---
aliases:
  - Lagrangian function
  - Lagrangian functions
  - Lagrange multiplier
  - Lagrange multipliers
  - method of Lagrange multiplier
  - method of Lagrange multipliers
tags:
  - flashcard/active/general/eng/Lagrange_multiplier
  - language/in/English
---

# Lagrange multiplier

- {@{"Lagrangian function"}@} redirects here; not to be confused with {@{[Lagrangian \(physics\)](Lagrangian%20(physics).md)}@}. <!--SR:!2026-01-30,244,330!2026-03-13,272,330-->

In {@{[mathematical optimization](mathematical%20optimization.md)}@}, {@{the __method of Lagrange multipliers__}@} is {@{a strategy for finding the local [maxima and minima](maxima%20and%20minima.md) of a [function](function%20(mathematics).md) subject to [equation constraints](constraint%20(mathematics).md) \(i.e., subject to the condition that one or more [equations](equation.md) have to be satisfied exactly by the chosen values of the [variables](variable%20(mathematics).md)\)}@}.<sup>[\[1\]](#^ref-1)</sup> It is named after {@{the mathematician [Joseph-Louis Lagrange](Joseph-Louis%20Lagrange.md)}@}. <!--SR:!2026-02-13,254,330!2026-04-01,292,330!2026-03-17,276,330!2026-01-27,241,330-->

## summary and rationale

The basic idea is to {@{convert a constrained problem into a form such that the [derivative test](derivative%20test.md) of an unconstrained problem can still be applied}@}. The relationship between {@{the gradient of the function and gradients of the constraints}@} rather {@{naturally leads to a reformulation of the original problem, known as the __Lagrangian function__ or Lagrangian}@}.<sup>[\[2\]](#^ref-2)</sup> In the general case, the Lagrangian is defined as {@{$${\mathcal {L} }(x,\lambda )\equiv f(x)+\langle \lambda ,g(x)\rangle$$}@} for {@{functions $f,g$; the notation $\langle \cdot ,\cdot \rangle$ denotes an [inner product](inner%20product.md)}@}. {@{The value $\lambda$}@} is called {@{the Lagrange multiplier}@}. <!--SR:!2026-11-23,458,310!2026-03-16,275,330!2026-03-13,272,330!2026-04-04,294,330!2026-03-14,273,330!2026-02-01,245,330!2026-03-24,284,330-->

In simple cases, where {@{the inner product is defined as the [dot product](dot%20product.md)}@}, the Lagrangian is {@{$${\mathcal {L} }(x,\lambda )\equiv f(x)+\lambda \cdot g(x)$$}@} <!--SR:!2026-03-08,267,330!2026-02-03,246,330-->

The method can be summarized as follows: in order to {@{find the maximum or minimum of a function $f$ subject to the equality constraint $g(x)=0$}@}, find {@{the [stationary points](stationary%20point.md) of ${\mathcal {L} }$ considered as a function of $x$ and the Lagrange multiplier $\lambda ~$}@}. This means that {@{all [partial derivatives](partial%20derivative.md) should be zero, including the partial derivative with respect to $\lambda ~$}@}.<sup>[\[3\]](#^ref-3)</sup> <p> {@{${\frac {\partial {\mathcal {L} } }{\partial x} }=0$   and   ${\frac {\ \partial {\mathcal {L} }\ }{\partial \lambda } }=0\ ;$}@} <p> or equivalently <p> {@{${\frac {\partial f(x)}{\partial x} }+\lambda \cdot {\frac {\partial g(x)}{\partial x} }=0$   and  $g(x)=0~$.}@} <p> {@{The solution corresponding to the original constrained optimization}@} is {@{always a [saddle point](saddle%20point.md) of the Lagrangian function}@},<sup>[\[4\]](#^ref-4)</sup><sup>[\[5\]](#^ref-5)</sup> which can be {@{identified among the stationary points}@} from {@{the [definiteness](definiteness%20of%20a%20matrix.md) of the [bordered Hessian matrix](bordered%20Hessian.md#bordered%20Hessian)}@}.<sup>[\[6\]](#^ref-6)</sup> <!--SR:!2026-02-03,245,330!2025-11-15,176,310!2026-02-14,255,330!2026-03-13,272,330!2026-02-04,247,330!2026-03-18,277,330!2026-02-16,256,330!2026-02-22,261,330!2027-02-16,508,310-->

{@{The great advantage of this method}@} is that it {@{allows the optimization to be solved without explicit [parameterization](parametrization%20(geometry).md) in terms of the constraints}@}. As a result, the method of Lagrange multipliers is {@{widely used to solve challenging constrained optimization problems}@}. Further, the method of Lagrange multipliers is {@{generalized by the [Karush–Kuhn–Tucker conditions](Karush–Kuhn–Tucker%20conditions.md)}@}, which can also {@{take into account inequality constraints of the form $h(\mathbf {x} )\leq c$ for a given constant $c$}@}. <!--SR:!2026-03-10,269,330!2026-02-01,245,330!2026-03-19,278,330!2026-01-28,242,330!2026-01-24,239,330-->

## statement

The following is known as {@{the Lagrange multiplier theorem}@}.<sup>[\[7\]](#^ref-7)</sup> <!--SR:!2026-03-25,284,330-->

Let {@{$f:\mathbb {R} ^{n}\to \mathbb {R}$ be the [objective function](objective%20function.md)}@} and let {@{$g:\mathbb {R} ^{n}\to \mathbb {R} ^{c}$ be the constraints function}@}, both {@{belonging to $C^{1}$ \(that is, having continuous first derivatives\)}@}. Let {@{$x_{\star }$ be an optimal solution to the following optimization problem}@} such that, for {@{the matrix of partial derivatives ${\Bigl [}\operatorname {D} g(x_{\star }){\Bigr ]}_{j,k}={\frac {\ \partial g_{j}\ }{\partial x_{k} } }$}@}, {@{$\operatorname {rank} (\operatorname {D} g(x_{\star }))=c\leq n$ \(annotation: _constraint qualification_\)}@}: {@{$${\begin{aligned}&{\text{maximize } }f(x)\\&{\text{subject to: } }g(x)=0\end{aligned} }$$}@} Then {@{there exists a unique Lagrange multiplier $\lambda _{\star }\in \mathbb {R} ^{c}$}@} such that {@{$\operatorname {D} f(x_{\star })=\lambda _{\star }^{\mathsf {T} }\operatorname {D} g(x_{\star })~$}@}. \(Note that this is {@{a somewhat conventional thing where $\lambda _{\star }$ is clearly treated as a column vector to ensure that the dimensions match}@}. But, we might as well {@{make it just a row vector without taking the transpose}@}.\) <!--SR:!2025-11-22,183,310!2026-03-12,271,330!2026-01-24,238,330!2026-03-19,279,330!2026-02-01,245,330!2026-02-13,254,330!2026-03-24,283,330!2026-03-29,288,330!2026-03-31,291,330!2026-01-28,242,330!2025-12-25,194,310-->

{@{The Lagrange multiplier theorem}@} states that {@{at any local maximum \(or minimum\) of the function evaluated under the equality constraints}@}, if {@{constraint qualification applies \(explained below\)}@}, then {@{the [gradient](gradient.md) of the function \(at that point\) can be expressed as a [linear combination](linear%20combination.md) of the gradients of the constraints \(at that point\)}@}, with {@{the Lagrange multipliers acting as [coefficients](coefficient.md)}@}.<sup>[\[8\]](#^ref-8)</sup> This is equivalent to saying that {@{any direction perpendicular to all gradients of the constraints is also perpendicular to the gradient of the function}@}. Or still, saying that {@{the [directional derivative](directional%20derivative.md) of the function is 0 in every feasible direction}@}. <!--SR:!2026-03-25,284,330!2026-03-13,272,330!2026-03-06,266,330!2026-03-15,274,330!2026-03-26,285,330!2026-02-18,258,330!2026-03-27,288,330-->

## single constraint

> {@{![Figure 1: The red curve shows the constraint _g_\(_x_, _y_\) = _c_.](../../archives/Wikimedia%20Commons/LagrangeMultipliers2D.svg)}@}
>
> Figure 1: {@{The red curve shows the constraint <span style="color: DarkRed;">_g_\(_x_, _y_\) = _c_</span>}@}. The blue curves are {@{contours of <span style="color: DarkSlateBlue;">_f_\(_x_, _y_\)</span>}@}. {@{The point where the red constraint tangentially touches a blue contour}@} is {@{the maximum of <span style="color: DarkSlateBlue;">_f_\(_x_, _y_\)</span> along the constraint}@}, since {@{_d_<sub>1</sub> \> _d_<sub>2</sub>}@}. <!--SR:!2026-03-23,282,330!2026-03-24,285,330!2026-03-31,290,330!2026-02-04,248,330!2026-04-04,294,330!2026-03-12,271,330-->

For the case of {@{only one constraint and only two choice variables}@} \(as exemplified in Figure 1\), consider {@{the [optimization problem](optimization%20problem.md) $${\begin{aligned}{\underset {x,y}{\text{maximize} } }\quad &f(x,y)\\{\text{subject to} }\quad &g(x,y)=0.\end{aligned} }$$}@} \(Sometimes {@{an additive constant is shown separately rather than being included in $g$}@}, in which case {@{the constraint is written $g(x,y)=c$}@}, as in Figure 1.\) We assume that {@{both $f$ and $g$ have continuous first [partial derivatives](partial%20derivative.md)}@}. We introduce {@{a new variable \($\lambda$\) called a __Lagrange multiplier__ \(or __Lagrange undetermined multiplier__\)}@} and study {@{the __Lagrange function__ \(or __Lagrangian__ or __Lagrangian expression__\)}@} defined by {@{$${\mathcal {L} }(x,y,\lambda )=f(x,y)+\lambda \cdot g(x,y),$$}@} where {@{the $\lambda$ term may be either added or subtracted}@}. If {@{$f(x_{0},y_{0})$ is a maximum of $f(x,y)$ for the original constrained problem and $\nabla g(x_{0},y_{0})\neq 0$}@}, then there {@{exists $\lambda _{0}$ such that \($x_{0},y_{0},\lambda _{0}$\) is a _[stationary point](stationary%20point.md)_ for the Lagrange function}@} \(stationary points are {@{those points where the first partial derivatives of ${\mathcal {L} }$ are zero}@}\). {@{The assumption $\nabla g\neq 0$}@} is called {@{constraint qualification}@}. However, {@{not all stationary points yield a solution of the original problem}@}, as the method of Lagrange multipliers {@{yields only a [necessary condition](necessary%20condition.md#necessity) for optimality in constrained problems}@}.<sup>[\[9\]](#^ref-9)</sup><sup>[\[10\]](#^ref-10)</sup><sup>[\[11\]](#^ref-11)</sup><sup>[\[12\]](#^ref-12)</sup><sup>[\[13\]](#^ref-13)</sup> {@{Sufficient conditions for a minimum or maximum}@} {@{[also exist](bordered%20Hessian.md#bordered%20Hessian)}@}, but if {@{a particular [candidate solution](candidate%20solution.md#candidate%20solution) satisfies the sufficient conditions}@}, it is {@{only guaranteed that that solution is the best one _locally_}@} – that is, it is {@{better than any permissible nearby points}@}. {@{The _global_ optimum}@} can be found by {@{comparing the values of the original objective function at the points satisfying the necessary and locally sufficient conditions}@}. <!--SR:!2026-03-23,282,330!2026-03-03,262,330!2026-03-02,261,330!2026-03-31,290,330!2026-02-04,248,330!2026-03-31,291,330!2026-01-23,237,330!2026-03-30,290,330!2026-03-13,272,330!2026-03-14,273,330!2026-03-29,288,330!2026-01-30,243,330!2026-02-07,249,330!2026-04-02,292,330!2026-01-28,242,330!2026-03-24,283,330!2027-01-27,472,310!2027-02-01,475,310!2026-04-02,292,330!2026-02-12,255,330!2026-03-29,288,330!2026-03-11,272,330!2025-12-03,181,310-->

{@{The method of Lagrange multipliers}@} relies on {@{the intuition that at a maximum}@}, _f_\(_x_, _y_\) {@{cannot be increasing in the direction of any such neighboring point that also has _g_ = 0}@}. If {@{it were, we could walk along _g_ = 0 to get higher}@}, meaning that {@{the starting point wasn't actually the maximum}@}. Viewed in this way, it is {@{an exact analogue to testing if the derivative of an unconstrained function is 0}@}, that is, we are {@{verifying that the directional derivative is 0 in any relevant \(viable\) direction}@}. <!--SR:!2026-02-05,249,330!2026-03-04,263,330!2025-11-25,186,310!2026-02-01,246,330!2026-03-03,262,330!2026-03-30,289,330!2026-02-08,251,330-->

We can {@{visualize [contours](contour%20line.md) of _f_}@} given by {@{_f_\(_x_, _y_\) = _d_ for various values of _d_}@}, and {@{the contour of _g_}@} given by {@{_g_\(_x_, _y_\) = _c_}@}. <!--SR:!2026-02-14,255,330!2026-02-13,254,330!2026-02-15,255,330!2026-02-22,261,330-->

Suppose we {@{walk along the contour line with _g_ = _c_}@}. We are interested in {@{finding points where _f_ almost does not change as we walk}@}, since {@{these points might be maxima}@}. <!--SR:!2026-03-28,287,330!2026-03-07,266,330!2026-01-26,240,330-->

There are {@{two ways this could happen}@}: <!--SR:!2025-11-13,174,310-->

1. \(annotation: nonzero gradient\) ::@:: We could touch a contour line of _f_, since by definition _f_ does not change as we walk along its contour lines. This would mean that the tangents to the contour lines of _f_ and _g_ are parallel here. <!--SR:!2025-11-21,182,310!2025-11-30,179,310-->
2. \(annotation: zero gradient\) ::@:: We have reached a "level" part of _f_, meaning that _f_ does not change in any direction. <!--SR:!2025-11-11,172,310!2025-11-12,173,310-->

To {@{check the first possibility \(we touch a contour line of _f_\)}@}, notice that since {@{the [gradient](gradient.md) of a function is perpendicular to the contour lines}@}, {@{the tangents to the contour lines of _f_ and _g_ are parallel}@} {@{if and only if the gradients of _f_ and _g_ are parallel}@}. Thus we want {@{points \(_x_, _y_\) where _g_\(_x_, _y_\) = _c_}@} and {@{$$\nabla _{x,y}f=\lambda \,\nabla _{x,y}g,$$ for some $\lambda$}@} where {@{$$\nabla _{x,y}f=\left({\frac {\partial f}{\partial x} },{\frac {\partial f}{\partial y} }\right),\qquad \nabla _{x,y}g=\left({\frac {\partial g}{\partial x} },{\frac {\partial g}{\partial y} }\right)$$ are the respective gradients}@}. {@{The constant $\lambda$ is required}@} because {@{although the two gradient vectors are parallel}@}, {@{the magnitudes of the gradient vectors are generally not equal}@}. This constant is called {@{the Lagrange multiplier}@}. \(In some conventions {@{$\lambda$ is preceded by a minus sign}@}\). <!--SR:!2026-03-10,269,330!2026-02-14,255,330!2026-02-06,250,330!2026-03-17,276,330!2026-03-08,268,330!2026-01-25,239,330!2026-03-30,289,330!2026-03-11,272,330!2026-02-13,254,330!2026-04-04,294,330!2026-01-20,235,330!2026-02-01,246,330-->

Notice that this method also {@{solves the second possibility, that _f_ is level}@}: if {@{_f_ is level, then its gradient is zero}@}, and {@{setting $\lambda =0$ is a solution regardless of $\nabla _{x,y}g$}@}. <!--SR:!2026-03-15,276,330!2026-03-15,274,330!2026-02-24,263,330-->

To {@{incorporate these conditions into one equation}@}, we introduce {@{an auxiliary function $${\mathcal {L} }(x,y,\lambda )\equiv f(x,y)+\lambda \cdot g(x,y)\,,$$}@} and solve {@{$$\nabla _{x,y,\lambda }{\mathcal {L} }(x,y,\lambda )=0~.$$}@} Note that this amounts to {@{solving three equations in three unknowns}@}. This is {@{the method of Lagrange multipliers}@}. <!--SR:!2026-03-28,287,330!2025-12-21,192,310!2026-01-29,240,330!2026-01-23,237,330!2026-02-09,252,330-->

Note that {@{$\ \nabla _{\lambda }{\mathcal {L} }(x,y,\lambda )=0\ {}$ implies $\ g(x,y)=0\ {}$}@}, as {@{the partial derivative of ${\mathcal {L} }$ with respect to $\lambda$ is $\ g(x,y)~$}@}. <!--SR:!2026-02-01,246,330!2026-03-07,268,330-->

To summarize {@{$$\nabla _{x,y,\lambda }{\mathcal {L} }(x,y,\lambda )=0\iff {\begin{cases}\nabla _{x,y}f(x,y)=-\lambda \,\nabla _{x,y}g(x,y)\\g(x,y)=0\end{cases} }$$}@} The method {@{generalizes readily to functions on $n$ variables}@} {@{$$\nabla _{x_{1},\dots ,x_{n},\lambda }{\mathcal {L} }(x_{1},\dots ,x_{n},\lambda )=0$$}@} which amounts to {@{solving _n_ + 1 equations in _n_ + 1 unknowns}@}. <!--SR:!2026-03-25,284,330!2026-03-29,290,330!2026-03-18,277,330!2026-01-30,244,330-->

{@{The constrained extrema of _f_}@} are {@{_[critical points](critical%20point%20(mathematics).md)_ of the Lagrangian ${\mathcal {L} }$}@}, but they are {@{not necessarily _local extrema_ of ${\mathcal {L} }$}@} \(see [§ Example 2](#example%202) below\). <!--SR:!2026-02-06,250,330!2026-03-30,289,330!2026-03-04,263,330-->

One may {@{[reformulate the Lagrangian](Hamiltonian%20mechanics.md#As%20a%20reformulation%20of%20Lagrangian%20mechanics) as a [Hamiltonian](Hamiltonian%20(control%20theory).md)}@}, in which case {@{the solutions are local minima for the Hamiltonian}@}. This is done in {@{[optimal control](optimal%20control.md) theory}@}, in the form of {@{[Pontryagin's maximum principle](Pontryagin's%20maximum%20principle.md)}@}. <!--SR:!2026-04-02,292,330!2026-02-11,254,330!2026-04-04,294,330!2026-02-12,255,330-->

The fact that {@{solutions of the method of Lagrange multipliers are not necessarily extrema of the Lagrangian}@}, also {@{poses difficulties for numerical optimization}@}. This can be addressed by {@{minimizing the _magnitude_ of the gradient of the Lagrangian \(annotation: instead of finding critical points of the Lagrangian\)}@}, as {@{these minima are the same as the zeros of the magnitude \(annotation: corresponding to critical points of the Lagrangian\)}@}, as illustrated in [Example 5: Numerical optimization](#example%205). <!--SR:!2026-04-04,294,330!2026-03-05,266,330!2026-03-05,264,330!2026-02-12,255,330-->

## multiple constraints

> {@{![Figure 2: A paraboloid constrained along two intersecting lines.](../../archives/Wikimedia%20Commons/As%20wiki%20lgm%20parab.svg)}@}
>
> Figure 2: {@{A paraboloid constrained along two intersecting lines}@}. <!--SR:!2026-03-25,285,330!2026-04-03,293,330-->

<!-- markdownlint MD028 -->

> {@{![Figure 3: Contour map of Figure 2.](../../archives/Wikimedia%20Commons/As%20wiki%20lgm%20levelsets.svg)}@}
>
> Figure 3: {@{Contour map of Figure 2}@}. <!--SR:!2026-02-23,262,330!2026-03-26,287,330-->

The method of Lagrange multipliers can be {@{extended to solve problems with multiple constraints using a similar argument}@}. Consider {@{a [paraboloid](paraboloid.md) subject to two line constraints that intersect at a single point}@}. As {@{the only feasible solution}@}, {@{this point is obviously a constrained extremum}@}. However, {@{the [level set](level%20set.md) of $f$}@} is {@{clearly not parallel to either constraint at the intersection point}@} \(see Figure 3\); instead, it is {@{a linear combination of the two constraints' gradients}@}. In the case of {@{multiple constraints}@}, that {@{will be what we seek in general}@}: The method of Lagrange seeks {@{points not at which the gradient of $f$ is a multiple of any single constraint's gradient necessarily}@}, but {@{in which it is a linear combination of all the constraints' gradients}@}. <!--SR:!2026-03-16,276,330!2026-02-04,248,330!2026-03-16,276,330!2026-02-12,255,330!2026-04-02,292,330!2026-03-07,267,330!2026-03-16,276,330!2026-04-03,293,330!2026-02-12,255,330!2026-04-02,293,330!2026-04-02,293,330-->

Concretely, suppose we have {@{$M$ constraints}@} and are {@{walking along the set of points satisfying $g_{i}(\mathbf {x} )=0,i=1,\dots ,M\,$}@}. {@{Every point $\mathbf {x}$ on the contour of a given constraint function $g_{i}$}@} has {@{a space of allowable directions}@}: {@{the space of vectors perpendicular to $\nabla g_{i}(\mathbf {x} )\,$}@}. {@{The set of directions that are allowed by all constraints}@} is thus {@{the space of directions perpendicular to all of the constraints' gradients}@}. Denote {@{this space of allowable moves by $\ A\ {}$}@} and denote {@{the span of the constraints' gradients by $S\,$}@}. Then {@{$A=S^{\perp }\,$}@}, the space of {@{vectors perpendicular to every element of $S\,$}@}. <!--SR:!2026-02-12,255,330!2026-02-14,255,330!2026-03-09,268,330!2026-03-21,280,330!2026-01-25,240,330!2026-02-06,249,330!2026-04-01,291,330!2026-01-30,244,330!2026-12-16,475,310!2026-02-09,252,330!2026-04-03,293,330-->

We are still interested in {@{finding points where $f$ does not change as we walk}@}, since {@{these points might be \(constrained\) extrema}@}. We therefore {@{seek $\mathbf {x}$}@} such that {@{any allowable direction of movement away from $\mathbf {x}$ is perpendicular to $\nabla f(\mathbf {x} )$}@} \(otherwise we could {@{increase $f$ by moving along that allowable direction}@}\). In other words, {@{$\nabla f(\mathbf {x} )\in A^{\perp }=S\,$}@}. Thus there are {@{scalars $\lambda _{1},\lambda _{2},\ \dots ,\lambda _{M}$}@} such that {@{$$\nabla f(\mathbf {x} )=\sum _{k=1}^{M}\lambda _{k}\,\nabla g_{k}(\mathbf {x} )\quad \iff \quad \nabla f(\mathbf {x} )-\sum _{k=1}^{M}{\lambda _{k}\nabla g_{k}(\mathbf {x} )}=0~.$$}@} <!--SR:!2026-03-28,289,330!2026-04-01,292,330!2026-01-24,238,330!2026-02-03,247,330!2026-03-01,260,330!2026-01-23,237,330!2026-03-07,268,330!2026-03-16,275,330-->

These scalars are {@{the Lagrange multipliers}@}. We now have {@{$M$ of them, one for every constraint}@}. <!--SR:!2026-03-19,278,330!2026-02-05,248,330-->

As before, we introduce {@{an auxiliary function $${\mathcal {L} }\left(x_{1},\ldots ,x_{n},\lambda _{1},\ldots ,\lambda _{M}\right)=f\left(x_{1},\ldots ,x_{n}\right)-\sum \limits _{k=1}^{M}{\lambda _{k}g_{k}\left(x_{1},\ldots ,x_{n}\right)}\ {}$$}@} and solve {@{$$\nabla _{x_{1},\ldots ,x_{n},\lambda _{1},\ldots ,\lambda _{M} }{\mathcal {L} }(x_{1},\ldots ,x_{n},\lambda _{1},\ldots ,\lambda _{M})=0\iff {\begin{cases}\nabla f(\mathbf {x} )-\sum _{k=1}^{M}{\lambda _{k}\,\nabla g_{k}(\mathbf {x} )}=0\\g_{1}(\mathbf {x} )=\cdots =g_{M}(\mathbf {x} )=0\end{cases} }$$}@} which amounts to {@{solving $n+M$ equations in $\ n+M\ {}$ unknowns}@}. <!--SR:!2026-03-20,279,330!2026-03-30,289,330!2026-03-12,271,330-->

{@{The constraint qualification assumption}@} when there are multiple constraints is that {@{the constraint gradients at the relevant point are linearly independent}@}. <!--SR:!2026-04-01,292,330!2026-03-25,286,330-->

## modern formulation via differentiable manifolds

{@{The problem of finding the local maxima and minima subject to constraints}@} can be {@{generalized to finding local maxima and minima on a [differentiable manifold](differentiable%20manifold.md) $\ M~$}@}.<sup>[\[14\]](#^ref-14)</sup> In what follows, it is not {@{necessary that $M$ be a Euclidean space, or even a Riemannian manifold}@}. {@{All appearances of the gradient $\ \nabla \ {}$}@} \(which {@{depends on a choice of Riemannian metric}@}\) can be {@{replaced with the [exterior derivative](exterior%20derivative.md) $\ \operatorname {d} ~$}@}. <!--SR:!2026-03-27,286,330!2026-03-24,285,330!2026-03-22,281,330!2026-03-27,286,330!2026-03-16,276,330!2026-02-12,255,330-->

<!-- markdownlint-disable-next-line MD024 -->
### single constraint

Let {@{$\ M\ {}$ be a [smooth manifold](smooth%20manifold.md#definition) of dimension $\ m~$}@}. Suppose that we wish to {@{find the stationary points $\ x\ {}$ of a smooth function $\ f:M\to \mathbb {R} \ {}$}@} when {@{restricted to the submanifold $\ N\ {}$ defined by $\ g(x)=0\ {}$}@}, where {@{$\ g:M\to \mathbb {R} \ {}$ is a smooth function for which 0 is a [regular value](regular%20value.md)}@}. <!--SR:!2026-03-17,276,330!2026-03-18,277,330!2026-03-31,290,330!2026-01-27,241,330-->

Let {@{$\ \operatorname {d} f\ {}$ and $\ \operatorname {d} g\ {}$ be the [exterior derivatives](exterior%20derivative.md) of $\ f\ {}$ and $\ g\ {}$}@}. {@{Stationarity for the restriction $\ f|_{N}\ {}$ at $\ x\in N\ {}$}@} means {@{$\ \operatorname {d} (f|_{N})_{x}=0~$ \(annotation: the exterior derivatives restricted of $f$ to the submanifold is 0 at $x$\)}@}. Equivalently, {@{the kernel $\ \ker(\operatorname {d} f_{x})\ {}$}@} \(annotation: {@{The kernel}@} is {@{the movement space on the manifold that does not change the value of $f$}@}.\) {@{contains $\ T_{x}N=\ker(\operatorname {d} g_{x})~$}@}. \(annotation: {@{The tangent bundle of the submanifold at $x$}@} is also {@{the movement space on the manifold that does not change the value of $g$}@}, and thus {@{stays on the submanifold}@}.\) In other words, {@{$\ \operatorname {d} f_{x}\ {}$ and $\ \operatorname {d} g_{x}\ {}$ are proportional 1-forms \(annotation: TODO: What?\)}@}. For this it is {@{necessary and sufficient that the following system of $\ {\tfrac {1}{2} }m(m-1)\ {}$ equations holds}@}: {@{$$\operatorname {d} f_{x}\wedge \operatorname {d} g_{x}=0\in \Lambda ^{2}(T_{x}^{\ast }M)$$}@} \(annotation: {@{The exterior derivatives}@} are elements of {@{the cotangent bundle of the manifold at $x$, $T_{x}^{\ast} M$}@}. The result is {@{a bivector, represented by $\Lambda^2$}@}.\) where {@{$\ \wedge \ {}$ denotes the [exterior product](exterior%20algebra.md)}@}. \(annotation: It is related to {@{the much more familiar cross product}@}. The above results can be roughly thought of as {@{the two exterior derivatives being "parallel"}@}.\) {@{The stationary points $\ x\ {}$}@} are {@{the solutions of the above system of equations plus the constraint $\ g(x)=0~$}@}. Note that {@{the $\ {\tfrac {1}{2} }m(m-1)\ {}$ equations are not independent}@}, since {@{the left-hand side of the equation belongs to the subvariety of $\ \Lambda ^{2}(T_{x}^{\ast }M)\ {}$}@} consisting of {@{[decomposable elements](exterior%20algebra.md) \(annotation: TODO: What?\)}@}. <!--SR:!2026-02-06,249,330!2026-02-14,255,330!2025-11-28,177,310!2025-11-17,178,310!2026-03-20,232,270!2026-01-08,208,310!2025-12-24,193,310!2026-04-23,285,290!2026-01-31,244,330!2026-02-18,258,330!2026-03-22,281,330!2027-04-12,550,310!2026-03-31,290,330!2026-02-05,248,330!2026-03-03,124,387!2026-03-09,129,387!2026-03-09,129,387!2025-11-29,44,347!2026-03-03,124,387!2026-02-27,117,388!2026-03-06,123,388!2026-01-27,86,368!2026-02-01,90,368!2026-03-12,128,388-->

In this formulation, it is {@{not necessary to explicitly find the Lagrange multiplier}@}, {@{a number $\ \lambda \ {}$ such that $\ \operatorname {d} f_{x}=\lambda \cdot \operatorname {d} g_{x}~$}@}. <!--SR:!2026-03-28,287,330!2026-03-28,287,330-->

<!-- markdownlint-disable-next-line MD024 -->
### multiple constraints

Let {@{$\ M\ {}$ and $\ f\ {}$ be as in the above section regarding the case of a single constraint}@}. Rather than {@{the function $g$ described there}@}, now consider {@{a smooth function $\ G:M\to \mathbb {R} ^{p}(p>1)\ {}$}@}, with {@{component functions $\ g_{i}:M\to \mathbb {R} \ {}$}@}, for which {@{$0\in \mathbb {R} ^{p}$ is a [regular value](regular%20value.md)}@}. Let {@{$N$ be the submanifold of $\ M\ {}$ defined by $\ G(x)=0~$}@}. <!--SR:!2026-03-25,285,330!2026-01-30,243,330!2026-03-27,287,330!2026-01-02,203,310!2026-01-31,242,330!2026-02-12,255,330-->

{@{$\ x\ {}$ is a stationary point of $f|_{N}$}@} {@{if and only if $\ \ker(\operatorname {d} f_{x})\ {}$}@} \(annotation: {@{The kernel}@} is {@{the movement space on the manifold that does not change the value of $f$}@}.\) {@{contains $\ \ker(\operatorname {d} G_{x})~$}@}. \(annotation: {@{The kernel}@} is {@{the movement space on the manifold that does not change the value of $g$}@}, and thus {@{stays on the submanifold}@}.\) For convenience let {@{$\ L_{x}=\operatorname {d} f_{x}\ {}$ and $\ K_{x}=\operatorname {d} G_{x}\ {}$}@}, where {@{$\ \operatorname {d} G$ denotes the tangent map or Jacobian $\ TM\to T\mathbb {R} ^{p}~$}@} \($\ T_{x}\mathbb {R} ^{p}$ can be {@{canonically identified with $\ \mathbb {R} ^{p}$}@}\). {@{The subspace $\ker(K_{x})$}@} has {@{dimension smaller than that of $\ker(L_{x})$}@}, namely {@{$\ \dim(\ker(L_{x}))=n-1\ {}$ \(annotation: There is one direction to move to change the value of $f$.\)}@} and {@{$\ \dim(\ker(K_{x}))=n-p~$ \(annotation: There are $p$ directions to move to change the value of $G$\)}@}. {@{$\ker(K_{x})$ belongs to $\ \ker(L_{x})\ {}$ \(annotation: same as the first statement of this paragraph\)}@} {@{if and only if $L_{x}\in T_{x}^{\ast }M$ belongs to the image of $\ K_{x}^{\ast }:\mathbb {R} ^{p\ast }\to T_{x}^{\ast }M~$}@}. \(annotation: It is {@{the dual of the tangent map}@}. $K_x^*$ can be interpreted as {@{exterior derivatives of the output of $G$ being mapped to exterior derivatives of the manifold at $x$}@}. So this statement is roughly saying {@{the exterior derivatives of $f$, which causes changes in $f$, belongs to exterior derivatives of $G$, which causes changes in $G$}@}.\) Computationally speaking, the condition is that {@{$L_{x}$ \(annotation: roughly gradient of $f$\) belongs to the row space of the matrix of $\ K_{x}\ {}$ \(annotation: Jacobian of $G$\)}@}, or equivalently {@{the column space of the matrix of $K_{x}^{\ast }$ \(the transpose\)}@}. If {@{$\ \omega _{x}\in \Lambda ^{p}(T_{x}^{\ast }M)\ {}$}@} denotes {@{the exterior product of the columns of the matrix of $\ K_{x}^{\ast }\ {}$}@}, {@{the stationary condition for $\ f|_{N}\ {}$ at $\ x\ {}$}@} becomes {@{$$L_{x}\wedge \omega _{x}=0\in \Lambda ^{p+1}\left(T_{x}^{\ast }M\right) \,.$$}@} \(annotation: It is related to the {@{much more familiar cross product}@}. The above results can be roughly thought of as {@{the two exterior derivatives being "parallel"}@}.\) Once again, in this formulation it is not {@{necessary to explicitly find the Lagrange multipliers}@}, {@{the numbers $\ \lambda _{1},\ldots ,\lambda _{p}\ {}$ such that $$\ \operatorname {d} f_{x}=\sum _{i=1}^{p}\lambda _{i}\operatorname {d} (g_{i})_{x}~.$$}@} <!--SR:!2026-01-26,240,330!2025-12-03,66,270!2026-03-22,281,330!2025-12-01,180,310!2026-03-21,280,330!2026-04-03,293,330!2027-11-16,742,330!2026-03-02,261,330!2026-01-23,237,330!2025-12-23,192,310!2026-03-17,276,330!2025-12-28,198,310!2025-11-14,175,310!2026-02-02,245,330!2026-03-05,265,330!2026-03-06,265,330!2026-01-24,197,270!2025-11-24,185,310!2025-11-10,171,310!2026-02-18,216,270!2026-03-31,290,330!2026-02-24,263,330!2025-12-23,64,367!2026-03-09,129,387!2026-03-02,123,387!2026-01-04,75,367!2026-03-09,129,387!2026-02-28,121,387!2026-03-13,129,388!2026-03-07,124,388-->

## interpretation of the Lagrange multipliers

In this section, we modify {@{the constraint equations from the form $g_{i}({\bf {x} })=0$ to the form $\ g_{i}({\bf {x} })=c_{i}\ {}$}@}, where {@{the $\ c_{i}\ {}$ are _m_ real constants}@} that are {@{considered to be additional arguments of the Lagrangian expression ${\mathcal {L} }$}@}. <!--SR:!2026-03-25,284,330!2026-01-23,238,330!2026-02-16,256,330-->

Often {@{the Lagrange multipliers}@} have an interpretation as {@{some quantity of interest}@}. For example, by {@{parametrising the constraint's contour line}@}, that is, if the Lagrangian expression is {@{$${\begin{aligned}&{\mathcal {L} }(x_{1},x_{2},\ldots ;\lambda _{1},\lambda _{2},\ldots ;c_{1},c_{2},\ldots )\\[4pt]={}&f(x_{1},x_{2},\ldots )+\lambda _{1}(c_{1}-g_{1}(x_{1},x_{2},\ldots ))+\lambda _{2}(c_{2}-g_{2}(x_{1},x_{2},\dots ))+\cdots \end{aligned} }$$}@} then {@{$$\ {\frac {\partial {\mathcal {L} } }{\partial c_{k} } }=\lambda _{k}~.$$}@} So, _λ<sub>k</sub>_ is {@{the rate of change of the quantity being optimized as a function of the constraint parameter}@}. As examples, in {@{[Lagrangian mechanics](Lagrangian%20mechanics.md)}@} {@{the equations of motion}@} are derived by {@{finding stationary points of the [action](action%20(physics).md)}@}, {@{the time integral of the difference between kinetic and potential energy}@}. Thus, {@{the force on a particle due to a scalar potential}@}, {@{_F_ = −∇<!-- markdown separator -->_V_}@}, can be interpreted as {@{a Lagrange multiplier determining the change in action \(transfer of potential to kinetic energy\)}@} following {@{a variation in the particle's constrained trajectory}@}. \(annotation: If {@{the trajectory over time is varied}@}, the trajectory {@{moves up or down on the scalar potential, so the action changes}@}.\) In {@{control theory}@} this is {@{formulated instead as [costate equations](costate%20equations.md)}@}. <!--SR:!2026-03-15,276,330!2026-03-28,289,330!2026-02-15,255,330!2026-03-09,268,330!2026-03-09,268,330!2026-02-24,263,330!2026-01-09,209,310!2026-03-11,270,330!2026-01-28,242,330!2026-03-17,276,330!2025-12-22,191,310!2026-03-26,286,330!2026-03-13,272,330!2026-03-12,271,330!2026-03-24,285,330!2025-12-27,196,310!2026-01-25,239,330!2026-03-05,266,330-->

Moreover, by {@{the [envelope theorem](envelope%20theorem.md)}@} {@{the optimal value of a Lagrange multiplier}@} has an interpretation as {@{the marginal effect of the corresponding constraint constant}@} upon {@{the optimal attainable value of the original objective function}@}: If we denote {@{values at the optimum with a star \($\star$\)}@}, then it can be shown that {@{$${\frac {\ \operatorname {d} f\left(\ x_{1\star }(c_{1},c_{2},\dots ),\ x_{2\star }(c_{1},c_{2},\dots ),\ \dots \ \right)\ }{\operatorname {d} c_{k} } }=\lambda _{\star k}~.$$}@} For example, in {@{economics}@} {@{the optimal profit to a player}@} is {@{calculated subject to a constrained space of actions}@}, where a Lagrange multiplier is {@{the change in the optimal value of the objective function \(profit\) due to the relaxation of a given constraint \(e.g. through a change in income\)}@}; in such a context $\ \lambda _{\star k}\ {}$ is {@{the [marginal cost](marginal%20cost.md) of the constraint}@}, and is referred to as {@{the [shadow price](shadow%20price.md)}@}.<sup>[\[15\]](#^ref-15)</sup> <!--SR:!2026-02-17,257,330!2026-03-15,274,330!2026-03-23,282,330!2026-03-12,272,330!2026-03-05,264,330!2026-03-29,288,330!2026-03-19,278,330!2027-08-08,661,330!2026-03-24,283,330!2026-04-03,293,330!2026-02-03,247,330!2026-01-29,243,330-->

## sufficient conditions

- Main article: ::@:: [Bordered Hessian](bordered%20Hessian.md#bordered%20Hessian) <!--SR:!2026-04-02,292,330!2026-02-06,250,330-->

{@{Sufficient conditions for a constrained local maximum or minimum}@} can be stated in terms of {@{a sequence of principal minors \(determinants of upper-left-justified sub-matrices\)}@} of {@{the bordered [Hessian matrix](Hessian%20matrix.md) of second derivatives of the Lagrangian expression}@}.<sup>[\[6\]](#^ref-6)</sup><sup>[\[16\]](#^ref-16)</sup> <!--SR:!2026-03-11,270,330!2026-02-07,250,330!2026-03-13,272,330-->

## examples

### example 1

> {@{![Illustration of the constrained optimization problem __1__](../../archives/Wikimedia%20Commons/Lagrange%20very%20simple.svg)}@}
>
> Illustration of {@{the constrained optimization problem __1__}@} <!--SR:!2026-03-24,283,330!2026-01-20,235,330-->

Suppose we wish to {@{maximize $\ f(x,y)=x+y\ {}$}@} subject to {@{the constraint $\ x^{2}+y^{2}=1~$}@}. {@{The [feasible set](candidate%20solution.md#candidate%20solution) is the unit circle}@}, and {@{the [level sets](level%20set.md) of _f_ are diagonal lines \(with slope −1\)}@}, so we can see graphically that {@{the maximum occurs at $\ \left({\tfrac {1}{\sqrt {2} } },{\tfrac {1}{\sqrt {2} } }\right)\ {}$}@}, and that {@{the minimum occurs at $\ \left(-{\tfrac {1}{\sqrt {2} } },-{\tfrac {1}{\sqrt {2} } }\right)~$}@}. <!--SR:!2026-01-31,242,330!2026-01-28,241,330!2026-04-01,291,330!2026-02-19,259,330!2026-03-09,268,330!2026-03-27,286,330-->

For the method of Lagrange multipliers, the constraint is {@{$$g(x,y)=x^{2}+y^{2}-1=0\ ,$$}@} hence {@{the Lagrangian function, $${\begin{aligned}{\mathcal {L} }(x,y,\lambda )&=f(x,y)+\lambda \cdot g(x,y)\\[4pt]&=x+y+\lambda (x^{2}+y^{2}-1)\ ,\end{aligned} }$$}@} is a function that is {@{equivalent to $\ f(x,y)\ {}$ when $\ g(x,y)\ {}$ is set to 0}@}. <!--SR:!2026-02-22,261,330!2026-03-09,268,330!2026-02-06,248,330-->

Now we can calculate {@{the gradient: $${\begin{aligned}\nabla _{x,y,\lambda }{\mathcal {L} }(x,y,\lambda )&=\left({\frac {\partial {\mathcal {L} } }{\partial x} },{\frac {\partial {\mathcal {L} } }{\partial y} },{\frac {\partial {\mathcal {L} } }{\partial \lambda } }\right)\\[4pt]&=\left(1+2\lambda x,1+2\lambda y,x^{2}+y^{2}-1\right)\ \color {gray}{,}\end{aligned} }$$}@} and therefore: {@{$$\nabla _{x,y,\lambda }{\mathcal {L} }(x,y,\lambda )=0\quad \Leftrightarrow \quad {\begin{cases}1+2\lambda x=0\\1+2\lambda y=0\\x^{2}+y^{2}-1=0\end{cases} }$$}@} Notice that {@{the last equation is the original constraint}@}. <!--SR:!2026-03-10,269,330!2026-02-08,250,330!2026-01-23,237,330-->

{@{The first two equations}@} yield {@{$$x=y=-{\frac {1}{2\lambda } },\qquad \lambda \neq 0~.$$}@} By {@{substituting into the last equation}@} we have: {@{$${\frac {1}{4\lambda ^{2} } }+{\frac {1}{4\lambda ^{2} } }-1=0\ ,$$}@} so {@{$$\lambda =\pm {\frac {1}{\sqrt {2\ } } }\ ,$$}@} which implies that {@{the stationary points of ${\mathcal {L} }$}@} are {@{$$\left({\tfrac {\sqrt {2\ } }{2} },{\tfrac {\sqrt {2\ } }{2} },-{\tfrac {1}{\sqrt {2\ } } }\right),\qquad \left(-{\tfrac {\sqrt {2\ } }{2} },-{\tfrac {\sqrt {2\ } }{2} },{\tfrac {1}{\sqrt {2\ } } }\right)~.$$}@} {@{Evaluating the objective function _f_ at these points}@} yields {@{$$f\left({\tfrac {\sqrt {2\ } }{2} },{\tfrac {\sqrt {2\ } }{2} }\right)={\sqrt {2\ } }\ ,\qquad f\left(-{\tfrac {\sqrt {2\ } }{2} },-{\tfrac {\sqrt {2\ } }{2} }\right)=-{\sqrt {2\ } }~.$$}@} Thus {@{the constrained maximum is $\ {\sqrt {2\ } }\ {}$ and the constrained minimum is $-{\sqrt {2} }$}@}. <!--SR:!2026-03-12,272,330!2025-12-06,183,310!2025-12-18,187,310!2026-03-27,287,330!2026-03-13,272,330!2026-03-30,290,330!2026-02-23,262,330!2026-02-22,261,330!2025-11-18,179,310!2026-02-13,254,330-->

### example 2

> {@{![Illustration of the constrained optimization problem __2__](../../archives/Wikimedia%20Commons/Lagrange%20very%20simple-1b.svg)}@}
>
> Illustration of {@{the constrained optimization problem __2__}@} <!--SR:!2026-03-13,272,330!2026-04-01,291,330-->

Now we modify {@{the objective function of Example __1__}@} so that we {@{minimize $\ f(x,y)=(x+y)^{2}\ {}$ instead of $\ f(x,y)=x+y\ {}$}@}, again {@{along the circle $\ g(x,y)=x^{2}+y^{2}-1=0~$}@}. Now {@{the level sets of $f$ are still lines of slope −1}@}, and {@{the points on the circle tangent to these level sets are again $\ ({\sqrt {2} }/2,{\sqrt {2} }/2)\ {}$ and $\ (-{\sqrt {2} }/2,-{\sqrt {2} }/2)~$}@}. These tangency points are {@{maxima of $\ f~$}@}. <!--SR:!2026-02-01,243,330!2026-01-22,237,330!2026-03-28,288,330!2026-02-23,262,330!2027-02-28,519,310!2026-03-27,287,330-->

On the other hand, {@{the minima occur on the level set for $\ f=0\ {}$}@} \(since {@{by its construction $\ f\ {}$ cannot take negative values}@}\), at {@{$\ ({\sqrt {2} }/2,-{\sqrt {2} }/2)\ {}$ and $\ (-{\sqrt {2} }/2,{\sqrt {2} }/2)\ {}$}@}, where {@{the level curves of $\ f\ {}$ are not tangent to the constraint}@}. {@{The condition that $\ \nabla _{x,y,\lambda }\left(f(x,y)+\lambda \cdot g(x,y)\right)=0\ {}$}@} {@{correctly identifies all four points as extrema}@}; the minima are {@{characterized in by $\ \lambda =0\ {}$ and the maxima by $\ \lambda =-2~$}@}. <!--SR:!2026-01-23,237,330!2026-03-26,286,330!2026-03-14,274,330!2026-03-11,270,330!2026-03-26,285,330!2026-03-21,280,330!2026-02-12,255,330-->

### example 3

> {@{![Illustration of constrained optimization problem __3__.](../../archives/Wikimedia%20Commons/Lagrange%20simple.svg)}@}
>
> Illustration of {@{constrained optimization problem __3__}@}. <!--SR:!2026-03-17,276,330!2026-02-12,255,330-->

This example deals {@{with more strenuous calculations}@}, but it is {@{still a single constraint problem}@}. <!--SR:!2026-02-13,254,330!2026-04-04,294,330-->

Suppose one wants to {@{find the maximum values of $$f(x,y)=x^{2}y$$}@} with the condition that {@{the $\ x\ {}$- and $\ y\ {}$-coordinates lie on the circle around the origin with radius $\ {\sqrt {3\ } }~$}@}. That is, subject to {@{the constraint $$g(x,y)=x^{2}+y^{2}-3=0~.$$}@} As {@{there is just a single constraint}@}, there is {@{a single multiplier, say $\ \lambda ~$}@}. <!--SR:!2026-01-28,239,330!2026-03-20,281,330!2026-03-20,281,330!2026-03-23,282,330!2026-01-28,241,330-->

{@{The constraint $\ g(x,y)\ {}$}@} is {@{identically zero on the circle of radius $\ {\sqrt {3\ } }~$}@}. {@{Any multiple of $\ g(x,y)\ {}$ may be added to $\ g(x,y)\ {}$}@} leaving {@{$\ g(x,y)\ {}$ unchanged in the region of interest \(on the circle where our original constraint is satisfied\)}@}. <!--SR:!2026-01-30,241,330!2026-03-22,281,330!2026-02-06,250,330!2026-03-09,268,330-->

{@{Applying the ordinary Lagrange multiplier method}@} yields {@{$${\begin{aligned}{\mathcal {L} }(x,y,\lambda )&=f(x,y)+\lambda \cdot g(x,y)\\&=x^{2}y+\lambda (x^{2}+y^{2}-3)\ ,\end{aligned} }$$}@} from which {@{the gradient can be calculated}@}: {@{$${\begin{aligned}\nabla _{x,y,\lambda }{\mathcal {L} }(x,y,\lambda )&=\left({\frac {\partial {\mathcal {L} } }{\partial x} },{\frac {\partial {\mathcal {L} } }{\partial y} },{\frac {\partial {\mathcal {L} } }{\partial \lambda } }\right)\\&=\left(2xy+2\lambda x,x^{2}+2\lambda y,x^{2}+y^{2}-3\right)~.\end{aligned} }$$}@} And therefore: {@{$$\nabla _{x,y,\lambda }{\mathcal {L} }(x,y,\lambda )=0\quad \iff \quad {\begin{cases}2xy+2\lambda x=0\\x^{2}+2\lambda y=0\\x^{2}+y^{2}-3=0\end{cases} }\quad \iff \quad {\begin{cases}x(y+\lambda )=0&{\text{(i)} }\\x^{2}=-2\lambda y&{\text{(ii)} }\\x^{2}+y^{2}=3&{\text{(iii)} }\end{cases} }$$}@} \(iii\) is {@{just the original constraint}@}. \(i\) implies {@{$\ x=0\ {}$ or $\ \lambda =-y~$}@}. If {@{$x=0$ then $\ y=\pm {\sqrt {3\ } }\ {}$ by \(iii\) and consequently $\ \lambda =0\ {}$ from \(ii\)}@}. If {@{$\ \lambda =-y\ {}$, substituting this into \(ii\) yields $\ x^{2}=2y^{2}~$}@}. Substituting {@{this into \(iii\) and solving for $\ y\ {}$ gives $\ y=\pm 1~$}@}. Thus there are {@{six critical points of $\ {\mathcal {L} }\ :$}@} {@{$$({\sqrt {2\ } },1,-1);\quad (-{\sqrt {2\ } },1,-1);\quad ({\sqrt {2\ } },-1,1);\quad (-{\sqrt {2\ } },-1,1);\quad (0,{\sqrt {3\ } },0);\quad (0,-{\sqrt {3\ } },0)~.$$}@} Evaluating {@{the objective at these points}@}, one finds that {@{$$f(\pm {\sqrt {2\ } },1)=2;\quad f(\pm {\sqrt {2\ } },-1)=-2;\quad f(0,\pm {\sqrt {3\ } })=0~.$$}@} Therefore, the objective function {@{attains the [global maximum](global%20maximum.md) \(subject to the constraints\) at $\ (\pm {\sqrt {2\ } },1\ )$ and the [global minimum](global%20minimum.md) at $\ (\pm {\sqrt {2\ } },-1)~$}@}. The point $\ (0,{\sqrt {3\ } })\ {}$ is {@{a [local minimum](local%20minimum.md) of $\ f\ {}$}@} and $\ (0,-{\sqrt {3\ } })\ {}$ is {@{a [local maximum](local%20maximum.md) of $\ f\ {}$}@}, as may be determined by {@{consideration of the [Hessian matrix](Hessian%20(mathematics).md#bordered%20Hessian) of $\ {\mathcal {L} }(x,y,0)~$}@}. <!--SR:!2026-01-31,245,330!2026-03-13,272,330!2026-01-27,241,330!2026-03-26,285,330!2026-03-28,288,330!2026-03-27,286,330!2026-02-14,255,330!2026-04-04,294,330!2026-07-17,346,290!2026-04-02,292,330!2026-02-05,247,330!2026-03-01,260,330!2026-03-17,276,330!2026-04-03,293,330!2026-03-23,283,330!2026-03-31,291,330!2026-03-13,272,330!2025-11-15,176,310-->

Note that while {@{$\ ({\sqrt {2\ } },1,-1)\ {}$ is a critical point of $\ {\mathcal {L} }\ {}$}@}, it is {@{not a local extremum of $\ {\mathcal {L} }~$}@}. We have {@{$${\mathcal {L} }\left({\sqrt {2\ } }+\varepsilon ,1,-1+\delta \right)=2+\delta \left(\varepsilon ^{2}+\left(2{\sqrt {2\ } }\right)\varepsilon \right)~.$$}@} Given {@{any neighbourhood of $\ ({\sqrt {2\ } },1,-1)\ {}$}@}, one can {@{choose a small positive $\ \varepsilon \ {}$ and a small $\ \delta \ {}$ of either sign}@} to {@{get $\ {\mathcal {L} }$ values both greater and less than $\ 2~$}@}. This can also be seen from {@{the Hessian matrix of $\ {\mathcal {L} }\ {}$ evaluated at this point \(or indeed at any of the critical points\)}@} which is {@{an [indefinite matrix](indefinite%20matrix.md)}@}. {@{Each of the critical points of $\ {\mathcal {L} }\ {}$}@} is {@{a [saddle point](saddle%20point.md) of $\ {\mathcal {L} }~$}@}.<sup>[\[4\]](#^ref-4)</sup> <!--SR:!2025-11-29,178,310!2026-02-05,249,330!2026-10-11,378,290!2026-01-31,244,330!2026-02-23,262,330!2026-01-28,241,330!2026-03-14,273,330!2026-03-30,289,330!2026-01-31,245,330!2026-03-06,267,330-->

### Example 4 – Entropy

Suppose we wish to {@{find the [discrete probability distribution](probability%20distribution.md#discrete%20probability%20distribution)}@} on {@{the points $\ \{p_{1},p_{2},\ldots ,p_{n}\}\ {}$ with maximal [information entropy](information%20entropy.md)}@}. This is the same as saying that we wish to {@{find the [least structured](principle%20of%20maximum%20entropy.md) probability distribution on the points $\ \{p_{1},p_{2},\cdots ,p_{n}\}~$}@}. In other words, we wish to {@{maximize the [Shannon entropy](Shannon%20entropy.md) equation}@}: {@{$$f(p_{1},p_{2},\ldots ,p_{n})=-\sum _{j=1}^{n}p_{j}\log _{2}p_{j}~.$$}@} <!--SR:!2026-03-21,281,330!2026-03-17,276,330!2026-04-04,294,330!2026-01-20,235,330!2026-04-04,294,330-->

For {@{this to be a probability distribution}@} {@{the sum of the probabilities $\ p_{i}\ {}$ at each point $\ x_{i}\ {}$ must equal 1}@}, so our constraint is: {@{$$g(p_{1},p_{2},\ldots ,p_{n})=\sum _{j=1}^{n}p_{j}=1~.$$}@} <!--SR:!2026-03-23,282,330!2026-03-26,286,330!2026-03-22,282,330-->

We use Lagrange multipliers to {@{find the point of maximum entropy, $\ {\vec {p} }^{\,*}\ {}$}@}, across {@{all discrete probability distributions $\ {\vec {p} }\ {}$ on $\ \{x_{1},x_{2},\ldots ,x_{n}\}~$}@}. We require that: {@{$$\left.{\frac {\partial }{\partial {\vec {p} } } }(f+\lambda (g-1))\right|_{ {\vec {p} }={\vec {p} }^{\,*} }=0\ ,$$}@} which gives {@{a system of _n_ equations, $\ k=1,\ \ldots ,n\ {}$}@}, such that: {@{$$\left.{\frac {\partial }{\partial p_{k} } }\left\{-\left(\sum _{j=1}^{n}p_{j}\log _{2}p_{j}\right)+\lambda \left(\sum _{j=1}^{n}p_{j}-1\right)\right\}\right|_{p_{k}=p_{\star k} }=0~.$$}@} Carrying out {@{the differentiation of these _n_ equations}@}, we get {@{$$-\left({\frac {1}{\ln 2} }+\log _{2}p_{\star k}\right)+\lambda =0~.$$}@} This shows that {@{all $\ p_{\star k}\ {}$ are equal \(because they depend on _λ_ only\)}@}. By {@{using the constraint $$\sum _{j}p_{j}=1\ ,$$}@} we find {@{$$p_{\star k}={\frac {1}{n} }~.$$}@} Hence, {@{the uniform distribution is the distribution with the greatest entropy, among distributions on _n_ points}@}. <!--SR:!2026-03-26,285,330!2026-03-28,287,330!2026-01-29,242,330!2026-01-29,243,330!2025-11-28,188,310!2026-03-16,276,330!2027-11-04,733,330!2026-02-12,255,330!2026-03-16,275,330!2026-02-07,250,330!2027-04-26,533,310-->

### Example 5 – Numerical optimization

> {@{![Lagrange multipliers cause the critical points to occur at saddle points \(Example __5__\).](../../archives/Wikimedia%20Commons/Lagnum1.png)}@}
>
> Lagrange multipliers {@{cause the critical points to occur at saddle points \(Example __5__\)}@}. <!--SR:!2025-11-27,187,310!2026-01-25,239,330-->

<!-- markdownlint MD028 -->

> {@{![The magnitude of the gradient can be used to force the critical points to occur at local minima \(Example __5__\).](../../archives/Wikimedia%20Commons/Lagnum2.png)}@}
>
> {@{The magnitude of the gradient}@} can be used to {@{force the critical points to occur at local minima \(Example __5__\)}@}. <!--SR:!2026-03-30,289,330!2026-03-20,279,330!2026-03-24,283,330-->

{@{The critical points of Lagrangians}@} occur at {@{[saddle points](saddle%20point.md), rather than at local maxima \(or minima\)}@}.<sup>[\[4\]](#^ref-4)</sup><sup>[\[17\]](#^ref-17)</sup> Unfortunately, {@{many numerical optimization techniques}@}, such as {@{[hill climbing](hill%20climbing.md), [gradient descent](gradient%20descent.md), some of the [quasi-Newton methods](quasi-Newton%20method.md)}@}, among others, are designed to {@{find local maxima \(or minima\) and not saddle points}@}. For this reason, one must {@{either modify the formulation to ensure that it's a minimization problem}@} \(for example, by {@{extremizing the square of the [gradient](gradient.md) of the Lagrangian as below}@}\), or else use {@{an optimization technique that finds [stationary points](stationary%20points.md)}@} \(such as {@{[Newton's method](Newton's%20method%20in%20optimization.md) without an extremum seeking [line search](line%20search.md)}@}\) and not {@{necessarily extrema}@}. <!--SR:!2026-01-29,240,330!2026-03-20,280,330!2026-01-26,241,330!2026-02-21,260,330!2026-03-12,271,330!2025-11-11,172,310!2026-02-14,255,330!2026-04-01,291,330!2026-03-11,270,330!2025-11-12,173,310-->

As a simple example, consider {@{the problem of finding the value of _x_ that minimizes $\ f(x)=x^{2}\ {}$}@}, constrained such that {@{$\ x^{2}=1~$}@}. \(This problem is {@{somewhat untypical because there are only two values that satisfy this constraint}@}, but it is {@{useful for illustration purposes}@} because {@{the corresponding unconstrained function can be visualized in three dimensions}@}.\) <!--SR:!2026-03-28,288,330!2026-04-01,291,330!2026-02-06,250,330!2026-01-21,236,330!2026-01-30,241,330-->

Using {@{Lagrange multipliers}@}, this problem can be {@{converted into an unconstrained optimization problem}@}: {@{$${\mathcal {L} }(x,\lambda )=x^{2}+\lambda (x^{2}-1)~.$$}@} {@{The two critical points}@} occur at {@{saddle points where _x_ = 1 and _x_ = −1}@}. <!--SR:!2026-02-10,253,330!2026-03-27,286,330!2026-02-19,259,330!2026-04-04,294,330!2026-01-29,243,330-->

In order to {@{solve this problem with a numerical optimization technique}@}, we must first {@{transform this problem such that the critical points occur at local minima}@}. This is done by {@{computing the magnitude of the gradient of the unconstrained optimization problem}@}. <!--SR:!2026-01-27,241,330!2026-03-25,284,330!2026-03-29,289,330-->

First, we compute {@{the partial derivative of the unconstrained problem with respect to each variable}@}: {@{$${\begin{aligned}&{\frac {\partial {\mathcal {L} } }{\partial x} }=2x+2x\lambda \\[5pt]&{\frac {\partial {\mathcal {L} } }{\partial \lambda } }=x^{2}-1~.\end{aligned} }$$}@} If {@{the target function is not easily differentiable}@}, {@{the differential with respect to each variable}@} can be {@{approximated as $${\begin{aligned}{\frac {\ \partial {\mathcal {L} }\ }{\partial x} }\approx {\frac { {\mathcal {L} }(x+\varepsilon ,\lambda )-{\mathcal {L} }(x,\lambda )}{\varepsilon } },\\[5pt]{\frac {\ \partial {\mathcal {L} }\ }{\partial \lambda } }\approx {\frac { {\mathcal {L} }(x,\lambda +\varepsilon )-{\mathcal {L} }(x,\lambda )}{\varepsilon } },\end{aligned} }$$}@} where {@{$\varepsilon$ is a small value}@}. <!--SR:!2026-02-12,255,330!2027-04-18,555,310!2026-03-20,279,330!2026-04-04,294,330!2026-04-03,294,330!2026-02-04,248,330-->

Next, we compute {@{the magnitude of the gradient}@}, which is {@{the square root of the sum of the squares of the partial derivatives}@}: {@{$${\begin{aligned}h(x,\lambda )&={\sqrt {(2x+2x\lambda )^{2}+(x^{2}-1)^{2}\ } }\\[4pt]&\approx {\sqrt {\left({\frac {\ {\mathcal {L} }(x+\varepsilon ,\lambda )-{\mathcal {L} }(x,\lambda )\ }{\varepsilon } }\right)^{2}+\left({\frac {\ {\mathcal {L} }(x,\lambda +\varepsilon )-{\mathcal {L} }(x,\lambda )\ }{\varepsilon } }\right)^{2}\ } }~.\end{aligned} }$$}@} \(Since {@{magnitude is always non-negative}@}, {@{optimizing over the squared-magnitude}@} is {@{equivalent to optimizing over the magnitude}@}. Thus, {@{the "square root" may be omitted from these equations with no expected difference in the results of optimization}@}.\) <!--SR:!2026-01-27,241,330!2026-03-11,270,330!2025-11-13,174,310!2026-02-02,246,330!2026-02-02,244,330!2026-03-08,268,330!2026-03-26,285,330-->

{@{The critical points of _h_}@} occur at {@{_x_ = 1 and _x_ = −1, just as in ${\mathcal {L} }~$}@}. Unlike {@{the critical points in ${\mathcal {L} }\,$}@}, however, the critical points in _h_ {@{occur at local minima}@}, so {@{numerical optimization techniques can be used to find them}@}. <!--SR:!2026-03-10,269,330!2026-02-12,255,330!2026-02-12,255,330!2026-02-17,257,330!2026-03-29,288,330-->

## applications

### control theory

In {@{[optimal control](optimal%20control.md) theory}@}, {@{the Lagrange multipliers}@} are {@{interpreted as [costate](costate.md) variables}@}, and Lagrange multipliers are {@{reformulated as the minimization of the [Hamiltonian](Hamiltonian%20(control%20theory).md)}@}, in {@{[Pontryagin's maximum principle](Pontryagin's%20maximum%20principle.md)}@}. <!--SR:!2026-03-14,274,330!2026-03-29,288,330!2026-03-15,275,330!2026-02-05,249,330!2026-03-15,275,330-->

### nonlinear programming

The Lagrange multiplier method has {@{several generalizations}@}. In {@{[nonlinear programming](nonlinear%20programming.md)}@} there are {@{several multiplier rules}@}, e.g. {@{the Carathéodory–John Multiplier Rule and the Convex Multiplier Rule}@}, for {@{inequality constraints}@}.<sup>[\[18\]](#^ref-18)</sup> <!--SR:!2026-01-23,237,330!2026-03-15,276,330!2026-03-17,276,330!2026-03-17,276,330!2026-03-08,267,330-->

### economics

In {@{many models in [mathematical economics](mathematical%20economics.md)}@} such as {@{[general equilibrium models](general%20equilibrium%20model.md)}@}, {@{consumer behavior is implemented as [utility maximization](utility%20maximization%20problem.md)}@} and {@{firm behavior as [profit maximization](profit%20maximization.md)}@}, both entities being {@{subject to constraints such as [budget constraints](budget%20constraint.md) and [production constraints](production%20function.md)}@}. {@{The usual way to determine an optimal solution}@} is achieved by {@{maximizing some function}@}, where {@{the constraints are enforced using Lagrangian multipliers}@}.<sup>[\[19\]](#^ref-19)</sup><sup>[\[20\]](#^ref-20)</sup><sup>[\[21\]](#^ref-21)</sup><sup>[\[22\]](#^ref-22)</sup> <!--SR:!2026-03-11,270,330!2026-02-12,255,330!2026-03-24,285,330!2026-02-03,247,330!2026-02-02,246,330!2026-03-15,276,330!2026-03-10,269,330!2026-01-31,245,330-->

### power systems

Methods based on Lagrange multipliers have {@{applications in [power systems](power%20systems.md)}@}, e.g. in {@{distributed-energy-resources \(DER\) placement and load shedding}@}.<sup>[\[23\]](#^ref-23)</sup> <!--SR:!2026-01-27,240,330!2026-01-21,236,330-->

### safe reinforcement learning

The method of Lagrange multipliers applies to {@{[constrained Markov decision processes](constrained%20Markov%20decision%20processes.md)}@}.<sup>[\[24\]](#^ref-24)</sup> It naturally {@{produces gradient-based primal-dual algorithms in safe reinforcement learning}@}.<sup>[\[25\]](#^ref-25)</sup> <!--SR:!2026-03-09,268,330!2026-02-08,251,330-->

### [normalized solutions](normalized%20solution%20(mathematics).md)

Considering {@{the PDE problems with constraints}@}, i.e., the study of {@{the properties of the normalized solutions}@}, {@{Lagrange multipliers play an important role}@}. <!--SR:!2026-03-12,271,330!2026-03-18,278,330!2026-02-04,246,330-->

## see also

- [Adjustment of observations](adjustment%20of%20observations.md)
- [Duality](duality%20(optimization).md)
- [Gittins index](Gittins%20index.md)
- [Karush–Kuhn–Tucker conditions](Karush–Kuhn–Tucker%20conditions.md): ::@:: generalization of the method of Lagrange multipliers <!--SR:!2027-11-08,729,330!2027-11-15,741,330-->
- [Lagrange multipliers on Banach spaces](Lagrange%20multipliers%20on%20Banach%20spaces.md): ::@:: another generalization of the method of Lagrange multipliers <!--SR:!2026-02-12,255,330!2026-01-29,242,330-->
- [Lagrange multiplier test](Lagrange%20multiplier%20test.md) ::@:: in maximum likelihood estimation <!--SR:!2025-11-13,174,310!2026-01-23,237,330-->
- [Lagrangian relaxation](Lagrangian%20relaxation.md)

## references

This text incorporates [content](https://en.wikipedia.org/wiki/Lagrange_multiplier) from [Wikipedia](Wikipedia.md) available under the [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) license.

1. <a id="CITEREFHoffmannBradley2004"></a> Hoffmann, Laurence D.; Bradley, Gerald L. \(2004\). _Calculus for Business, Economics, and the Social and Life Sciences_ \(8th ed.\). McGraw Hill Higher Education. pp. 575–588. [ISBN](ISBN%20(identifier).md) [0-07-242432-X](https://en.wikipedia.org/wiki/Special:BookSources/0-07-242432-X). <a id="^ref-1"></a>^ref-1
2. <a id="CITEREFBeavisDobbs1990"></a> Beavis, Brian; Dobbs, Ian M. \(1990\). ["Static Optimization"](https://books.google.com/books?id=L7HMACFgnXMC&pg=PA40). _Optimization and Stability Theory for Economic Analysis_. New York: Cambridge University Press. p. 40. [ISBN](ISBN%20(identifier).md) [0-521-33605-8](https://en.wikipedia.org/wiki/Special:BookSources/0-521-33605-8). <a id="^ref-2"></a>^ref-2
3. <a id="CITEREFProtterMorrey1985"></a> [Protter, Murray H.](Murray%20H.%20Protter.md); [Morrey, Charles B. Jr.](Charles%20B.%20Morrey%20Jr..md) \(1985\). _Intermediate Calculus_ \(2nd ed.\). New York, NY: Springer. p. 267. [ISBN](ISBN%20(identifier).md) [0-387-96058-9](https://en.wikipedia.org/wiki/Special:BookSources/0-387-96058-9). <a id="^ref-3"></a>^ref-3
4. <a id="CITEREFWalsh1975"></a> Walsh, G.R. \(1975\). ["Saddle-point Property of Lagrangian Function"](https://books.google.com/books?id=K0EZAQAAIAAJ&pg=PA39). _Methods of Optimization_. New York, NY: John Wiley & Sons. pp. 39–44. [ISBN](ISBN%20(identifier).md) [0-471-91922-5](https://en.wikipedia.org/wiki/Special:BookSources/0-471-91922-5). <a id="^ref-4"></a>^ref-4
5. <a id="CITEREFKalman2009"></a> Kalman, Dan \(2009\). "Leveling with Lagrange: An alternate view of constrained optimization". _[Mathematics Magazine](Mathematics%20Magazine.md)_. __82__ \(3\): 186–196. [doi](doi%20(identifier).md):[10.1080/0025570X.2009.11953617](https://doi.org/10.1080%2F0025570X.2009.11953617). [JSTOR](JSTOR%20(identifier).md#content) [27765899](https://www.jstor.org/stable/27765899). [S2CID](S2CID%20(identifier).md#S2CID) [121070192](https://api.semanticscholar.org/CorpusID:121070192). <a id="^ref-5"></a>^ref-5
6. <a id="CITEREFSilberbergSuen2001"></a> Silberberg, Eugene; Suen, Wing \(2001\). _The Structure of Economics: A Mathematical Analysis_ \(Third ed.\). Boston: Irwin McGraw-Hill. pp. 134–141. [ISBN](ISBN%20(identifier).md) [0-07-234352-4](https://en.wikipedia.org/wiki/Special:BookSources/0-07-234352-4). <a id="^ref-6"></a>^ref-6
7. <a id="CITEREFde la Fuente2000"></a> de la Fuente, Angel \(2000\). [_Mathematical Methods and Models for Economists_](https://archive.org/details/mathematicalmeth00fuen). Cambridge: Cambridge University Press. p. [285](https://archive.org/details/mathematicalmeth00fuen/page/n288). [doi](doi%20(identifier).md):[10.1017/CBO9780511810756](https://doi.org/10.1017%2FCBO9780511810756). [ISBN](ISBN%20(identifier).md) [978-0-521-58512-5](https://en.wikipedia.org/wiki/Special:BookSources/978-0-521-58512-5). <a id="^ref-7"></a>^ref-7
8. <a id="CITEREFLuenberger1969"></a> [Luenberger, David G.](David%20Luenberger.md) \(1969\). _Optimization by Vector Space Methods_. New York: John Wiley & Sons. pp. 188–189. <a id="^ref-8"></a>^ref-8
9. <a id="CITEREFBertsekas1999"></a> [Bertsekas, Dimitri P.](Dimitri%20P.%20Bertsekas.md) \(1999\). _Nonlinear Programming_ \(Second ed.\). Cambridge, MA: Athena Scientific. [ISBN](ISBN%20(identifier).md) [1-886529-00-0](https://en.wikipedia.org/wiki/Special:BookSources/1-886529-00-0). <a id="^ref-9"></a>^ref-9
10. <a id="CITEREFVapnyarskii2001"></a> Vapnyarskii, I.B. \(2001\) \[1994\], ["Lagrange multipliers"](https://www.encyclopediaofmath.org/index.php?title=Lagrange_multipliers), _[Encyclopedia of Mathematics](Encyclopedia%20of%20Mathematics.md)_, [EMS Press](European%20Mathematical%20Society.md). <a id="^ref-10"></a>^ref-10
11. <a id="CITEREFLasdon2002"></a> Lasdon, Leon S. \(2002\) \[1970\]. _Optimization Theory for Large Systems_ \(reprint ed.\). Mineola, New York, NY: Dover. [ISBN](ISBN%20(identifier).md) [0-486-41999-1](https://en.wikipedia.org/wiki/Special:BookSources/0-486-41999-1). [MR](MR%20(identifier).md) [1888251](https://mathscinet.ams.org/mathscinet-getitem?mr=1888251). <a id="^ref-11"></a>^ref-11
12. <a id="CITEREFHiriart-UrrutyLemaréchal1993"></a> Hiriart-Urruty, Jean-Baptiste; [Lemaréchal, Claude](Claude%20Lemaréchal.md) \(1993\). "Chapter XII: Abstract duality for practitioners". _Convex analysis and minimization algorithms_. Grundlehren der Mathematischen Wissenschaften \[Fundamental Principles of Mathematical Sciences\]. Vol. 306. Berlin, DE: Springer-Verlag. pp. 136–193 \(and Bibliographical comments pp. 334–335\). [ISBN](ISBN%20(identifier).md) [3-540-56852-2](https://en.wikipedia.org/wiki/Special:BookSources/3-540-56852-2). [MR](MR%20(identifier).md) [1295240](https://mathscinet.ams.org/mathscinet-getitem?mr=1295240). Volume II: Advanced theory and bundle methods. <a id="^ref-12"></a>^ref-12
13. <a id="CITEREFLemaréchal2000"></a> [Lemaréchal, Claude](Claude%20Lemaréchal.md) \(15–19 May 2000\). "Lagrangian relaxation". In Jünger, Michael; Naddef, Denis \(eds.\). _Computational combinatorial optimization: Papers from the Spring School held in Schloß Dagstuhl_. Spring School held in Schloß Dagstuhl, May 15–19, 2000. Lecture Notes in Computer Science. Vol. 2241. Berlin, DE: Springer-Verlag \(published 2001\). pp. 112–156. [doi](doi%20(identifier).md):[10.1007/3-540-45586-8\_4](https://doi.org/10.1007%2F3-540-45586-8_4). [ISBN](ISBN%20(identifier).md) [3-540-42877-1](https://en.wikipedia.org/wiki/Special:BookSources/3-540-42877-1). [MR](MR%20(identifier).md) [1900016](https://mathscinet.ams.org/mathscinet-getitem?mr=1900016). [S2CID](S2CID%20(identifier).md#S2CID) [9048698](https://api.semanticscholar.org/CorpusID:9048698). <a id="^ref-13"></a>^ref-13
14. <a id="CITEREFLafontaine2015"></a> Lafontaine, Jacques \(2015\). [_An Introduction to Differential Manifolds_](https://books.google.com/books?id=KNhJCgAAQBAJ&pg=PA70). Springer. p. 70. [ISBN](ISBN%20(identifier).md) [978-3-319-20735-3](https://en.wikipedia.org/wiki/Special:BookSources/978-3-319-20735-3). <a id="^ref-14"></a>^ref-14
15. <a id="CITEREFDixit1990"></a> [Dixit, Avinash K.](Avinash%20Dixit.md) \(1990\). ["Shadow Prices"](https://books.google.com/books?id=dHrsHz0VocUC&pg=PA40). _Optimization in Economic Theory_ \(2nd ed.\). New York: Oxford University Press. pp. 40–54. [ISBN](ISBN%20(identifier).md) [0-19-877210-6](https://en.wikipedia.org/wiki/Special:BookSources/0-19-877210-6). <a id="^ref-15"></a>^ref-15
16. <a id="CITEREFChiang1984"></a> [Chiang, Alpha C.](Alpha%20Chiang.md) \(1984\). [_Fundamental Methods of Mathematical Economics_](https://archive.org/details/fundamentalmetho0000chia_h4v2) \(Third ed.\). McGraw-Hill. p. [386](https://archive.org/details/fundamentalmetho0000chia_h4v2/page/386). [ISBN](ISBN%20(identifier).md) [0-07-010813-7](https://en.wikipedia.org/wiki/Special:BookSources/0-07-010813-7). <a id="^ref-16"></a>^ref-16
17. <a id="CITEREFHeath2005"></a> [Heath, Michael T.](Michael%20Heath%20(computer%20scientist).md) \(2005\). [_Scientific Computing: An introductory survey_](https://books.google.com/books?id=gwBrMAEACAAJ). McGraw-Hill. p. 203. [ISBN](ISBN%20(identifier).md) [978-0-07-124489-3](https://en.wikipedia.org/wiki/Special:BookSources/978-0-07-124489-3). <a id="^ref-17"></a>^ref-17
18. <a id="CITEREFPourciau1980"></a> Pourciau, Bruce H. \(1980\). ["Modern multiplier rules"](http://www.maa.org/programs/maa-awards/writing-awards/modern-multiplier-rules). _[American Mathematical Monthly](American%20Mathematical%20Monthly.md)_. __87__ \(6\): 433–452. [doi](doi%20(identifier).md):[10.2307/2320250](https://doi.org/10.2307%2F2320250). [JSTOR](JSTOR%20(identifier).md#content) [2320250](https://www.jstor.org/stable/2320250). <a id="^ref-18"></a>^ref-18
19. <a id="CITEREFKamienSchwartz1991"></a> [Kamien, M. I.](Morton%20Kamien.md); [Schwartz, N. L.](Nancy%20Schwartz.md) \(1991\). [_Dynamic Optimization: The Calculus of Variations and Optimal Control in Economics and Management_](https://books.google.com/books?id=0IoGUn8wjDQC) \(Second ed.\). New York: Elsevier. [ISBN](ISBN%20(identifier).md) [0-444-01609-0](https://en.wikipedia.org/wiki/Special:BookSources/0-444-01609-0). <a id="^ref-19"></a>^ref-19
20. <a id="CITEREFGlötzlGlötzlRichters2019"></a> Glötzl, Erhard; Glötzl, Florentin; Richters, Oliver \(2019\). "From constrained optimization to constrained dynamics: extending analogies between economics and mechanics". _Journal of Economic Interaction and Coordination_. __14__ \(3\): 623–642. [doi](doi%20(identifier).md):[10.1007/s11403-019-00252-7](https://doi.org/10.1007%2Fs11403-019-00252-7). [hdl](hdl%20(identifier).md):[10419/171974](https://hdl.handle.net/10419%2F171974). <a id="^ref-20"></a>^ref-20
21. <a id="CITEREFBaxleyMoorhouse1984"></a> Baxley, John V.; Moorhouse, John C. \(1984\). "Lagrange Multiplier Problems in Economics". _The American Mathematical Monthly_. __91__ \(7\): 404–412. [doi](doi%20(identifier).md):[10.1080/00029890.1984.11971446](https://doi.org/10.1080%2F00029890.1984.11971446).. <a id="^ref-21"></a>^ref-21
22. <a id="CITEREFJanová2011"></a> Janová, Jitka \(2011\). "Applications of a constrained mechanics methodology in economics". _European Journal of Physics_. __32__ \(6\): 1443–1463. [arXiv](ArXiv%20(identifier).md):[1106.3455](https://arxiv.org/abs/1106.3455). [Bibcode](bibcode%20(identifier).md):[2011EJPh...32.1443J](https://ui.adsabs.harvard.edu/abs/2011EJPh...32.1443J). [doi](doi%20(identifier).md):[10.1088/0143-0807/32/6/001](https://doi.org/10.1088%2F0143-0807%2F32%2F6%2F001). <a id="^ref-22"></a>^ref-22
23. <a id="CITEREFGautamBhusalBenidris2020"></a> Gautam, Mukesh; Bhusal, Narayan; Benidris, Mohammed \(2020\). _A sensitivity-based approach to adaptive under-frequency load shedding_. 2020 IEEE Texas Power and Energy Conference \(TPEC\). [Institute of Electronic and Electrical Engineers](Institute%20of%20Electronic%20and%20Electrical%20Engineers.md). pp. 1–5. [doi](doi%20(identifier).md):[10.1109/TPEC48276.2020.9042569](https://doi.org/10.1109%2FTPEC48276.2020.9042569). <a id="^ref-23"></a>^ref-23
24. <a id="CITEREFAltman2021"></a> Altman, Eitan \(2021\). _Constrained Markov Decision Processes_. [Routledge](routledge.md). <a id="^ref-24"></a>^ref-24
25. <a id="CITEREFDingZhangJovanovicBasar2020"></a> Ding, Dongsheng; Zhang, Kaiqing; Jovanovic, Mihailo; Basar, Tamer \(2020\). _Natural policy gradient primal-dual method for constrained Markov decision processes_. Advances in Neural Information Processing Systems. <a id="^ref-25"></a>^ref-25

## further reading

- <a id="CITEREFBeavisDobbs1990"></a> Beavis, Brian; Dobbs, Ian M. \(1990\). ["Static Optimization"](https://books.google.com/books?id=L7HMACFgnXMC&pg=PA32). _Optimization and Stability Theory for Economic Analysis_. New York, NY: Cambridge University Press. pp. 32–72. [ISBN](ISBN%20(identifier).md) [0-521-33605-8](https://en.wikipedia.org/wiki/Special:BookSources/0-521-33605-8).
- <a id="CITEREFBertsekas1982"></a> [Bertsekas, Dimitri P.](Dimitri%20Bertsekas.md) \(1982\). _Constrained optimization and Lagrange multiplier methods_. New York, NY: Academic Press. [ISBN](ISBN%20(identifier).md) [0-12-093480-9](https://en.wikipedia.org/wiki/Special:BookSources/0-12-093480-9).
- <a id="CITEREFBeveridgeSchechter1970"></a> Beveridge, Gordon S.G.; Schechter, Robert S. \(1970\). ["Lagrangian multipliers"](https://books.google.com/books?id=TfhVXlWtOPQC&pg=PA244). _Optimization: Theory and Practice_. New York, NY: McGraw-Hill. pp. 244–259. [ISBN](ISBN%20(identifier).md) [0-07-005128-3](https://en.wikipedia.org/wiki/Special:BookSources/0-07-005128-3).
- <a id="CITEREFBingerHoffman1998"></a> Binger, Brian R.; Hoffman, Elizabeth \(1998\). "Constrained optimization". _Microeconomics with Calculus_ \(2nd ed.\). Reading: Addison-Wesley. pp. 56–91. [ISBN](ISBN%20(identifier).md) [0-321-01225-9](https://en.wikipedia.org/wiki/Special:BookSources/0-321-01225-9).
- <a id="CITEREFCarter2001"></a> Carter, Michael \(2001\). ["Equality constraints"](https://books.google.com/books?id=KysvrGGfzq0C&pg=PA516). _Foundations of Mathematical Economics_. Cambridge, MA: MIT Press. pp. 516–549. [ISBN](ISBN%20(identifier).md) [0-262-53192-5](https://en.wikipedia.org/wiki/Special:BookSources/0-262-53192-5).
- <a id="CITEREFHestenes1966"></a> [Hestenes, Magnus R.](Magnus%20Hestenes.md) \(1966\). "Minima of functions subject to equality constraints". _Calculus of Variations and Optimal Control Theory_. New York, NY: Wiley. pp. 29–34.
- <a id="CITEREFWylieBarrett1995"></a> Wylie, C. Ray; Barrett, Louis C. \(1995\). "The extrema of integrals under constraint". _Advanced Engineering Mathematics_ \(Sixth ed.\). New York, NY: McGraw-Hill. pp. 1096–1103. [ISBN](ISBN%20(identifier).md) [0-07-072206-4](https://en.wikipedia.org/wiki/Special:BookSources/0-07-072206-4).

## external links

> ![Wikibooks logo](../../archives/Wikimedia%20Commons/Wikibooks-logo-en-noslogan.svg) The Wikibook _[Calculus optimization methods](https://en.wikibooks.org/wiki/Calculus_optimization_methods)_ has a page on the topic of: ___[Lagrange multipliers](https://en.wikibooks.org/wiki/Calculus_optimization_methods/Lagrange_multipliers)___

### exposition

- <a id="CITEREFSteuard"></a> Steuard. ["Conceptual introduction"](http://www.slimy.com/~steuard/teaching/tutorials/Lagrange.html). _slimy.com_. — plus a brief discussion of Lagrange multipliers in the [calculus of variations](calculus%20of%20variations.md) as used in physics.
- <a id="CITEREFCarpenter, Kenneth H."></a> Carpenter, Kenneth H. ["Lagrange multipliers for quadratic forms with linear constraints"](http://ece.k-state.edu/people/faculty/carpenter/documents/lagrange.pdf) \(PDF\). [Kansas State University](Kansas%20State%20University.md).

### additional text and interactive applets

- <a id="CITEREFResnik"></a> Resnik. ["Simple explanation with an example of governments using taxes as Lagrange multipliers"](http://www.umiacs.umd.edu/~resnik/ling848_fa2004/lagrange.html). _umiacs.umd.edu_. [University of Maryland](University%20of%20Maryland.md).
- <a id="CITEREFKlein, Dan"></a> Klein, Dan. ["Lagrange multipliers without permanent scarring\] Explanation with focus on the intuition"](http://nlp.cs.berkeley.edu/tutorials/lagrange-multipliers.pdf) \(PDF\). _nlp.cs.berkeley.edu_. [University of California, Berkeley](University%20of%20California,%20Berkeley.md).
- <a id="CITEREFSathyanarayana, Shashi"></a> Sathyanarayana, Shashi. ["Geometric representation of method of Lagrange multipliers"](http://demonstrations.wolfram.com/GeometricRepresentationOfMethodOfLagrangeMultipliers). _wolfram.com_ \(_Mathematica_ demonstration\). [Wolfram Research](Wolfram%20Research.md). Needs Internet Explorer / Firefox / Safari. — Provides compelling insight in 2 dimensions that at a minimizing point, the direction of steepest descent must be perpendicular to the tangent of the constraint curve at that point.
- ["Lagrange multipliers – two variables"](http://ocw.mit.edu/ans7870/18/18.02/f07/tools/LagrangeMultipliersTwoVariables.html). _MIT Open Courseware \(ocw.mit.edu\)_ \(Applet\). [Massachusetts Institute of Technology](Massachusetts%20Institute%20of%20Technology.md).
- ["Lagrange multipliers"](http://ocw.mit.edu/courses/mathematics/18-02-multivariable-calculus-fall-2007/video-lectures/lecture-13-lagrange-multipliers/). _MIT Open Courseware \(ocw.mit.edu\)_ \(video lecture\). Mathematics 18-02: Multivariable calculus. [Massachusetts Institute of Technology](Massachusetts%20Institute%20of%20Technology.md). Fall 2007.
- <a id="CITEREFBertsekas"></a> Bertsekas. ["Details on Lagrange multipliers"](http://www.athenasc.com/NLP_Slides.pdf) \(PDF\). _athenasc.com_ \(slides / course lecture\). Non-Linear Programming. — Course slides accompanying text on nonlinear optimization
- <a id="CITEREFWyatt, John2004"></a> Wyatt, John \(7 April 2004\) \[19 November 2002\]. ["Legrange multipliers, constrained optimization, and the maximum entropy principle"](http://www-mtl.mit.edu/Courses/6.050/2004/unit9/wyatt.apr.7.pdf) \(PDF\). _www-mtl.mit.edu_. Elec E & C S / Mech E 6.050 – Information, entropy, and computation. — Geometric idea behind Lagrange multipliers
- ["Using Lagrange multipliers in optimization"](http://matlab.cheme.cmu.edu/2011/12/24/using-lagrange-multipliers-in-optimization/). _matlab.cheme.cmu.edu_ \(MATLAB example\). Pittsburgh, PA: Carnegie Mellon University. 24 December 2011.

|                                                           | <!-- - [v](https://en.wikipedia.org/wiki/Template:Calculus%20topics) <br/> - [t](https://en.wikipedia.org/wiki/Template%20talk:Calculus%20topics) <br/> - [e](https://en.wikipedia.org/wiki/Special:EditPage/Template%3ACalculus%20topics) <br/> --> [Calculus](calculus.md)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| --------------------------------------------------------: | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
|                         __[Precalculus](precalculus.md)__ | - [Binomial theorem](binomial%20theorem.md) <br/> - [Concave function](concave%20function.md) <br/> - [Continuous function](continuous%20function.md) <br/> - [Factorial](factorial.md) <br/> - [Finite difference](finite%20difference.md) <br/> - [Free variables and bound variables](free%20variables%20and%20bound%20variables.md) <br/> - [Graph of a function](graph%20of%20a%20function.md) <br/> - [Linear function](linear%20function.md) <br/> - [Radian](radian.md) <br/> - [Rolle's theorem](Rolle's%20theorem.md) <br/> - [Secant](secant%20line.md) <br/> - [Slope](slope.md) <br/> - [Tangent](tangent.md)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
|                    __[Limits](limit%20(mathematics).md)__ | - [Indeterminate form](indeterminate%20form.md) <br/> - [Limit of a function](limit%20of%20a%20function.md)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [One-sided limit](one-sided%20limit.md) <br/> - [Limit of a sequence](limit%20of%20a%20sequence.md) <br/> - [Order of approximation](order%20of%20approximation.md) <br/> - [\(ε, δ\)-definition of limit]((ε,%20δ)-definition%20of%20limit.md#(ε,%20δ)-definition%20of%20limit)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
|   __[Differential calculus](differential%20calculus.md)__ | - [Derivative](derivative.md) <br/> - [Second derivative](second%20derivative.md) <br/> - [Partial derivative](partial%20derivative.md) <br/> - [Differential](differential%20(mathematics).md) <br/> - [Differential operator](differential%20operator.md) <br/> - [Mean value theorem](mean%20value%20theorem.md) <br/> - [Notation](notation%20for%20differentiation.md)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Leibniz's notation](Leibniz's%20notation.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Newton's notation](Newton's%20notation%20for%20differentiation.md#Newton's%20notation) <br/> - [Rules of differentiation](differentiation%20rules.md)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [linearity](linearity%20of%20differentiation.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Power](power%20rule.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Sum](sum%20rule%20in%20differentiation.md#differentiation%20is%20linear) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Chain](chain%20rule.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [L'Hôpital's](L'Hôpital's%20rule.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Product](product%20rule.md)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [General Leibniz's rule](general%20Leibniz%20rule.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Quotient](quotient%20rule.md) <br/> - Other techniques  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Implicit differentiation](implicit%20differentiation.md#implicit%20differentiation) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Inverse functions and differentiation](inverse%20functions%20and%20differentiation.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Logarithmic derivative](logarithmic%20derivative.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Related rates](related%20rates.md) <br/> - [Stationary points](stationary%20point.md)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [First derivative test](first%20derivative%20test.md#first-derivative%20test) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Second derivative test](second%20derivative%20test.md#second-derivative%20test%20(single%20variable)) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Extreme value theorem](extreme%20value%20theorem.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Maximum and minimum](maximum%20and%20minimum.md) <br/> - Further applications  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Newton's method](Newton's%20method.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Taylor's theorem](Taylor's%20theorem.md) <br/> - [Differential equation](differential%20equation.md)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Ordinary differential equation](ordinary%20differential%20equation.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Partial differential equation](partial%20differential%20equation.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Stochastic differential equation](stochastic%20differential%20equation.md) |
|           __[Integral calculus](integral%20calculus.md)__ | - [Antiderivative](antiderivative.md) <br/> - [Arc length](arc%20length.md) <br/> - [Riemann integral](Riemann%20integral.md) <br/> - [Basic properties](integral.md#properties) <br/> - [Constant of integration](constant%20of%20integration.md) <br/> - [Fundamental theorem of calculus](fundamental%20theorem%20of%20calculus.md)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Differentiating under the integral sign](Leibniz%20integral%20rule.md) <br/> - [Integration by parts](integration%20by%20parts.md) <br/> - [Integration by substitution](integration%20by%20substitution.md)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [trigonometric](trigonometric%20substitution.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Euler](Euler%20substitution.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Tangent half-angle substitution](tangent%20half-angle%20substitution.md) <br/> - [Partial fractions in integration](partial%20fractions%20in%20integration.md#application%20to%20symbolic%20integration)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Quadratic integral](quadratic%20integral.md) <br/> - [Trapezoidal rule](trapezoidal%20rule.md) <br/> - Volumes  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Washer method](disc%20integration.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Shell method](shell%20integration.md) <br/> - [Integral equation](integral%20equation.md) <br/> - [Integro-differential equation](integro-differential%20equation.md)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
|               __[Vector calculus](vector%20calculus.md)__ | - Derivatives  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Curl](curl%20(mathematics).md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Directional derivative](directional%20derivative.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Divergence](divergence.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Gradient](gradient.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Laplacian](Laplace%20operator.md) <br/> - Basic theorems  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Line integrals](fundamental%20theorem%20of%20line%20integrals.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Green's](Green's%20theorem.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Stokes'](Stokes'%20theorem.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Gauss'](divergence%20theorem.md)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| __[Multivariable calculus](multivariable%20calculus.md)__ | - [Divergence theorem](divergence%20theorem.md) <br/> - [Geometric](geometric%20calculus.md) <br/> - [Hessian matrix](Hessian%20matrix.md) <br/> - [Jacobian matrix and determinant](Jacobian%20matrix%20and%20determinant.md) <br/> - [Lagrange multiplier](Lagrange%20multiplier.md) <br/> - [Line integral](line%20integral.md) <br/> - [Matrix](matrix%20calculus.md) <br/> - [Multiple integral](multiple%20integral.md) <br/> - [Partial derivative](partial%20derivative.md) <br/> - [Surface integral](surface%20integral.md) <br/> - [Volume integral](volume%20integral.md) <br/> - Advanced topics  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Differential forms](differential%20form.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Exterior derivative](exterior%20derivative.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Generalized Stokes' theorem](generalized%20Stokes'%20theorem.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Tensor calculus](tensor%20calculus.md)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
|                                  __Sequences and series__ | - [Arithmetico-geometric sequence](arithmetico-geometric%20sequence.md) <br/> - Types of series  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Alternating](alternating%20series.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Binomial](binomial%20series.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Fourier](Fourier%20series.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Geometric](geometric%20series.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Harmonic](harmonic%20series%20(mathematics).md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Infinite](infinite%20series.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Power](power%20series.md)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [Maclaurin](Maclaurin%20series.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [Taylor](Taylor%20series.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Telescoping](telescoping%20series.md) <br/> - Tests of convergence  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Abel's](Abel's%20test.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Alternating series](alternating%20series%20test.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Cauchy condensation](Cauchy%20condensation%20test.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Direct comparison](direct%20comparison%20test.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Dirichlet's](Dirichlet's%20test.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Integral](integral%20test%20for%20convergence.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Limit comparison](limit%20comparison%20test.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Ratio](ratio%20test.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Root](root%20test.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Term](term%20test.md)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
|                   __Special functions <br/> and numbers__ | - [Bernoulli numbers](Bernoulli%20number.md) <br/> - [e \(mathematical constant\)](e%20(mathematical%20constant).md) <br/> - [Exponential function](exponential%20function.md) <br/> - [Natural logarithm](natural%20logarithm.md) <br/> - [Stirling's approximation](Stirling's%20approximation.md)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
|     __[History of calculus](history%20of%20calculus.md)__ | - [Adequality](adequality.md) <br/> - [Brook Taylor](Brook%20Taylor.md) <br/> - [Colin Maclaurin](Colin%20Maclaurin.md) <br/> - [Generality of algebra](generality%20of%20algebra.md) <br/> - [Gottfried Wilhelm Leibniz](Gottfried%20Wilhelm%20Leibniz.md) <br/> - [Infinitesimal](infinitesimal.md) <br/> - [Infinitesimal calculus](infinitesimal%20calculus.md) <br/> - [Isaac Newton](Isaac%20Newton.md) <br/> - [Fluxion](fluxion.md) <br/> - [Law of Continuity](law%20of%20continuity.md) <br/> - [Leonhard Euler](Leonhard%20Euler.md) <br/> - _[Method of Fluxions](Method%20of%20Fluxions.md)_ <br/> - _[The Method of Mechanical Theorems](The%20Method%20of%20Mechanical%20Theorems.md)_                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
|                                                 __Lists__ | __[Integrals](lists%20of%20integrals.md)__ <p>  - [rational functions](list%20of%20integrals%20of%20rational%20functions.md) <br/> - [irrational algebraic functions](list%20of%20integrals%20of%20irrational%20algebraic%20functions.md) <br/> - [exponential functions](list%20of%20integrals%20of%20exponential%20functions.md) <br/> - [logarithmic functions](list%20of%20integrals%20of%20logarithmic%20functions.md) <br/> - [hyperbolic functions](list%20of%20integrals%20of%20hyperbolic%20functions.md)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [inverse](list%20of%20integrals%20of%20inverse%20hyperbolic%20functions.md) <br/> - [trigonometric functions](list%20of%20integrals%20of%20trigonometric%20functions.md)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [inverse](list%20of%20integrals%20of%20inverse%20trigonometric%20functions.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Secant](integral%20of%20the%20secant%20function.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Secant cubed](integral%20of%20secant%20cubed.md)  <p>  - [List of limits](list%20of%20limits.md) <br/> - [List of derivatives](differentiation%20rules.md)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
|                                  __Miscellaneous topics__ | - Complex calculus  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Contour integral](contour%20integral.md) <br/> - Differential geometry  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Manifold](manifold.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Curvature](curvature.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [of curves](differential%20geometry%20of%20curves.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [of surfaces](differential%20geometry%20of%20surfaces.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Tensor](tensor.md) <br/> - [Euler–Maclaurin formula](Euler–Maclaurin%20formula.md) <br/> - [Gabriel's horn](Gabriel's%20horn.md) <br/> - [Integration Bee](Integration%20Bee.md) <br/> - [Proof that 22/7 exceeds π](proof%20that%2022_7%20exceeds%20π.md) <br/> - [Regiomontanus' angle maximization problem](regiomontanus'%20angle%20maximization%20problem.md) <br/> - [Steinmetz solid](Steinmetz%20solid.md)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |

| <!-- hide <p> - [v](https://en.wikipedia.org/wiki/Template:Joseph-Louis%20Lagrange) <br/> - [t](https://en.wikipedia.org/wiki/Template%20talk:Joseph-Louis%20Lagrange) <br/> - [e](https://en.wikipedia.org/wiki/Special:EditPage/Template%3AJoseph-Louis%20Lagrange) <p>  <p>  <br/> --> [Joseph-Louis Lagrange](Joseph-Louis%20Lagrange.md)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|
| - [Lagrange multiplier](Lagrange%20multiplier.md) <br/> - [Lagrange polynomial](Lagrange%20polynomial.md) <br/> - [Lagrange's four-square theorem](Lagrange's%20four-square%20theorem.md) <br/> - [Lagrange's theorem \(group theory\)](Lagrange's%20theorem%20(group%20theory).md) <br/> - [Lagrange's identity](Lagrange's%20identity.md) <br/> - [Lagrange's identity \(boundary value problem\)](Lagrange's%20identity%20(boundary%20value%20problem).md) <br/> - [Lagrange's trigonometric identities](Lagrange's%20trigonometric%20identities.md#Lagrange's%20trigonometric%20identities) <br/> - [Lagrange multiplier](Lagrange%20multiplier.md) <br/> - [Lagrangian mechanics](Lagrangian%20mechanics.md) <br/> - [Lagrange's mean value theorem](mean%20value%20theorem.md) <br/> - [Lagrange stability](Lagrange%20stability.md) <br/> - [Lagrange point](Lagrange%20point.md) |

> [Categories](https://en.wikipedia.org/wiki/Help:Category):
>
> - [Multivariable calculus](https://en.wikipedia.org/wiki/Category:Multivariable%20calculus)
> - [Mathematical optimization](https://en.wikipedia.org/wiki/Category:Mathematical%20optimization)
> - [Mathematical and quantitative methods \(economics\)](https://en.wikipedia.org/wiki/Category:Mathematical%20and%20quantitative%20methods%20%28economics%29)
