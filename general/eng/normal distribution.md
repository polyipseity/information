---
aliases:
  - Gaussian distribution
  - Gaussian distributions
  - bell curve
  - bell curves
  - normal distribution
  - normal distributions
tags:
  - flashcard/active/general/eng/normal_distribution
  - language/in/English
---

# normal distribution

- {@{"Bell curve"}@} redirects here. For other uses, see [Bell curve \(disambiguation\)](bell%20curve%20(disambiguation).md). <!--SR:!2026-03-16,18,323-->

<!-- | ![icon](../../archives/Wikimedia%20Commons/Question%20book-new.svg) | This article __needs additional citations for [verification](https://en.wikipedia.org/wiki/Wikipedia:Verifiability)__. Please help [improve this article](https://en.wikipedia.org/wiki/Special:EditPage/Normal%20distribution) by [adding citations to reliable sources](https://en.wikipedia.org/wiki/Help:Referencing%20for%20beginners). Unsourced material may be challenged and removed. <br/> _Find sources:_ ["Normal distribution"](https://www.google.com/search?as_eq=wikipedia&q=%22Normal+distribution%22) – [news](https://www.google.com/search?tbm=nws&q=%22Normal+distribution%22+-wikipedia&tbs=ar:1) __·__ [newspapers](https://www.google.com/search?&q=%22Normal+distribution%22&tbs=bkt:s&tbm=bks) __·__ [books](https://www.google.com/search?tbs=bks:1&q=%22Normal+distribution%22+-wikipedia) __·__ [scholar](https://scholar.google.com/scholar?q=%22Normal+distribution%22) __·__ [JSTOR](https://www.jstor.org/action/doBasicSearch?Query=%22Normal+distribution%22&acc=on&wc=on) _\(December 2024\)__\([Learn how and when to remove this message](https://en.wikipedia.org/wiki/Help:Maintenance%20template%20removal)\)_ | -->

|                                                                     | __Normal distribution__                                                                                                                                                                                                                                                                                                                                                                                          |
| ------------------------------------------------------------------: | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
|                                                                     | __Probability density function__ <br/> ![A selection of Normal Distribution Probability Density Functions \(PDFs\).](../../archives/Wikimedia%20Commons/Normal%20Distribution%20PDF.svg) <br/> The red curve is the [_standard normal distribution_](#standard%20normal%20distribution).                                                                                                                         |
|                                                                     | __Cumulative distribution function__ <br/> ![A selection of Normal Distribution Cumulative Density Functions \(CDFs\).](../../archives/Wikimedia%20Commons/Normal%20Distribution%20CDF.svg)                                                                                                                                                                                                                      |
|                                                        __Notation__ | ${\mathcal {N} }(\mu ,\sigma ^{2})$                                                                                                                                                                                                                                                                                                                                                                              |
|                        __[Parameters](statistical%20parameter.md)__ | $\mu \in \mathbb {R}$ = [mean](mean.md) \([location](location%20parameter.md)\) <br/> $\sigma ^{2}\in \mathbb {R} _{>0}$ = [variance](variance.md) \(squared [scale](scale%20parameter.md)\)                                                                                                                                                                                                                     |
|                           __[Support](support%20(mathematics).md)__ | $x\in \mathbb {R}$                                                                                                                                                                                                                                                                                                                                                                                               |
|                      __[PDF](probability%20density%20function.md)__ | ${\frac {1}{\sqrt {2\pi \sigma ^{2} } } }e^{-{\frac {(x-\mu )^{2} }{2\sigma ^{2} } } }$                                                                                                                                                                                                                                                                                                                          |
|                  __[CDF](cumulative%20distribution%20function.md)__ | $\Phi \left({\frac {x-\mu }{\sigma } }\right)={\frac {1}{2} }\left[1+\operatorname {erf} \left({\frac {x-\mu }{\sigma {\sqrt {2} } } }\right)\right]$                                                                                                                                                                                                                                                            |
|                              __[Quantile](quantile%20function.md)__ | $\mu +\sigma {\sqrt {2} }\operatorname {erf} ^{-1}(2p-1)$                                                                                                                                                                                                                                                                                                                                                        |
|                                     __[Mean](expected%20value.md)__ | $\mu$                                                                                                                                                                                                                                                                                                                                                                                                            |
|                                             __[Median](median.md)__ | $\mu$                                                                                                                                                                                                                                                                                                                                                                                                            |
|                                  __[Mode](mode%20(statistics).md)__ | $\mu$                                                                                                                                                                                                                                                                                                                                                                                                            |
|                                         __[Variance](variance.md)__ | $\sigma ^{2}$                                                                                                                                                                                                                                                                                                                                                                                                    |
|                         __[MAD](median%20absolute%20deviation.md)__ | $\sigma {\sqrt {2} }\,\operatorname {erf} ^{-1}(1/2)$                                                                                                                                                                                                                                                                                                                                                            |
|                           __[AAD](mean%20absolute%20deviation.md)__ | $\sigma {\sqrt {2/\pi } }$                                                                                                                                                                                                                                                                                                                                                                                       |
|                                         __[Skewness](skewness.md)__ | $0$                                                                                                                                                                                                                                                                                                                                                                                                              |
|       __[Excess kurtosis](excess%20kurtosis.md#excess%20kurtosis)__ | $0$                                                                                                                                                                                                                                                                                                                                                                                                              |
|                             __[Entropy](information%20entropy.md)__ | ${\tfrac {1}{2} }\log(2\pi e\sigma ^{2})$                                                                                                                                                                                                                                                                                                                                                                        |
|                          __[MGF](moment-generating%20function.md)__ | $\exp(\mu t+\sigma ^{2}t^{2}/2)$                                                                                                                                                                                                                                                                                                                                                                                 |
|     __[CF](characteristic%20function%20(probability%20theory).md)__ | $\exp(i\mu t-\sigma ^{2}t^{2}/2)$                                                                                                                                                                                                                                                                                                                                                                                |
|                   __[Fisher information](Fisher%20information.md)__ | ${\mathcal {I} }(\mu ,\sigma )={\begin{pmatrix}1/\sigma ^{2}&0\\0&2/\sigma ^{2}\end{pmatrix} }$ <br/> ${\mathcal {I} }(\mu ,\sigma ^{2})={\begin{pmatrix}1/\sigma ^{2}&0\\0&1/(2\sigma ^{4})\end{pmatrix} }$                                                                                                                                                                                                     |
| __[Kullback–Leibler divergence](Kullback–Leibler%20divergence.md)__ | ${1 \over 2}\left\{\left({\frac {\sigma _{0} }{\sigma _{1} } }\right)^{2}+{\frac {(\mu _{1}-\mu _{0})^{2} }{\sigma _{1}^{2} } }-1+\ln {\sigma _{1}^{2} \over \sigma _{0}^{2} }\right\}$ \(annotation: form with the approximating distribution always dividing: $\frac 1 2 \left\{\frac {\sigma_0^2} {\sigma_1^2} + \frac {(\mu_1 - \mu_0)^2} {\sigma_1^2} - 1 - \ln \frac {\sigma_0^2} {\sigma_1^2} \right\}$\) |
|                   __[Expected shortfall](expected%20shortfall.md)__ | $\mu +\sigma {\frac { {\frac {1}{\sqrt {2\pi } } }e^{\frac {-\left(q_{p}\left({\frac {X-\mu }{\sigma } }\right)\right)^{2} }{2} } }{1-p} }$<sup>[\[1\]](#^ref-1)</sup>                                                                                                                                                                                                                                           |

> __flashcards__
>
> - __Probability density function__ ::@:: ![A selection of Normal Distribution Probability Density Functions \(PDFs\).](../../archives/Wikimedia%20Commons/Normal%20Distribution%20PDF.svg) <br/> The red curve is the [_standard normal distribution_](#standard%20normal%20distribution). <!--SR:!2026-03-12,14,290!2026-03-15,17,323-->
> - __Cumulative distribution function__ ::@:: ![ A selection of Normal Distribution Cumulative Density Functions \(CDFs\).](../../archives/Wikimedia%20Commons/Normal%20Distribution%20CDF.svg) <!--SR:!2026-03-15,17,323!2026-03-15,17,323-->
> - __Notation__ ::@:: ${\mathcal {N} }(\mu ,\sigma ^{2})$ <!--SR:!2026-03-15,17,323!2026-03-12,14,290-->
> - __[Parameters](statistical%20parameter.md)__ ::@:: $\mu \in \mathbb {R}$ = [mean](mean.md) \([location](location%20parameter.md)\) <br/> $\sigma ^{2}\in \mathbb {R} _{>0}$ = [variance](variance.md) \(squared [scale](scale%20parameter.md)\) <!--SR:!2026-03-14,16,323!2026-03-16,18,323-->
> - __[Support](support%20(mathematics).md)__ ::@:: $x\in \mathbb {R}$ <!--SR:!2026-03-16,18,323!2026-03-13,15,309-->
> - __[PDF](probability%20density%20function.md)__ ::@:: ${\frac {1}{\sqrt {2\pi \sigma ^{2} } } }e^{-{\frac {(x-\mu )^{2} }{2\sigma ^{2} } } }$ <!--SR:!2026-03-12,14,290!2026-03-15,17,323-->
> - __[CDF](cumulative%20distribution%20function.md)__ ::@:: $\Phi \left({\frac {x-\mu }{\sigma } }\right)={\frac {1}{2} }\left[1+\operatorname {erf} \left({\frac {x-\mu }{\sigma {\sqrt {2} } } }\right)\right]$ <!--SR:!2026-03-14,16,332!2026-04-20,42,290-->
> - __[Quantile](quantile%20function.md)__ ::@:: $\mu +\sigma {\sqrt {2} }\operatorname {erf} ^{-1}(2p-1)$ <!--SR:!2026-03-14,16,332!2026-03-15,17,323-->
> - __[Mean](expected%20value.md)__ ::@:: $\mu$ <!--SR:!2026-03-14,16,316!2026-03-16,18,323-->
> - __[Median](median.md)__ ::@:: $\mu$ <!--SR:!2026-03-12,14,290!2026-03-14,16,316-->
> - __[Mode](mode%20(statistics).md)__ ::@:: $\mu$ <!--SR:!2026-03-12,14,290!2026-03-12,14,290-->
> - __[Variance](variance.md)__ ::@:: $\sigma ^{2}$ <!--SR:!2026-03-16,18,323!2026-03-13,15,309-->
> - __[MAD](median%20absolute%20deviation.md)__ ::@:: $\sigma {\sqrt {2} }\,\operatorname {erf} ^{-1}(1/2)$ <!--SR:!2026-04-14,35,303!2026-03-23,14,263-->
> - __[AAD](mean%20absolute%20deviation.md)__ ::@:: $\sigma {\sqrt {2/\pi } }$ <!--SR:!2026-03-14,16,316!2026-03-18,8,230-->
> - __[Skewness](skewness.md)__ ::@:: $0$ <!--SR:!2026-03-14,16,309!2026-03-16,18,323-->
> - __[Excess kurtosis](excess%20kurtosis.md#excess%20kurtosis)__ ::@:: $0$ <!--SR:!2026-03-12,14,290!2026-03-14,16,309-->
> - __[Entropy](information%20entropy.md)__ ::@:: ${\tfrac {1}{2} }\log(2\pi e\sigma ^{2})$ <!--SR:!2026-04-26,47,290!2026-03-15,17,323-->
> - __[MGF](moment-generating%20function.md)__ ::@:: $\exp(\mu t+\sigma ^{2}t^{2}/2)$ <!--SR:!2026-03-14,16,332!2026-03-12,14,290-->
> - __[CF](characteristic%20function%20(probability%20theory).md)__ ::@:: $\exp(i\mu t-\sigma ^{2}t^{2}/2)$ <!--SR:!2026-03-14,16,314!2026-03-12,14,290-->
> - __[Fisher information](Fisher%20information.md)__ ::@:: ${\mathcal {I} }(\mu ,\sigma )={\begin{pmatrix}1/\sigma ^{2}&0\\0&2/\sigma ^{2}\end{pmatrix} }$ <br/> ${\mathcal {I} }(\mu ,\sigma ^{2})={\begin{pmatrix}1/\sigma ^{2}&0\\0&1/(2\sigma ^{4})\end{pmatrix} }$ <!--SR:!2026-04-21,43,290!2026-04-19,39,303-->
> - __[Kullback–Leibler divergence](Kullback–Leibler%20divergence.md)__ ::@:: ${1 \over 2}\left\{\left({\frac {\sigma _{0} }{\sigma _{1} } }\right)^{2}+{\frac {(\mu _{1}-\mu _{0})^{2} }{\sigma _{1}^{2} } }-1+\ln {\sigma _{1}^{2} \over \sigma _{0}^{2} }\right\}$  \(annotation: form with the approximating distribution always dividing: $\frac 1 2 \left\{\frac {\sigma_0^2} {\sigma_1^2} + \frac {(\mu_1 - \mu_0)^2} {\sigma_1^2} - 1 - \ln \frac {\sigma_0^2} {\sigma_1^2} \right\}$\) <!--SR:!2026-04-05,28,291!2026-05-07,57,323-->
> - __[Expected shortfall](expected%20shortfall.md)__ ::@:: $\mu +\sigma {\frac { {\frac {1}{\sqrt {2\pi } } }e^{\frac {-\left(q_{p}\left({\frac {X-\mu }{\sigma } }\right)\right)^{2} }{2} } }{1-p} }$<sup>[\[1\]](#^ref-1)</sup> <!--SR:!2026-03-16,18,332!2026-04-16,37,296-->

| Part of a series on [statistics](statistics.md) <br/> <big>[Probability theory](probability%20theory.md)</big>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ![Standard deviation](../../archives/Wikimedia%20Commons/Standard%20deviation%20diagram%20micro.svg)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| - [Probability](probability.md)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Axioms](probability%20axioms.md) <br/> - [Determinism](determinism.md)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [System](deterministic%20system.md) <br/> - [Indeterminism](indeterminism.md) <br/> - [Randomness](randomness.md)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| - [Probability space](probability%20space.md) <br/> - [Sample space](sample%20space.md) <br/> - [Event](event%20(probability%20theory).md)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Collectively exhaustive events](collectively%20exhaustive%20events.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Elementary event](elementary%20event.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Mutual exclusivity](mutual%20exclusivity.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Outcome](outcome%20(probability).md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Singleton](singleton%20(mathematics).md) <br/> - [Experiment](experiment%20(probability%20theory).md)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Bernoulli trial](Bernoulli%20trial.md) <br/> - [Probability distribution](probability%20distribution.md)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Bernoulli distribution](Bernoulli%20distribution.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Binomial distribution](binomial%20distribution.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Exponential distribution](exponential%20distribution.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Normal distribution](normal%20distribution.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Pareto distribution](Pareto%20distribution.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Poisson distribution](Poisson%20distribution.md) <br/> - [Probability measure](probability%20measure.md) <br/> - [Random variable](random%20variable.md)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Bernoulli process](Bernoulli%20process.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Continuous or discrete](continuous%20or%20discrete%20variable.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Expected value](expected%20value.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Variance](variance.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Markov chain](Markov%20chain.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Observed value](realization%20(probability).md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Random walk](random%20walk.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Stochastic process](stochastic%20process.md) |
| - [Complementary event](complementary%20event.md) <br/> - [Joint probability](joint%20probability%20distribution.md) <br/> - [Marginal probability](marginal%20distribution.md) <br/> - [Conditional probability](conditional%20probability.md)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| - [Independence](independence%20(probability%20theory).md) <br/> - [Conditional independence](conditional%20independence.md) <br/> - [Law of total probability](law%20of%20total%20probability.md) <br/> - [Law of large numbers](law%20of%20large%20numbers.md) <br/> - [Bayes' theorem](Bayes'%20theorem.md) <br/> - [Boole's inequality](Boole's%20inequality.md)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| - [Venn diagram](Venn%20diagram.md) <br/> - [Tree diagram](tree%20diagram%20(probability%20theory).md)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
<!-- -->
<!-- | - [v](https://en.wikipedia.org/wiki/Template:Probability%20fundamentals) <br/> - [t](https://en.wikipedia.org/wiki/Template%20talk:Probability%20fundamentals) <br/> - [e](https://en.wikipedia.org/wiki/Special:EditPage/Template%3AProbability%20fundamentals) | -->

In {@{[probability theory](probability%20theory.md) and [statistics](statistics.md)}@}, {@{a __normal distribution__ or __Gaussian distribution__}@} is {@{a type of [continuous probability distribution](continuous%20probability%20distribution.md#absolutely%20continuous%20probability%20distribution) for a [real-valued](real%20number.md) [random variable](random%20variable.md)}@}. {@{The general form of its [probability density function](probability%20density%20function.md)}@} is<sup>[\[2\]](#^ref-2)</sup><sup>[\[3\]](#^ref-3)</sup><sup>[\[4\]](#^ref-4)</sup> {@{$$f(x)={\frac {1}{\sqrt {2\pi \sigma ^{2} } } }e^{-{\frac {(x-\mu )^{2} }{2\sigma ^{2} } } }\,.$$}@} {@{The parameter ⁠$\mu$}@}⁠ is {@{the [mean](mean.md#mean%20of%20a%20probability%20distribution) or [expectation](expected%20value.md)}@} of the distribution \(and also its {@{[median](median.md) and [mode](mode%20(statistics).md)}@}\), while {@{the parameter $\sigma ^{2}$}@} is {@{the [variance](variance.md)}@}. {@{The [standard deviation](standard%20deviation.md)}@} of the distribution is {@{⁠$\sigma$⁠ \(sigma\)}@}. {@{A random variable with a Gaussian distribution}@} is said to be {@{__normally distributed__ and is called a __normal deviate__}@}. <!--SR:!2026-03-13,15,316!2026-03-16,18,323!2026-03-14,16,323!2026-03-16,18,323!2026-03-13,15,316!2026-03-16,18,332!2026-03-13,15,309!2026-03-14,16,316!2026-03-16,18,323!2026-03-14,16,323!2026-03-12,14,290!2026-03-16,18,323!2026-03-16,18,332!2026-03-13,15,314-->

Normal distributions are {@{important in [statistics](statistics.md)}@} and are often used in {@{the [natural](natural%20science.md) and [social sciences](social%20science.md)}@} to represent {@{real-valued [random variables](random%20variable.md) whose distributions are not known}@}.<sup>[\[5\]](#^ref-5)</sup><sup>[\[6\]](#^ref-6)</sup> Their importance is partly due to {@{the [central limit theorem](central%20limit%20theorem.md)}@}. It states that, under {@{some conditions}@}, {@{the average of many samples \(observations\)}@} of {@{a random variable with finite mean and variance}@} is itself {@{a random variable—whose distribution [converges](convergence%20in%20distribution.md#convergence%20in%20distribution) to a normal distribution}@} as {@{the number of samples increases}@}. Therefore, {@{physical quantities}@} that are expected to be {@{the sum of many independent processes, such as [measurement errors](measurement%20error.md)}@}, often have {@{distributions that are nearly normal}@}.<sup>[\[7\]](#^ref-7)</sup> <!--SR:!2026-03-14,16,332!2026-03-14,16,311!2026-03-16,18,332!2026-03-16,18,323!2026-03-14,16,332!2026-03-16,18,323!2026-03-16,18,323!2026-03-12,14,290!2026-03-15,17,323!2026-03-12,14,290!2026-03-15,17,323!2026-03-14,16,323-->

Moreover, Gaussian distributions have some {@{unique properties that are valuable in analytic studies}@}. For instance, {@{any [linear combination](linear%20combination.md) of a fixed collection of independent normal deviates}@} is {@{a normal deviate}@}. Many {@{results and methods}@}, such as {@{[propagation of uncertainty](propagation%20of%20uncertainty.md) and [least squares](least%20squares.md)<sup>[\[8\]](#^ref-8)</sup> parameter fitting}@}, can be derived {@{analytically in explicit form when the relevant variables are normally distributed}@}. <!--SR:!2026-03-16,18,323!2026-03-13,15,316!2026-03-12,14,290!2026-03-12,14,290!2026-03-16,18,332!2026-03-14,16,323-->

{@{A normal distribution}@} is sometimes informally called {@{a __bell curve__}@}.<sup>[\[9\]](#^ref-9)</sup><sup>[\[10\]](#^ref-10)</sup> However, many {@{other distributions are [bell-shaped](bell-shaped%20function.md)}@} \(such as {@{the [Cauchy](Cauchy%20distribution.md), [Student's _t_](Student's%20t-distribution.md), and [logistic](logistic%20distribution.md) distributions}@}\). \(For other names, see _[Naming](#naming)_.\) <!--SR:!2026-03-14,16,323!2026-03-15,17,323!2026-03-15,17,323!2026-03-15,17,309-->

{@{The [univariate probability distribution](univariate%20distribution.md)}@} is generalized for {@{[vectors](vector%20(mathematics%20and%20physics).md) in the [multivariate normal distribution](multivariate%20normal%20distribution.md)}@} and for {@{matrices in the [matrix normal distribution](matrix%20normal%20distribution.md)}@}. <!--SR:!2026-03-15,17,314!2026-03-13,15,316!2026-03-12,14,290-->

## definitions

### standard normal distribution

{@{The simplest case}@} of a normal distribution is known as {@{the __standard normal distribution__ or __unit normal distribution__}@}. This is {@{a special case when $\mu =0$ and $\sigma ^{2}=1$}@}, and it is described by {@{this [probability density function](probability%20density%20function.md) \(or density\)}@}:<sup>[\[11\]](#^ref-11)</sup> {@{$$\varphi (z)={\frac {e^{-z^{2}/2} }{\sqrt {2\pi } } }\,.$$}@} {@{The variable ⁠$z$}@}⁠ has {@{a mean of 0 and a variance and standard deviation of 1}@}. {@{The density $\varphi (z)$}@} has its {@{peak ${\frac {1}{\sqrt {2\pi } } }$ at $z=0$}@} and {@{[inflection points](inflection%20point.md) at $z=+1$ and ⁠$z=-1$⁠}@}. <!--SR:!2026-03-14,16,290!2026-03-15,17,323!2026-03-13,15,316!2026-03-13,15,316!2026-03-12,14,290!2026-03-16,18,323!2026-03-12,14,290!2026-03-14,16,311!2026-03-16,18,323!2026-03-16,18,323-->

Although the density above is {@{most commonly known as the _standard normal_}@}, a few authors have used {@{that term to describe other versions of the normal distribution}@}. {@{[Carl Friedrich Gauss](Carl%20Friedrich%20Gauss.md)}@}, for example, once defined the standard normal as {@{$$\varphi (z)={\frac {e^{-z^{2} } }{\sqrt {\pi } } },$$ which has a variance of ⁠${\frac {1}{2} }$}@}⁠, and {@{[Stephen Stigler](Stephen%20Stigler.md)}@}<sup>[\[12\]](#^ref-12)</sup> once defined the standard normal as {@{$$\varphi (z)=e^{-\pi z^{2} },$$}@} which has {@{a simple functional form and a variance of $\sigma ^{2}={\frac {1}{2\pi } }$}@}. <!--SR:!2026-03-16,18,323!2026-03-12,14,290!2026-03-13,15,311!2026-03-12,14,290!2026-03-16,18,323!2026-03-13,15,316!2026-03-14,16,323-->

### general normal distribution

{@{Every normal distribution}@} is {@{a version of the standard normal distribution}@} whose domain has been {@{stretched by a factor ⁠$\sigma$⁠ \(the standard deviation\) and then translated by ⁠$\mu$⁠ \(the mean value\)}@}: {@{$$f(x\mid \mu ,\sigma ^{2})={\frac {1}{\sigma } }\varphi \left({\frac {x-\mu }{\sigma } }\right)\,.$$}@} The probability density must be {@{scaled by $1/\sigma$ so that the [integral](integral.md) is still 1}@}. <!--SR:!2026-03-12,14,290!2026-03-14,16,323!2026-03-12,14,290!2026-03-13,15,314!2026-03-14,16,290-->

If ⁠$Z$⁠ is {@{a [standard normal deviate](standard%20normal%20deviate.md)}@}, then {@{$X=\sigma Z+\mu$}@} will have {@{a normal distribution with expected value ⁠$\mu$⁠ and standard deviation ⁠$\sigma$}@}⁠. This is equivalent to saying that {@{the standard normal distribution ⁠$Z$}@}⁠ can be {@{scaled/stretched by a factor of ⁠$\sigma$⁠ and shifted by ⁠$\mu$}@}⁠ to yield {@{a different normal distribution, called ⁠$X$}@}⁠. Conversely, if ⁠$X$⁠ is {@{a normal deviate with parameters ⁠$\mu$⁠ and $\sigma ^{2}$}@}, then this ⁠$X$⁠ distribution can be {@{re-scaled and shifted via the formula $Z=(X-\mu )/\sigma$}@} to convert it to {@{the standard normal distribution}@}. This variate is also called {@{the standardized form of ⁠$X$⁠}@}. <!--SR:!2026-03-15,17,323!2026-03-16,18,323!2026-03-14,16,316!2026-03-13,15,314!2026-03-16,18,323!2026-03-12,14,290!2026-03-15,17,332!2026-03-12,14,290!2026-03-15,17,323!2026-03-15,17,323-->

### notation

{@{The probability density of the standard Gaussian distribution}@} \(standard normal distribution, with {@{zero mean and unit variance}@}\) is often denoted with {@{the Greek letter ⁠$\phi$⁠ \([phi](phi.md)\)}@}.<sup>[\[13\]](#^ref-13)</sup> {@{The alternative form of the Greek letter phi, ⁠$\varphi$⁠}@}, is also {@{used quite often}@}. <!--SR:!2026-03-16,18,332!2026-03-15,17,323!2026-03-13,15,316!2026-03-15,17,316!2026-03-15,17,314-->

{@{The normal distribution}@} is often referred to as {@{$N(\mu ,\sigma ^{2})$ or ⁠${\mathcal {N} }(\mu ,\sigma ^{2})$}@}⁠.<sup>[\[14\]](#^ref-14)</sup> Thus when {@{a random variable ⁠$X$}@}⁠ is {@{normally distributed with mean ⁠$\mu$⁠ and standard deviation ⁠$\sigma$}@}⁠, one may write {@{$$X\sim {\mathcal {N} }(\mu ,\sigma ^{2}).$$}@} <!--SR:!2026-03-13,15,316!2026-03-13,15,309!2026-03-14,16,316!2026-03-14,16,323!2026-03-13,15,316-->

### alternative parameterizations

Some authors advocate using {@{the [precision](precision%20(statistics).md) ⁠$\tau$⁠}@} as {@{the parameter defining the width of the distribution}@}, instead of {@{the standard deviation ⁠$\sigma$⁠ or the variance ⁠$\sigma ^{2}$}@}⁠. {@{The precision}@} is normally defined as {@{the reciprocal of the variance, ⁠$1/\sigma ^{2}$}@}⁠.<sup>[\[15\]](#^ref-15)</sup> {@{The formula for the distribution}@} then becomes {@{$$f(x)={\sqrt {\frac {\tau }{2\pi } } }e^{-\tau (x-\mu )^{2}/2}.$$}@} This choice is claimed to have {@{advantages in numerical computations when ⁠$\sigma$⁠ is very close to zero}@}, and simplifies {@{formulas in some contexts}@}, such as in {@{the [Bayesian inference](Bayesian%20statistics.md) of variables with [multivariate normal distribution](multivariate%20normal%20distribution.md)}@}. <!--SR:!2026-03-15,17,316!2026-03-16,18,323!2026-03-12,14,290!2026-03-12,14,290!2026-03-14,16,316!2026-03-13,15,290!2026-03-12,14,290!2026-03-16,18,323!2026-03-15,17,323!2026-03-12,14,290-->

Alternatively, {@{the reciprocal of the standard deviation $\tau '=1/\sigma$}@} might be defined as {@{the _precision_}@}, in which case {@{the expression of the normal distribution}@} becomes {@{$$f(x)={\frac {\tau '}{\sqrt {2\pi } } }e^{-(\tau ')^{2}(x-\mu )^{2}/2}.$$}@} According to {@{Stigler, this formulation is advantageous}@} because of {@{a much simpler and easier-to-remember formula}@}, and simple {@{approximate formulas for the [quantiles](quantile.md) of the distribution}@}. <!--SR:!2026-03-16,18,332!2026-03-16,18,332!2026-03-16,18,332!2026-03-13,15,316!2026-03-16,18,323!2026-03-13,15,311!2026-03-12,14,290-->

{@{Normal distributions}@} form {@{an [exponential family](exponential%20family.md)}@} with {@{[natural parameters](natural%20parameter.md) $\textstyle \theta _{1}={\frac {\mu }{\sigma ^{2} } }$ and $\textstyle \theta _{2}={\frac {-1}{2\sigma ^{2} } }$}@}, and {@{natural statistics _x_ and _x_<sup>2</sup>}@}. (annotation: In {@{the canonical exponential family form}@} {@{$$p(x\mid\mu,\sigma^2)=\frac{1}{\sqrt{2\pi} }\exp\Big\{\underbrace{\frac{\mu}{\sigma^2} }_{\theta_1}x+\underbrace{\Big(-\frac{1}{2\sigma^2}\Big)}_{\theta_2}x^2-\underbrace{\Big(\frac{\mu^2}{2\sigma^2}+\ln\lvert\sigma\rvert)\Big)}_{A(\theta)}\Big\}$$}@} the natural parameters {@{$\theta_1=\mu/\sigma^2$ and $\theta_2=-1/(2\sigma^2)$}@} are simply {@{the coefficients that multiply the natural statistics $T(x)=(x,x^2)$}@} inside the exponent, turning {@{the normal density}@} into {@{a linear combination of data features}@} in {@{the exponential family structure}@}.) {@{The dual expectation parameters}@} for normal distribution are {@{_η_<sub>1</sub> = _μ_ and _η_<sub>2</sub> = _μ_<sup>2</sup> + _σ_<sup>2</sup>}@}. (annotation: The dual expectation parameters {@{$\eta_1=\mathbb{E}\,[X]=\mu$ and $\eta_2=\mathbb{E}\,[X^2]=\mu^2+\sigma^2$}@} are {@{the expected values of those natural statistics}@} under the distribution, connecting {@{the canonical representation back to familiar moments (mean and second moment)}@}.) <!--SR:!2026-03-12,14,290!2026-03-16,18,323!2026-03-13,15,290!2026-03-13,15,316!2026-03-14,16,309!2026-03-16,18,323!2026-03-16,18,323!2026-03-15,17,332!2026-03-15,17,323!2026-03-14,16,323!2026-03-13,15,314!2026-03-16,18,323!2026-03-12,14,290!2026-03-15,17,323!2026-03-16,18,323!2026-03-15,17,323-->

### cumulative distribution function

{@{The [cumulative distribution function](cumulative%20distribution%20function.md) \(CDF\)}@} of the standard normal distribution, usually denoted with {@{the capital Greek letter ⁠$\Phi$}@}⁠, is {@{the integral $$\Phi (x)={\frac {1}{\sqrt {2\pi } } }\int _{-\infty }^{x}e^{-t^{2}/2}\,dt\,.$$}@} <!--SR:!2026-03-15,17,323!2026-03-12,14,290!2026-03-14,16,323-->

### error function

{@{The related [error function](error%20function.md) $\operatorname {erf} (x)$}@} gives {@{the probability of a random variable, with normal distribution of mean 0 and variance 1/2}@} falling {@{in the range ⁠$[-x,x]$}@}⁠. That is: {@{$$\operatorname {erf} (x)={\frac {1}{\sqrt {\pi } } }\int _{-x}^{x}e^{-t^{2} }\,dt={\frac {2}{\sqrt {\pi } } }\int _{0}^{x}e^{-t^{2} }\,dt\,.$$}@} These integrals cannot be expressed {@{in terms of elementary functions}@}, and are often said to be {@{[special functions](special%20function.md)}@}. However, many {@{numerical approximations}@} are known; see [below](#numerical%20approximations%20for%20the%20normal%20cumulative%20distribution%20function%20and%20normal%20quantile%20function) for more. <!--SR:!2026-03-12,14,290!2026-03-12,14,290!2026-03-14,16,309!2026-03-14,16,323!2026-03-15,17,323!2026-03-12,14,290!2026-03-13,15,314-->

{@{The two functions}@} are {@{closely related}@}, namely {@{$$\Phi (x)={\frac {1}{2} }\left[1+\operatorname {erf} \left({\frac {x}{\sqrt {2} } }\right)\right]\,.$$}@} (annotation: {@{Distributing $\tfrac{1}{2}$}@} gives {@{$\Phi(x) = \tfrac{1}{2} + \tfrac{1}{2}\,\operatorname{erf}\!\big(\tfrac{x}{\sqrt{2} }\big)$}@} and since $\operatorname{erf}(x)$ is {@{the probability that a $N(0,\tfrac{1}{2})$ variable falls in $[-x,x]$}@}, {@{the term $\tfrac{1}{2}\,\operatorname{erf}\!\big(\tfrac{x}{\sqrt{2} }\big)$}@} is exactly {@{the symmetric probability mass around zero}@} translated into {@{a one-sided cumulative above the baseline $\tfrac{1}{2}$}@}, with {@{the division by $\sqrt{2}$}@} aligning {@{the variance}@}.) <!--SR:!2026-03-14,16,332!2026-03-13,15,316!2026-03-12,14,290!2026-03-16,18,332!2026-03-13,15,309!2026-03-14,16,316!2026-03-13,15,316!2026-03-16,18,323!2026-03-12,14,290!2026-03-15,17,332!2026-03-15,17,323-->

For {@{a generic normal distribution}@} with {@{density ⁠$f$⁠, mean ⁠$\mu$⁠ and variance $\sigma ^{2}$}@}, {@{the cumulative distribution function}@} is {@{$$F(x)=\Phi {\left({\frac {x-\mu }{\sigma } }\right)}={\frac {1}{2} }\left[1+\operatorname {erf} \left({\frac {x-\mu }{\sigma {\sqrt {2} } } }\right)\right]\,.$$}@} <!--SR:!2026-03-14,16,309!2026-03-12,14,290!2026-03-14,16,323!2026-03-16,18,323-->

{@{The complement of the standard normal cumulative distribution function}@}, {@{$Q(x)=1-\Phi (x)$}@}, is often called {@{the [Q-function](Q-function.md), especially in engineering texts}@}.<sup>[\[16\]](#^ref-16)</sup><sup>[\[17\]](#^ref-17)</sup> It gives the probability that {@{the value of a standard normal random variable ⁠$X$⁠ will exceed ⁠$x$}@}⁠: {@{⁠$P(X>x)$}@}⁠. {@{Other definitions of the ⁠$Q$⁠-function}@}, all of which are {@{simple transformations of ⁠$\Phi$⁠}@}, are also {@{used occasionally}@}.<sup>[\[18\]](#^ref-18)</sup> <!--SR:!2026-03-16,18,323!2026-03-15,17,332!2026-03-14,16,323!2026-03-13,15,316!2026-03-12,14,290!2026-03-13,15,309!2026-03-15,17,323!2026-03-14,16,332-->

{@{The [graph](graph%20of%20a%20function.md)}@} of the standard normal cumulative distribution function ⁠$\Phi$⁠ has {@{2-fold [rotational symmetry](rotational%20symmetry.md) around the point \(0,1/2\)}@}; that is, {@{⁠$\Phi (-x)=1-\Phi (x)$}@}⁠. Its {@{[antiderivative](antiderivative.md) \(indefinite integral\)}@} can be expressed as follows: {@{$$\int \Phi (x)\,dx=x\Phi (x)+\varphi (x)+C.$$}@} (annotation: Noting that {@{$\varphi'(x) = -x \varphi(x)$}@}.) <!--SR:!2026-03-14,16,323!2026-03-13,15,316!2026-03-13,15,316!2026-03-14,16,332!2026-03-13,15,314!2026-03-14,16,309-->

{@{The cumulative distribution function}@} of the standard normal distribution can be expanded by {@{[integration by parts](integration%20by%20parts.md) into a series}@}: {@{$$\Phi (x)={\frac {1}{2} }+{\frac {1}{\sqrt {2\pi } } }\cdot e^{-x^{2}/2}\left[x+{\frac {x^{3} }{3} }+{\frac {x^{5} }{3\cdot 5} }+\cdots +{\frac {x^{2n+1} }{(2n+1)!!} }+\cdots \right]\,.$$}@} where $!!$ denotes {@{the [double factorial](double%20factorial.md)}@}. (annotation: {@{The zero step}@} uses {@{symmetry}@}: since {@{the normal distribution is centered at zero, $\Phi(0)=\tfrac{1}{2}$}@}, so {@{$$\Phi(x)=\tfrac{1}{2}+\int_0^x\varphi(t)\,dt = \tfrac 1 2 + \frac 1 {\sqrt{2\pi} } \int_0^x e^{-t^2/2} \,dt \,.$$}@} After {@{one integration by parts}@}, we obtain {@{$$\Phi(x)=\tfrac{1}{2}+\frac{e^{-x^2/2} }{\sqrt{2\pi} }[x]+\frac{1}{\sqrt{2\pi} }\int_0^x t^2 e^{-t^2/2}\,dt \,.$$}@} Applying {@{a second integration by parts}@} gives {@{$$\Phi(x)=\tfrac{1}{2}+\frac{e^{-x^2/2} }{\sqrt{2\pi} }\big[x+\tfrac{x^3}{3}\big]+\frac{1}{\sqrt{2\pi} }\int_0^x \tfrac{t^4}{3}e^{-t^2/2}\,dt \,.$$}@} The pattern is clear: each step adds {@{an odd-power term with coefficient $1/(2n+1)!!$}@}, while {@{the constant $\tfrac{1}{2}$}@} reflects {@{the symmetry of the normal distribution at zero}@}.) <!--SR:!2026-03-16,18,323!2026-03-15,17,316!2026-03-14,16,323!2026-03-15,17,323!2026-03-16,18,323!2026-03-15,17,323!2026-03-16,18,323!2026-03-13,15,316!2026-04-16,39,290!2026-03-12,14,290!2026-03-15,17,323!2026-03-13,15,316!2026-03-16,18,323!2026-03-13,15,311!2026-03-14,16,309-->

{@{An [asymptotic expansion](asymptotic%20expansion.md)}@} of {@{the cumulative distribution function for large _x_}@} can also be derived {@{using integration by parts}@}. For more, see {@{[Error function § Asymptotic expansion](error%20function.md#asymptotic%20expansion)}@}.<sup>[\[19\]](#^ref-19)</sup> <!--SR:!2026-03-16,18,323!2026-03-14,16,323!2026-03-13,15,316!2026-03-12,14,290-->

{@{A quick approximation to the standard normal distribution's cumulative distribution function}@} can be found by using {@{a Taylor series approximation}@}: {@{$$\Phi (x)\approx {\frac {1}{2} }+{\frac {1}{\sqrt {2\pi } } }\sum _{k=0}^{n}{\frac {(-1)^{k}x^{(2k+1)} }{2^{k}k!(2k+1)} }\,.$$}@} (annotation: Start from {@{the bases and recurrence when $x_0 = 0$}@}: {@{$$\Phi(0)=\tfrac{1}{2},\qquad \Phi^{(1)}(0)=\tfrac{1}{\sqrt{2\pi} },\qquad \Phi^{(n)}(0)=-(n-2)\,\Phi^{(n-2)}(0)\ \text{for }n\ge2.$$}@} All {@{even derivatives vanish}@}, and odd derivatives {@{alternate in sign and grow by $(n-2)$}@}. So: {@{$$\Phi^{(1)}(0)=\tfrac{1}{\sqrt{2\pi} },\ \Phi^{(3)}(0)=\frac {(-1)} {\sqrt {2\pi} },\ \Phi^{(5)}(0)=\frac {(-1) (3)} {\sqrt {2\pi} },\dots$$}@} With the convention {@{$(-1)!!=1$ (so the $k=0$ term is well-defined)}@} and using {@{$(2k-1)!!=\dfrac{(2k)!}{2^{k}k!}$}@}, we obtain that {@{only odd powers contribute to the Taylor series at $0$}@}, yielding {@{$$\Phi(x)\approx \frac 1 2 + \frac 1 {\sqrt{2\pi} } \sum_{k = 0}^n \frac {(-1)^k (2k - 1)!!} {(2k + 1)!} x^{2k + 1} = \frac{1}{2}+\frac{1}{\sqrt{2\pi} }\sum_{k=0}^n \frac{(-1)^k\,x^{2k+1} }{2^k\,k!\,(2k+1)} \,.$$}@}) <!--SR:!2026-03-15,17,323!2026-03-15,17,309!2026-03-16,18,323!2026-03-13,15,316!2026-03-13,15,309!2026-03-16,18,323!2026-03-15,17,332!2026-03-16,18,332!2026-03-16,18,323!2026-04-19,41,290!2026-03-13,15,316!2026-04-16,39,290-->

#### recursive computation with Taylor series expansion

{@{The recursive nature}@} of {@{the $e^{ax^{2} }$ family of derivatives}@} may be used to easily construct {@{a rapidly converging [Taylor series](Taylor%20series.md) expansion}@} using {@{recursive entries about any point of known value of the distribution, $\Phi (x_{0})$}@}: {@{$$\Phi (x)=\sum _{n=0}^{\infty }{\frac {\Phi ^{(n)}(x_{0})}{n!} }(x-x_{0})^{n}\,,$$}@} where: {@{$${\begin{aligned}\Phi ^{(0)}(x_{0})&={\frac {1}{\sqrt {2\pi } } }\int _{-\infty }^{x_{0} }e^{-t^{2}/2}\,dt\\\Phi ^{(1)}(x_{0})&={\frac {1}{\sqrt {2\pi } } }e^{-x_{0}^{2}/2}\\\Phi ^{(n)}(x_{0})&=-\left(x_{0}\Phi ^{(n-1)}(x_{0})+(n-2)\Phi ^{(n-2)}(x_{0})\right),&n\geq 2\,.\end{aligned} }$$}@} (annotation: Assuming $\phi$ is {@{real-analytic at $x_0$}@} (true for {@{the Gaussian $\phi(x)=(2\pi)^{-1/2}e^{-x^2/2}$, which is entire}@}), {@{the EGF $$F(t)=\sum_{n\ge0}\frac{\phi^{(n)}(x_0)}{n!}\,t^n$$}@} equals {@{the Taylor shift $F(t)=\phi(x_0+t)$}@}. Begin with {@{the ODE $ \phi'(x)=-x\,\phi(x)$}@}. Differentiating {@{$F(t)=\phi(x_0+t)$}@} gives {@{$F'(t)=-(x_0+t)F(t)$}@}. {@{Expanding and equating coefficients}@} yields {@{$$\frac{\phi^{(n+1)}(x_0)}{n!}=-x_0\frac{\phi^{(n)}(x_0)}{n!}-\frac{\phi^{(n-1)}(x_0)}{(n-1)!} \,,$$}@} valid for {@{$n\ge1$ to ensure $\phi^{(n-1)}$ is defined after the index shift}@}. {@{Multiplying by $n!$}@} gives {@{$$\phi^{(1)}(x_0)=-x_0\,\phi(x_0), \qquad \phi^{(n+1)}(x_0)=-x_0\,\phi^{(n)}(x_0)-n\,\phi^{(n-1)}(x_0)\quad(n\ge1) \,.$$}@} Since {@{$\Phi'(x)=\phi(x)$ for the normal CDF $\Phi$}@}, shifting {@{derivative order by one}@} yields {@{$$\boxed{\;\Phi^{(m+2)}(x_0)=-x_0\,\Phi^{(m+1)}(x_0)-m\,\Phi^{(m)}(x_0)\quad(m\ge1),\;}$$}@} with bases {@{$\Phi'(x_0)=\phi(x_0)$ and $\Phi''(x_0)=-x_0\,\phi(x_0)$}@}.) <!--SR:!2026-03-14,16,316!2026-03-15,17,323!2026-03-15,17,323!2026-03-15,17,323!2026-03-14,16,332!2026-03-13,15,316!2026-03-15,17,323!2026-03-14,16,323!2026-03-16,18,323!2026-03-13,15,290!2026-03-27,21,276!2026-03-14,3,216!2026-03-15,17,323!2026-03-14,16,323!2026-03-13,15,309!2026-03-14,16,332!2026-03-14,16,323!2026-04-26,47,290!2026-03-14,16,316!2026-03-16,18,323!2026-03-16,18,332!2026-03-16,18,332-->

#### using the Taylor series and Newton's method for the inverse function

{@{An application for the above Taylor series expansion}@} is to use {@{[Newton's method](Newton's%20method.md) to reverse the computation}@}. That is, if we have {@{a value for the [cumulative distribution function](cumulative%20distribution%20function.md), $\Phi (x)$}@}, but do not {@{know the x needed to obtain the $\Phi (x)$}@}, we can use {@{Newton's method to find x}@}, and use {@{the Taylor series expansion above}@} to minimize {@{the number of computations}@}. Newton's method is {@{ideal to solve this problem}@} because {@{the first derivative of $\Phi (x)$}@}, which is {@{an integral of the normal standard distribution}@}, is {@{the normal standard distribution}@}, and is readily {@{available to use in the Newton's method solution}@}. <!--SR:!2026-03-12,14,290!2026-03-12,14,290!2026-03-16,18,323!2026-03-16,18,323!2026-03-16,18,323!2026-03-16,18,323!2026-03-12,14,290!2026-03-16,18,323!2026-03-16,18,323!2026-03-15,17,323!2026-03-13,15,316!2026-03-15,17,332-->

To solve, select {@{a known approximate solution, $x_{0}$}@}, to {@{the desired ⁠$\Phi (x)$}@}⁠. $x_{0}$ may be {@{a value from a distribution table, or an intelligent estimate}@} followed by a computation of {@{$\Phi (x_{0})$ using any desired means to compute}@}. Use {@{this value of $x_{0}$ and the Taylor series expansion above}@} to minimize {@{computations}@}. <!--SR:!2026-03-13,15,314!2026-03-15,17,323!2026-03-14,16,323!2026-03-12,14,290!2026-03-15,17,332!2026-03-12,14,290-->

Repeat {@{the following process until the difference}@} between {@{the computed $\Phi (x_{n})$ and the desired ⁠$\Phi$⁠, which we will call $\Phi ({\text{desired} })$}@}, is below {@{a chosen acceptably small error, such as 10<sup>−5</sup>, 10<sup>−15</sup>, etc.}@}: {@{$$x_{n+1}=x_{n}-{\frac {\Phi (x_{n},x_{0},\Phi (x_{0}))-\Phi ({\text{desired} })}{\Phi '(x_{n})} }\,,$$}@} where <p> &emsp; {@{$\Phi (x,x_{0},\Phi (x_{0}))$}@} is {@{the $\Phi (x)$ from a Taylor series solution using $x_{0}$ and $\Phi (x_{0})$}@} <br/> &emsp; {@{$\Phi '(x_{n})={\frac {1}{\sqrt {2\pi } } }e^{-x_{n}^{2}/2}$}@}. <!--SR:!2026-03-12,14,290!2026-03-16,18,332!2026-03-13,15,316!2026-03-13,15,316!2026-03-12,14,290!2026-03-13,15,316!2026-03-12,14,290-->

When the repeated computations {@{converge to an error below the chosen acceptably small value}@}, _x_ will be {@{the value needed to obtain a $\Phi (x)$ of the desired value, ⁠$\Phi ({\text{desired} })$}@}⁠. <!--SR:!2026-03-15,17,323!2026-03-13,15,311-->

#### standard deviation and coverage

- Further information: ::@:: [Interval estimation](interval%20estimation.md) and [Coverage probability](coverage%20probability.md) <!--SR:!2026-03-16,18,323!2026-03-15,17,316-->

> {@{![For the normal distribution, the values less than one standard deviation from the mean account for 68.27% of the set; while two standard deviations from the mean account for 95.45%; and three standard deviations account for 99.73%.](../../archives/Wikimedia%20Commons/Standard%20deviation%20diagram.svg)}@}
>
> For {@{the normal distribution}@}, the values {@{less than one standard deviation from the mean}@} account for {@{68.27% of the set}@}; while {@{two standard deviations from the mean}@} account for {@{95.45%}@}; and {@{three standard deviations}@} account for {@{99.73%}@}. <!--SR:!2026-03-12,14,290!2026-03-13,15,316!2026-03-13,15,290!2026-03-14,16,323!2026-03-12,14,290!2026-03-14,16,323!2026-03-12,14,290!2026-03-12,14,290-->

{@{About 68% of values drawn}@} from a normal distribution are {@{within one standard deviation _σ_ from the mean}@}; {@{about 95% of the values}@} lie {@{within two standard deviations}@}; and {@{about 99.7%}@} are within {@{three standard deviations}@}.<sup>[\[9\]](#^ref-9)</sup> This is known as {@{the [68–95–99.7 \(empirical\) rule](68–95–99.7%20rule.md), or the _3-sigma rule_}@}. <!--SR:!2026-03-16,18,323!2026-03-13,15,316!2026-03-14,16,323!2026-03-16,18,323!2026-03-12,14,290!2026-03-13,15,316!2026-03-16,18,323-->

More precisely, the probability that {@{a normal deviate lies in the range between $\mu -n\sigma$ and $\mu +n\sigma$}@} is given by {@{$$F(\mu +n\sigma )-F(\mu -n\sigma )=\Phi (n)-\Phi (-n)=\operatorname {erf} \left({\frac {n}{\sqrt {2} } }\right).$$}@} To 12 significant digits, the values for $n=1,2,\ldots ,6$ are: <!--SR:!2026-03-15,17,332!2026-03-16,18,323-->

|  ⁠$n$ ⁠ | $p=F(\mu +n\sigma )-F(\mu -n\sigma )$ |              $1-p$               | ${\text{or } }1{\text{ in } }(1-p)$                                                                                                         |                                          [OEIS](OEIS.md)                                          |
| :---: | :-----------------------------------: | :------------------------------: | :------------------------------------------------------------------------------------------------------------------------------------------ | :-----------------------------------------------------------------------------------------------: |
|   1   |   0.682&nbsp;689&nbsp;492&nbsp;137    | 0.317&nbsp;310&nbsp;507&nbsp;863 | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.151&nbsp;487&nbsp;187&nbsp;53 | [OEIS](On-Line%20Encyclopedia%20of%20Integer%20Sequences.md): [A178647](https://oeis.org/A178647) |
|   2   |   0.954&nbsp;499&nbsp;736&nbsp;104    | 0.045&nbsp;500&nbsp;263&nbsp;896 | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;21.977&nbsp;894&nbsp;5080                   | [OEIS](On-Line%20Encyclopedia%20of%20Integer%20Sequences.md): [A110894](https://oeis.org/A110894) |
|   3   |   0.997&nbsp;300&nbsp;203&nbsp;937    | 0.002&nbsp;699&nbsp;796&nbsp;063 | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;370.398&nbsp;347&nbsp;345                               | [OEIS](On-Line%20Encyclopedia%20of%20Integer%20Sequences.md): [A270712](https://oeis.org/A270712) |
|   4   |   0.999&nbsp;936&nbsp;657&nbsp;516    | 0.000&nbsp;063&nbsp;342&nbsp;484 | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;15&nbsp;787.192&nbsp;7673                                                             |                                                                                                   |
|   5   |   0.999&nbsp;999&nbsp;426&nbsp;697    | 0.000&nbsp;000&nbsp;573&nbsp;303 | &nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;744&nbsp;277.893&nbsp;62                                                                                     |                                                                                                   |
|   6   |   0.999&nbsp;999&nbsp;998&nbsp;027    | 0.000&nbsp;000&nbsp;001&nbsp;973 | 506&nbsp;797&nbsp;345.897                                                                                                                   |                                                                                                   |

For {@{large ⁠$n$}@}⁠, one can use the approximation {@{$1-p\approx {\frac {e^{-n^{2}/2} }{n{\sqrt {\pi /2} } } }$}@}. (annotation: It can be written as {@{$1 - p \approx \frac {2 \varphi(n)} n$}@} instead.) <!--SR:!2026-03-13,15,314!2026-03-12,14,290!2026-03-16,18,332-->

#### quantile function

- Further information: ::@:: [Quantile function § Normal distribution](quantile%20function.md#normal%20distribution) <!--SR:!2026-03-15,17,323!2026-03-16,18,332-->

{@{The [quantile function](quantile%20function.md) of a distribution}@} is {@{the inverse of the cumulative distribution function}@}. {@{The quantile function of the standard normal distribution}@} is called {@{the [probit function](probit%20function.md)}@}, and can be expressed in terms of {@{the inverse [error function](error%20function.md)}@}: {@{$$\Phi ^{-1}(p)={\sqrt {2} }\operatorname {erf} ^{-1}(2p-1),\quad p\in (0,1).$$}@} (annotation: It uses {@{the inverse error function}@}, {@{scaled by $\sqrt{2}$}@} to account for {@{the variance difference}@}, while {@{the term $(2p - 1)$}@} translates {@{the baseline from $[0,1]$}@} to {@{the symmetric range $[-1,1]$ required by $\operatorname{erf}^{-1}$}@}.) For {@{a normal random variable with mean ⁠$\mu$⁠ and variance $\sigma ^{2}$}@}, the quantile function is {@{$$F^{-1}(p)=\mu +\sigma \Phi ^{-1}(p)=\mu +\sigma {\sqrt {2} }\operatorname {erf} ^{-1}(2p-1),\quad p\in (0,1).$$}@} {@{The [quantile](quantile.md) $\Phi ^{-1}(p)$ of the standard normal distribution}@} is commonly denoted {@{as ⁠$z_{p}$}@}⁠. These values are used in {@{[hypothesis testing](hypothesis%20testing.md), construction of [confidence intervals](confidence%20interval.md) and [Q–Q plots](Q–Q%20plot.md)}@}. {@{A normal random variable ⁠$X$⁠}@} will exceed {@{$\mu +z_{p}\sigma$ with probability $1-p$}@}, and will lie {@{outside the interval $\mu \pm z_{p}\sigma$}@} with {@{probability ⁠$2(1-p)$}@}⁠. In particular, {@{the quantile $z_{0.975}$}@} is {@{[1.96](1.96.md)}@}; therefore {@{a normal random variable will lie outside the interval $\mu \pm 1.96\sigma$}@} in {@{only 5% of cases}@}. <!--SR:!2026-03-14,16,323!2026-03-12,14,290!2026-03-14,16,316!2026-03-12,14,290!2026-03-15,17,332!2026-03-13,15,316!2026-03-13,15,316!2026-03-13,15,316!2026-03-14,16,316!2026-03-16,18,323!2026-03-13,15,316!2026-03-14,16,316!2026-03-12,14,290!2026-03-14,16,323!2026-03-16,18,332!2026-03-15,17,323!2026-03-16,18,323!2026-03-14,16,316!2026-03-13,15,316!2026-03-14,16,316!2026-03-13,15,309!2026-03-12,14,290!2026-03-15,17,316!2026-03-14,16,323!2026-03-14,16,323-->

{@{The following table}@} gives {@{the quantile $z_{p}$}@} such that ⁠$X$⁠ will lie {@{in the range $\mu \pm z_{p}\sigma$ with a specified probability ⁠$p$}@}⁠. These values are useful to determine {@{[tolerance interval](tolerance%20interval.md) for [sample averages](sample%20mean%20and%20sample%20covariance.md#sample%20mean)}@} and other {@{statistical [estimators](estimator.md) with normal \(or [asymptotically](asymptotic.md) normal\) distributions}@}.<sup>[\[20\]](#^ref-20)</sup> The following table shows {@{${\sqrt {2} }\operatorname {erf} ^{-1}(p)=\Phi ^{-1}\left({\frac {p+1}{2} }\right)$}@}, not {@{$\Phi ^{-1}(p)$ as defined above}@}. <!--SR:!2026-03-16,18,323!2026-03-14,16,316!2026-03-12,14,290!2026-03-14,16,323!2026-03-15,17,314!2026-03-15,17,323!2026-03-16,18,323-->

| __⁠$p$⁠__ | __$z_{p}$__                      |
| ------- | -------------------------------- |
| 0.80    | 1.281&nbsp;551&nbsp;565&nbsp;545 |
| 0.90    | 1.644&nbsp;853&nbsp;626&nbsp;951 |
| 0.95    | 1.959&nbsp;963&nbsp;984&nbsp;540 |
| 0.98    | 2.326&nbsp;347&nbsp;874&nbsp;041 |
| 0.99    | 2.575&nbsp;829&nbsp;303&nbsp;549 |
| 0.995   | 2.807&nbsp;033&nbsp;768&nbsp;344 |
| 0.998   | 3.090&nbsp;232&nbsp;306&nbsp;168 |

| __⁠$p$⁠__     | __$z_{p}$__                      |
| ----------- | -------------------------------- |
| 0.999       | 3.290&nbsp;526&nbsp;731&nbsp;492 |
| 0.9999      | 3.890&nbsp;591&nbsp;886&nbsp;413 |
| 0.99999     | 4.417&nbsp;173&nbsp;413&nbsp;469 |
| 0.999999    | 4.891&nbsp;638&nbsp;475&nbsp;699 |
| 0.9999999   | 5.326&nbsp;723&nbsp;886&nbsp;384 |
| 0.99999999  | 5.730&nbsp;728&nbsp;868&nbsp;236 |
| 0.999999999 | 6.109&nbsp;410&nbsp;204&nbsp;869 |

For {@{small ⁠$p$⁠, the quantile function}@} has {@{the useful [asymptotic expansion](asymptotic%20expansion.md)}@} {@{$\Phi ^{-1}(p)=-{\sqrt {\ln {\frac {1}{p^{2} } }-\ln \ln {\frac {1}{p^{2} } }-\ln(2\pi )} }+{\mathcal {o} }(1)$}@}.<sup>\[_[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation%20needed)_\]</sup> <!--SR:!2026-03-15,17,323!2026-03-15,17,323!2026-05-05,55,316-->

## properties

{@{The normal distribution}@} is {@{the only distribution whose [cumulants](cumulant.md) beyond the first two \(i.e., other than the mean and [variance](variance.md)\) are zero}@}. It is also {@{the continuous distribution with the [maximum entropy](maximum%20entropy%20probability%20distribution.md)}@} for {@{a specified mean and variance}@}.<sup>[\[21\]](#^ref-21)</sup><sup>[\[22\]](#^ref-22)</sup> {@{Geary has shown, assuming that the mean and variance are finite}@}, that the normal distribution is {@{the only distribution where the mean and variance calculated}@} from {@{a set of independent draws are independent of each other}@}.<sup>[\[23\]](#^ref-23)</sup><sup>[\[24\]](#^ref-24)</sup> <!--SR:!2026-03-16,18,323!2026-03-12,14,290!2026-03-14,16,316!2026-03-15,17,323!2026-03-15,17,323!2026-03-12,14,290!2026-03-15,17,316-->

The normal distribution is a subclass of {@{the [elliptical distributions](elliptical%20distribution.md)}@}. The normal distribution is {@{[symmetric](symmetric%20distribution.md) about its mean, and is non-zero over the entire real line}@}. As such it may not be {@{a suitable model for variables that are inherently positive or strongly skewed}@}, such as {@{the [weight](weight.md) of a person or the price of a [share](share%20(finance).md)}@}. Such variables may be better {@{described by other distributions}@}, such as {@{the [log-normal distribution](log-normal%20distribution.md) or the [Pareto distribution](Pareto%20distribution.md)}@}. <!--SR:!2026-03-13,15,316!2026-03-14,16,323!2026-03-12,14,290!2026-03-13,15,316!2026-03-15,17,332!2026-03-13,15,316-->

{@{The value of the normal density is practically zero}@} when the value ⁠$x$⁠ lies {@{more than a few [standard deviations](standard%20deviation.md) away from the mean}@} \(e.g., {@{a spread of three standard deviations}@} covers {@{all but 0.27% of the total distribution}@}\). Therefore, it may not be {@{an appropriate model}@} when one expects {@{a significant fraction of [outliers](outlier.md)}@}—values that lie {@{many standard deviations away from the mean}@}—and {@{least squares and other [statistical inference](statistical%20inference.md) methods}@} that are {@{optimal for normally distributed variables}@} often become {@{highly unreliable when applied to such data}@}. In those cases, {@{a more [heavy-tailed](heavy-tailed.md) distribution}@} should be assumed and {@{the appropriate [robust statistical inference](robust%20statistics.md) methods}@} applied. <!--SR:!2026-03-13,15,316!2026-03-15,17,311!2026-03-14,16,332!2026-03-15,17,323!2026-03-14,16,332!2026-03-12,14,290!2026-03-14,16,332!2026-03-16,18,323!2026-03-16,18,332!2026-03-13,15,316!2026-03-15,17,323!2026-03-15,17,311-->

The Gaussian distribution belongs to {@{the family of [stable distributions](stable%20distribution.md)}@} which are {@{the attractors of sums of [independent, identically distributed](independent,%20identically%20distributed.md) distributions}@} whether or not {@{the mean or variance is finite}@}. Except for {@{the Gaussian which is a limiting case}@}, {@{all stable distributions}@} have {@{heavy tails and infinite variance}@}. It is {@{one of the few distributions}@} that are {@{stable and that have probability density functions that can be expressed analytically}@}, the others being {@{the [Cauchy distribution](Cauchy%20distribution.md) and the [Lévy distribution](Lévy%20distribution.md)}@}. <!--SR:!2026-03-12,14,290!2026-03-14,16,323!2026-03-16,18,323!2026-03-14,16,323!2026-03-13,15,316!2026-03-13,15,314!2026-03-15,17,323!2026-03-15,17,323!2026-03-12,14,290-->

### symmetries and derivatives

The normal distribution with {@{density $f(x)$ \(mean ⁠$\mu$⁠ and variance $\sigma ^{2}>0$\)}@} has the following properties: <!--SR:!2026-03-13,15,316-->

- It is {@{symmetric around the point $x=\mu$}@}, which is at the same time {@{the [mode](mode%20(statistics).md), the [median](median.md) and the [mean](mean.md)}@} of the distribution.<sup>[\[25\]](#^ref-25)</sup>
- It is {@{[unimodal](unimodal.md)}@}: its {@{first [derivative](derivative.md)}@} is {@{positive for $x<\mu$, negative for $x>\mu$}@}, and {@{zero only at $x=\mu$}@}.
- {@{The area bounded by the curve and the ⁠$x$⁠-axis}@} is {@{unity \(i.e. equal to one\)}@}.
- Its {@{first derivative}@} is {@{$f'(x)=-{\frac {x-\mu }{\sigma ^{2} } }f(x)$}@}. (annotation: Compare with {@{$Q'(x) = -x Q(x)$}@}. Seems like replacing {@{$x$ with $(x - \mu) / \sigma$}@} and then {@{divide by $\sigma$}@}.)
- Its {@{second derivative}@} is $f''(x)={\frac {(x-\mu )^{2}-\sigma ^{2} }{\sigma ^{4} } }f(x)$. (annotation: Compare with {@{$Q''(x) = \left(x^2 - 1\right) Q(x)$}@}. Seems like replacing {@{$x$ with $(x - \mu) / \sigma$}@} and then {@{divide by $\sigma^2$}@}.)
- Its {@{density}@} has {@{two [inflection points](inflection%20point.md) \(where the second derivative of ⁠$f$⁠ is zero and changes sign\)}@}, located {@{one standard deviation away from the mean}@}, namely at {@{$x=\mu -\sigma$ and $x=\mu +\sigma$}@}.<sup>[\[25\]](#^ref-25)</sup>
- Its {@{density}@} is {@{[log-concave](logarithmically%20concave%20function.md)}@}.<sup>[\[25\]](#^ref-25)</sup>
- Its {@{density}@} is {@{infinitely [differentiable](differentiable.md)}@}, indeed {@{[supersmooth](supersmooth.md) of order 2}@}.<sup>[\[26\]](#^ref-26)</sup> <!--SR:!2026-03-13,15,311!2026-03-15,17,323!2026-03-15,17,323!2026-03-15,17,323!2026-03-16,18,323!2026-03-12,14,290!2026-03-15,17,323!2026-03-12,14,290!2026-03-13,15,309!2026-03-15,17,316!2026-03-14,16,323!2026-03-15,17,323!2026-04-19,41,290!2026-03-14,16,311!2026-03-16,18,323!2026-03-13,15,311!2026-03-12,14,290!2026-03-12,14,290!2026-03-15,17,332!2026-03-15,17,332!2026-03-14,16,332!2026-03-12,14,290!2026-03-16,18,323!2026-03-14,16,323!2026-03-13,15,314!2026-03-16,18,323-->

Furthermore, {@{the density ⁠$\varphi$⁠ of the standard normal distribution}@} \(i.e. {@{$\mu =0$ and $\sigma =1$}@}\) also has the following properties: <!--SR:!2026-03-14,16,316!2026-03-12,14,290-->

- Its {@{first derivative}@} is {@{$\varphi '(x)=-x\varphi (x)$}@}.
- Its {@{second derivative}@} is {@{$\varphi ''(x)=(x^{2}-1)\varphi (x)$}@}
- More generally, its {@{_n_<!-- markdown separator -->th derivative}@} is {@{$\varphi ^{(n)}(x)=(-1)^{n}\operatorname {He} _{n}(x)\varphi (x)$}@}, where {@{$\operatorname {He} _{n}(x)$ is the _n_<!-- markdown separator -->th \(probabilist\) [Hermite polynomial](Hermite%20polynomial.md)}@}.<sup>[\[27\]](#^ref-27)</sup>
- The probability that {@{a normally distributed variable ⁠$X$⁠ with known ⁠$\mu$⁠ and $\sigma ^{2}$}@} is in {@{a particular set, can be calculated}@} given that {@{the fraction $Z=(X-\mu )/\sigma$ has a standard normal distribution}@}. <!--SR:!2026-03-13,15,316!2026-03-16,18,323!2026-03-12,14,290!2026-03-15,17,323!2026-03-13,15,314!2026-04-19,42,309!2026-03-16,18,332!2026-03-13,15,290!2026-03-15,17,314!2026-03-14,16,311-->

### moments

- See also: ::@:: [List of integrals of Gaussian functions](List%20of%20integrals%20of%20Gaussian%20functions.md) <!--SR:!2026-03-13,15,311!2026-03-15,17,323-->

{@{The plain and absolute [moments](moment%20(mathematics).md)}@} of a variable ⁠$X$⁠ are {@{the expected values of $X^{p}$ and $|X|^{p}$}@}, respectively. If {@{the expected value ⁠$\mu$⁠ of ⁠$X$⁠ is zero}@}, these parameters are called {@{_central moments_}@}; otherwise, these parameters are called {@{_non-central moments_}@}. Usually we are interested {@{only in moments with integer order ⁠$p$⁠}@}. <!--SR:!2026-03-14,16,323!2026-03-15,17,332!2026-03-12,14,290!2026-03-13,15,316!2026-03-14,16,332!2026-03-13,15,316-->

If ⁠$X$⁠ has {@{a normal distribution}@}, {@{the non-central moments}@} {@{exist and are finite for any ⁠$p$⁠ whose real part is greater than −1}@}. For {@{any non-negative integer ⁠$p$}@}⁠, {@{the plain central moments}@} are:<sup>[\[28\]](#^ref-28)</sup> {@{$$\operatorname {E} \left[(X-\mu )^{p}\right]={\begin{cases}0&{\text{if } }p{\text{ is odd,} }\\\sigma ^{p}(p-1)!!&{\text{if } }p{\text{ is even.} }\end{cases} }$$}@} Here $n!!$ denotes {@{the [double factorial](double%20factorial.md)}@}, that is, {@{the product of all numbers from ⁠$n$⁠ to 1 that have the same parity as $n$}@}. <!--SR:!2026-03-13,15,316!2026-03-13,15,316!2026-03-16,18,332!2026-03-12,14,290!2026-03-16,18,323!2026-03-14,16,332!2026-03-16,18,323!2026-03-14,16,323-->

{@{The central absolute moments}@} coincide with {@{plain moments for all even orders}@}, but are {@{nonzero for odd orders}@}. For {@{any non-negative integer $p$}@}, {@{$${\begin{aligned}\operatorname {E} \left[|X-\mu |^{p}\right]&=\sigma ^{p}(p-1)!!\cdot {\begin{cases}{\sqrt {\frac {2}{\pi } } }&{\text{if } }p{\text{ is odd} }\\1&{\text{if } }p{\text{ is even} }\end{cases} }\\[8pt]&=\sigma ^{p}\cdot {\frac {2^{p/2}\Gamma \left({\frac {p+1}{2} }\right)}{\sqrt {\pi } } }.\end{aligned} }$$}@} {@{The last formula}@} is {@{valid also for any non-integer $p>-1$}@}. When {@{the mean $\mu \neq 0$}@}, {@{the plain and absolute moments}@} can be expressed in terms of {@{[confluent hypergeometric functions](confluent%20hypergeometric%20function.md) ${}_{1}F_{1}$ and $U$}@}.<sup>[\[29\]](#^ref-29)</sup> {@{$${\begin{aligned}\operatorname {E} \left[X^{p}\right]&=\sigma ^{p}\cdot {\left(-i{\sqrt {2} }\right)}^{p}\,U{\left(-{\frac {p}{2} },{\frac {1}{2} },-{\frac {\mu ^{2} }{2\sigma ^{2} } }\right)},\\\operatorname {E} \left[|X|^{p}\right]&=\sigma ^{p}\cdot 2^{p/2}{\frac {\Gamma {\left({\frac {1+p}{2} }\right)} }{\sqrt {\pi } } }\,{}_{1}F_{1}{\left(-{\frac {p}{2} },{\frac {1}{2} },-{\frac {\mu ^{2} }{2\sigma ^{2} } }\right)}.\end{aligned} }$$}@} {@{These expressions remain valid}@} even if {@{⁠$p$⁠ is not an integer}@}. See also {@{[generalized Hermite polynomials](Hermite%20polynomials.md#%22negative%20variance%22)}@}. <!--SR:!2026-03-14,16,316!2026-03-12,14,290!2026-03-12,14,290!2026-03-13,15,316!2026-03-27,21,250!2026-03-13,15,316!2026-03-12,14,290!2026-03-13,15,316!2026-03-16,18,323!2026-03-16,18,323!2026-03-29,20,263!2026-03-16,18,323!2026-03-15,17,323!2026-04-30,51,311-->

| Order | Non-central moment, $\operatorname {E} \left[X^{p}\right]$                                    | Central moment, $\operatorname {E} \left[(X-\mu )^{p}\right]$ |
| ----- | --------------------------------------------------------------------------------------------- | ------------------------------------------------------------- |
| 0     | ⁠$1$⁠                                                                                           | ⁠$1$⁠                                                           |
| 1     | ⁠$\mu$⁠                                                                                         | ⁠$0$⁠                                                           |
| 2     | $\mu ^{2}+\sigma ^{2}$                                                                        | $\sigma ^{2}$                                                 |
| 3     | $\mu ^{3}+3\mu \sigma ^{2}$                                                                   | ⁠$0$⁠                                                           |
| 4     | $\mu ^{4}+6\mu ^{2}\sigma ^{2}+3\sigma ^{4}$                                                  | $3\sigma ^{4}$                                                |
| 5     | $\mu ^{5}+10\mu ^{3}\sigma ^{2}+15\mu \sigma ^{4}$                                            | ⁠$0$⁠                                                           |
| 6     | $\mu ^{6}+15\mu ^{4}\sigma ^{2}+45\mu ^{2}\sigma ^{4}+15\sigma ^{6}$                          | $15\sigma ^{6}$                                               |
| 7     | $\mu ^{7}+21\mu ^{5}\sigma ^{2}+105\mu ^{3}\sigma ^{4}+105\mu \sigma ^{6}$                    | ⁠$0$⁠                                                           |
| 8     | $\mu ^{8}+28\mu ^{6}\sigma ^{2}+210\mu ^{4}\sigma ^{4}+420\mu ^{2}\sigma ^{6}+105\sigma ^{8}$ | $105\sigma ^{8}$                                              |

{@{The expectation of ⁠$X$⁠}@} conditioned on the event that {@{⁠$X$⁠ lies in an interval $[a,b]$}@} is given by {@{$$\operatorname {E} \left[X\mid a<X<b\right]=\mu -\sigma ^{2}{\frac {f(b)-f(a)}{F(b)-F(a)} }\,,$$}@} where {@{⁠$f$⁠ and ⁠$F$}@}⁠ respectively are {@{the density and the cumulative distribution function of ⁠$X$}@}⁠. (annotation: You _could_ derive {@{this by integration by parts}@}, but {@{that's mechanical}@}. More tersely, for {@{$X\sim\mathcal N(\mu,\sigma^2)$ and $I=\mathbf 1_{(a,b)}(X)$}@}, {@{$$\mathbb E[X\mid a<X<b]=\frac{\mathbb E[XI]}{\mathbb P(a<X<b)} =\frac{\mu\,\mathbb P(a<X<b)+\operatorname{Cov}(X,I)}{F(b)-F(a)},$$}@} using {@{$\mathbb E[AB]=\mathbb E[A]\mathbb E[B]+\operatorname{Cov}(A,B)$}@}. By {@{Stein's lemma for normals}@}, {@{$\operatorname{Cov}(X,g(X))=\sigma^2\,\mathbb E[g'(X)]$}@} for {@{suitable $g$}@}; taking {@{$g(x)=\mathbf 1_{(a,b)}(x)$}@} and interpreting {@{$g'$ in the distributional sense (or via smooth approximations)}@}, {@{$g'(x)=\delta(x-a)-\delta(x-b)$}@}, hence {@{$\mathbb E[g'(X)]=f(a)-f(b)$}@}. Therefore {@{$$\operatorname{Cov}(X,I)=\sigma^2\,[f(a)-f(b)] \quad\Rightarrow\quad \mathbb E[X\mid a<X<b] =\mu-\sigma^2\,\frac{f(b)-f(a)}{F(b)-F(a)} \,.$$}@} This makes {@{the correction}@} depend {@{only on the endpoint densities}@}, reflecting how {@{truncation shifts the mean}@}.) For {@{$b=\infty$ (annotation: so that $f(b) = 0$ and $F(b) = 1$)}@} this is known as {@{the [inverse Mills ratio](inverse%20Mills%20ratio.md#inverse%20Mills%20ratio)}@}. Note that above, {@{density ⁠$f$⁠ of ⁠$X$⁠}@} is used instead of {@{standard normal density as in inverse Mills ratio}@}, so here we have {@{$\sigma ^{2}$ instead of ⁠$\sigma$⁠}@}. <!--SR:!2026-03-12,14,290!2026-03-13,15,309!2026-03-12,14,290!2026-03-12,14,290!2026-03-15,17,323!2026-03-16,18,332!2026-03-12,14,290!2026-03-12,14,290!2026-03-12,14,290!2026-03-16,18,323!2026-04-16,39,290!2026-03-16,18,332!2026-03-12,14,290!2026-03-13,15,314!2026-03-15,17,323!2026-03-13,15,316!2026-03-13,15,309!2026-03-16,18,323!2026-03-13,15,316!2026-03-14,16,332!2026-03-13,15,316!2026-03-12,14,290!2026-03-14,16,309!2026-03-15,17,323!2026-03-14,16,311!2026-03-13,15,311-->

### Fourier transform and characteristic function

{@{The [Fourier transform](Fourier%20transform.md)}@} of {@{a normal density ⁠$f$⁠ with mean ⁠$\mu$⁠ and variance $\sigma ^{2}$}@} is<sup>[\[30\]](#^ref-30)</sup> {@{$${\hat {f} }(t)=\int _{-\infty }^{\infty }f(x)e^{-itx}\,dx=e^{-i\mu t}e^{-{\frac {1}{2} }\sigma ^{2}t^{2} }\,,$$}@} where ⁠{@{$i$⁠ is the [imaginary unit](imaginary%20unit.md)}@}. If {@{the mean $\mu =0$}@}, {@{the first factor is 1}@}, and the Fourier transform is, {@{apart from a constant factor, a normal density on the [frequency domain](frequency%20domain.md)}@}, with {@{mean 0 and variance ⁠$1/\sigma ^{2}$}@}⁠. In particular, {@{the standard normal distribution ⁠$\varphi$⁠}@} is {@{an [eigenfunction](Fourier%20transform.md#eigenfunctions) of the Fourier transform}@}. <!--SR:!2026-03-16,18,323!2026-03-14,16,323!2026-03-14,16,316!2026-03-16,18,323!2026-03-13,15,316!2026-03-14,16,316!2026-03-13,15,311!2026-03-16,18,332!2026-03-15,17,332!2026-03-14,16,323-->

In {@{probability theory}@}, the Fourier transform of {@{the probability distribution of a real-valued random variable ⁠$X$}@}⁠ is closely connected to {@{the [characteristic function](characteristic%20function%20(probability%20theory).md) $\varphi _{X}(t)$ of that variable}@}, which is defined as {@{the [expected value](expected%20value.md) of $e^{itX}$, as a function of the real variable ⁠$t$}@}⁠ \({@{the [frequency](frequency.md) parameter}@} of the Fourier transform\). This definition can be {@{analytically extended to a complex-value variable ⁠$t$}@}⁠.<sup>[\[31\]](#^ref-31)</sup> {@{The relation between both}@} is: {@{$$\varphi _{X}(t)={\hat {f} }(-t)\,.$$}@} <!--SR:!2026-03-12,14,290!2026-03-15,17,332!2026-03-16,18,323!2026-03-13,15,316!2026-03-14,16,323!2026-03-13,15,316!2026-03-13,15,316!2026-03-15,17,323-->

\(annotation: Note $x$ follows {@{a normal distribution $\mathcal N\!\left(\mu,\sigma^2\right)$}@}.\) {@{The real and imaginary parts}@} of {@{${\hat {f} }(t)=\operatorname {E} [e^{-itx}]=e^{-i\mu t}e^{-{\frac {1}{2} }\sigma ^{2}t^{2} }$}@} give: {@{$$\operatorname {E} [\cos(tx)]=\cos(\mu t)e^{-{\frac {1}{2} }\sigma ^{2}t^{2} }$$}@} and {@{$$\operatorname {E} [\sin(tx)]=\sin(\mu t)e^{-{\frac {1}{2} }\sigma ^{2}t^{2} }.$$}@} Similarly, {@{$$\operatorname {E} [\cosh(tx)]=\cosh(\mu t)e^{ {\frac {1}{2} }\sigma ^{2}t^{2} }$$}@} and {@{$$\operatorname {E} [\sinh(tx)]=\sinh(\mu t)e^{ {\frac {1}{2} }\sigma ^{2}t^{2} }.$$}@} \(annotation: {@{The latter two can be derived}@} by {@{the substitution of $t$ with $it$ in the former two}@}.\) <!--SR:!2026-03-12,14,290!2026-03-14,16,316!2026-03-13,15,314!2026-03-12,14,290!2026-05-05,55,311!2026-03-15,17,323!2026-03-23,18,354!2026-03-24,19,354!2026-03-23,18,354-->

{@{These formulas evaluated at $t=1$}@} give {@{the expected value of these basic trigonometric and hyperbolic functions}@} over {@{a Gaussian random variable $X\sim N(\mu ,\sigma ^{2})$}@}, which also could be seen as {@{consequences of the [Isserlis's theorem](Isserlis's%20theorem.md)}@}. <!--SR:!2026-03-14,16,323!2026-03-12,14,290!2026-03-16,18,323!2026-03-13,15,311-->

### moment- and cumulant-generating functions

{@{The [moment generating function](moment%20generating%20function.md) of a real random variable ⁠$X$}@}⁠ is {@{the expected value of $e^{tX}$, as a function of the real parameter ⁠$t$⁠}@}. For {@{a normal distribution with density ⁠$f$⁠, mean ⁠$\mu$⁠ and variance $\sigma ^{2}$}@}, {@{the moment generating function exists and is equal}@} to {@{$$M(t)=\operatorname {E} \left[e^{tX}\right]={\hat {f} }(it)=e^{\mu t}e^{\sigma ^{2}t^{2}/2}\,.$$}@} For {@{any ⁠$k$}@}⁠, {@{the coefficient of ⁠$t^{k}/k!$⁠}@} in the moment generating function \(expressed as {@{an [exponential power series](generating%20function.md#Exponential%20generating%20function%20(EGF)) in ⁠$t$}@}⁠\) is {@{the normal distribution's expected value ⁠$\operatorname {E} [X^{k}]$⁠}@}. <!--SR:!2026-03-12,14,290!2026-03-13,15,316!2026-03-12,14,290!2026-03-12,14,290!2026-03-15,17,323!2026-03-15,17,316!2026-03-16,18,323!2026-03-12,14,290!2026-03-15,17,323-->

{@{The [cumulant generating function](cumulant%20generating%20function.md)}@} is {@{the logarithm of the moment generating function}@}, namely {@{$$g(t)=\ln M(t)=\mu t+{\tfrac {1}{2} }\sigma ^{2}t^{2}\,.$$}@} <!--SR:!2026-03-12,14,290!2026-03-13,15,290!2026-03-12,14,290-->

{@{The coefficients of this exponential power series}@} define {@{the cumulants}@}, but because this is {@{a quadratic polynomial in ⁠$t$}@}⁠, {@{only the first two [cumulants](cumulant.md) are nonzero}@}, namely {@{the mean ⁠$\mu$⁠ and the variance ⁠$\sigma ^{2}$⁠}@}. <!--SR:!2026-03-15,17,323!2026-03-13,15,316!2026-03-15,17,311!2026-03-16,18,323!2026-03-15,17,323-->

{@{Some authors}@} prefer to instead work with {@{the [characteristic function](characteristic%20function%20(probability%20theory).md) E\[_e_<sup>_itX_</sup>\] = _e_<sup>_iμt_ − _σ_<sup>2</sup>_t_<sup>2</sup>/2</sup>}@} and {@{$\ln \operatorname{E} [e^{itX}] = i\mu t - \frac{1}{2} \sigma^2 t^2$}@}. <!--SR:!2026-03-15,17,323!2026-03-12,14,290!2026-03-12,14,290-->

### Stein operator and class

Within {@{[Stein's method](Stein's%20method.md)}@} (annotation: motivated by {@{the need for a practical way}@} to quantify {@{how close a distribution is to a target distribution}@} without {@{relying on explicit density formulas}@}) {@{the Stein operator and class}@} of {@{a random variable $X\sim {\mathcal {N} }(\mu ,\sigma ^{2})$}@} are {@{${\mathcal {A} }f(x)=\sigma ^{2}f'(x)-(x-\mu )f(x)$}@} (annotation: equivalently, {@{${\mathcal {A} } = \sigma ^{2}{\frac {d}{dx} } -(x-\mu ){\mathcal {I} }$}@} where {@{${\mathcal {I} }$ is the identity operator}@}⁠) and {@{${\mathcal {F} }$ the class}@} of {@{all absolutely continuous functions ⁠$\textstyle f:\mathbb {R} \to \mathbb {R}$⁠ such that ⁠$\operatorname {E} [\vert f'(X)\vert ]<\infty$⁠}@}. <!--SR:!2026-03-13,15,290!2026-03-13,15,316!2026-03-15,17,323!2026-03-14,16,323!2026-03-14,16,323!2026-03-13,15,316!2026-04-08,30,270!2026-05-07,57,323!2026-03-14,16,316!2026-03-12,14,290!2026-03-14,16,323-->

### zero-variance limit

In {@{the [limit](limit%20(mathematics).md) when $\sigma ^{2}$ approaches zero}@}, {@{the probability density $f$}@} approaches {@{zero everywhere except at $\mu$}@}, where it {@{approaches $\infty$, while its integral remains equal to 1}@}. {@{An extension of the normal distribution to the case with zero variance}@} can be defined using {@{the [Dirac delta measure](Dirac%20measure.md) $\delta _{\mu }$}@}, although {@{the resulting random variables}@} are {@{not [absolutely continuous](absolutely%20continuous%20random%20variable.md#absolutely%20continuous%20probability%20distribution)}@} and thus {@{do not have [probability density functions](probability%20density%20function.md)}@}. {@{The cumulative distribution function}@} of such a random variable is then {@{the [Heaviside step function](Heaviside%20step%20function.md) translated by the mean $\mu$}@}, namely {@{$$F(x)={\begin{cases}0&{\text{if } }x<\mu \\1&{\text{if } }x\geq \mu .\end{cases} }$$}@} <!--SR:!2026-03-14,16,316!2026-03-14,16,323!2026-03-15,17,323!2026-03-13,15,316!2026-03-13,15,316!2026-03-14,16,309!2026-03-16,18,323!2026-03-12,14,290!2026-03-13,15,314!2026-03-13,15,309!2026-03-12,14,290!2026-03-13,15,314-->

### maximum entropy

Of {@{all probability distributions}@} over {@{the reals with a specified finite mean ⁠$\mu$⁠ and finite variance ⁠$\sigma ^{2}$⁠}@}, {@{the normal distribution $N(\mu ,\sigma ^{2})$}@} is {@{the one with [maximum entropy](maximum%20entropy%20probability%20distribution.md)}@}.<sup>[\[21\]](#^ref-21)</sup> To see this, let ⁠$X$⁠ be {@{a [continuous random variable](continuous%20random%20variable.md#absolutely%20continuous%20probability%20distribution) with [probability density](probability%20density.md) ⁠$f(x)$}@}⁠. {@{The (annotation: differential) entropy of ⁠$X$}@}⁠ is defined as<sup>[\[32\]](#^ref-32)</sup><sup>[\[33\]](#^ref-33)</sup><sup>[\[34\]](#^ref-34)</sup> {@{$$H(X)=-\int _{-\infty }^{\infty }f(x)\ln f(x)\,dx\,,$$}@} where {@{$f(x)\log f(x)$ is understood to be zero whenever ⁠$f(x)=0$}@}⁠. {@{This functional can be maximized}@}, subject to the constraints that {@{the distribution is properly normalized and has a specified mean and variance}@}, by using {@{[variational calculus](variational%20calculus.md)}@}. {@{A function with three [Lagrange multipliers](Lagrange%20multipliers.md)}@} is defined: {@{$$L=-\int _{-\infty }^{\infty }f(x)\ln f(x)\,dx-\lambda _{0}\left(1-\int _{-\infty }^{\infty }f(x)\,dx\right)-\lambda _{1}\left(\mu -\int _{-\infty }^{\infty }f(x)x\,dx\right)-\lambda _{2}\left(\sigma ^{2}-\int _{-\infty }^{\infty }f(x)(x-\mu )^{2}\,dx\right)\,.$$}@} <!--SR:!2026-03-12,14,290!2026-03-14,16,311!2026-03-15,17,332!2026-03-14,16,316!2026-03-15,17,323!2026-03-15,17,323!2026-03-15,17,323!2026-03-12,14,290!2026-03-14,16,323!2026-03-12,14,290!2026-03-12,14,290!2026-03-12,14,290!2026-03-16,18,323-->

At {@{maximum entropy}@}, {@{a small variation $\delta f(x)$ about $f(x)$}@} will produce {@{a variation $\delta L$ about ⁠$L$⁠ which is equal to 0}@}: {@{$$0=\delta L=\int _{-\infty }^{\infty }\delta f(x)\left(-\ln f(x)-1+\lambda _{0}+\lambda _{1}x+\lambda _{2}(x-\mu )^{2}\right)\,dx\,.$$}@} Since this must {@{hold for any small ⁠$\delta f(x)$⁠}@}, {@{the factor multiplying ⁠$\delta f(x)$⁠ must be zero}@}, and {@{solving for ⁠$f(x)$}@}⁠ yields: {@{$$f(x)=\exp \left(-1+\lambda _{0}+\lambda _{1}x+\lambda _{2}(x-\mu )^{2}\right)\,.$$}@} {@{The Lagrange constraints}@} that ⁠{@{$f(x)$⁠ is properly normalized and has the specified mean and variance}@} are {@{satisfied if and only if ⁠$\lambda _{0}$⁠, ⁠$\lambda _{1}$⁠, and ⁠$\lambda _{2}$⁠}@} are chosen so that {@{$$f(x)={\frac {1}{\sqrt {2\pi \sigma ^{2} } } }e^{-{\frac {(x-\mu )^{2} }{2\sigma ^{2} } } }\,.$$}@} {@{The entropy of a normal distribution $X\sim N(\mu ,\sigma ^{2})$}@} is equal to {@{$$H(X)={\tfrac {1}{2} }(1+\ln 2\sigma ^{2}\pi )\,,$$ \(annotation: easy-to-remember form: $\frac 1 2 \log \det\!\left(2\pi e \sigma^2\right)$\)}@} which is {@{independent of the mean ⁠$\mu$⁠}@}. <!--SR:!2026-03-14,16,323!2026-03-16,18,323!2026-03-16,18,323!2026-03-12,14,290!2026-03-16,18,332!2026-03-14,16,316!2026-03-16,18,332!2026-03-13,15,316!2026-03-14,16,323!2026-03-14,16,311!2026-03-12,14,290!2026-03-14,16,323!2026-03-15,17,323!2026-05-03,54,332!2026-03-14,16,290-->

### other properties

1. If {@{the characteristic function $\phi _{X}$ of some random variable ⁠$X$⁠}@} is of {@{the form $\phi _{X}(t)=\exp Q(t)$ in a neighborhood of zero, where $Q(t)$ is a [polynomial](polynomial.md)}@}, then {@{the __Marcinkiewicz theorem__ \(named after [Józef Marcinkiewicz](Józef%20Marcinkiewicz.md)\) asserts that ⁠$Q$⁠ can be at most a quadratic polynomial}@}, and therefore {@{⁠$X$⁠ is a normal random variable}@}.<sup>[\[35\]](#^ref-35)</sup> {@{The consequence of this result}@} is that {@{the normal distribution}@} is {@{the only distribution with a finite number \(two\) of non-zero [cumulants](cumulant.md)}@}. <!--SR:!2026-03-16,18,323!2026-03-15,17,323!2026-03-12,14,290!2026-03-15,17,323!2026-03-12,14,290!2026-03-14,16,323!2026-03-16,18,323-->

2. If ⁠$X$⁠ and ⁠$Y$⁠ are {@{[jointly normal](jointly%20normal.md) and [uncorrelated](uncorrelated.md)}@}, then {@{they are [independent](independence%20(probability%20theory).md)}@}. {@{The requirement that ⁠$X$⁠ and ⁠$Y$⁠ should be _jointly_ normal}@} is {@{essential; without it the property does not hold}@}.<sup>[\[36\]](#^ref-36)</sup><sup>[\[37\]](#^ref-37)</sup><sup>[\[proof\]](normally%20distributed%20and%20uncorrelated%20does%20not%20imply%20independent.md)</sup> For {@{non-normal random variables}@} {@{uncorrelatedness does not imply independence}@}. <!--SR:!2026-03-13,15,311!2026-03-15,17,323!2026-03-14,16,316!2026-03-13,15,316!2026-03-15,17,323!2026-03-13,15,316-->

3. {@{The [Kullback–Leibler divergence](Kullback–Leibler%20divergence.md)}@} of {@{one normal distribution $X_{1}\sim N(\mu _{1},\sigma _{1}^{2})$ from another $X_{2}\sim N(\mu _{2},\sigma _{2}^{2})$}@} is given by:<sup>[\[38\]](#^ref-38)</sup> {@{$$D_{\mathrm {KL} }(X_{1}\parallel X_{2})={\frac {(\mu _{1}-\mu _{2})^{2} }{2\sigma _{2}^{2} } }+{\frac {1}{2} }\left({\frac {\sigma _{1}^{2} }{\sigma _{2}^{2} } }-1-\ln {\frac {\sigma _{1}^{2} }{\sigma _{2}^{2} } }\right)$$ \(annotation: form with the approximating distribution always dividing: $\frac 1 2 \left\{\frac {\sigma_1^2} {\sigma_2^2} + \frac {(\mu_1 - \mu_2)^2} {\sigma_2^2} - 1 - \ln \frac {\sigma_1^2} {\sigma_2^2} \right\}$\)}@} {@{The [Hellinger distance](Hellinger%20distance.md) between the same distributions}@} is equal to {@{$$H^{2}(X_{1},X_{2})=1-{\sqrt {\frac {2\sigma _{1}\sigma _{2} }{\sigma _{1}^{2}+\sigma _{2}^{2} } } }\exp \left(-{\frac {1}{4} }{\frac {(\mu _{1}-\mu _{2})^{2} }{\sigma _{1}^{2}+\sigma _{2}^{2} } }\right)$$}@} <!--SR:!2026-03-14,16,316!2026-04-13,37,312!2026-04-23,43,312!2026-03-12,14,290!2026-03-22,17,303-->

4. {@{The [Fisher information matrix](Fisher%20information%20matrix.md#matrix%20form)}@} for {@{a normal distribution w.r.t. ⁠$\mu$⁠ and $\sigma ^{2}$}@} is {@{diagonal and takes the form $${\mathcal {I} }(\mu ,\sigma ^{2})={\begin{pmatrix}{\frac {1}{\sigma ^{2} } }&0\\0&{\frac {1}{2\sigma ^{4} } }\end{pmatrix} }$$}@} <!--SR:!2026-03-13,15,316!2026-03-16,18,323!2026-03-13,15,316-->

5. {@{The [conjugate prior](conjugate%20prior.md) of the mean of a normal distribution}@} is {@{another normal distribution}@}.<sup>[\[39\]](#^ref-39)</sup> Specifically, if {@{$x_{1},\ldots ,x_{n}$ are iid $\sim N(\mu ,\sigma ^{2})$}@} and {@{the prior is $\mu \sim N(\mu _{0},\sigma _{0}^{2})$}@}, then {@{the posterior distribution for the estimator of ⁠$\mu$}@}⁠ will be {@{$$\mu \mid x_{1},\ldots ,x_{n}\sim {\mathcal {N} }\left({\frac { {\frac {\sigma ^{2} }{n} }\mu _{0}+\sigma _{0}^{2}{\bar {x} } }{ {\frac {\sigma ^{2} }{n} }+\sigma _{0}^{2} } },\left({\frac {n}{\sigma ^{2} } }+{\frac {1}{\sigma _{0}^{2} } }\right)^{-1}\right)$$}@} (annotation: {@{Multiply the numerator and denominator of the mean}@} by {@{$\tfrac{n}{\sigma^2\sigma_0^2}$}@} to obtain {@{the precision-weighted form $\mu_n=\frac{\tau_0\mu_0+\tau\,\bar x}{\tau_0+\tau}\quad\text{and}\quad \sigma_n^2=(\tau_0+\tau)^{-1},$}@} where {@{$\tau_0=1/\sigma_0^2$ (prior precision) and $\tau=n/\sigma^2$ (data precision)}@}; the reason {@{the data precision multiplies by $n$}@} is that for {@{$n$ i.i.d. normal observations with known variance $\sigma^2$}@}, {@{the variance of the sample mean is $\sigma^2/n$}@}, so {@{its precision—being the inverse variance—is $n/\sigma^2$}@}.) <!--SR:!2026-03-14,16,332!2026-03-16,18,323!2026-03-14,16,316!2026-03-15,17,323!2026-03-16,18,332!2026-03-14,16,316!2026-03-15,17,323!2026-04-21,43,290!2026-03-14,16,323!2026-03-15,17,323!2026-03-16,18,332!2026-03-12,14,290!2026-03-14,16,316!2026-03-14,16,316-->

6. {@{The family of normal distributions}@} not only {@{forms an [exponential family](exponential%20family.md) \(EF\)}@}, but in fact forms {@{a [natural exponential family](natural%20exponential%20family.md) \(NEF\) with quadratic [variance function](variance%20function.md) \([NEF-QVF](NEF-QVF.md#NEF-QVF)\)}@}. {@{Many properties of normal distributions}@} generalize to properties of {@{NEF-QVF distributions, NEF distributions, or EF distributions generally}@}. {@{NEF-QVF distributions}@} comprises {@{6 families}@}, including {@{Poisson, Gamma, binomial, and negative binomial distributions}@}, while {@{many of the common families studied in probability and statistics}@} are {@{NEF or EF}@}. <!--SR:!2026-03-13,15,316!2026-03-12,14,290!2026-03-15,17,316!2026-03-15,17,323!2026-03-13,15,290!2026-03-15,17,323!2026-03-16,18,323!2026-03-12,14,290!2026-03-14,16,323!2026-03-15,17,323-->

7. In {@{[information geometry](information%20geometry.md)}@}, {@{the family of normal distributions}@} forms {@{a [statistical manifold](statistical%20manifold.md) with [constant curvature](constant%20curvature.md) ⁠$-1$}@}⁠. {@{The same family is [flat](flat%20manifold.md)}@} with respect to {@{the \(±1\)-connections $\nabla ^{(e)}$ and $\nabla ^{(m)}$}@}.<sup>[\[40\]](#^ref-40)</sup> (annotation: TODO: What does this mean?) <!--SR:!2026-03-13,15,316!2026-03-16,18,323!2026-03-14,16,316!2026-03-13,15,311!2026-03-13,15,290-->

8. If {@{$X_{1},\dots ,X_{n}$ are distributed according to $N(0,\sigma ^{2})$}@}, then {@{$E[\max _{i}X_{i}]\leq \sigma {\sqrt {2\ln n} }$}@}. Note that there is {@{no assumption of independence}@}.<sup>[\[41\]](#^ref-41)</sup>  \(annotation: {@{A natural way to control a maximum}@} is to notice that {@{exponentials turn a hard, non-smooth object into something additive and easier to average}@}: {@{$\max_i X_i \le \tfrac1t \log\sum_i e^{tX_i}$}@}, so taking {@{expectations and using Jensen}@} gives {@{$\mathbb{E}[\max_i X_i]\le \tfrac1t\log\sum_i \mathbb{E}e^{tX_i}$}@}. For {@{Gaussian $X_i\sim N(0,\sigma^2)$}@}, {@{$\mathbb{E}e^{tX_i}=e^{\frac12\sigma^2 t^2}$}@}, yielding {@{the upper bound $\tfrac{\ln n}{t} + \tfrac12\sigma^2 t$}@}. To think of {@{the "right" $t$}@}, notice this expression has {@{the classic shape "something over $t$" plus "something times $t$"}@}—{@{a convex function minimized}@} when {@{the two terms balance}@}, which is {@{a common pattern in large deviations/entropy arguments}@}. Setting {@{$\tfrac{\ln n}{t} \approx \tfrac12\sigma^2 t$}@} immediately suggests {@{$t \approx \sqrt{2\ln n}/\sigma$}@}, and {@{plugging this in}@} gives {@{$\mathbb{E}[\max_i X_i]\le \sigma\sqrt{2\ln n}$}@}. {@{Independence}@} is never needed; only {@{Gaussian tails and the exponential trick}@}.\) <!--SR:!2026-03-14,16,323!2026-03-13,15,316!2026-03-14,16,323!2026-03-24,19,354!2026-03-22,17,354!2026-03-24,19,354!2026-03-22,17,354!2026-03-24,19,354!2026-03-24,19,354!2026-03-23,18,354!2026-03-23,18,354!2026-03-23,18,354!2026-03-22,17,354!2026-03-22,17,354!2026-03-22,17,354!2026-03-22,17,354!2026-03-22,17,354!2026-03-23,18,354!2026-03-23,18,354!2026-03-24,19,354!2026-03-22,17,354!2026-03-22,17,354-->

## related distributions

### central limit theorem

> {@{![As the number of discrete events increases, the function begins to resemble a normal distribution.](../../archives/Wikimedia%20Commons/De%20moivre-laplace.gif)}@}
>
> As {@{the number of discrete events increases}@}, the function {@{begins to resemble a normal distribution}@}. <!--SR:!2026-03-12,14,290!2026-03-14,16,311!2026-03-13,15,309-->

<!-- markdownlint MD028 -->

> {@{![Comparison of probability density functions, _p_\(_k_\) for the sum of _n_ fair 6-sided dice to show their convergence to a normal distribution with increasing _na_, in accordance to the central limit theorem.](../../archives/Wikimedia%20Commons/Dice%20sum%20central%20limit%20theorem.svg)}@}
>
> Comparison of {@{probability density functions, _p_\(_k_\)}@} for {@{the sum of _n_ fair 6-sided dice}@} to show their {@{convergence to a normal distribution with increasing _na_}@}, in accordance to {@{the central limit theorem}@}. In {@{the bottom-right graph}@}, {@{smoothed profiles of the previous graphs}@} are {@{rescaled, superimposed and compared with a normal distribution \(black curve\)}@}. <!--SR:!2026-03-12,14,290!2026-03-12,14,290!2026-03-12,14,290!2026-03-16,18,323!2026-03-15,17,323!2026-03-15,17,332!2026-03-15,17,332!2026-03-15,17,323-->

- Main article: ::@:: [Central limit theorem](central%20limit%20theorem.md) <!--SR:!2026-03-15,17,323!2026-03-16,18,323-->

{@{The central limit theorem}@} states that under {@{certain \(fairly common\) conditions}@}, {@{the sum of many random variables}@} will have {@{an approximately normal distribution}@}. More specifically, where {@{$X_{1},\ldots ,X_{n}$ are [independent and identically distributed](independent%20and%20identically%20distributed.md) random variables}@} with {@{the same arbitrary distribution, zero mean, and variance $\sigma ^{2}$}@} and ⁠$Z$⁠ is {@{their mean scaled by ${\sqrt {n} }$}@} {@{$$Z={\sqrt {n} }\left({\frac {1}{n} }\sum _{i=1}^{n}X_{i}\right)$$}@} Then, as {@{⁠$n$⁠ increases}@}, {@{the probability distribution of ⁠$Z$}@}⁠ will tend to {@{the normal distribution with zero mean and variance ⁠$\sigma ^{2}$⁠}@}. <!--SR:!2026-03-15,17,332!2026-03-16,18,332!2026-03-16,18,323!2026-03-13,15,316!2026-03-15,17,332!2026-03-12,14,290!2026-03-13,15,316!2026-03-12,14,290!2026-03-16,18,332!2026-03-15,17,323!2026-03-15,17,332-->

{@{The theorem}@} can be extended to {@{variables $(X_{i})$ that are not independent and/or not identically distributed}@} if {@{certain constraints are placed}@} on {@{the degree of dependence and the moments of the distributions}@}. <!--SR:!2026-03-12,14,290!2026-03-16,18,323!2026-03-12,14,290!2026-03-14,16,323-->

{@{Many [test statistics](test%20statistic.md), [scores](score%20(statistics).md), and [estimators](estimator.md)}@} encountered in practice contain {@{sums of certain random variables in them}@}, and {@{even more estimators}@} can be represented as {@{sums of random variables through the use of [influence functions](influence%20function%20(statistics).md#empirical%20influence%20function)}@}. {@{The central limit theorem}@} implies that {@{those statistical parameters will have asymptotically normal distributions}@}. <!--SR:!2026-03-15,17,323!2026-03-14,16,316!2026-03-16,18,323!2026-03-16,18,332!2026-03-13,15,316!2026-03-15,17,323-->

{@{The central limit theorem}@} also implies that {@{certain distributions}@} can be {@{approximated by the normal distribution}@}, for example: (annotation: 4 items: {@{binomial distribution, Poisson distribution, chi-squared distribution, and Student's t-distribution}@}) <!--SR:!2026-03-15,17,323!2026-03-15,17,323!2026-03-12,14,290!2026-03-13,15,290-->

- The [binomial distribution](binomial%20distribution.md) $B(n,p)$ ::@:: is [approximately normal](De%20Moivre–Laplace%20theorem.md) with mean $np$ and variance $np(1-p)$ for large ⁠$n$⁠ and for ⁠$p$⁠ not too close to 0 or 1. <!--SR:!2026-03-12,14,290!2026-03-16,18,323-->
- The [Poisson distribution](Poisson%20distribution.md) with parameter ⁠$\lambda$⁠ ::@:: is approximately normal with mean ⁠$\lambda$⁠ and variance ⁠$\lambda$⁠, for large values of ⁠$\lambda$⁠.<sup>[\[42\]](#^ref-42)</sup> <!--SR:!2026-03-12,14,290!2026-03-15,17,323-->
- The [chi-squared distribution](chi-squared%20distribution.md) $\chi ^{2}(k)$ ::@:: is approximately normal with mean ⁠$k$⁠ and variance $2k$, for large ⁠$k$⁠. <!--SR:!2026-03-15,17,323!2026-03-14,16,332-->
- The [Student's t-distribution](Student's%20t-distribution.md) $t(\nu )$ ::@:: is approximately normal with mean 0 and variance 1 when ⁠$\nu$⁠ is large. <!--SR:!2026-03-14,16,311!2026-03-13,15,309-->

Whether {@{these approximations are sufficiently accurate}@} depends on {@{the purpose for which they are needed}@}, and {@{the rate of convergence}@} to {@{the normal distribution}@}. It is typically the case that {@{such approximations are less accurate}@} in {@{the tails of the distribution}@}. <!--SR:!2026-03-16,18,323!2026-03-16,18,323!2026-03-14,16,323!2026-03-16,18,323!2026-03-15,17,323!2026-03-16,18,332-->

{@{A general upper bound for the approximation error}@} in the central limit theorem is given by {@{the [Berry–Esseen theorem](Berry–Esseen%20theorem.md)}@}, {@{improvements of the approximation}@} are given by {@{the [Edgeworth expansions](Edgeworth%20expansion.md)}@}. <!--SR:!2026-03-15,17,323!2026-03-16,18,323!2026-03-14,16,323!2026-03-14,16,323-->

{@{This theorem}@} can also be used to justify {@{modeling the sum of many uniform noise sources as [Gaussian noise](gaussian%20noise.md)}@}. See {@{[AWGN](AWGN.md)}@}. <!--SR:!2026-03-15,17,323!2026-03-13,15,316!2026-03-13,15,311-->

### operations and functions of normal variables

#### operations on a single normal variable

If ⁠{@{$X$⁠}@} is {@{distributed normally with mean ⁠$\mu$⁠ and variance $\sigma ^{2}$}@}, then <!--SR:!2026-03-16,18,332!2026-03-16,18,323-->

- {@{$aX+b$, for any real numbers ⁠$a$⁠ and ⁠$b$}@}⁠, is {@{also normally distributed, with mean $a\mu +b$ and variance $a^{2}\sigma ^{2}$}@}. That is, {@{the family of normal distributions}@} is {@{closed under [linear transformations](linear%20transformations.md)}@}.
- {@{The exponential of ⁠$X$⁠}@} is {@{distributed [log-normally](log-normal%20distribution.md)}@}: {@{$e^{X}\sim \ln(N(\mu ,\sigma ^{2}))$}@}.
- {@{The standard [sigmoid](logistic%20function.md) of ⁠$X$⁠}@} is {@{[logit-normally distributed](logit-normal%20distribution.md)}@}: {@{$\sigma (X)\sim P({\mathcal {N} }(\mu ,\,\sigma ^{2}))$}@}.
- {@{The absolute value of ⁠$X$}@}⁠ has {@{[folded normal distribution](folded%20normal%20distribution.md)}@}: {@{${\left|X\right|\sim N_{f}(\mu ,\sigma ^{2})}$}@}. If {@{$\mu =0$}@} this is known as {@{the [half-normal distribution](half-normal%20distribution.md)}@}.
- {@{The absolute value of normalized residuals, $|X-\mu |/\sigma$}@}, has {@{[chi distribution](chi%20distribution.md) with one degree of freedom}@}: {@{$|X-\mu |/\sigma \sim \chi _{1}$}@}.
- {@{The square of $X/\sigma$}@} has {@{the [noncentral chi-squared distribution](noncentral%20chi-squared%20distribution.md) with one degree of freedom}@}: {@{$X^{2}/\sigma ^{2}\sim \chi _{1}^{2}(\mu ^{2}/\sigma ^{2})$}@}. If {@{$\mu =0$}@}, the distribution is called simply {@{[chi-squared](chi-squared%20distribution.md)}@}.
- {@{The log-likelihood of a normal variable ⁠$x$}@}⁠ is simply {@{the log of its [probability density function](probability%20density%20function.md)}@}: {@{$$\ln p(x)=-{\frac {1}{2} }\left({\frac {x-\mu }{\sigma } }\right)^{2}-\ln \left(\sigma {\sqrt {2\pi } }\right).$$}@} Since this is {@{a scaled and shifted square of a standard normal variable}@}, it is distributed as {@{a scaled and shifted [chi-squared](chi-squared%20distribution.md) variable}@}.
- {@{The distribution of the variable ⁠$X$⁠}@} {@{restricted to an interval $[a,b]$}@} is called {@{the [truncated normal distribution](truncated%20normal%20distribution.md)}@}.
- {@{$(X-\mu )^{-2}$}@} has {@{a [Lévy distribution](Lévy%20distribution.md)}@} with {@{location 0 and scale $\sigma ^{-2}$}@}. <!--SR:!2026-03-12,14,290!2026-03-14,16,316!2026-03-13,15,316!2026-03-14,16,316!2026-03-15,17,323!2026-03-13,15,316!2026-03-15,17,311!2026-03-16,18,323!2026-03-16,18,332!2026-03-13,15,311!2026-03-15,17,316!2026-03-12,14,290!2026-03-15,17,332!2026-03-12,14,290!2026-03-12,14,290!2026-03-16,18,323!2026-03-13,15,314!2026-03-13,15,316!2026-03-12,14,290!2026-03-15,17,311!2026-03-16,18,323!2026-03-15,17,323!2026-03-15,17,323!2026-03-14,16,290!2026-03-15,17,323!2026-04-26,48,323!2026-03-12,14,290!2026-03-12,14,290!2026-03-15,17,332!2026-03-14,16,323!2026-03-14,16,323!2026-03-16,18,323!2026-03-14,16,332!2026-03-13,15,316-->

##### operations on two independent normal variables

- If $X_{1}$ and $X_{2}$ are {@{two [independent](independence%20(probability%20theory).md) normal random variables}@}, with {@{means $\mu _{1}$, $\mu _{2}$ and variances $\sigma _{1}^{2}$, $\sigma _{2}^{2}$}@}, then {@{their sum $X_{1}+X_{2}$ will also be normally distributed}@},<sup>[\[proof\]](sum%20of%20normally%20distributed%20random%20variables.md)</sup> with {@{mean $\mu _{1}+\mu _{2}$ and variance $\sigma _{1}^{2}+\sigma _{2}^{2}$}@}.
- In particular, if ⁠$X$⁠ and ⁠$Y$⁠ are {@{independent normal deviates with zero mean and variance $\sigma ^{2}$}@}, then {@{$X+Y$ and $X-Y$ are also independent and normally distributed}@}, with {@{zero mean and variance $2\sigma ^{2}$}@}. This is a special case of {@{the [polarization identity](polarization%20identity.md)}@}.<sup>[\[43\]](#^ref-43)</sup>
- If $X_{1}$, $X_{2}$ are {@{two independent normal deviates with mean ⁠$\mu$⁠ and variance $\sigma ^{2}$}@}, and ⁠$a$⁠, ⁠$b$⁠ are {@{arbitrary real numbers}@}, then {@{the variable $$X_{3}={\frac {aX_{1}+bX_{2}-(a+b)\mu }{\sqrt {a^{2}+b^{2} } } }+\mu$$}@} is {@{also normally distributed with mean ⁠$\mu$⁠ and variance $\sigma ^{2}$}@}. It follows that {@{the normal distribution is [stable](stable%20distribution.md)}@} \(with {@{exponent $\alpha =2$}@}\).
- If {@{$X_{k}\sim {\mathcal {N} }(m_{k},\sigma _{k}^{2})$, $k\in \{0,1\}$}@} are {@{normal distributions}@}, then {@{their normalized [geometric mean](geometric%20mean.md)}@} {@{${\frac {1}{\int _{\mathbb {R} ^{n} }X_{0}^{\alpha }(x)X_{1}^{1-\alpha }(x)\,{\text{d} }x} }X_{0}^{\alpha }X_{1}^{1-\alpha }$}@} (annotation: {@{The notation $X_0^\alpha X_1^{1 - \alpha}$}@} does not mean {@{exponentiation and multiplication of random variables}@}, but rather {@{the pointwise exponentiation and multiplication of their density functions}@}. Hence the need for {@{the normalization factor in front}@}.) is {@{a normal distribution ${\mathcal {N} }(m_{\alpha },\sigma _{\alpha }^{2})$}@} with {@{$m_{\alpha }={\frac {\alpha m_{0}\sigma _{1}^{2}+(1-\alpha )m_{1}\sigma _{0}^{2} }{\alpha \sigma _{1}^{2}+(1-\alpha )\sigma _{0}^{2} } }$ and $\sigma _{\alpha }^{2}={\frac {\sigma _{0}^{2}\sigma _{1}^{2} }{\alpha \sigma _{1}^{2}+(1-\alpha )\sigma _{0}^{2} } }$}@}. (annotation: Using precision {@{⁠$\tau _{k}={\frac {1}{\sigma _{k}^{2} } }$⁠}@} for {@{$k\in \{0,1\}$}@}, this can be rewritten as {@{$m_{\alpha }={\frac {\alpha \tau _{0}m_{0}+(1-\alpha )\tau _{1}m_{1} }{\alpha \tau _{0}+(1-\alpha )\tau _{1} } }$ and $\tau _{\alpha }=\alpha \tau _{0}+(1-\alpha )\tau _{1}$}@}. {@{This form for mean and variance}@} is more easily remembered as {@{a precision-weighted average of the means, and a precision-weighted harmonic mean of the variances}@}.) <!--SR:!2026-03-13,15,316!2026-03-14,16,332!2026-03-15,17,323!2026-03-13,15,316!2026-04-20,43,314!2026-03-14,16,316!2026-03-15,17,323!2026-03-12,14,290!2026-03-16,18,332!2026-03-16,18,332!2026-03-16,18,323!2026-03-12,14,290!2026-03-15,17,323!2026-03-15,17,316!2026-03-16,18,323!2026-03-14,16,323!2026-03-14,16,316!2026-05-07,57,323!2026-03-13,15,314!2026-03-12,14,290!2026-03-12,14,290!2026-03-14,16,316!2026-03-14,16,323!2026-03-15,17,323!2026-03-15,17,332!2026-03-16,18,323!2026-03-15,17,323!2026-04-15,38,290!2026-03-15,17,332-->

##### operations on two independent standard normal variables

If {@{$X_{1}$ and $X_{2}$}@} are {@{two independent standard normal random variables with mean 0 and variance 1}@}, then <!--SR:!2026-03-16,18,323!2026-03-12,14,290-->

- {@{Their sum and difference}@} is distributed {@{normally with mean zero and variance two}@}: {@{$X_{1}\pm X_{2}\sim {\mathcal {N} }(0,2)$}@}.
- {@{Their product $Z=X_{1}X_{2}$}@} follows {@{the [product distribution](product%20distribution.md#independent%20central-normal%20distributions)}@}<sup>[\[44\]](#^ref-44)</sup> with {@{density function $f_{Z}(z)=\pi ^{-1}K_{0}(|z|)$}@} where {@{$K_{0}$ is the [modified Bessel function of the second kind](Macdonald%20function.md#modified%20Bessel%20functions)}@}. This distribution is {@{symmetric around zero, unbounded at $z=0$}@}, and has {@{the [characteristic function](characteristic%20function%20(probability%20theory).md) $\phi _{Z}(t)=(1+t^{2})^{-1/2}$}@}.
- {@{Their ratio}@} follows {@{the standard [Cauchy distribution](Cauchy%20distribution.md)}@}: {@{$X_{1}/X_{2}\sim \operatorname {Cauchy} (0,1)$}@}.
- {@{Their Euclidean norm ${\sqrt {X_{1}^{2}+X_{2}^{2} } }$}@} has {@{the [Rayleigh distribution](Rayleigh%20distribution.md)}@}. <!--SR:!2026-03-14,16,323!2026-03-15,17,323!2026-03-15,17,332!2026-03-16,18,323!2026-03-16,18,323!2026-03-16,18,323!2026-03-12,14,290!2026-03-14,16,309!2026-04-23,43,312!2026-03-12,14,290!2026-03-15,17,323!2026-04-15,38,290!2026-03-12,14,290!2026-03-15,17,332-->

#### operations on multiple independent normal variables

- {@{Any [linear combination](linear%20combination.md)}@} of {@{independent normal deviates}@} is {@{a normal deviate}@}.
- If {@{$X_{1},X_{2},\ldots ,X_{n}$}@} are {@{independent standard normal random variables}@}, then {@{the sum of their squares}@} has {@{the [chi-squared distribution](chi-squared%20distribution.md) with ⁠$n$⁠ degrees of freedom}@} {@{$$X_{1}^{2}+\cdots +X_{n}^{2}\sim \chi _{n}^{2}.$$}@}
- If {@{$X_{1},X_{2},\ldots ,X_{n}$}@} are {@{independent normally distributed random variables with means ⁠$\mu$⁠ and variances $\sigma ^{2}$}@}, then their {@{[sample mean](sample%20mean.md) is independent from the sample [standard deviation](standard%20deviation.md)}@},<sup>[\[45\]](#^ref-45)</sup> which can be demonstrated using {@{[Basu's theorem](Basu's%20theorem.md) or [Cochran's theorem](Cochran's%20theorem.md)}@}.<sup>[\[46\]](#^ref-46)</sup> {@{The ratio of these two quantities}@} will have {@{the [Student's t-distribution](Student's%20t-distribution.md) with $n-1$ degrees of freedom}@}: {@{$$t={\frac { {\overline {X} }-\mu }{S/{\sqrt {n} } } }={\frac { {\frac {1}{n} }(X_{1}+\cdots +X_{n})-\mu }{\sqrt { {\frac {1}{n(n-1)} }\left[(X_{1}-{\overline {X} })^{2}+\cdots +(X_{n}-{\overline {X} })^{2}\right]} } }\sim t_{n-1}.$$}@}
- If {@{$X_{1},X_{2},\ldots ,X_{n}$, $Y_{1},Y_{2},\ldots ,Y_{m}$}@} are {@{independent standard normal random variables}@}, then {@{the ratio of their normalized sums of squares}@} will have {@{the [F-distribution](F-distribution.md) with \(_n_, _m_\) degrees of freedom}@}:<sup>[\[47\]](#^ref-47)</sup> {@{$$F={\frac {\left(X_{1}^{2}+X_{2}^{2}+\cdots +X_{n}^{2}\right)/n}{\left(Y_{1}^{2}+Y_{2}^{2}+\cdots +Y_{m}^{2}\right)/m} }\sim F_{n,m}.$$}@} <!--SR:!2026-03-16,18,323!2026-05-05,55,316!2026-03-15,17,323!2026-03-13,15,316!2026-03-13,15,316!2026-03-16,18,323!2026-03-12,14,290!2026-03-12,14,290!2026-03-12,14,290!2026-03-12,14,290!2026-03-14,16,323!2026-03-13,15,316!2026-03-13,15,316!2026-03-15,17,323!2026-03-16,18,332!2026-03-14,16,316!2026-03-15,17,323!2026-03-15,17,323!2026-03-13,15,311!2026-03-14,16,316-->

#### operations on multiple correlated normal variables

- {@{A [quadratic form](quadratic%20form.md) of a normal vector}@}, i.e. {@{a quadratic function $q=\sum x_{i}^{2}+\sum x_{j}+c$}@} of {@{multiple independent or correlated normal variables}@}, is {@{a [generalized chi-square](generalized%20chi-square%20distribution.md) variable}@}. <!--SR:!2026-03-13,15,316!2026-03-15,17,323!2026-03-12,14,290!2026-03-15,17,332-->

### operations on the density function

{@{The [split normal distribution](split%20normal%20distribution.md)}@} is most directly defined in terms of {@{joining scaled sections of the density functions of different normal distributions}@} and {@{rescaling the density to integrate to one}@}. {@{The [truncated normal distribution](truncated%20normal%20distribution.md)}@} results from {@{rescaling a section of a single density function}@}. <!--SR:!2026-03-16,18,323!2026-03-16,18,323!2026-03-14,16,323!2026-03-16,18,323!2026-03-16,18,323-->

### infinite divisibility and Cramér's theorem

For {@{any positive integer _n_}@}, {@{any normal distribution with mean ⁠$\mu$⁠ and variance $\sigma ^{2}$}@} is {@{the distribution of the sum of _n_ independent normal deviates}@}, each with {@{mean ${\frac {\mu }{n} }$ and variance ${\frac {\sigma ^{2} }{n} }$}@}. This property is called {@{[infinite divisibility](infinite%20divisibility%20(probability).md)}@}.<sup>[\[48\]](#^ref-48)</sup> <!--SR:!2026-03-15,17,316!2026-03-16,18,323!2026-03-12,14,290!2026-03-13,15,314!2026-03-15,17,332-->

Conversely, if {@{$X_{1}$ and $X_{2}$ are independent random variables and their sum $X_{1}+X_{2}$ has a normal distribution}@}, then {@{both $X_{1}$ and $X_{2}$ must be normal deviates}@}.<sup>[\[49\]](#^ref-49)</sup> <!--SR:!2026-03-16,18,323!2026-03-14,16,316-->

This result is known as {@{[Cramér's decomposition theorem](Cramér's%20decomposition%20theorem.md)}@}, and is equivalent to saying that {@{the [convolution](convolution.md) of two distributions is normal if and only if both are normal}@}. Cramér's theorem implies that {@{a linear combination of independent non-Gaussian variables}@} will {@{never have an exactly normal distribution}@}, although it {@{may approach it arbitrarily closely}@}.<sup>[\[35\]](#^ref-35)</sup> <!--SR:!2026-03-15,17,332!2026-03-15,17,323!2026-03-16,18,323!2026-03-14,16,332!2026-03-14,16,323-->

### the Kac–Bernstein theorem

{@{The [Kac–Bernstein theorem](Kac–Bernstein%20theorem.md)}@} states that if {@{$X$ and ⁠$Y$⁠ are independent and $X+Y$ and $X-Y$ are also independent}@}, then {@{both _X_ and _Y_ must necessarily have normal distributions}@}.<sup>[\[50\]](#^ref-50)</sup><sup>[\[51\]](#^ref-51)</sup> <!--SR:!2026-03-13,15,316!2026-03-13,15,309!2026-03-14,16,323-->

More generally, if {@{$X_{1},\ldots ,X_{n}$ are independent random variables}@}, then {@{two distinct linear combinations $\sum {a_{k}X_{k} }$ and $\sum {b_{k}X_{k} }$ will be independent}@} {@{if and only if all $X_{k}$ are normal and $\sum {a_{k}b_{k}\sigma _{k}^{2}=0}$}@}, (annotation: {@{The second condition}@} means that {@{the two linear combinations must be uncorrelated}@}.) where {@{$\sigma _{k}^{2}$ denotes the variance of $X_{k}$}@}.<sup>[\[50\]](#^ref-50)</sup> (annotation: The intuition is that {@{the independence of two linear combinations of independent random variables}@} is {@{a very strong condition that can only be satisfied if the original variables are normal}@}.) <!--SR:!2026-03-12,14,290!2026-03-12,14,290!2026-03-16,18,323!2026-03-16,18,323!2026-03-16,18,323!2026-03-16,18,323!2026-03-16,18,323!2026-03-12,14,290-->

### extensions

The notion of {@{normal distribution}@}, being {@{one of the most important distributions in probability theory}@}, has been {@{extended far beyond the standard framework of the univariate \(that is one-dimensional\) case}@} \(Case 1\). {@{All these extensions}@} are also called {@{_normal_ or _Gaussian_ laws}@}, so {@{a certain ambiguity in names}@} exists. <!--SR:!2026-03-14,16,332!2026-03-13,15,316!2026-03-15,17,323!2026-03-12,14,290!2026-03-16,18,323!2026-03-14,16,323-->

- {@{The [multivariate normal distribution](multivariate%20normal%20distribution.md)}@} describes {@{the Gaussian law in the _k_-dimensional [Euclidean space](Euclidean%20space.md)}@}. {@{A vector _X_ ∈ __R__<sup>_k_</sup> is multivariate-normally distributed}@} if {@{any linear combination of its components Σ<sup>_k_</sup><sub>_j_=1</sub>_a_<sub>_j_</sub> _X_<sub>_j_</sub> has a \(univariate\) normal distribution}@}. {@{The variance of _X_}@} is {@{a _k_<!-- markdown separator --> × <!-- markdown separator -->_k_ symmetric positive-definite matrix _V_}@}. {@{The multivariate normal distribution}@} is a special case of {@{the [elliptical distributions](elliptical%20distribution.md)}@}. As such, its {@{iso-density loci in the _k_ = 2}@} case are {@{[ellipses](ellipse.md)}@} and in the case of {@{arbitrary _k_ are [ellipsoids](ellipsoid.md)}@}. <!--SR:!2026-03-16,18,323!2026-03-16,18,332!2026-03-14,16,323!2026-03-12,14,290!2026-03-13,15,316!2026-03-15,17,323!2026-03-14,16,323!2026-03-12,14,290!2026-03-16,18,323!2026-03-15,17,332!2026-03-16,18,323-->

- {@{[Rectified Gaussian distribution](rectified%20Gaussian%20distribution.md)}@} {@{a rectified version of normal distribution}@} with {@{all the negative elements reset to 0}@}. <!--SR:!2026-03-13,15,316!2026-03-16,18,323!2026-03-16,18,323-->

- {@{[Complex normal distribution](complex%20normal%20distribution.md)}@} deals with {@{the complex normal vectors}@}. {@{A complex vector _X_ ∈ __C__<sup>_k_</sup> is said to be normal}@} if {@{both its real and imaginary components jointly possess a 2<!-- markdown separator -->_k_-dimensional multivariate normal distribution}@}. {@{The variance-covariance structure of _X_}@} is described by {@{two matrices: the _variance_ matrix Γ, and the _relation_ matrix _C_}@}. <!--SR:!2026-04-22,42,312!2026-03-13,15,314!2026-03-14,16,332!2026-03-16,18,332!2026-03-12,14,290!2026-03-16,18,323-->

- {@{[Matrix normal distribution](matrix%20normal%20distribution.md)}@} describes {@{the case of normally distributed matrices}@}. <!--SR:!2026-03-13,15,316!2026-03-14,16,323-->

- {@{[Gaussian processes](Gaussian%20process.md)}@} are {@{the normally distributed [stochastic processes](stochastic%20process.md)}@}. These can be viewed as {@{elements of some infinite-dimensional [Hilbert space](Hilbert%20space.md) _H_}@}, and thus are {@{the analogues of multivariate normal vectors for the case _k_ = ∞}@}. {@{A random element _h_ ∈ _H_ is said to be normal}@} if {@{for any constant _a_ ∈ _H_ the [scalar product](scalar%20product.md) \(_a_, _h_\) has a \(univariate\) normal distribution}@}. {@{The variance structure of such Gaussian random element}@} can be described in terms of {@{the linear _covariance operator_ _K_: _H_ → _H_}@}. {@{Several Gaussian processes became popular enough}@} to {@{have their own names}@}: (annotation: 3 items: {@{Brownian motion (Wiener process), Brownian bridge, Ornstein–Uhlenbeck process}@})
  - [Brownian motion](Wiener%20process.md);
  - [Brownian bridge](Brownian%20bridge.md); and
  - [Ornstein–Uhlenbeck process](Ornstein–Uhlenbeck%20process.md). <!--SR:!2026-04-13,34,294!2026-03-16,18,323!2026-03-12,14,290!2026-03-12,14,290!2026-03-16,18,323!2026-03-13,15,316!2026-03-14,16,323!2026-03-16,18,332!2026-03-16,18,323!2026-03-14,16,323!2026-03-15,17,332-->

- {@{[Gaussian q-distribution](Gaussian%20q-distribution.md)}@} is {@{an abstract mathematical construction}@} that represents {@{a [q-analogue](q-analogue.md) of the normal distribution}@}. <!--SR:!2026-04-15,38,290!2026-03-12,14,290!2026-03-12,14,290-->

- {@{the [q-Gaussian](q-Gaussian.md)}@} is {@{an analogue of the Gaussian distribution}@}, in the sense that it {@{maximises the [Tsallis entropy](Tsallis%20entropy.md), and is one type of [Tsallis distribution](Tsallis%20distribution.md)}@}. This distribution is different from {@{the [Gaussian q-distribution](Gaussian%20q-distribution.md)}@} above. <!--SR:!2026-03-15,17,332!2026-03-16,18,323!2026-03-15,17,323!2026-03-15,17,332-->

- {@{The [Kaniadakis _κ_-Gaussian distribution](Kaniadakis%20Gaussian%20distribution.md)}@} is {@{a generalization of the Gaussian distribution}@} which arises from {@{the [Kaniadakis statistics](Kaniadakis%20statistics.md), being one of the [Kaniadakis distributions](Kaniadakis%20distribution.md)}@}. <!--SR:!2026-03-13,15,316!2026-03-15,17,332!2026-03-15,17,316-->

A random variable _X_ has {@{a two-piece normal distribution}@} if it has {@{a distribution $$f_{X}(x)={\begin{cases}N(\mu ,\sigma _{1}^{2}),&{\text{ if } }x\leq \mu \\N(\mu ,\sigma _{2}^{2}),&{\text{ if } }x\geq \mu \end{cases} }$$}@} where _μ_ is {@{the mean (annotation: which is not its _actual_ mean E\(_X_\))}@} and $\sigma_1^2$ and $\sigma_2^2$ are {@{the variances of the distribution to the left and right of the mean respectively}@}. <!--SR:!2026-03-16,18,332!2026-03-12,14,290!2026-03-12,14,290!2026-03-15,17,316-->

{@{The mean E\(_X_\), variance V\(_X_\), and third central moment T\(_X_\)}@} of this distribution have been determined<sup>[\[52\]](#^ref-52)</sup> {@{$${\begin{aligned}\operatorname {E} (X)&=\mu +{\sqrt {\frac {2}{\pi } } }(\sigma _{2}-\sigma _{1}),\\\operatorname {V} (X)&=\left(1-{\frac {2}{\pi } }\right)(\sigma _{2}-\sigma _{1})^{2}+\sigma _{1}\sigma _{2},\\\operatorname {T} (X)&={\sqrt {\frac {2}{\pi } } }(\sigma _{2}-\sigma _{1})\left[\left({\frac {4}{\pi } }-1\right)(\sigma _{2}-\sigma _{1})^{2}+\sigma _{1}\sigma _{2}\right].\end{aligned} }$$}@} <!--SR:!2026-03-12,14,290!2026-04-26,47,290-->

One of {@{the main practical uses of the Gaussian law}@} is to model {@{the empirical distributions of many different random variables encountered in practice}@}. In such case {@{a possible extension}@} would be {@{a richer family of distributions}@}, having {@{more than two parameters}@} and therefore being able to fit {@{the empirical distribution more accurately}@}. {@{The examples of such extensions}@} are: (annotation: 2 items: {@{Pearson distribution, generalized normal distribution}@}) <!--SR:!2026-03-13,15,311!2026-03-12,14,290!2026-03-16,18,323!2026-03-12,14,290!2026-03-14,16,316!2026-03-13,15,314!2026-03-12,14,290!2026-03-16,18,323-->

- [Pearson distribution](Pearson%20distribution.md) ::@:: — a four-parameter family of probability distributions that extend the normal law to include different skewness and kurtosis values. <!--SR:!2026-03-14,16,323!2026-03-14,16,323-->
- The [generalized normal distribution](generalized%20normal%20distribution.md), also known as the exponential power distribution, ::@:: allows for distribution tails with thicker or thinner asymptotic behaviors. <!--SR:!2026-03-13,15,316!2026-03-13,15,311-->

## statistical inference

### estimation of parameters

- See also: ::@:: [Maximum likelihood § Continuous distribution, continuous parameter space](maximum%20likelihood.md#continuous%20distribution,%20continuous%20parameter%20space); and [Gaussian function § Estimation of parameters](Gaussian%20function.md#estimation%20of%20parameters) <!--SR:!2026-03-12,14,290!2026-03-16,18,323-->

It is often the case that we do not {@{know the parameters of the normal distribution}@}, but instead want to {@{[estimate](estimation%20theory.md) them}@}. That is, having {@{a sample $(x_{1},\ldots ,x_{n})$ from a normal ${\mathcal {N} }(\mu ,\sigma ^{2})$ population}@} we would like to learn {@{the approximate values of parameters ⁠$\mu$⁠ and $\sigma ^{2}$}@}. {@{The standard approach}@} to this problem is {@{the [maximum likelihood](maximum%20likelihood.md) method}@}, which requires {@{maximization of the _[log-likelihood function](log-likelihood%20function.md#log-likelihood)_}@}: {@{$$\ln {\mathcal {L} }(\mu ,\sigma ^{2})=\sum _{i=1}^{n}\ln f(x_{i}\mid \mu ,\sigma ^{2})=-{\frac {n}{2} }\ln(2\pi )-{\frac {n}{2} }\ln \sigma ^{2}-{\frac {1}{2\sigma ^{2} } }\sum _{i=1}^{n}(x_{i}-\mu )^{2}.$$}@} Taking {@{derivatives with respect to ⁠$\mu$⁠ and $\sigma ^{2}$ and solving the resulting system of first order conditions}@} yields {@{the _maximum likelihood estimates_}@}: {@{$${\hat {\mu } }={\overline {x} }\equiv {\frac {1}{n} }\sum _{i=1}^{n}x_{i},\qquad {\hat {\sigma } }^{2}={\frac {1}{n} }\sum _{i=1}^{n}(x_{i}-{\overline {x} })^{2}.$$}@} Then {@{$\ln {\mathcal {L} }({\hat {\mu } },{\hat {\sigma } }^{2})$}@} is as follows: {@{$$\ln {\mathcal {L} }({\hat {\mu } },{\hat {\sigma } }^{2})=(-n/2)[\ln(2\pi {\hat {\sigma } }^{2})+1]$$}@} <!--SR:!2026-03-12,14,290!2026-03-16,18,332!2026-03-15,17,332!2026-03-15,17,323!2026-03-13,15,316!2026-03-14,16,311!2026-03-12,14,290!2026-03-14,16,323!2026-03-13,15,316!2026-03-15,17,323!2026-03-13,15,309!2026-03-16,18,323!2026-03-16,18,323-->

#### sample mean

- See also: ::@:: [Standard error of the mean](standard%20error%20of%20the%20mean.md#standard%20error%20of%20the%20sample%20mean) <!--SR:!2026-03-12,14,290!2026-03-12,14,290-->

{@{Estimator $\textstyle {\hat {\mu } }$}@} is called {@{the _[sample mean](sample%20mean.md)_}@}, since it is {@{the arithmetic mean of all observations}@}. {@{The statistic $\textstyle {\overline {x} }$}@} is {@{[complete](complete%20statistic.md) and [sufficient](sufficient%20statistic.md) for ⁠$\mu$}@}⁠, and therefore by {@{the [Lehmann–Scheffé theorem](Lehmann–Scheffé%20theorem.md)}@}, {@{$\textstyle {\hat {\mu } }$ is the [uniformly minimum variance unbiased](uniformly%20minimum%20variance%20unbiased.md) \(UMVU\) estimator}@}.<sup>[\[53\]](#^ref-53)</sup> In {@{finite samples}@} it is {@{distributed normally}@}: {@{$${\hat {\mu } }\sim {\mathcal {N} }(\mu ,\sigma ^{2}/n).$$}@} {@{The variance of this estimator}@} is equal to {@{the _μμ_-element of the inverse [Fisher information matrix](Fisher%20information%20matrix.md#matrix%20form) $\textstyle {\mathcal {I} }^{-1}$}@}. This implies that the estimator is {@{[finite-sample efficient](efficient%20estimator.md#efficient%20estimators)}@}. Of {@{practical importance}@} is {@{the [standard error](standard%20error.md) of $\textstyle {\hat {\mu } }$ being proportional to $\textstyle 1/{\sqrt {n} }$}@}, that is, if one wishes to {@{decrease the standard error by a factor of 10}@}, one must {@{increase the number of points in the sample by a factor of 100}@}. This fact is widely used in determining {@{sample sizes for opinion polls and the number of trials in [Monte Carlo simulations](Monte%20Carlo%20simulation.md)}@}. <!--SR:!2026-03-13,15,316!2026-03-16,18,332!2026-03-16,18,323!2026-03-12,14,290!2026-03-12,14,290!2026-03-14,16,316!2026-03-16,18,323!2026-03-15,17,323!2026-03-15,17,323!2026-03-14,16,309!2026-03-12,14,290!2026-03-15,17,332!2026-03-12,14,290!2026-03-12,14,290!2026-03-16,18,323!2026-03-16,18,332!2026-03-13,15,316!2026-03-14,16,311-->

From the standpoint of {@{the [asymptotic theory](asymptotic%20theory%20(statistics).md)}@}, $\textstyle {\hat {\mu } }$ is {@{[consistent](consistent%20estimator.md), that is, it [converges in probability](converges%20in%20probability.md#convergence%20in%20probability) to ⁠$\mu$⁠ as $n\rightarrow \infty$}@}. The estimator is also {@{[asymptotically normal](asymptotic%20normality.md)}@}, which is {@{a simple corollary of it being normal in finite samples}@}: {@{$${\sqrt {n} }({\hat {\mu } }-\mu )\,\xrightarrow {d} \,{\mathcal {N} }(0,\sigma ^{2}).$$}@} <!--SR:!2026-03-16,18,323!2026-03-13,15,316!2026-03-15,17,316!2026-03-13,15,316!2026-03-12,14,290-->

#### sample variance

- See also: ::@:: [Standard deviation § Estimation](standard%20deviation.md#estimation), and [Variance § Estimation](variance.md#estimation) <!--SR:!2026-03-12,14,290!2026-03-16,18,323-->

{@{The estimator $\textstyle {\hat {\sigma } }^{2}$}@} is called {@{the _[sample variance](sample%20variance.md#sample%20variance)_}@}, since it is {@{the variance of the sample \($(x_{1},\ldots ,x_{n})$\)}@}. In practice, another estimator is {@{often used instead of the $\textstyle {\hat {\sigma } }^{2}$}@}. This other estimator is {@{denoted $s^{2}$, and is also called the _sample variance_}@}, which represents {@{a certain ambiguity in terminology}@}; {@{its square root ⁠$s$⁠}@} is called {@{the _sample standard deviation_}@}. {@{The estimator $s^{2}$ differs from $\textstyle {\hat {\sigma } }^{2}$}@} by having {@{\(_n_ − 1\) instead of _n_ in the denominator \(the so-called [Bessel's correction](Bessel's%20correction.md)\)}@}: {@{$$s^{2}={\frac {n}{n-1} }{\hat {\sigma } }^{2}={\frac {1}{n-1} }\sum _{i=1}^{n}(x_{i}-{\overline {x} })^{2}.$$}@} {@{The difference between $s^{2}$ and $\textstyle {\hat {\sigma } }^{2}$}@} becomes {@{negligibly small for large _n_'s}@}. In {@{finite samples}@} however, {@{the motivation behind the use of $s^{2}$}@} is that it is {@{an [unbiased estimator](unbiased%20estimator.md) of the underlying parameter $\sigma ^{2}$, whereas $\textstyle {\hat {\sigma } }^{2}$ is biased}@}. Also, by {@{the Lehmann–Scheffé theorem}@} the estimator $s^{2}$ is {@{uniformly minimum variance unbiased \([UMVU](UMVU.md)\)}@},<sup>[\[53\]](#^ref-53)</sup> which makes it {@{the "best" estimator among all unbiased ones}@}. However it can be shown that {@{the biased estimator $\textstyle {\hat {\sigma } }^{2}$}@} is {@{better than the $s^{2}$ in terms of the [mean squared error](mean%20squared%20error.md) \(MSE\) criterion}@}. In {@{finite samples}@} {@{both $s^{2}$ and $\textstyle {\hat {\sigma } }^{2}$}@} have {@{scaled [chi-squared distribution](chi-squared%20distribution.md) with \(_n_ − 1\) degrees of freedom}@}: {@{$$s^{2}\sim {\frac {\sigma ^{2} }{n-1} }\cdot \chi _{n-1}^{2},\qquad {\hat {\sigma } }^{2}\sim {\frac {\sigma ^{2} }{n} }\cdot \chi _{n-1}^{2}.$$}@} {@{The first of these expressions}@} shows that {@{the variance of $s^{2}$ is equal to $2\sigma ^{4}/(n-1)$}@} (annotation: recall that {@{the variance of a chi-squared variable with $k$ degrees of freedom}@} is {@{equal to $2k$}@}), which is {@{slightly greater than the _σσ_-element of the inverse Fisher information matrix $\textstyle {\mathcal {I} }^{-1}$}@}, which is {@{$2\sigma ^{4}/n$}@}. Thus, $s^{2}$ is {@{not an efficient estimator for $\sigma ^{2}$}@}, and moreover, since {@{$s^{2}$ is UMVU}@}, we can conclude that {@{the finite-sample efficient estimator for $\sigma ^{2}$ does not exist}@}. <!--SR:!2026-03-12,14,290!2026-03-13,15,309!2026-03-15,17,316!2026-03-12,14,290!2026-03-15,17,316!2026-03-16,18,323!2026-03-15,17,316!2026-03-14,16,323!2026-03-15,17,323!2026-03-13,15,316!2026-03-13,15,316!2026-03-13,15,314!2026-03-16,18,323!2026-03-13,15,309!2026-03-14,16,323!2026-03-16,18,323!2026-03-13,15,316!2026-03-15,17,323!2026-03-15,17,323!2026-03-12,14,290!2026-03-13,15,290!2026-03-14,16,323!2026-03-16,18,323!2026-03-14,16,323!2026-03-15,17,323!2026-03-13,15,290!2026-03-14,16,311!2026-03-16,18,323!2026-03-15,17,323!2026-03-13,15,311!2026-04-14,37,290!2026-03-13,15,316!2026-03-16,18,323!2026-03-16,18,323-->

Applying {@{the asymptotic theory}@}, {@{both estimators $s^{2}$ and $\textstyle {\hat {\sigma } }^{2}$ are consistent}@}, that is they {@{converge in probability to $\sigma ^{2}$ as the sample size $n\rightarrow \infty$}@}. {@{The two estimators}@} are {@{also both asymptotically normal}@}: {@{$${\sqrt {n} }({\hat {\sigma } }^{2}-\sigma ^{2})\simeq {\sqrt {n} }(s^{2}-\sigma ^{2})\,\xrightarrow {d} \,{\mathcal {N} }(0,2\sigma ^{4}).$$}@} In particular, {@{both estimators}@} are {@{asymptotically efficient for $\sigma ^{2}$}@}. <!--SR:!2026-03-15,17,323!2026-03-12,14,290!2026-03-15,17,323!2026-03-12,14,290!2026-03-12,14,290!2026-03-15,17,323!2026-03-14,16,323!2026-03-13,15,314-->

### confidence intervals

- See also: ::@:: [Studentization](Studentization.md) and [3-sigma rule](3-sigma%20rule.md#three-sigma%20rule) <!--SR:!2026-03-12,14,290!2026-03-13,15,309-->

By {@{[Cochran's theorem](Cochran's%20theorem.md)}@}, for {@{normal distributions}@} {@{the sample mean $\textstyle {\hat {\mu } }$ and the sample variance _s_<sup>2</sup> are [independent](independence%20(probability%20theory).md)}@}, which means there can be {@{no gain in considering their [joint distribution](joint%20distribution.md)}@}. There is also {@{a converse theorem}@}: if in {@{a sample the sample mean and sample variance are independent}@}, then {@{the sample must have come from the normal distribution}@}. {@{The independence between $\textstyle {\hat {\mu } }$ and _s_}@} can be employed to {@{construct the so-called _t-statistic_}@}: {@{$$t={\frac { {\hat {\mu } }-\mu }{s/{\sqrt {n} } } }={\frac { {\overline {x} }-\mu }{\sqrt { {\frac {1}{n(n-1)} }\sum (x_{i}-{\overline {x} })^{2} } } }\sim t_{n-1}$$}@} {@{This quantity _t_}@} has {@{the [Student's t-distribution](Student's%20t-distribution.md) with \(_n_ − 1\) degrees of freedom}@}, and it is {@{an [ancillary statistic](ancillary%20statistic.md) \(independent of the value of the parameters\)}@}. {@{Inverting the distribution of this _t_-statistics}@} will allow us to {@{construct the [confidence interval](confidence%20interval.md) for _μ_}@};<sup>[\[54\]](#^ref-54)</sup> similarly, {@{inverting the _χ_<sup>2</sup> distribution of the statistic _s_<sup>2</sup>}@} will give us {@{the confidence interval for _σ_<sup>2</sup>}@}:<sup>[\[55\]](#^ref-55)</sup> {@{$$\mu \in \left[{\hat {\mu } }-t_{n-1,1-\alpha /2}{\frac {s}{\sqrt {n} } },\,{\hat {\mu } }+t_{n-1,1-\alpha /2}{\frac {s}{\sqrt {n} } }\right]$$}@} {@{$$\sigma ^{2}\in \left[{\frac {n-1}{\chi _{n-1,1-\alpha /2}^{2} } }s^{2},\,{\frac {n-1}{\chi _{n-1,\alpha /2}^{2} } }s^{2}\right]$$}@} where {@{_t_<sub>_k_,_p_</sub> and _χ_<sup>2</sup><sub>_k,p_</sub>}@} are {@{the _p_<!-- markdown separator -->th [quantiles](quantile.md) of the _t_- and _χ_<sup>2</sup>-distributions respectively}@}. {@{These confidence intervals}@} are of {@{the _[confidence level](confidence%20level.md)_ 1 − _α_}@}, meaning that {@{the true values _μ_ and _σ_<sup>2</sup>}@} fall {@{outside of these intervals with probability \(or [significance level](significance%20level.md)\) _α_}@}. In practice people usually take {@{_α_ = 5%, resulting in the 95% confidence intervals}@}. {@{The confidence interval for _σ_}@} can be found by taking {@{the square root of the interval bounds for _σ_<sup>2</sup>}@}. <!--SR:!2026-03-14,16,323!2026-03-13,15,311!2026-03-14,16,316!2026-03-14,16,323!2026-03-15,17,323!2026-03-15,17,323!2026-03-14,16,316!2026-03-14,16,323!2026-03-16,18,323!2026-03-16,18,323!2026-03-12,14,290!2026-03-14,16,323!2026-03-16,18,323!2026-03-12,14,290!2026-03-16,18,323!2026-03-15,17,316!2026-03-13,15,290!2026-03-15,17,323!2026-03-13,15,311!2026-03-16,18,323!2026-03-14,16,316!2026-03-14,16,323!2026-03-13,15,316!2026-03-13,15,311!2026-03-12,14,290!2026-03-12,14,290!2026-03-13,15,316!2026-03-16,18,323-->

{@{Approximate formulas}@} can be derived from {@{the asymptotic distributions of $\textstyle {\hat {\mu } }$ and _s_<sup>2</sup>}@}: {@{$$\mu \in \left[{\hat {\mu } }-{\frac {|z_{\alpha /2}|}{\sqrt {n} } }s,\,{\hat {\mu } }+{\frac {|z_{\alpha /2}|}{\sqrt {n} } }s\right]$$}@} (annotation: As {@{$n\rightarrow \infty$}@}, {@{the _t_-distribution converges to the standard normal distribution}@}, so we can replace {@{the quantiles of the _t_-distribution with those of the standard normal distribution}@}.) and {@{$$\sigma ^{2}\in \left[s^{2}-{\sqrt {2} }{\frac {|z_{\alpha /2}|}{\sqrt {n} } }s^{2},\,s^{2}+{\sqrt {2} }{\frac {|z_{\alpha /2}|}{\sqrt {n} } }s^{2}\right]$$}@} (annotation: As {@{$n\rightarrow \infty$}@}, {@{the distribution of $s^2$ approaches normal with mean $\sigma^2$ and variance $2\sigma^4/n$}@}, so we can construct {@{the confidence interval using the standard normal quantiles}@}.) {@{The approximate formulas}@} become {@{valid for large values of _n_}@}, and are {@{more convenient for the manual calculation}@} since {@{the standard normal quantiles _z_<sub>_α_<!-- markdown separator -->/2</sub> do not depend on _n_}@}. In particular, {@{the most popular value of _α_ = 5%}@}, results in {@{\|_z_<sub>0.025</sub>\| = [1.96](1.96.md)}@}. <!--SR:!2026-03-16,18,323!2026-03-15,17,323!2026-03-12,14,290!2026-03-14,16,332!2026-03-13,15,316!2026-03-13,15,311!2026-04-20,42,290!2026-03-12,14,290!2026-03-14,16,323!2026-03-16,18,323!2026-03-12,14,290!2026-03-14,16,323!2026-03-13,15,316!2026-03-15,17,323!2026-03-14,16,316!2026-03-15,17,323-->

### normality tests

- Main article: ::@:: [Normality tests](normality%20tests.md) <!--SR:!2026-03-13,15,311!2026-03-12,14,290-->

{@{Normality tests}@} assess {@{the likelihood that the given data set {_x_<sub>1</sub>, ..., _x_<sub>_n_</sub>} comes from a normal distribution}@}. Typically {@{the [null hypothesis](null%20hypothesis.md) _H_<sub>0</sub>}@} is that {@{the observations are distributed normally with unspecified mean _μ_ and variance _σ_<sup>2</sup>}@}, versus the alternative {@{_H_<sub>_a_</sub> that the distribution is arbitrary}@}. {@{Many tests \(over 40\)}@} have been {@{devised for this problem}@}. {@{The more prominent of them}@} are outlined below: <!--SR:!2026-03-14,16,323!2026-03-12,14,290!2026-03-16,18,323!2026-03-16,18,323!2026-03-15,17,323!2026-03-12,14,290!2026-03-12,14,290!2026-03-14,16,323-->

{@{__Diagnostic plots__}@} are {@{more intuitively appealing but subjective at the same time}@}, as they rely on {@{informal human judgement to accept or reject the null hypothesis}@}. <!--SR:!2026-03-13,15,316!2026-03-13,15,316!2026-03-16,18,323-->

- {@{[Q–Q plot](Q–Q%20plot.md)}@}, also known as {@{[normal probability plot](normal%20probability%20plot.md) or [rankit](rankit.md) plot}@}—is {@{a plot of the sorted values from the data set}@} against {@{the expected values of the corresponding quantiles from the standard normal distribution}@}. That is, it is {@{a plot of point of the form \(_Φ_<sup>−1</sup>\(_p_<sub>_k_</sub>\), _x_<sub>\(_k_\)</sub>\)}@} (annotation: _x_<sub>\(_k_\)</sub> is {@{the _k_-th data point value}@}), where {@{plotting points _p_<sub>_k_</sub>}@} are {@{equal to _p_<sub>_k_</sub> = \(_k_ − _α_\)/\(_n_ + 1 − 2<!-- markdown separator -->_α_\)}@} and _α_ is {@{an adjustment constant, which can be anything between 0 and 1}@}. (annotation: When {@{_a_ = 0, _p_<sub>_k_</sub> = _k_/\(_n_ + 1\)}@}, and when {@{_a_ = 1, _p_<sub>_k_</sub> = \(_k_ − 1\)/\(_n_ − 1\)}@}.) If {@{the null hypothesis is true}@}, {@{the plotted points}@} should {@{approximately lie on a straight line}@}.
- {@{[P–P plot](P–P%20plot.md)}@} – similar to {@{the Q–Q plot, but used much less frequently}@}. This method consists of plotting {@{the points \(_Φ_\(_z_<sub>\(_k_\)</sub>\), _p_<sub>_k_</sub>\)}@}, where {@{$\textstyle z_{(k)}=(x_{(k)}-{\hat {\mu } })/{\hat {\sigma } }$}@} (annotation: _x_<sub>\(_k_\)</sub> is {@{the _k_-th data point value}@}). For {@{normally distributed data}@} this plot should {@{lie on a straight line between \(0, 0\) and \(1, 1\)}@}. <!--SR:!2026-03-13,15,309!2026-03-15,17,323!2026-03-13,15,316!2026-03-13,15,311!2026-03-12,14,290!2026-03-14,16,316!2026-03-13,15,316!2026-03-15,17,323!2026-03-13,15,316!2026-03-16,18,332!2026-03-14,16,323!2026-03-15,17,323!2026-03-16,18,323!2026-03-13,15,314!2026-03-15,17,323!2026-03-12,14,290!2026-03-15,17,323!2026-03-13,15,316!2026-03-12,14,290!2026-03-15,17,323!2026-03-13,15,316-->

__Goodness-of-fit tests__: ::@:: (annotation: 2 items: moment-based tests, tests based on the empirical distribution function) <!--SR:!2026-03-13,15,309!2026-03-12,14,290-->

_Moment-based tests_: ::@:: (annotation: 3 items: D'Agostino's K-squared test, Jarque–Bera test, Shapiro–Wilk test) <!--SR:!2026-03-15,17,323!2026-03-16,18,323-->

- [D'Agostino's K-squared test](D'Agostino's%20K-squared%20test.md)
- [Jarque–Bera test](Jarque–Bera%20test.md)
- [Shapiro–Wilk test](Shapiro–Wilk%20test.md): ::@:: This is based on the line in the Q–Q plot having the slope of _σ_. The test compares the least squares estimate of that slope with the value of the sample variance, and rejects the null hypothesis if these two quantities differ significantly. <!--SR:!2026-03-16,18,332!2026-03-14,16,332-->

_Tests based on the empirical distribution function_: ::@:: (annotation: 2 items: Anderson–Darling test, Lilliefors test) <!--SR:!2026-03-15,17,332!2026-03-12,14,290-->

- [Anderson–Darling test](Anderson–Darling%20test.md)
- [Lilliefors test](Lilliefors%20test.md) ::@:: \(an adaptation of the [Kolmogorov–Smirnov test](Kolmogorov–Smirnov%20test.md)\) <!--SR:!2026-03-12,14,290!2026-03-14,16,323-->

### Bayesian analysis of the normal distribution

{@{Bayesian analysis of normally distributed data}@} is complicated by {@{the many different possibilities that may be considered}@}: <!--SR:!2026-03-14,16,323!2026-03-12,14,290-->

- {@{Either the mean, or the variance, or neither}@}, may be considered {@{a fixed quantity}@}.
- When {@{the variance is unknown}@}, analysis may be done {@{directly in terms of the variance, or in terms of the [precision](precision%20(statistics).md), the reciprocal of the variance}@}. The reason for {@{expressing the formulas in terms of precision}@} is that {@{the analysis of most cases is simplified}@}.
- Both {@{univariate and [multivariate](multivariate%20normal%20distribution.md) cases}@} need to be {@{considered}@}.
- {@{Either [conjugate](conjugate%20prior.md) or [improper](improper%20prior.md#improper%20priors) [prior distributions](prior%20distribution.md)}@} may be placed {@{on the unknown variables}@}.
- {@{An additional set of cases}@} occurs in {@{[Bayesian linear regression](Bayesian%20linear%20regression.md)}@}, where in {@{the basic model}@} the data is {@{assumed to be normally distributed}@}, and {@{normal priors}@} are placed on {@{the [regression coefficients](regression%20coefficient.md)}@}. The resulting analysis is similar to {@{the basic cases of [independent identically distributed](independent%20identically%20distributed.md) data}@}. <!--SR:!2026-03-12,14,290!2026-03-13,15,316!2026-03-16,18,323!2026-03-15,17,323!2026-03-16,18,323!2026-03-13,15,316!2026-03-14,16,332!2026-03-12,14,290!2026-03-12,14,290!2026-03-15,17,323!2026-03-15,17,316!2026-03-14,16,332!2026-03-15,17,323!2026-03-14,16,309!2026-03-12,14,290!2026-03-13,15,316!2026-03-13,15,314-->

The formulas for {@{the non-linear-regression cases}@} are {@{summarized in the [conjugate prior](conjugate%20prior.md) article}@}. <!--SR:!2026-03-16,18,323!2026-03-14,16,323-->

#### sum of two quadratics

##### scalar form

{@{The following auxiliary formula}@} is useful for simplifying {@{the [posterior](posterior%20distribution.md) update equations}@}, which otherwise {@{become fairly tedious}@}. {@{$$a(x-y)^{2}+b(x-z)^{2}=(a+b)\left(x-{\frac {ay+bz}{a+b} }\right)^{2}+{\frac {ab}{a+b} }(y-z)^{2}$$}@} This equation rewrites {@{the sum of two quadratics in _x_}@} by expanding {@{the squares, grouping the terms in _x_, and [completing the square](completing%20the%20square.md)}@}. Note the following about {@{the complex constant factors}@} attached to some of the terms: <!--SR:!2026-03-16,18,332!2026-03-15,17,323!2026-03-16,18,323!2026-04-19,39,296!2026-03-16,18,323!2026-03-14,16,316!2026-03-14,16,309-->

1. {@{The factor ${\frac {ay+bz}{a+b} }$}@} has the form of {@{a [weighted average](weighted%20average.md) of _y_ and _z_}@}.
2. {@{${\frac {ab}{a+b} }={\frac {1}{ {\frac {1}{a} }+{\frac {1}{b} } } }=(a^{-1}+b^{-1})^{-1}$}@}. This shows that this factor can be thought of as resulting from {@{a situation where the [reciprocals](multiplicative%20inverse.md) of quantities _a_ and _b_ add directly}@}, so to combine {@{_a_ and _b_ themselves}@}, it is necessary to {@{reciprocate, add, and reciprocate the result again to get back into the original units}@}. This is exactly the sort of operation {@{performed by the [harmonic mean](harmonic%20mean.md)}@}, so it is not surprising that {@{${\frac {ab}{a+b} }$ is one-half the [harmonic mean](harmonic%20mean.md) of _a_ and _b_}@}. <!--SR:!2026-03-13,15,316!2026-03-16,18,332!2026-03-16,18,323!2026-03-15,17,323!2026-03-15,17,323!2026-03-14,16,323!2026-03-16,18,323!2026-03-15,17,323-->

##### vector form

{@{A similar formula}@} can be written for {@{the sum of two vector quadratics}@}: If {@{__x__, __y__, __z__}@} are {@{vectors of length _k_}@}, and {@{__A__ and __B__}@} are {@{[symmetric](symmetric%20matrix.md), [invertible matrices](invertible%20matrices.md) of size $k\times k$}@}, then {@{$${\begin{aligned}&(\mathbf {y} -\mathbf {x} )'\mathbf {A} (\mathbf {y} -\mathbf {x} )+(\mathbf {x} -\mathbf {z} )'\mathbf {B} (\mathbf {x} -\mathbf {z} )\\={}&(\mathbf {x} -\mathbf {c} )'(\mathbf {A} +\mathbf {B} )(\mathbf {x} -\mathbf {c} )+(\mathbf {y} -\mathbf {z} )'(\mathbf {A} ^{-1}+\mathbf {B} ^{-1})^{-1}(\mathbf {y} -\mathbf {z} )\end{aligned} }$$}@} where {@{$$\mathbf {c} =(\mathbf {A} +\mathbf {B} )^{-1}(\mathbf {A} \mathbf {y} +\mathbf {B} \mathbf {z} )$$}@} \(annotation: Expand {@{the squares, group the terms in __x__, and complete the square}@} to verify this formula. Note that {@{the factor $\mathbf {c}$}@} has the form of {@{a weighted average of __y__ and __z__}@}, where the weights are given by {@{the matrices __A__ and __B__}@}. {@{The factor $(\mathbf {A} ^{-1}+\mathbf {B} ^{-1})^{-1}$}@} can be thought of as {@{one-half the harmonic mean of __A__ and __B__}@}, in the sense that it is obtained by reciprocating {@{__A__ and __B__, adding them, and reciprocating the result again to get back into the original units}@}.\) <!--SR:!2026-03-13,15,311!2026-03-15,17,323!2026-03-16,18,323!2026-03-12,14,290!2026-03-14,16,316!2026-03-15,17,323!2026-03-14,16,323!2026-04-03,26,270!2026-03-15,17,334!2026-03-15,17,334!2026-03-15,17,334!2026-03-14,16,334!2026-03-15,17,334!2026-03-16,18,334!2026-03-15,17,334-->

{@{The form __x__<!-- markdown separator -->′ __A__ __x__}@} is called {@{a [quadratic form](quadratic%20form.md) and is a [scalar](scalar%20(mathematics).md)}@}: {@{$$\mathbf {x} '\mathbf {A} \mathbf {x} =\sum _{i,j}a_{ij}x_{i}x_{j}$$}@} In other words, it sums up {@{all possible combinations of products of pairs of elements from __x__}@}, with {@{a separate coefficient for each}@}. In addition, since {@{$x_{i}x_{j}=x_{j}x_{i}$}@}, {@{only the sum $a_{ij}+a_{ji}$}@} matters for {@{any off-diagonal elements of __A__}@}, and there is no {@{loss of generality in assuming that __A__ is [symmetric](symmetric%20matrix.md)}@}. Furthermore, if {@{__A__ is symmetric}@}, then the form {@{$\mathbf {x} '\mathbf {A} \mathbf {y} =\mathbf {y} '\mathbf {A} \mathbf {x}$}@}. <!--SR:!2026-03-16,18,323!2026-03-12,14,290!2026-03-16,18,332!2026-03-13,15,316!2026-03-14,16,323!2026-04-19,41,290!2026-03-13,15,314!2026-03-16,18,323!2026-03-15,17,323!2026-03-13,15,316!2026-03-12,14,290-->

#### sum of differences from the mean

{@{Another useful formula}@} is as follows: {@{$$\sum _{i=1}^{n}(x_{i}-\mu )^{2}=\sum _{i=1}^{n}(x_{i}-{\bar {x} })^{2}+n({\bar {x} }-\mu )^{2}$$}@} where {@{${\bar {x} }={\frac {1}{n} }\sum _{i=1}^{n}x_{i}$}@}. <!--SR:!2026-03-15,17,323!2026-03-14,16,332!2026-03-12,14,290-->

### with known variance

For {@{a set of [i.i.d.](i.i.d..md) normally distributed data points __X__ of size _n_}@} where each individual point _x_ follows {@{$x\sim {\mathcal {N} }(\mu ,\sigma ^{2})$ with known [variance](variance.md) _σ_<sup>2</sup>}@}, {@{the [conjugate prior](conjugate%20prior.md) distribution}@} is {@{also normally distributed}@}. <!--SR:!2026-03-15,17,323!2026-03-13,15,316!2026-03-14,16,316!2026-03-14,16,316-->

{@{This can be shown more easily}@} by rewriting {@{the variance as the [precision](precision%20(statistics).md)}@}, i.e. using {@{_τ_ = 1/<!-- markdown separator -->_σ_<sup>2</sup>}@}. Then if {@{$x\sim {\mathcal {N} }(\mu ,1/\tau )$ and $\mu \sim {\mathcal {N} }(\mu _{0},1/\tau _{0})$}@}, we {@{proceed as follows}@}. <!--SR:!2026-03-15,17,323!2026-03-15,17,323!2026-03-16,18,323!2026-03-13,15,316!2026-03-12,14,290-->

First, {@{the [likelihood function](likelihood%20function.md)}@} is \(using the formula above for {@{the sum of differences from the mean}@}\): {@{$${\begin{aligned}p(\mathbf {X} \mid \mu ,\tau )&=\prod _{i=1}^{n}{\sqrt {\frac {\tau }{2\pi } } }\exp \left(-{\frac {1}{2} }\tau (x_{i}-\mu )^{2}\right)\\&=\left({\frac {\tau }{2\pi } }\right)^{n/2}\exp \left(-{\frac {1}{2} }\tau \sum _{i=1}^{n}(x_{i}-\mu )^{2}\right)\\&=\left({\frac {\tau }{2\pi } }\right)^{n/2}\exp \left[-{\frac {1}{2} }\tau \left(\sum _{i=1}^{n}(x_{i}-{\bar {x} })^{2}+n({\bar {x} }-\mu )^{2}\right)\right].\end{aligned} }$$}@} Then, we {@{proceed as follows}@}: {@{$${\begin{aligned}p(\mu \mid \mathbf {X} )&\propto p(\mathbf {X} \mid \mu )p(\mu )\\&=\left({\frac {\tau }{2\pi } }\right)^{n/2}\exp \left[-{\frac {1}{2} }\tau \left(\sum _{i=1}^{n}(x_{i}-{\bar {x} })^{2}+n({\bar {x} }-\mu )^{2}\right)\right]{\sqrt {\frac {\tau _{0} }{2\pi } } }\exp \left(-{\frac {1}{2} }\tau _{0}(\mu -\mu _{0})^{2}\right)\\&\propto \exp \left(-{\frac {1}{2} }\left(\tau \left(\sum _{i=1}^{n}(x_{i}-{\bar {x} })^{2}+n({\bar {x} }-\mu )^{2}\right)+\tau _{0}(\mu -\mu _{0})^{2}\right)\right)\\&\propto \exp \left(-{\frac {1}{2} }\left(n\tau ({\bar {x} }-\mu )^{2}+\tau _{0}(\mu -\mu _{0})^{2}\right)\right)\\&=\exp \left(-{\frac {1}{2} }(n\tau +\tau _{0})\left(\mu -{\dfrac {n\tau {\bar {x} }+\tau _{0}\mu _{0} }{n\tau +\tau _{0} } }\right)^{2}+{\frac {n\tau \tau _{0} }{n\tau +\tau _{0} } }({\bar {x} }-\mu _{0})^{2}\right)\\&\propto \exp \left(-{\frac {1}{2} }(n\tau +\tau _{0})\left(\mu -{\dfrac {n\tau {\bar {x} }+\tau _{0}\mu _{0} }{n\tau +\tau _{0} } }\right)^{2}\right)\end{aligned} }$$}@} <!--SR:!2026-03-16,18,323!2026-03-14,16,316!2026-03-29,24,270!2026-03-16,18,332!2026-03-14,16,316-->

In {@{the above derivation}@}, we used {@{the formula above for the sum of two quadratics}@} and eliminated {@{all constant factors not involving _μ_}@}. The result is {@{the [kernel](kernel%20(statistics).md) of a normal distribution}@}, with {@{mean ${\frac {n\tau {\bar {x} }+\tau _{0}\mu _{0} }{n\tau +\tau _{0} } }$ and precision $n\tau +\tau _{0}$}@}, i.e. {@{$$p(\mu \mid \mathbf {X} )\sim {\mathcal {N} }\left({\frac {n\tau {\bar {x} }+\tau _{0}\mu _{0} }{n\tau +\tau _{0} } },{\frac {1}{n\tau +\tau _{0} } }\right)$$}@} <!--SR:!2026-03-15,17,323!2026-03-13,15,316!2026-03-12,14,290!2026-03-13,15,316!2026-03-14,16,316!2026-03-16,18,332-->

This can be written as {@{a set of Bayesian update equations for the posterior parameters}@} in terms of {@{the prior parameters}@}: {@{$${\begin{aligned}\tau _{0}'&=\tau _{0}+n\tau \\[5pt]\mu _{0}'&={\frac {n\tau {\bar {x} }+\tau _{0}\mu _{0} }{n\tau +\tau _{0} } }\\[5pt]{\bar {x} }&={\frac {1}{n} }\sum _{i=1}^{n}x_{i}\end{aligned} }$$}@} That is, to combine {@{_n_ data points with total precision of _nτ_}@} \(or equivalently, {@{total variance of _n_<!-- markdown separator -->/<!-- markdown separator -->_σ_<sup>2</sup>}@}\) and {@{mean of values ${\bar {x} }$}@}, derive {@{a new total precision simply by adding the total precision of the data to the prior total precision}@}, and form {@{a new mean through a _precision-weighted average_}@}, i.e. {@{a [weighted average](weighted%20average.md) of the data mean and the prior mean, each weighted by the associated total precision}@}. This makes {@{logical sense}@} if the precision is thought of {@{as indicating the certainty of the observations}@}: In the distribution of {@{the posterior mean}@}, each of {@{the input components is weighted by its certainty}@}, and the certainty of {@{this distribution is the sum of the individual certainties}@}. \(For {@{the intuition of this}@}, compare the expression {@{"the whole is \(or is not\) greater than the sum of its parts"}@}. In addition, consider that {@{the knowledge of the posterior}@} comes from {@{a combination of the knowledge of the prior and likelihood}@}, so it makes sense that we are {@{more certain of it than of either of its components}@}.\) <!--SR:!2026-03-12,14,290!2026-03-14,16,323!2026-03-13,15,316!2026-03-14,16,323!2026-03-13,15,316!2026-03-16,18,323!2026-03-13,15,316!2026-03-15,17,332!2026-03-15,17,323!2026-03-15,17,332!2026-03-12,14,290!2026-03-16,18,323!2026-03-14,16,323!2026-03-14,16,323!2026-03-12,14,290!2026-03-15,17,332!2026-03-13,15,316!2026-03-13,15,311!2026-03-12,14,290-->

{@{The above formula}@} reveals why it is {@{more convenient to do [Bayesian analysis](Bayesian%20analysis.md) of [conjugate priors](conjugate%20prior.md)}@} for {@{the normal distribution in terms of the precision}@}. {@{The posterior precision}@} is simply {@{the sum of the prior and likelihood precisions}@}, and {@{the posterior mean}@} is {@{computed through a precision-weighted average}@}, as described above. {@{The same formulas}@} can be written {@{in terms of variance by reciprocating all the precisions}@}, yielding {@{the more ugly formulas}@} {@{$${\begin{aligned}{\sigma _{0}^{2} }'&={\frac {1}{ {\frac {n}{\sigma ^{2} } }+{\frac {1}{\sigma _{0}^{2} } } } }\\[5pt]\mu _{0}'&={\frac { {\frac {n{\bar {x} } }{\sigma ^{2} } }+{\frac {\mu _{0} }{\sigma _{0}^{2} } } }{ {\frac {n}{\sigma ^{2} } }+{\frac {1}{\sigma _{0}^{2} } } } }\\[5pt]{\bar {x} }&={\frac {1}{n} }\sum _{i=1}^{n}x_{i}\end{aligned} }$$}@} <!--SR:!2026-03-13,15,314!2026-03-14,16,316!2026-03-14,16,316!2026-03-12,14,290!2026-03-12,14,290!2026-03-13,15,316!2026-03-15,17,323!2026-03-13,15,316!2026-03-12,14,290!2026-03-14,16,323!2026-03-13,15,316-->

#### with known mean

For {@{a set of [i.i.d.](i.i.d..md) normally distributed data points __X__ of size _n_}@} where each individual point _x_ follows {@{$x\sim {\mathcal {N} }(\mu ,\sigma ^{2})$ with known mean _μ_}@}, {@{the [conjugate prior](conjugate%20prior.md) of the [variance](variance.md)}@} has {@{an [inverse gamma distribution](inverse%20gamma%20distribution.md) or a [scaled inverse chi-squared distribution](scaled%20inverse%20chi-squared%20distribution.md)}@}. {@{The two are equivalent}@} except for having {@{different [parameterizations](parameter.md)}@}. Although {@{the inverse gamma is more commonly used}@}, we use {@{the scaled inverse chi-squared for the sake of convenience}@}. {@{The prior for _σ_<sup>2</sup>}@} is as follows: {@{$$p(\sigma ^{2}\mid \nu _{0},\sigma _{0}^{2})={\frac {(\sigma _{0}^{2}{\frac {\nu _{0} }{2} })^{\nu _{0}/2} }{\Gamma \left({\frac {\nu _{0} }{2} }\right)} }~{\frac {\exp \left[{\frac {-\nu _{0}\sigma _{0}^{2} }{2\sigma ^{2} } }\right]}{(\sigma ^{2})^{1+{\frac {\nu _{0} }{2} } } } }\propto {\frac {\exp \left[{\frac {-\nu _{0}\sigma _{0}^{2} }{2\sigma ^{2} } }\right]}{(\sigma ^{2})^{1+{\frac {\nu _{0} }{2} } } } }$$}@} (annotation: In this context, {@{the term after proportionality}@} is {@{more important}@}. It may be noted that {@{the parameters $\nu _{0}$ and $\sigma _{0}^{2}$}@} can be thought of as {@{the number of data points and the variance of those data points, respectively}@}, in {@{a hypothetical data set that would yield this prior distribution}@}. This is because {@{the update equations for the posterior parameters}@} have {@{the same form}@} as if we were to combine {@{the prior with a likelihood function derived from such a hypothetical data set}@}.) <!--SR:!2026-03-13,15,316!2026-03-12,14,290!2026-03-15,17,323!2026-03-16,18,323!2026-03-12,14,290!2026-03-13,15,316!2026-03-13,15,316!2026-03-15,17,332!2026-04-21,43,290!2026-03-18,7,292!2026-03-14,16,316!2026-03-12,14,290!2026-03-14,16,316!2026-03-15,17,332!2026-03-15,17,332!2026-03-15,17,309!2026-03-16,18,323!2026-03-14,16,323-->

{@{The [likelihood function](likelihood%20function.md)}@} from above, written in terms of the variance, is: {@{$${\begin{aligned}p(\mathbf {X} \mid \mu ,\sigma ^{2})&=\left({\frac {1}{2\pi \sigma ^{2} } }\right)^{n/2}\exp \left[-{\frac {1}{2\sigma ^{2} } }\sum _{i=1}^{n}(x_{i}-\mu )^{2}\right]\\&=\left({\frac {1}{2\pi \sigma ^{2} } }\right)^{n/2}\exp \left[-{\frac {S}{2\sigma ^{2} } }\right]\end{aligned} }$$}@} where {@{$$S=\sum _{i=1}^{n}(x_{i}-\mu )^{2}.$$}@} Then: {@{$${\begin{aligned}p(\sigma ^{2}\mid \mathbf {X} )&\propto p(\mathbf {X} \mid \sigma ^{2})p(\sigma ^{2})\\&=\left({\frac {1}{2\pi \sigma ^{2} } }\right)^{n/2}\exp \left[-{\frac {S}{2\sigma ^{2} } }\right]{\frac {(\sigma _{0}^{2}{\frac {\nu _{0} }{2} })^{\frac {\nu _{0} }{2} } }{\Gamma \left({\frac {\nu _{0} }{2} }\right)} }~{\frac {\exp \left[{\frac {-\nu _{0}\sigma _{0}^{2} }{2\sigma ^{2} } }\right]}{(\sigma ^{2})^{1+{\frac {\nu _{0} }{2} } } } }\\&\propto \left({\frac {1}{\sigma ^{2} } }\right)^{n/2}{\frac {1}{(\sigma ^{2})^{1+{\frac {\nu _{0} }{2} } } } }\exp \left[-{\frac {S}{2\sigma ^{2} } }+{\frac {-\nu _{0}\sigma _{0}^{2} }{2\sigma ^{2} } }\right]\\&={\frac {1}{(\sigma ^{2})^{1+{\frac {\nu _{0}+n}{2} } } } }\exp \left[-{\frac {\nu _{0}\sigma _{0}^{2}+S}{2\sigma ^{2} } }\right]\end{aligned} }$$}@}  (annotation: It may be noted that {@{the parameters $\nu _{0}$ and $\sigma _{0}^{2}$}@} can be thought of as {@{the number of data points and the variance of those data points, respectively}@}, in {@{a hypothetical data set that would yield this prior distribution}@}. This is because {@{the update equations for the posterior parameters}@} have {@{the same form}@} as if we were to combine {@{the prior with a likelihood function derived from such a hypothetical data set}@}.) <!--SR:!2026-03-15,17,323!2026-03-14,16,311!2026-03-16,18,323!2026-03-15,17,323!2026-03-15,17,323!2026-04-25,46,290!2026-03-12,14,290!2026-03-15,17,316!2026-03-15,17,314!2026-03-14,16,316-->

The above is also {@{a scaled inverse chi-squared distribution}@} where {@{$${\begin{aligned}\nu _{0}'&=\nu _{0}+n\\\nu _{0}'{\sigma _{0}^{2} }'&=\nu _{0}\sigma _{0}^{2}+\sum _{i=1}^{n}(x_{i}-\mu )^{2}\end{aligned} }$$}@} or equivalently {@{$${\begin{aligned}\nu _{0}'&=\nu _{0}+n\\{\sigma _{0}^{2} }'&={\frac {\nu _{0}\sigma _{0}^{2}+\sum _{i=1}^{n}(x_{i}-\mu )^{2} }{\nu _{0}+n} }\end{aligned} }$$}@} Reparameterizing {@{in terms of an [inverse gamma distribution](inverse%20gamma%20distribution.md)}@}, the result is: {@{$${\begin{aligned}\alpha '&=\alpha +{\frac {n}{2} }\\\beta '&=\beta +{\frac {\sum _{i=1}^{n}(x_{i}-\mu )^{2} }{2} }\end{aligned} }$$}@} <!--SR:!2026-03-16,18,332!2026-03-13,15,316!2026-03-12,14,290!2026-03-15,17,323!2026-03-12,14,290-->

#### with unknown mean and unknown variance

For {@{a set of [i.i.d.](i.i.d..md) normally distributed data points __X__ of size _n_}@} where each individual point _x_ follows {@{$x\sim {\mathcal {N} }(\mu ,\sigma ^{2})$ with unknown mean _μ_ and unknown [variance](variance.md) _σ_<sup>2</sup>}@}, {@{a combined \(multivariate\) [conjugate prior](conjugate%20prior.md)}@} is placed {@{over the mean and variance}@}, consisting of {@{a [normal-inverse-gamma distribution](normal-inverse-gamma%20distribution.md)}@}. Logically, this {@{originates as follows}@}: (annotation: 6 steps: {@{known variance only → known mean only → posterior as prior}@} → {@{unknown variance and mean → conditional prior → normal-inverse-gamma distribution}@}) <!--SR:!2026-03-14,16,323!2026-03-14,16,323!2026-03-16,18,323!2026-03-12,14,290!2026-03-15,17,332!2026-03-14,16,311!2026-03-12,14,290!2026-03-14,16,334-->

1. From the analysis of {@{the case with unknown mean but known variance}@}, we see that {@{the update equations involve [sufficient statistics](sufficient%20statistic.md)}@} computed from {@{the data consisting of the mean of the data points (annotation: $\overline x$ in above) and the total variance of the data points (annotation: $n \tau$ in terms of precision in above)}@}, computed in turn from {@{the known variance divided by the number of data points}@}. <!--SR:!2026-03-13,15,311!2026-03-15,17,323!2026-03-14,16,323!2026-03-13,15,316-->

2. From the analysis of {@{the case with unknown variance but known mean}@}, we see that {@{the update equations involve sufficient statistics}@} over {@{the data consisting of the number of data points (annotation: $n$ in above) and [sum of squared deviations](sum%20of%20squared%20deviations.md) (annotation: $S$ in above)}@}. <!--SR:!2026-03-16,18,323!2026-03-16,18,323!2026-03-14,16,323-->

3. Keep in mind that {@{the posterior update values}@} serve as {@{the prior distribution when further data is handled}@}. Thus, we should logically think of {@{our priors}@} in terms of {@{the sufficient statistics just described}@}, with the same {@{semantics kept in mind as much as possible}@}. <!--SR:!2026-03-14,16,323!2026-03-14,16,323!2026-03-12,14,290!2026-03-16,18,323!2026-03-15,17,323-->

4. To handle the case where {@{both mean and variance are unknown}@}, we could place {@{independent priors over the mean and variance}@}, with fixed {@{estimates of the average mean (annotation: $\overline x$ in above), total variance (annotation: $S$ in above), number of data points (annotation: $n$ in above)}@} used to compute {@{the variance prior, and sum of squared deviations}@}. Note however that in reality, {@{the total variance of the mean (annotation: that updates the unknown mean)}@} depends on {@{the unknown variance}@}, and {@{the sum of squared deviations that goes into the variance prior \(appears to\)}@} depend on {@{the unknown mean}@}. In practice, {@{the latter dependence}@} is {@{relatively unimportant}@}: {@{Shifting the actual mean}@} shifts {@{the generated points by an equal amount}@}, and on average {@{the squared deviations will remain the same}@}. This is not {@{the case, however, with the total variance of the mean}@}: As {@{the unknown variance increases}@}, {@{the total variance of the mean will increase proportionately}@}, and we would like to {@{capture this dependence}@}. <!--SR:!2026-03-12,14,290!2026-03-14,16,323!2026-03-15,17,332!2026-03-15,17,323!2026-03-13,15,316!2026-03-16,18,323!2026-03-16,18,323!2026-03-12,14,290!2026-03-14,16,311!2026-03-16,18,323!2026-03-14,16,332!2026-04-24,46,311!2026-03-16,18,323!2026-03-14,16,316!2026-03-15,17,309!2026-03-16,18,323!2026-03-15,17,332-->

5. This suggests that we create {@{a _conditional prior_ of the mean on the unknown variance}@}, with a hyperparameter specifying {@{the mean of the [pseudo-observations](pseudo-observation.md#pseudo-observations) associated with the prior}@}, and another parameter specifying {@{the number of pseudo-observations}@}. This number serves as {@{a scaling parameter on the variance}@}, making it possible to control {@{the overall variance of the mean relative to the actual variance parameter}@}. {@{The prior for the variance}@} also has {@{two hyperparameters}@}, one specifying {@{the sum of squared deviations of the pseudo-observations associated with the prior}@}, and another specifying {@{once again the number of pseudo-observations}@}. {@{Each of the priors}@} has {@{a hyperparameter specifying the number of pseudo-observations}@}, and in each case this controls {@{the relative variance of that prior}@}. These are given as {@{two separate hyperparameters}@} so that {@{the variance \(aka the confidence\) of the two priors}@} can be {@{controlled separately}@}. <!--SR:!2026-03-13,15,314!2026-03-15,17,309!2026-03-12,14,290!2026-03-12,14,290!2026-03-13,15,311!2026-03-15,17,323!2026-03-12,14,290!2026-03-12,14,290!2026-03-13,15,316!2026-03-13,15,309!2026-03-14,16,323!2026-03-13,15,316!2026-03-15,17,332!2026-03-16,18,323!2026-03-14,16,323-->

6. This leads immediately to {@{the [normal-inverse-gamma distribution](normal-inverse-gamma%20distribution.md)}@}, which is {@{the product of the two distributions just defined}@}, with {@{[conjugate priors](conjugate%20prior.md) used \(an [inverse gamma distribution](inverse%20gamma%20distribution.md) over the variance}@}, and {@{a normal distribution over the mean, _conditional_ on the variance\)}@} and with {@{the same four parameters just defined}@}. <!--SR:!2026-03-15,17,311!2026-03-15,17,316!2026-03-14,16,323!2026-03-14,16,323!2026-03-15,17,323-->

{@{The priors}@} are normally defined as follows: {@{$${\begin{aligned}p(\mu \mid \sigma ^{2};\mu _{0},n_{0})&\sim {\mathcal {N} }(\mu _{0},\sigma ^{2}/n_{0})\\p(\sigma ^{2};\nu _{0},\sigma _{0}^{2})&\sim I\chi ^{2}(\nu _{0},\sigma _{0}^{2})=IG(\nu _{0}/2,\nu _{0}\sigma _{0}^{2}/2)\end{aligned} }$$}@} {@{The update equations}@} can be derived, and look as follows: {@{$${\begin{aligned}{\bar {x} }&={\frac {1}{n} }\sum _{i=1}^{n}x_{i}\\\mu _{0}'&={\frac {n_{0}\mu _{0}+n{\bar {x} } }{n_{0}+n} }\\n_{0}'&=n_{0}+n\\\nu _{0}'&=\nu _{0}+n\\\nu _{0}'{\sigma _{0}^{2} }'&=\nu _{0}\sigma _{0}^{2}+\sum _{i=1}^{n}(x_{i}-{\bar {x} })^{2}+{\frac {n_{0}n}{n_{0}+n} }(\mu _{0}-{\bar {x} })^{2}\end{aligned} }$$}@} {@{The respective numbers of pseudo-observations}@} add {@{the number of actual observations to them}@}. {@{The new mean hyperparameter}@} is once again {@{a weighted average}@}, this time {@{weighted by the relative numbers of observations}@}. Finally, {@{the update for $\nu _{0}'{\sigma _{0}^{2} }'$}@} is similar to {@{the case with known mean}@}, but in this case {@{the sum of squared deviations is taken with respect to the observed data mean rather than the true mean}@}, and as a result {@{a new interaction term needs to be added}@} to take care of {@{the additional error source stemming from the deviation between prior and data mean}@}. <!--SR:!2026-03-16,18,323!2026-04-21,43,290!2026-03-13,15,316!2026-04-20,40,303!2026-03-14,16,332!2026-03-14,16,314!2026-03-13,15,316!2026-04-16,39,290!2026-03-16,18,323!2026-03-13,15,316!2026-03-16,18,323!2026-03-13,15,316!2026-03-14,16,309!2026-03-14,16,323-->

## occurrence and applications

{@{The occurrence of normal distribution}@} in practical problems can be loosely {@{classified into four categories}@}: (annotation: 4 items: {@{exactness, central limit theorem, maximum entropy, regression}@}) <!--SR:!2026-03-12,14,290!2026-03-16,18,323!2026-03-13,15,316-->

1. Exactly ::@:: normal distributions; <!--SR:!2026-03-14,16,316!2026-03-16,18,323-->
2. Approximately normal laws, ::@:: for example when such approximation is justified by the [central limit theorem](central%20limit%20theorem.md); and <!--SR:!2026-03-16,18,323!2026-03-16,18,323-->
3. Distributions modeled as normal ::@:: – the normal distribution being the distribution with [maximum entropy](principle%20of%20maximum%20entropy.md) for a given mean and variance. <!--SR:!2026-03-15,17,309!2026-03-15,17,323-->
4. Regression problems ::@:: – the normal distribution being found after systematic effects have been modeled sufficiently well. <!--SR:!2026-03-14,16,311!2026-03-16,18,323-->

### exact normality

> {@{![The ground state of a [quantum harmonic oscillator](quantum%20harmonic%20oscillator.md) has the Gaussian distribution.](../../archives/Wikimedia%20Commons/QHarmonicOscillator.png)}@}
>
> {@{The ground state}@} of {@{a [quantum harmonic oscillator](quantum%20harmonic%20oscillator.md) has the Gaussian distribution}@}. <!--SR:!2026-03-14,16,323!2026-03-15,17,332!2026-03-15,17,323-->

{@{A normal distribution}@} occurs in {@{some [physical theories](physical%20theory.md)}@}: <!--SR:!2026-03-14,16,323!2026-03-14,16,323-->

- {@{The [velocity distribution](Maxwell–Boltzmann%20distribution.md#distribution%20for%20the%20velocity%20vector)}@} of {@{independently moving and perfectly elastic spheres}@}, which is {@{a consequence of [Maxwell's Dynamical Theory of Gases, Part I \(1860\)](Maxwell's%20theorem.md)}@}.<sup>[\[56\]](#^ref-56)</sup><sup>[\[57\]](#^ref-57)</sup>
- {@{The [ground state](ground%20state.md) [wave function](wave%20function.md)}@} in {@{[position space](position%20and%20momentum%20spaces.md#quantum%20mechanics) of the [quantum harmonic oscillator](quantum%20harmonic%20oscillator.md)}@}.<sup>[\[58\]](#^ref-58)</sup>
- {@{The position}@} of {@{a particle that experiences [diffusion](diffusion.md)}@}.<sup>\[_[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation%20needed)_\]</sup> If initially {@{the particle is located at a specific point}@} \(that is its {@{probability distribution is the [Dirac delta function](Dirac%20delta%20function.md)}@}\), then {@{after time _t_ its location is described by a normal distribution with variance _t_}@}, which satisfies {@{the [diffusion equation](diffusion%20equation.md) ${\frac {\partial }{\partial t} }f(x,t)={\frac {1}{2} }{\frac {\partial ^{2} }{\partial x^{2} } }f(x,t)$}@}. If {@{the initial location is given by a certain density function $g(x)$}@}, then {@{the density at time _t_ is the [convolution](convolution.md) of _g_ and the normal probability density function}@}. (annotation: See the related technique of {@{Green's functions in solving differential equations}@}.) <!--SR:!2026-03-12,14,290!2026-03-13,15,316!2026-03-12,14,290!2026-03-15,17,323!2026-03-14,16,316!2026-03-16,18,323!2026-03-14,16,311!2026-03-16,18,323!2026-03-13,15,316!2026-03-12,14,290!2026-03-16,18,332!2026-03-14,16,332!2026-03-12,14,290!2026-03-15,17,323-->

### approximate normality

{@{_Approximately_ normal distributions}@} occur in {@{many situations, as explained by the [central limit theorem](central%20limit%20theorem.md)}@}. When the outcome is produced {@{by many small effects acting _additively and independently_}@}, its {@{distribution will be close to normal}@}. {@{The normal approximation will not be valid}@} if the effects {@{act multiplicatively \(instead of additively\)}@}, or if there is a single {@{external influence that has a considerably larger magnitude than the rest of the effects}@}. <!--SR:!2026-03-14,16,311!2026-03-12,14,290!2026-03-14,16,332!2026-03-15,17,314!2026-03-16,18,323!2026-03-14,16,316!2026-03-15,17,323-->

- In {@{counting problems}@}, where {@{the central limit theorem includes a discrete-to-continuum approximation}@} and where {@{[infinitely divisible](infinitely%20divisible.md) and [decomposable](indecomposable%20distribution.md) distributions}@} are involved, such as (annotation: 2 items: {@{binomial random variables, Poisson random variables}@})
  - {@{[Binomial random variables](binomial%20distribution.md)}@}, {@{associated with binary response variables}@};
  - {@{[Poisson random variables](Poisson%20random%20variables.md)}@}, {@{associated with rare events}@};
- {@{[Thermal radiation](thermal%20radiation.md)}@} has {@{a [Bose–Einstein](Bose–Einstein%20statistics.md) distribution on very short time scales}@}, and {@{a normal distribution on longer timescales}@} due to {@{the central limit theorem}@}. <!--SR:!2026-03-14,16,332!2026-03-16,18,323!2026-03-12,14,290!2026-03-12,14,290!2026-03-13,15,314!2026-03-13,15,311!2026-03-13,15,311!2026-03-15,17,323!2026-03-12,14,290!2026-03-16,18,323!2026-03-13,15,316!2026-03-13,15,309-->

### assumed normality

> {@{![Histogram of sepal widths for _Iris versicolor_ from Fisher's [Iris flower data set](Iris%20flower%20data%20set.md), with superimposed best-fitting normal distribution](../../archives/Wikimedia%20Commons/Fisher%20iris%20versicolor%20sepalwidth.svg)}@}
>
> {@{Histogram of sepal widths for _Iris versicolor_ from Fisher's [Iris flower data set](Iris%20flower%20data%20set.md)}@}, with {@{superimposed best-fitting normal distribution}@} <!--SR:!2026-03-16,18,332!2026-03-12,14,290!2026-03-15,17,323-->

<!-- markdownlint MD028 -->

> I can only recognize {@{the occurrence of the normal curve – the Laplacian curve of errors}@} – as {@{a very abnormal phenomenon}@}. It is roughly {@{approximated to in certain distributions}@}; for {@{this reason, and on account for its beautiful simplicity}@}, we may, perhaps, use it as {@{a first approximation, particularly in theoretical investigations}@}.
>
> &emsp;&emsp;—&hairsp;{@{[Pearson \(1901\)](#CITEREFPearson1901)}@} <!--SR:!2026-03-15,17,323!2026-03-15,17,323!2026-03-16,18,323!2026-03-13,15,316!2026-03-16,18,332!2026-03-16,18,323-->

There are {@{statistical methods}@} to {@{empirically test that assumption}@}; see {@{the above [Normality tests](#normality%20tests)}@} section. <!--SR:!2026-03-13,15,316!2026-03-16,18,332!2026-03-13,15,316-->

- In {@{[biology](biology.md)}@}, {@{the _logarithm_ of various variables}@} tend to {@{have a normal distribution}@}, that is, they tend to {@{have a [log-normal distribution](log-normal%20distribution.md)}@} \(after {@{separation on male/female subpopulations}@}\), with examples including:
  - Measures of {@{size of living tissue}@} \({@{length, height, skin area, weight}@}\);<sup>[\[59\]](#^ref-59)</sup>
  - {@{The _length_ of _inert_ appendages}@} \({@{hair, claws, nails, teeth}@}\) of biological specimens, {@{_in the direction of growth_}@}; presumably {@{the thickness of tree bark}@} also falls under this category;
  - Certain {@{physiological measurements}@}, such as {@{blood pressure of adult humans}@}. <!--SR:!2026-03-13,15,316!2026-03-12,14,290!2026-03-13,15,316!2026-03-15,17,323!2026-03-14,16,323!2026-03-16,18,323!2026-03-12,14,290!2026-03-16,18,323!2026-03-13,15,311!2026-03-14,16,316!2026-03-14,16,323!2026-03-14,16,323!2026-03-12,14,290-->

- In {@{finance, in particular the [Black–Scholes model](Black–Scholes%20model.md)}@}, {@{changes in the _logarithm_ of exchange rates, price indices, and stock market indices}@} are {@{assumed normal}@} \(these variables behave like {@{[compound interest](compound%20interest.md), not like simple interest}@}, and so are {@{multiplicative}@}\). {@{Some mathematicians such as [Benoit Mandelbrot](Benoit%20Mandelbrot.md)}@} have argued that {@{[log-Levy distributions](Levy%20skew%20alpha-stable%20distribution.md)}@}, which {@{possess [heavy tails](heavy%20tails.md), would be a more appropriate model}@}, in particular for {@{the analysis for [stock market crashes](stock%20market%20crash.md)}@}. The use of {@{the assumption of normal distribution occurring in financial models}@} has also been criticized by {@{[Nassim Nicholas Taleb](Nassim%20Nicholas%20Taleb.md) in his works}@}. <!--SR:!2026-03-16,18,323!2026-03-16,18,323!2026-03-14,16,316!2026-03-12,14,290!2026-03-15,17,332!2026-03-13,15,311!2026-03-14,16,332!2026-03-13,15,311!2026-03-14,16,323!2026-03-14,16,332!2026-03-14,16,323-->

- {@{[Measurement errors](propagation%20of%20uncertainty.md) in physical experiments}@} are often {@{modeled by a normal distribution}@}. This use of {@{a normal distribution}@} does not imply that one is {@{assuming the measurement errors are normally distributed}@}, rather {@{using the normal distribution}@} produces {@{the most conservative predictions possible given only knowledge about the mean and variance}@} of the errors.<sup>[\[60\]](#^ref-60)</sup> <!--SR:!2026-03-15,17,323!2026-03-12,14,290!2026-03-13,15,316!2026-03-16,18,323!2026-03-15,17,316!2026-03-13,15,316-->

- In {@{[standardized testing](standardized%20testing%20(statistics).md), results can be made to have a normal distribution}@} by either selecting {@{the number and difficulty of questions \(as in the [IQ test](intelligence%20quotient.md)\)}@} or transforming {@{the raw test scores into output scores by fitting them to the normal distribution}@}. For example, {@{the [SAT](SAT.md)'s traditional range of 200–800}@} is based on {@{a normal distribution with a mean of 500 and a standard deviation of 100}@}. <!--SR:!2026-03-12,14,290!2026-03-13,15,309!2026-03-14,16,332!2026-03-16,18,323!2026-03-12,14,290-->

> {@{![Fitted cumulative normal distribution to October rainfalls](../../archives/Wikimedia%20Commons/FitNormDistr.tif)}@}
>
> Fitted {@{cumulative normal distribution to October rainfalls}@}, see {@{[distribution fitting](distribution%20fitting.md)}@} <!--SR:!2026-03-12,14,290!2026-03-15,17,323!2026-03-14,16,323-->

- {@{Many scores are derived}@} from the normal distribution, including {@{[percentile ranks](percentile%20rank.md) \(percentiles or quantiles\), [normal curve equivalents](normal%20curve%20equivalent.md)}@}, {@{[stanines](stanine.md), [z-scores](z-scores.md), and T-scores}@}. Additionally, {@{some [behavioral statistical](psychological%20statistics.md) procedures}@} assume that {@{scores are normally distributed}@}; for example, {@{[t-tests](t-tests.md) and [ANOVAs](analysis%20of%20variance.md)}@}. {@{[Bell curve grading](bell%20curve%20grading.md)}@} assigns {@{relative grades based on a normal distribution of scores}@}.
- In {@{[hydrology](hydrology.md) the distribution of long duration river discharge or rainfall}@}, e.g. {@{monthly and yearly totals}@}, is often thought to be {@{practically normal according to the [central limit theorem](central%20limit%20theorem.md)}@}.<sup>[\[61\]](#^ref-61)</sup> The plot on the right illustrates an example of fitting {@{the normal distribution to ranked October rainfalls}@} showing {@{the 90% [confidence belt](confidence%20belt.md) based on the [binomial distribution](binomial%20distribution.md)}@}. {@{The rainfall data}@} are represented by {@{[plotting positions](plotting%20position.md#plotting%20positions)}@} as part of {@{the [cumulative frequency analysis](cumulative%20frequency%20analysis.md)}@}. <!--SR:!2026-03-14,16,332!2026-03-16,18,323!2026-03-14,16,316!2026-03-16,18,323!2026-03-16,18,323!2026-03-14,16,323!2026-03-16,18,323!2026-03-13,15,314!2026-03-16,18,323!2026-03-13,15,309!2026-03-16,18,323!2026-03-13,15,316!2026-03-15,17,332!2026-03-14,16,323!2026-03-12,14,290!2026-03-16,18,323-->

### methodological problems and peer review

{@{[John Ioannidis](John%20Ioannidis.md) [argued](Why%20Most%20Published%20Research%20Findings%20Are%20False.md) that using normally distributed standard deviations}@} as standards for validating research findings leave {@{[falsifiable predictions](falsifiability.md) about phenomena that are not normally distributed}@} {@{untested}@}. This includes, for example, phenomena that {@{only appear when all necessary conditions are present}@} and one cannot be {@{a substitute for another in an addition-like way}@} (annotation: item separator) and phenomena that {@{are not randomly distributed}@}. {@{Ioannidis argues that standard deviation-centered validation}@} gives {@{a false appearance of validity to hypotheses and theories}@} where {@{some but not all falsifiable predictions}@} are {@{normally distributed}@} since {@{the portion of falsifiable predictions that there is evidence against}@} may and in some cases are {@{in the non-normally distributed parts of the range of falsifiable predictions}@}, as well as baselessly {@{dismissing hypotheses for which none of the falsifiable predictions are normally distributed}@} as if they were {@{unfalsifiable when in fact they do make falsifiable predictions}@}. It is argued by {@{Ioannidis that many cases of mutually exclusive theories being accepted as validated}@} by research journals are caused by failure of {@{the journals to take in empirical falsifications of non-normally distributed predictions}@}, and not because {@{mutually exclusive theories are true, which they cannot be}@}, although two {@{mutually exclusive theories can both be wrong and a third one correct}@}.<sup>[\[62\]](#^ref-62)</sup> <!--SR:!2026-03-13,15,316!2026-03-16,18,323!2026-03-13,15,316!2026-03-12,14,290!2026-03-14,16,316!2026-03-12,14,290!2026-03-16,18,323!2026-03-16,18,332!2026-03-12,14,290!2026-03-14,16,323!2026-03-14,16,323!2026-03-14,16,332!2026-03-16,18,323!2026-03-15,17,323!2026-03-16,18,323!2026-03-13,15,316!2026-03-14,16,311!2026-03-15,17,332-->

## computational methods

### generating values from normal distribution

> {@{![The [bean machine](bean%20machine.md), a device invented by [Francis Galton](Francis%20Galton.md), can be called the first generator of normal random variables.](../../archives/Wikimedia%20Commons/Planche%20de%20Galton.jpg)}@}
>
> {@{The [bean machine](bean%20machine.md), a device invented by [Francis Galton](Francis%20Galton.md)}@}, can be called {@{the first generator of normal random variables}@}. This machine consists of {@{a vertical board with interleaved rows of pins}@}. {@{Small balls}@} are {@{dropped from the top}@} and then {@{bounce randomly left or right as they hit the pins}@}. The balls are {@{collected into bins at the bottom}@} and settle down {@{into a pattern resembling the Gaussian curve}@}. <!--SR:!2026-03-16,18,323!2026-03-12,14,290!2026-03-12,14,290!2026-03-13,15,316!2026-03-15,17,316!2026-03-12,14,290!2026-03-13,15,316!2026-03-15,17,323!2026-03-12,14,290-->

In {@{computer simulations, especially in applications of the [Monte-Carlo method](Monte-Carlo%20method.md)}@}, it is often {@{desirable to generate values that are normally distributed}@}. {@{The algorithms listed below}@} {@{all generate the standard normal deviates}@}, since {@{a _N_\(_μ_, _σ_<sup>2</sup>\) can be generated as _X_ = _μ_ + _σZ_}@}, where {@{_Z_ is standard normal}@}. {@{All these algorithms}@} rely on {@{the availability of a [random number generator](random%20number%20generator.md) _U_}@} capable of {@{producing [uniform](uniform%20distribution%20(continuous).md) random variates}@}. <!--SR:!2026-03-13,15,316!2026-03-16,18,323!2026-03-12,14,290!2026-03-13,15,316!2026-03-12,14,290!2026-03-14,16,323!2026-03-12,14,290!2026-03-12,14,290!2026-03-12,14,290-->

- {@{The most straightforward method}@} is based on {@{the [probability integral transform](probability%20integral%20transform.md) property}@}: if _U_ is {@{distributed uniformly on \(0,1\)}@}, then {@{_Φ_<sup>−1</sup>\(_U_\) will have the standard normal distribution}@}. {@{The drawback of this method}@} is that it relies on {@{calculation of the [probit function](probit%20function.md) Φ<sup>−1</sup>}@}, which {@{cannot be done analytically}@}. {@{Some approximate methods}@} are described in {@{[Hart \(1968\)](#CITEREFHart1968) and in the [erf](error%20function.md) article}@}. {@{Wichura gives a fast algorithm}@} for computing {@{this function to 16 decimal places}@},<sup>[\[63\]](#^ref-63)</sup> which is used by {@{[R](R%20programming%20language.md) to compute random variates of the normal distribution}@}. <!--SR:!2026-03-16,18,332!2026-03-12,14,290!2026-03-12,14,290!2026-03-16,18,323!2026-03-16,18,323!2026-03-16,18,332!2026-03-16,18,323!2026-03-13,15,316!2026-03-14,16,323!2026-03-13,15,316!2026-03-16,18,323!2026-03-12,14,290-->

- {@{[An easy-to-program approximate approach](Irwin–Hall%20distribution.md#approximating%20a%20normal%20distribution) that relies on the [central limit theorem](central%20limit%20theorem.md)}@} is as follows: generate {@{12 uniform _U_\(0,1\) deviates, add them all up, and subtract 6}@} – {@{the resulting random variable}@} will have {@{approximately standard normal distribution}@}. In truth, the distribution will be {@{[Irwin–Hall](Irwin–Hall%20distribution.md)}@}, which is {@{a 12-section eleventh-order polynomial approximation}@} to the normal distribution. {@{This random deviate}@} will have {@{a limited range of \(−6, 6\)}@}.<sup>[\[64\]](#^ref-64)</sup> Note that in {@{a true normal distribution}@}, only {@{0.00034% of all samples will fall outside ±6<!-- markdown separator -->_σ_}@}. <!--SR:!2026-03-16,18,323!2026-03-14,16,323!2026-03-12,14,290!2026-03-16,18,332!2026-03-16,18,323!2026-03-13,15,309!2026-03-12,14,290!2026-03-16,18,332!2026-03-13,15,316!2026-03-15,17,323-->

- {@{The [Box–Muller method](Box–Muller%20method.md)}@} uses {@{two independent random numbers _U_ and _V_ distributed [uniformly](uniform%20distribution%20(continuous).md) on \(0,1\)}@}. Then {@{the two random variables _X_ and _Y_}@} {@{$$X={\sqrt {-2\ln U} }\,\cos(2\pi V),\qquad Y={\sqrt {-2\ln U} }\,\sin(2\pi V).$$}@} will both have {@{the standard normal distribution, and will be [independent](independence%20(probability%20theory).md)}@}. {@{This formulation arises}@} because for {@{a [bivariate normal](bivariate%20normal.md) random vector \(_X_, _Y_\)}@} {@{the squared norm _X_<sup>2</sup> + _Y_<sup>2</sup>}@} will have {@{the [chi-squared distribution](chi-squared%20distribution.md) with two degrees of freedom}@}, which is {@{an easily generated [exponential random variable](exponential%20random%20variable.md)}@} corresponding to {@{the quantity −2 ln\(_U_\)}@} in these equations; (annotation: {@{the quantile function of an exponential distribution}@} with {@{mean $1 / \lambda = 2$ is $-\ln(1-p) / \lambda$}@}) and {@{the angle}@} is {@{distributed uniformly around the circle}@}, chosen by {@{the random variable _V_}@}. (annotation: Intuitively, you draw {@{a random point in the 2D plane}@} by first {@{drawing a random angle and then a random radius}@}, and then you can convert {@{the point to Cartesian coordinates by using the sine and cosine of the angle}@}.) <!--SR:!2026-03-14,16,316!2026-03-15,17,323!2026-03-15,17,323!2026-03-16,18,332!2026-03-14,16,316!2026-03-13,15,316!2026-03-12,14,290!2026-03-13,15,314!2026-03-15,17,323!2026-03-15,17,309!2026-03-14,16,323!2026-03-15,17,323!2026-03-16,18,323!2026-03-15,17,332!2026-03-13,15,290!2026-03-13,15,290!2026-03-14,16,332!2026-03-13,15,316!2026-03-14,16,311-->

- {@{The [Marsaglia polar method](Marsaglia%20polar%20method.md)}@} is {@{a modification of the Box–Muller method}@} which {@{does not require computation of the sine and cosine functions}@}. In this method, _U_ and _V_ are {@{drawn from the uniform \(−1,1\) distribution}@}, and then {@{_S_ = _U_<sup>2</sup> + _V_<sup>2</sup> is computed}@}. If {@{_S_ is greater or equal to 1}@}, then {@{the method starts over}@}, otherwise {@{the two quantities $$X=U{\sqrt {\frac {-2\ln S}{S} } },\qquad Y=V{\sqrt {\frac {-2\ln S}{S} } }$$}@} are returned. Again, _X_ and _Y_ are {@{independent, standard normal random variables}@}. (annotation: Intuitively, to determine {@{the random phase without using trigonometric functions}@}, you can {@{draw a random point in the unit circle}@}. First, draw {@{a random point $(U, V)$ in the square \(−1,1\) × \(−1,1\)}@}, and if {@{the point is outside the unit disk}@}, then {@{start over}@}. Otherwise, {@{the angle of the point with respect to the origin}@} will be {@{distributed uniformly around the circle}@}. Divide both {@{the x and y coordinates of the point by the radius of the point $\sqrt S$}@} to get {@{the sine and cosine of the angle}@}. Now, note {@{the radius of the point $\sqrt S$}@} follows {@{the distribution of the radius of a point uniformly distributed in the unit disk}@}, which is {@{a distribution with the cumulative distribution function $F_{\sqrt S}(r) = r^2$}@}. Taking {@{its square $S$}@} – {@{$F_S(s) = F_{\sqrt S}(\sqrt s) = s$}@} – it follows that {@{the distribution of $S$ is uniform on \(0,1\)}@}. Finally, as before, multiply by {@{the radius of the point in the 2D plane $\sqrt{-2 \ln S}$}@} to get {@{the normally distributed random variable}@}.) <!--SR:!2026-03-13,15,316!2026-03-15,17,314!2026-03-15,17,323!2026-03-12,14,290!2026-03-16,18,323!2026-03-15,17,323!2026-03-15,17,316!2026-03-13,15,316!2026-03-13,15,316!2026-03-12,14,290!2026-03-16,18,323!2026-03-13,15,309!2026-03-14,16,323!2026-03-15,17,323!2026-03-13,15,311!2026-03-13,15,316!2026-03-13,15,314!2026-03-14,16,332!2026-03-15,17,309!2026-03-15,17,323!2026-03-13,15,316!2026-03-13,15,316!2026-03-15,17,332!2026-03-16,18,323!2026-03-12,14,290!2026-03-16,18,332-->

- {@{The Ratio method}@}<sup>[\[65\]](#^ref-65)</sup> is {@{a rejection method}@}. {@{The algorithm proceeds}@} as follows:
  - Generate {@{two independent uniform deviates _U_ and _V_ (annotation: on \(0, 1\))}@};
  - Compute {@{$X = \sqrt{8 / e} (V - 0.5) / U$}@};
  - {@{Optional}@}: if {@{_X_<sup>2</sup> ≤ 5 − 4<!-- markdown separator -->_e_<sup>1/4</sup>_U_}@} then {@{accept _X_ and terminate algorithm}@};
  - {@{Optional}@}: if {@{_X_<sup>2</sup> ≥ 4<!-- markdown separator -->_e_<sup>−1.35</sup>/<!-- markdown separator -->_U_ + 1.4}@} then {@{reject _X_ and start over from step 1}@};
  - If {@{_X_<sup>2</sup> ≤ −4 ln _U_}@} then {@{accept _X_, otherwise start over the algorithm}@}. <!--SR:!2026-03-16,18,323!2026-03-15,17,323!2026-03-12,14,290!2026-03-12,14,290!2026-03-16,18,323!2026-03-12,14,290!2026-03-14,16,323!2026-03-16,18,323!2026-03-13,15,316!2026-03-14,16,290!2026-03-13,15,311!2026-03-13,15,309!2026-03-15,17,316-->

  {@{The two optional steps}@} allow the evaluation of {@{the logarithm in the last step to be avoided in most cases}@}. These steps can be {@{greatly improved<sup>[\[66\]](#^ref-66)</sup> so that the logarithm is rarely evaluated}@}. <!--SR:!2026-03-12,14,290!2026-03-12,14,290!2026-03-13,15,316-->

- {@{The [ziggurat algorithm](ziggurat%20algorithm.md)}@}<sup>[\[67\]](#^ref-67)</sup> is {@{faster than the Box–Muller transform and still exact}@}. In {@{about 97% of all cases}@} it uses {@{only two random numbers, one random integer and one random uniform}@}, {@{one multiplication and an if-test}@}. Only in {@{3% of the cases}@}, where {@{the combination of those two falls}@} outside {@{the "core of the ziggurat" \(a kind of rejection sampling using logarithms\)}@}, do {@{exponentials and more uniform random numbers have to be employed}@}. <!--SR:!2026-03-16,18,323!2026-03-13,15,316!2026-03-14,16,323!2026-03-16,18,323!2026-03-14,16,323!2026-03-15,17,332!2026-03-12,14,290!2026-03-13,15,316!2026-03-12,14,290-->

- {@{Integer arithmetic}@} can be used to {@{sample from the standard normal distribution}@}.<sup>[\[68\]](#^ref-68)</sup><sup>[\[69\]](#^ref-69)</sup> {@{This method is exact}@} in the sense that it satisfies {@{the conditions of _ideal approximation_}@};<sup>[\[70\]](#^ref-70)</sup> i.e., it is equivalent to sampling {@{a real number from the standard normal distribution}@} and rounding {@{this to the nearest representable floating point number}@}. <!--SR:!2026-03-15,17,323!2026-03-16,18,323!2026-03-14,16,323!2026-03-14,16,323!2026-03-16,18,323!2026-03-14,16,323-->

- There is also some investigation<sup>[\[71\]](#^ref-71)</sup> into {@{the connection between the fast [Hadamard transform](Hadamard%20transform.md) and the normal distribution}@}, since the transform {@{employs just addition and subtraction}@} and by {@{the central limit theorem random}@} numbers from {@{almost any distribution will be transformed into the normal distribution}@}. In this regard {@{a series of Hadamard transforms}@} can be combined {@{with random permutations}@} to turn {@{arbitrary data sets into a normally distributed data}@}. <!--SR:!2026-03-12,14,290!2026-03-12,14,290!2026-03-15,17,323!2026-03-15,17,323!2026-03-14,16,323!2026-03-15,17,323!2026-03-15,17,323-->

### numerical approximations for the normal cumulative distribution function and normal quantile function

{@{The standard normal [cumulative distribution function](cumulative%20distribution%20function.md)}@} is widely used in {@{scientific and statistical computing}@}. <!--SR:!2026-03-16,18,323!2026-03-15,17,323-->

{@{The values _Φ_\(_x_\)}@} may be {@{approximated very accurately by a variety of methods}@}, such as {@{[numerical integration](numerical%20integration.md), [Taylor series](Taylor%20series.md), [asymptotic series](asymptotic%20series.md) and [continued fractions](Gauss's%20continued%20fraction.md#of%20Kummer's%20confluent%20hypergeometric%20function)}@}. {@{Different approximations}@} are used depending on {@{the desired level of accuracy}@}. <!--SR:!2026-03-13,15,316!2026-03-16,18,332!2026-03-12,14,290!2026-03-12,14,290!2026-03-14,16,332-->

- {@{[Zelen & Severo \(1964\)](#CITEREFZelenSevero1964)}@} give {@{the approximation for _Φ_\(_x_\) for _x_ \> 0}@} with the absolute error {@{\|_ε_\(_x_\)\| \< 7.5·10<sup>−8</sup>}@} \(algorithm [26.2.17](https://secure.math.ubc.ca/~cbm/aands/page_932.htm)\): {@{$$\Phi (x)=1-\varphi (x)\left(b_{1}t+b_{2}t^{2}+b_{3}t^{3}+b_{4}t^{4}+b_{5}t^{5}\right)+\varepsilon (x),\qquad t={\frac {1}{1+b_{0}x} },$$}@} where _ϕ_\(_x_\) is {@{the standard normal probability density function}@}, and {@{_b_<sub>0</sub> = 0.2316419, _b_<sub>1</sub> = 0.319381530, _b_<sub>2</sub> = −0.356563782, _b_<sub>3</sub> = 1.781477937, _b_<sub>4</sub> = −1.821255978, _b_<sub>5</sub> = 1.330274429}@}. (annotation: Don't memorize; just for fun!) <!--SR:!2026-03-14,16,316!2026-03-12,14,290!2026-03-16,18,323!2026-03-16,18,323!2026-03-13,15,316!2026-03-14,16,323-->

- {@{[Hart \(1968\)](#CITEREFHart1968)}@} lists some {@{dozens of approximations}@} – by means of {@{rational functions, with or without exponentials}@} – for {@{the erfc\(\) function}@}. {@{His algorithms}@} vary in {@{the degree of complexity and the resulting precision}@}, with {@{maximum absolute precision of 24 digits}@}. {@{An algorithm by [West \(2009\)](#CITEREFWest2009)}@} combines {@{Hart's algorithm 5666 with a [continued fraction](continued%20fraction.md) approximation in the tail}@} to provide {@{a fast computation algorithm with a 16-digit precision}@}. <!--SR:!2026-03-12,14,290!2026-03-14,16,323!2026-03-13,15,316!2026-03-16,18,332!2026-03-12,14,290!2026-03-14,16,323!2026-03-12,14,290!2026-03-12,14,290!2026-03-15,17,323!2026-03-16,18,323-->

- {@{[Cody \(1969\)](#CITEREFCody1969)}@} after recalling {@{Hart68 solution is not suited for erf}@}, gives {@{a solution for both erf and erfc, with maximal relative error bound}@}, via {@{[Rational Chebyshev Approximation](rational%20function.md)}@}. <!--SR:!2026-03-13,15,316!2026-03-15,17,323!2026-03-15,17,323!2026-03-14,16,316-->

- {@{[Marsaglia \(2004\)](#CITEREFMarsaglia2004)}@} suggested {@{a simple algorithm<sup>[\[note 1\]](#^ref-note%201)</sup> based on the Taylor series expansion}@} {@{$$\Phi (x)={\frac {1}{2} }+\varphi (x)\left(x+{\frac {x^{3} }{3} }+{\frac {x^{5} }{3\cdot 5} }+{\frac {x^{7} }{3\cdot 5\cdot 7} }+{\frac {x^{9} }{3\cdot 5\cdot 7\cdot 9} }+\cdots \right)$$}@} for calculating {@{_Φ_\(_x_\) with arbitrary precision}@}. {@{The drawback}@} of this algorithm is {@{comparatively slow calculation time}@} \(for example it takes {@{over 300 iterations}@} to calculate {@{the function with 16 digits of precision when _x_ = 10}@}\). <!--SR:!2026-03-12,14,290!2026-03-15,17,332!2026-03-13,15,309!2026-03-13,15,311!2026-03-14,16,323!2026-03-12,14,290!2026-03-16,18,323!2026-03-15,17,323-->

- {@{The [GNU Scientific Library](GNU%20Scientific%20Library.md)}@} calculates {@{values of the standard normal cumulative distribution function}@} using {@{Hart's algorithms and approximations with [Chebyshev polynomials](Chebyshev%20polynomial.md)}@}. <!--SR:!2026-03-13,15,316!2026-03-13,15,316!2026-03-14,16,311-->

- {@{[Dia \(2023\)](#CITEREFDia2023)}@} proposes {@{the following approximation of $1-\Phi$}@} with a maximum relative error {@{less than $2^{-53}$ $\left(\approx 1.1\times 10^{-16}\right)$ in absolute value}@}: for {@{$x\geq 0$}@} {@{$${\begin{aligned}1-\Phi \left(x\right)&=\left({\frac {0.39894228040143268}{x+2.92678600515804815} }\right)\left({\frac {x^{2}+8.42742300458043240x+18.38871225773938487}{x^{2}+5.81582518933527391x+8.97280659046817350} }\right)\\&\left({\frac {x^{2}+7.30756258553673541x+18.25323235347346525}{x^{2}+5.70347935898051437x+10.27157061171363079} }\right)\left({\frac {x^{2}+5.66479518878470765x+18.61193318971775795}{x^{2}+5.51862483025707963x+12.72323261907760928} }\right)\\&\left({\frac {x^{2}+4.91396098895240075x+24.14804072812762821}{x^{2}+5.26184239579604207x+16.88639562007936908} }\right)\left({\frac {x^{2}+3.83362947800146179x+11.61511226260603247}{x^{2}+4.92081346632882033x+24.12333774572479110} }\right)e^{-{\frac {x^{2} }{2} } }\end{aligned} }$$}@} and for {@{$x<0$}@}, {@{$$1-\Phi \left(x\right)=1-\left(1-\Phi \left(-x\right)\right)$$}@} (annotation: Don't memorize; just for fun!) <!--SR:!2026-03-15,17,323!2026-03-16,18,323!2026-03-13,15,316!2026-03-12,14,290!2026-03-16,18,323!2026-03-16,18,323!2026-03-14,16,323-->

{@{Shore \(1982\) introduced simple approximations}@} that may be incorporated in {@{stochastic optimization models of engineering and operations research}@}, like {@{reliability engineering and inventory analysis}@}. Denoting {@{_p_ = _Φ_\(_z_\)}@}, {@{the simplest approximation for the quantile function}@} is: {@{$$z=\Phi ^{-1}(p)=5.5556\left[1-\left({\frac {1-p}{p} }\right)^{0.1186}\right],\qquad p\geq 1/2$$}@} {@{This approximation delivers for _z_}@} {@{a maximum absolute error of 0.026}@} \(for {@{0.5 ≤ _p_ ≤ 0.9999}@}, corresponding to {@{0 ≤ _z_ ≤ 3.719}@}\). For {@{_p_ \< 1/2}@} {@{replace _p_ by 1 − _p_ and change sign}@}. {@{Another approximation, somewhat less accurate}@}, is {@{the single-parameter approximation}@}: {@{$$z=-0.4115\left\{ {\frac {1-p}{p} }+\log \left[{\frac {1-p}{p} }\right]-1\right\},\qquad p\geq 1/2$$}@} <!--SR:!2026-03-16,18,323!2026-03-16,18,323!2026-03-15,17,323!2026-03-16,18,323!2026-03-12,14,290!2026-03-15,17,332!2026-03-12,14,290!2026-03-12,14,290!2026-03-12,14,290!2026-03-16,18,323!2026-03-12,14,290!2026-03-16,18,332!2026-03-12,14,290!2026-03-14,16,316!2026-03-13,15,316-->

The latter (annotation: {@{$z=-0.4115\left\{ {\frac {1-p}{p} }+\log \left[{\frac {1-p}{p} }\right]-1\right\},\qquad p\geq 1/2$}@}) had served to derive {@{a simple approximation for the loss integral of the normal distribution}@}, defined by {@{$${\begin{aligned}L(z)&=\int _{z}^{\infty }(u-z)\varphi (u)\,du=\int _{z}^{\infty }[1-\Phi (u)]\,du\end{aligned} }$$}@} (annotation: {@{The loss integral}@} is {@{the expected value of the excess of a normal random variable over a threshold _z_}@}. The last equation may be {@{derived by integration by parts}@}, carefully handling {@{the limits of the boundary terms}@}. It may also be visualized as {@{the area of the shaded region}@} in {@{the plot of the cumulative distribution function of the normal distribution}@}, where {@{the excess of the random variable over the threshold _z_}@} is {@{the distance between the curve and the vertical line at _z_}@}.) {@{$${\begin{aligned}L(z)&\approx {\begin{cases}0.4115\left({\dfrac {p}{1-p} }\right)-z,&p<1/2,\\\\0.4115\left({\dfrac {1-p}{p} }\right),&p\geq 1/2.\end{cases} }\\[5pt]{\text{or, equivalently,} }\\L(z)&\approx {\begin{cases}0.4115\left\{1-\log \left[{\frac {p}{1-p} }\right]\right\},&p<1/2,\\\\0.4115{\dfrac {1-p}{p} },&p\geq 1/2.\end{cases} }\end{aligned} }$$}@} (annotation: Hard to {@{explain how the approximation comes from}@}; its {@{derivation is ugly}@}.) {@{This approximation}@} is {@{particularly accurate for the right far-tail}@} \(maximum error of {@{10<sup>−3</sup> for _z_ ≥ 1.4}@}\). {@{Highly accurate approximations}@} for the cumulative distribution function, based on {@{[Response Modeling Methodology](response%20modeling%20methodology.md) \(RMM, Shore, 2011, 2012\), are shown in Shore \(2005\)}@}. <!--SR:!2026-03-13,15,290!2026-03-15,17,323!2026-03-12,14,290!2026-03-12,14,290!2026-03-16,18,323!2026-03-12,14,290!2026-03-15,17,332!2026-03-16,18,323!2026-03-15,17,311!2026-03-14,16,323!2026-03-13,15,309!2026-04-13,34,291!2026-03-12,14,290!2026-03-13,15,311!2026-03-13,15,316!2026-03-15,17,309!2026-03-14,16,332!2026-03-15,17,323!2026-03-13,15,309-->

{@{Some more approximations}@} can be found at: {@{[Error function\#Approximation with elementary functions](error%20function.md#approximation%20with%20elementary%20functions)}@}. In {@{particular, small _relative_ error on the whole domain}@} for {@{the cumulative distribution function ⁠$\Phi$⁠ and the quantile function $\Phi ^{-1}$}@} as well, is achieved via {@{an explicitly invertible formula by Sergei Winitzki in 2008}@}. <!--SR:!2026-03-12,14,290!2026-03-15,17,323!2026-03-14,16,316!2026-03-12,14,290!2026-03-15,17,323-->

## history

### development

{@{Some authors}@}<sup>[\[72\]](#^ref-72)</sup><sup>[\[73\]](#^ref-73)</sup> attribute {@{the discovery of the normal distribution to [de Moivre](de%20Moivre.md)}@}, who in {@{1738<sup>[\[note 2\]](#^ref-note%202)</sup> published in the second edition of his _[The Doctrine of Chances](The%20Doctrine%20of%20Chances.md)_}@} {@{the study of the coefficients in the [binomial expansion](binomial%20expansion.md) of \(_a_ + _b_\)<sup>_n_</sup>}@}. De Moivre proved that {@{the middle term in this expansion}@} has {@{the approximate magnitude of $2^{n}/{\sqrt {2\pi n} }$ (annotation: $\binom n {n / 2} \approx \frac {2^n} {\sqrt{2\pi n} }$)}@}, and that "If {@{_m_ or $\frac 1 2 n$ be a Quantity infinitely great}@}, then {@{the Logarithm of the Ratio}@}, which a Term {@{distant from the middle by the Interval _ℓ_, has to the middle Term}@}, is {@{$-{\frac {2\ell \ell }{n} }$ (annotation: $\binom n {n / 2} / \binom n {n / 2 \pm \ell} \approx \exp\left(-\frac {2 \ell^2} n\right)$)}@}."<sup>[\[74\]](#^ref-74)</sup> Although this theorem can be interpreted as {@{the first obscure expression for the normal probability law}@}, {@{[Stigler](Stephen%20Stigler.md) points out that de Moivre himself}@} did not {@{interpret his results as anything more than the approximate rule for the binomial coefficients}@}, and in particular de Moivre lacked {@{the concept of the probability density function}@}.<sup>[\[75\]](#^ref-75)</sup> <!--SR:!2026-03-12,14,290!2026-03-16,18,323!2026-03-16,18,332!2026-03-13,15,316!2026-03-12,14,290!2026-03-16,18,323!2026-03-13,15,311!2026-03-12,14,290!2026-03-14,16,311!2026-03-16,18,323!2026-03-15,17,323!2026-03-16,18,323!2026-03-16,18,323!2026-03-12,14,290-->

> {@{![In 1809, [Carl Friedrich Gauss](Carl%20Friedrich%20Gauss.md) showed that the normal distribution provides a way to rationalize the [method of least squares](method%20of%20least%20squares.md).](../../archives/Wikimedia%20Commons/Carl%20Friedrich%20Gauss.jpg)}@}
>
> In {@{1809, [Carl Friedrich Gauss](Carl%20Friedrich%20Gauss.md)}@} showed that {@{the normal distribution}@} provides a way to {@{rationalize the [method of least squares](method%20of%20least%20squares.md)}@}. <!--SR:!2026-03-14,16,332!2026-03-14,16,309!2026-03-12,14,290!2026-03-16,18,323-->

In {@{1823 [Gauss](Gauss.md) published his monograph "_Theoria combinationis observationum erroribus minimis obnoxiae_"}@} where among other things he introduces {@{several important statistical concepts}@}, such as {@{the [method of least squares](method%20of%20least%20squares.md), the [method of maximum likelihood](method%20of%20maximum%20likelihood.md), and the _normal distribution_}@}. Gauss used {@{_M_, _M_<!-- markdown separator -->′, _M_<!-- markdown separator -->″, ...}@} to denote {@{the measurements of some unknown quantity _V_}@}, and sought {@{the most probable estimator of that quantity}@}: the one that maximizes {@{the probability _φ_\(_M_ − _V_\) · _φ_\(_M_<!-- markdown separator -->′ − _V_\) · _φ_\(_M_<!-- markdown separator -->″ − _V_\) · ...}@} of {@{obtaining the observed experimental results}@}. In {@{his notation}@} φΔ is {@{the probability density function of the measurement errors of magnitude Δ}@}. Not knowing {@{what the function _φ_ is}@}, Gauss requires that his method should {@{reduce to the well-known answer}@}: {@{the arithmetic mean of the measured values}@}.<sup>[\[note 3\]](#^ref-note%203)</sup> Starting from {@{these principles}@}, Gauss demonstrates that {@{the only law that rationalizes the choice of arithmetic mean as an estimator of the location parameter}@}, is {@{the normal law of errors}@}:<sup>[\[76\]](#^ref-76)</sup> {@{$$\varphi {\mathit {\Delta } }={\frac {h}{\surd \pi } }\,e^{-\mathrm {hh} \Delta \Delta },$$}@} where _h_ is {@{"the measure of the precision of the observations"}@}. Using this normal law as {@{a generic model for errors in the experiments}@}, Gauss formulates what is now known as {@{the [non-linear](non-linear%20least%20squares.md) [weighted least squares](weighted%20least%20squares.md) method}@}.<sup>[\[77\]](#^ref-77)</sup> <!--SR:!2026-03-13,15,316!2026-03-14,16,323!2026-03-16,18,323!2026-03-15,17,323!2026-03-15,17,323!2026-03-13,15,316!2026-03-15,17,323!2026-03-15,17,323!2026-03-15,17,323!2026-03-12,14,290!2026-03-12,14,290!2026-03-13,15,316!2026-03-16,18,323!2026-03-16,18,332!2026-03-12,14,290!2026-03-13,15,316!2026-03-13,15,316!2026-03-13,15,316!2026-03-15,17,311!2026-03-15,17,323-->

> {@{![[Pierre-Simon Laplace](Pierre-Simon%20Laplace.md) proved the [central limit theorem](central%20limit%20theorem.md) in 1810, consolidating the importance of the normal distribution in statistics.](../../archives/Wikimedia%20Commons/Pierre-Simon%20Laplace.jpg)}@}
>
> {@{[Pierre-Simon Laplace](Pierre-Simon%20Laplace.md)}@} proved {@{the [central limit theorem](central%20limit%20theorem.md) in 1810}@}, consolidating {@{the importance of the normal distribution in statistics}@}. <!--SR:!2026-04-29,50,309!2026-03-15,17,323!2026-03-15,17,332!2026-03-15,17,323-->

Although Gauss was {@{the first to suggest the normal distribution law}@}, {@{[Laplace](Laplace.md) made significant contributions}@}.<sup>[\[note 4\]](#^ref-note%204)</sup> It was Laplace who first posed {@{the problem of aggregating several observations in 1774}@},<sup>[\[78\]](#^ref-78)</sup> although his {@{own solution led to the [Laplacian distribution](Laplacian%20distribution.md)}@}. It was Laplace who first {@{calculated the value of the [integral ∫ _e_<sup>−<!-- markdown separator -->_t_<sup>2</sup></sup> _dt_ = √_π_](Gaussian%20integral.md) in 1782}@}, providing {@{the normalization constant for the normal distribution}@}.<sup>[\[79\]](#^ref-79)</sup> For {@{this accomplishment}@}, Gauss {@{acknowledged the priority of Laplace}@}.<sup>[\[80\]](#^ref-80)</sup> Finally, it was Laplace who in {@{1810 proved and presented to the academy the fundamental [central limit theorem](central%20limit%20theorem.md)}@}, which emphasized {@{the theoretical importance of the normal distribution}@}.<sup>[\[81\]](#^ref-81)</sup> <!--SR:!2026-03-15,17,332!2026-03-13,15,316!2026-03-15,17,323!2026-03-15,17,332!2026-03-14,16,323!2026-03-12,14,290!2026-03-16,18,332!2026-03-16,18,332!2026-03-14,16,323!2026-03-13,15,309-->

It is of {@{interest to note}@} that in {@{1809 an Irish-American mathematician [Robert Adrain](Robert%20Adrain.md)}@} published {@{two insightful but flawed derivations of the normal probability law}@}, {@{simultaneously and independently from Gauss}@}.<sup>[\[82\]](#^ref-82)</sup> His works remained {@{largely unnoticed by the scientific community}@}, until in {@{1871 they were exhumed by [Abbe](Cleveland%20Abbe.md)}@}.<sup>[\[83\]](#^ref-83)</sup> <!--SR:!2026-03-16,18,323!2026-03-14,16,323!2026-03-16,18,323!2026-03-14,16,323!2026-03-15,17,332!2026-03-16,18,323-->

In {@{the middle of the 19th century [Maxwell](James%20Clerk%20Maxwell.md)}@} demonstrated that {@{the normal distribution is not just a convenient mathematical tool}@}, but may also {@{occur in natural phenomena}@}:<sup>[\[56\]](#^ref-56)</sup> {@{The number of particles}@} whose {@{velocity, resolved in a certain direction, lies between _x_ and _x_ + _dx_}@} is {@{$$\operatorname {N} {\frac {1}{\alpha \;{\sqrt {\pi } } } }\;e^{-{\frac {x^{2} }{\alpha ^{2} } } }\,dx$$}@} <!--SR:!2026-03-13,15,309!2026-03-14,16,323!2026-03-12,14,290!2026-03-13,15,311!2026-03-13,15,290!2026-03-13,15,316-->

### naming

Today, {@{the concept}@} is usually known in English as {@{the __normal distribution__ or __Gaussian distribution__}@}. {@{Other less common names}@} include {@{Gauss distribution, Laplace–Gauss distribution, the law of error}@}, {@{the law of facility of errors, Laplace's second law, and Gaussian law}@}. <!--SR:!2026-03-13,15,316!2026-03-12,14,290!2026-03-15,17,309!2026-03-14,16,323!2026-03-15,17,332-->

Gauss himself apparently coined {@{the term with reference to the "normal equations" involved}@} in its applications, with normal having {@{its technical meaning of orthogonal rather than usual}@}.<sup>[\[84\]](#^ref-84)</sup> However, by {@{the end of the 19th century some authors}@}<sup>[\[note 5\]](#^ref-note%205)</sup> had started using {@{the name _normal distribution_}@}, where {@{the word "normal" was used as an adjective}@} – the term now being seen as {@{a reflection of this distribution being seen as typical, common}@} – and thus {@{normal}@}. {@{[Peirce](Charles%20Sanders%20Peirce.md) \(one of those authors\)}@} once defined {@{"normal" thus}@}: "... the 'normal' is not {@{the average \(or any other kind of mean\) of what actually occurs}@}, but of {@{what _would_, in the long run, occur under certain circumstances}@}."<sup>[\[85\]](#^ref-85)</sup> Around {@{the turn of the 20th century [Pearson](Karl%20Pearson.md)}@} popularized {@{the term _normal_ as a designation for this distribution}@}.<sup>[\[86\]](#^ref-86)</sup> <!--SR:!2026-03-15,17,323!2026-03-15,17,323!2026-03-14,16,311!2026-03-15,17,323!2026-03-16,18,323!2026-03-16,18,323!2026-03-15,17,323!2026-03-15,17,323!2026-03-15,17,316!2026-03-15,17,311!2026-03-13,15,316!2026-03-16,18,323!2026-03-13,15,316-->

> Many years ago I called {@{the Laplace–Gaussian curve the _normal_ curve}@}, which name, while it avoids {@{an international question of priority}@}, has {@{the disadvantage of leading people}@} to believe that {@{all other distributions of frequency are in one sense or another 'abnormal'}@}.
>
> &emsp;&emsp;—&hairsp;{@{[Pearson \(1920\)](#CITEREFPearson1920)}@} <!--SR:!2026-03-14,16,314!2026-03-13,15,314!2026-03-16,18,323!2026-03-16,18,323!2026-03-16,18,323-->

Also, it was Pearson who first wrote {@{the distribution in terms of the standard deviation _σ_}@} as in modern notation. Soon after this, in {@{year 1915, [Fisher](Ronald%20Fisher.md)}@} added {@{the location parameter to the formula for normal distribution}@}, expressing it in {@{the way it is written nowadays}@}: {@{$$df={\frac {1}{\sqrt {2\sigma ^{2}\pi } } }e^{-(x-m)^{2}/(2\sigma ^{2})}\,dx.$$}@} <!--SR:!2026-03-14,16,316!2026-03-13,15,316!2026-03-13,15,316!2026-03-14,16,316!2026-03-15,17,332-->

{@{The term _standard normal distribution_}@}, which denotes {@{the normal distribution with zero mean and unit variance}@} came into {@{general use around the 1950s}@}, appearing in {@{the popular textbooks by P. G. Hoel \(1947\) _Introduction to Mathematical Statistics_}@} and {@{[Alexander M. Mood](Alexander%20M.%20Mood.md) \(1950\) _Introduction to the Theory of Statistics_}@}.<sup>[\[87\]](#^ref-87)</sup><sup>[\[88\]](#^ref-88)</sup><sup>[\[89\]](#^ref-89)</sup> <!--SR:!2026-03-12,14,290!2026-03-12,14,290!2026-03-16,18,332!2026-03-14,16,316!2026-03-15,17,323-->

## see also

> ___![icon](../../archives/Wikimedia%20Commons/Nuvola%20apps%20edu%20mathematics%20blue-p.svg) [Mathematics portal](https://en.wikipedia.org/wiki/Portal:Mathematics)___

- [Bates distribution](Bates%20distribution.md) ::@:: – similar to the Irwin–Hall distribution, but rescaled back into the 0 to 1 range <!--SR:!2026-03-15,17,323!2026-03-12,14,290-->
- [Behrens–Fisher problem](Behrens–Fisher%20problem.md) ::@:: – the long-standing problem of testing whether two normal samples with different variances have same means; <!--SR:!2026-03-13,15,316!2026-03-16,18,323-->
- [Bhattacharyya distance](Bhattacharyya%20distance.md) ::@:: – method used to separate mixtures of normal distributions <!--SR:!2026-03-13,15,311!2026-03-12,14,290-->
- [Erdős–Kac theorem](Erdős–Kac%20theorem.md) ::@:: – on the occurrence of the normal distribution in [number theory](number%20theory.md) <!--SR:!2026-03-12,14,290!2026-03-12,14,290-->
- [Full width at half maximum](full%20width%20at%20half%20maximum.md)
- [Gaussian blur](Gaussian%20blur.md) ::@:: – [convolution](convolution.md), which uses the normal distribution as a kernel <!--SR:!2026-03-16,18,323!2026-03-13,15,316-->
- [Gaussian function](Gaussian%20function.md)
- [Modified half-normal distribution](modified%20half-normal%20distribution.md)<sup>[\[90\]](#^ref-90)</sup> ::@:: with the pdf on $(0,\infty )$ is given as $f(x)={\frac {2\beta ^{\alpha /2}x^{\alpha -1}\exp(-\beta x^{2}+\gamma x)}{\Psi \left({\frac {\alpha }{2} },{\frac {\gamma }{\sqrt {\beta } } }\right)} }$, where $\Psi (\alpha ,z)={}_{1}\Psi _{1}\left({\begin{matrix}\left(\alpha ,{\frac {1}{2} }\right)\\(1,0)\end{matrix} };z\right)$ denotes the [Fox–Wright Psi function](Fox–Wright%20Psi%20function.md). <!--SR:!2026-04-15,36,296!2026-03-13,15,316-->
- [Normally distributed and uncorrelated does not imply independent](normally%20distributed%20and%20uncorrelated%20does%20not%20imply%20independent.md)
- [Ratio normal distribution](ratio%20normal%20distribution.md#normal)
- [Reciprocal normal distribution](reciprocal%20normal%20distribution.md#reciprocal%20normal%20distribution)
- [Standard normal table](standard%20normal%20table.md)
- [Stein's lemma](Stein's%20lemma.md)
- [Sub-Gaussian distribution](sub-Gaussian%20distribution.md)
- [Sum of normally distributed random variables](sum%20of%20normally%20distributed%20random%20variables.md)
- [Tweedie distribution](Tweedie%20distribution.md) ::@:: – The normal distribution is a member of the family of Tweedie [exponential dispersion models](exponential%20dispersion%20model.md). <!--SR:!2026-03-12,14,290!2026-03-12,14,290-->
- [Wrapped normal distribution](wrapped%20normal%20distribution.md) ::@:: – the normal distribution applied to a circular domain <!--SR:!2026-03-15,17,323!2026-03-15,17,332-->
- [Z-test](Z-test.md) ::@:: – using the normal distribution <!--SR:!2026-03-16,18,323!2026-03-13,15,314-->

## notes

1. For example, {@{this algorithm}@} is given in {@{the article [Bc programming language](bc%20programming%20language.md#a%20translated%20C%20function)}@}. <a id="^ref-1"></a>^ref-1
2. {@{De Moivre first published his findings in 1733}@}, in {@{a pamphlet _Approximatio ad Summam Terminorum Binomii_ \(_a_ + _b_\)<sup>_n_</sup> _in Seriem Expansi_}@} that was designated {@{for private circulation only}@}. But it was not {@{until the year 1738 that he made his results publicly available}@}. {@{The original pamphlet}@} was {@{reprinted several times}@}, see for example [Walker \(1985\)](#CITEREFWalker1985). <a id="^ref-2"></a>^ref-2
3. "It has been {@{customary certainly to regard as an axiom}@} the hypothesis that if {@{any quantity has been determined by several direct observations}@}, made under {@{the same circumstances and with equal care}@}, {@{the arithmetical mean of the observed values}@} affords {@{the most probable value}@}, if not {@{rigorously, yet very nearly at least}@}, so that it is always {@{most safe to adhere to it}@}." — [Gauss \(1809](#CITEREFGauss1809), section 177\) <a id="^ref-3"></a>^ref-3
4. "{@{My custom of terming the curve}@} {@{the Gauss–Laplacian or _normal_ curve}@} saves us from proportioning {@{the merit of discovery between the two great astronomer mathematicians}@}." quote from [Pearson \(1905](#CITEREFPearson1905), p. 189\) <a id="^ref-4"></a>^ref-4
5. Besides {@{those specifically referenced here}@}, {@{such use (annotation: of the name _normal distribution_)}@} is encountered in {@{the works of [Peirce](Charles%20Sanders%20Peirce.md), [Galton](Galton.md) \([Galton \(1889](#CITEREFGalton1889), chapter V\)\) and [Lexis](Wilhelm%20Lexis.md) \([Lexis \(1878\)](#CITEREFLexis1878), [Rohrbasser & Véron \(2003\)](#CITEREFRohrbasserV%C3%A9ron2003)\) c. 1875}@}.<sup>\[_[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation%20needed)_\]</sup> <a id="^ref-5"></a>^ref-5 <!--SR:!2026-03-15,17,332!2026-03-12,14,290!2026-03-12,14,290!2026-03-13,15,316!2026-03-14,16,323!2026-03-14,16,316!2026-03-16,18,332!2026-03-14,16,316!2026-03-15,17,323!2026-03-15,17,332!2026-03-13,15,316!2026-03-13,15,316!2026-03-15,17,323!2026-03-12,14,290!2026-03-16,18,332!2026-03-16,18,323!2026-03-16,18,323!2026-03-16,18,332!2026-03-12,14,290!2026-03-14,16,316!2026-03-15,17,332-->

## references

This text incorporates [content](https://en.wikipedia.org/wiki/normal_distribution) from [Wikipedia](Wikipedia.md) available under the [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) license.

### citations

1. <a id="CITEREFNortonKhokhlovUryasev2019"></a> Norton, Matthew; Khokhlov, Valentyn; Uryasev, Stan \(2019\). ["Calculating CVaR and bPOE for common probability distributions with application to portfolio optimization and density estimation"](https://web.archive.org/web/20230331230821/http://uryasev.ams.stonybrook.edu/wp-content/uploads/2019/10/Norton2019_CVaR_bPOE.pdf) \(PDF\). _Annals of Operations Research_. __299__ \(1–2\). Springer: 1281–1315. [arXiv](ArXiv%20(identifier).md):[1811.11301](https://arxiv.org/abs/1811.11301). [doi](doi%20(identifier).md):[10.1007/s10479-019-03373-1](https://doi.org/10.1007%2Fs10479-019-03373-1). [S2CID](S2CID%20(identifier).md#S2CID) [254231768](https://api.semanticscholar.org/CorpusID:254231768). Archived from [the original](http://uryasev.ams.stonybrook.edu/wp-content/uploads/2019/10/Norton2019_CVaR_bPOE.pdf) \(PDF\) on March 31, 2023. Retrieved February 27, 2023. <a id="^ref-1"></a>^ref-1
2. <a id="CITEREFTsokosWooten2016"></a> Tsokos, Chris; Wooten, Rebecca \(January 1, 2016\). Tsokos, Chris; Wooten, Rebecca \(eds.\). [_The Joy of Finite Mathematics_](https://linkinghub.elsevier.com/retrieve/pii/B9780128029671000073). Boston: Academic Press. pp. 231–263. [doi](doi%20(identifier).md):[10.1016/b978-0-12-802967-1.00007-3](https://doi.org/10.1016%2Fb978-0-12-802967-1.00007-3). [ISBN](ISBN%20(identifier).md) [978-0-12-802967-1](https://en.wikipedia.org/wiki/Special:BookSources/978-0-12-802967-1). <a id="^ref-2"></a>^ref-2
3. <a id="CITEREFHarris2014"></a> Harris, Frank E. \(January 1, 2014\). Harris, Frank E. \(ed.\). [_Mathematics for Physical Science and Engineering_](https://linkinghub.elsevier.com/retrieve/pii/B9780128010006000183). Boston: Academic Press. pp. 663–709. [doi](doi%20(identifier).md):[10.1016/b978-0-12-801000-6.00018-3](https://doi.org/10.1016%2Fb978-0-12-801000-6.00018-3). [ISBN](ISBN%20(identifier).md) [978-0-12-801000-6](https://en.wikipedia.org/wiki/Special:BookSources/978-0-12-801000-6). <a id="^ref-3"></a>^ref-3
4. [Hoel \(1947](#CITEREFHoel1947), [p. 31](https://archive.org/details/in.ernet.dli.2015.263186/page/n39/mode/2up?q=%22normal+distribution%22)\) and [Mood \(1950](#CITEREFMood1950), [p. 109](https://archive.org/details/introductiontoth0000alex/page/108/mode/2up?q=%22normal+distribution%22)\) give this definition with slightly different notation. <a id="^ref-4"></a>^ref-4
5. [_Normal Distribution_](http://www.encyclopedia.com/topic/Normal_Distribution.aspx#3), Gale Encyclopedia of Psychology <a id="^ref-5"></a>^ref-5
6. [Casella & Berger \(2001](#CITEREFCasellaBerger2001), p. 102\) <a id="^ref-6"></a>^ref-6
7. Lyon, A. \(2014\). [Why are Normal Distributions Normal?](https://aidanlyon.com/normal_distributions.pdf), The British Journal for the Philosophy of Science. <a id="^ref-7"></a>^ref-7
8. <a id="CITEREFJorgeStephan2006"></a> Jorge, Nocedal; Stephan, J. Wright \(2006\). _Numerical Optimization_ \(2nd ed.\). Springer. p. 249. [ISBN](ISBN%20(identifier).md) [978-0387-30303-1](https://en.wikipedia.org/wiki/Special:BookSources/978-0387-30303-1). <a id="^ref-8"></a>^ref-8
9. ["Normal Distribution"](https://www.mathsisfun.com/data/standard-normal-distribution.html). _<www.mathsisfun.com>_. Retrieved August 15, 2020. <a id="^ref-9"></a>^ref-9
10. ["bell curve"](https://www.merriam-webster.com/dictionary/bell%20curve). _Merriam-Webster.com Dictionary_. Retrieved May 25, 2025. <a id="^ref-10"></a>^ref-10
11. [Mood \(1950](#CITEREFMood1950), [p. 112](https://archive.org/details/introductiontoth0000alex/page/112/mode/2up?q=%22standard+normal+distribution%22)\) explicitly defines the _standard normal distribution_. In contrast, [Hoel \(1947\)](#CITEREFHoel1947) explicitly defines the _standard normal curve_ [\(p. 33\)](https://archive.org/details/in.ernet.dli.2015.263186/page/n41/mode/2up?q=%22standard+normal+curve%22) and introduces the term _standard normal distribution_ [\(p. 69\)](https://archive.org/details/in.ernet.dli.2015.263186/page/n77/mode/2up?q=%22standard+normal+distribution%22). <a id="^ref-11"></a>^ref-11
12. [Stigler \(1982\)](#CITEREFStigler1982) <a id="^ref-12"></a>^ref-12
13. [Halperin, Hartley & Hoel \(1965](#CITEREFHalperinHartleyHoel1965), item 7\) <a id="^ref-13"></a>^ref-13
14. [McPherson \(1990](#CITEREFMcPherson1990), p. 110\) <a id="^ref-14"></a>^ref-14
15. [Bernardo & Smith \(2000](#CITEREFBernardoSmith2000), p. 121\) <a id="^ref-15"></a>^ref-15
16. <a id="CITEREFScottNowak2003"></a> Scott, Clayton; Nowak, Robert \(August 7, 2003\). ["The Q-function"](http://cnx.org/content/m11537/1.2/). _Connexions_. <a id="^ref-16"></a>^ref-16
17. <a id="CITEREFBarak2006"></a> Barak, Ohad \(April 6, 2006\). ["Q Function and Error Function"](https://web.archive.org/web/20090325160012/http://www.eng.tau.ac.il/~jo/academic/Q.pdf) \(PDF\). Tel Aviv University. Archived from [the original](http://www.eng.tau.ac.il/~jo/academic/Q.pdf) \(PDF\) on March 25, 2009. <a id="^ref-17"></a>^ref-17
18. <a id="CITEREFWeisstein"></a> [Weisstein, Eric W.](Eric%20W.%20Weisstein.md) ["Normal Distribution Function"](https://mathworld.wolfram.com/NormalDistributionFunction.html). _[MathWorld](MathWorld.md)_. <a id="^ref-18"></a>^ref-18
19. <a id="CITEREFAbramowitzStegun1983"></a> [Abramowitz, Milton](Milton%20Abramowitz.md); [Stegun, Irene Ann](Irene%20Stegun.md), eds. \(1983\) \[June 1964\]. ["Chapter 26, eqn 26.2.12"](http://www.math.ubc.ca/~cbm/aands/page_932.htm). [_Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables_](Abramowitz%20and%20Stegun.md). Applied Mathematics Series. Vol. 55 \(Ninth reprint with additional corrections of tenth original printing with corrections \(December 1972\); first ed.\). Washington D.C.; New York: United States Department of Commerce, National Bureau of Standards; Dover Publications. p. 932. [ISBN](ISBN%20(identifier).md) [978-0-486-61272-0](https://en.wikipedia.org/wiki/Special:BookSources/978-0-486-61272-0). [LCCN](LCCN%20(identifier).md) [64-60036](https://lccn.loc.gov/64-60036). [MR](MR%20(identifier).md) [0167642](https://mathscinet.ams.org/mathscinet-getitem?mr=0167642). [LCCN](LCCN%20(identifier).md) [65-12253](https://www.loc.gov/item/65012253). <a id="^ref-19"></a>^ref-19
20. <a id="CITEREFVaart1998"></a> Vaart, A. W. van der \(October 13, 1998\). [_Asymptotic Statistics_](https://dx.doi.org/10.1017/cbo9780511802256). Cambridge University Press. [doi](doi%20(identifier).md):[10.1017/cbo9780511802256](https://doi.org/10.1017%2Fcbo9780511802256). [ISBN](ISBN%20(identifier).md) [978-0-511-80225-6](https://en.wikipedia.org/wiki/Special:BookSources/978-0-511-80225-6). <a id="^ref-20"></a>^ref-20
21. [Cover & Thomas \(2006\)](#CITEREFCoverThomas2006), p. 254. <a id="^ref-21"></a>^ref-21
22. <a id="CITEREFParkBera2009"></a> Park, Sung Y.; Bera, Anil K. \(2009\). ["Maximum Entropy Autoregressive Conditional Heteroskedasticity Model"](https://web.archive.org/web/20160307144515/http://wise.xmu.edu.cn/uploadfiles/paper-masterdownload/2009519932327055475115776.pdf) \(PDF\). _Journal of Econometrics_. __150__ \(2\): 219–230. [Bibcode](bibcode%20(identifier).md):[2009JEcon.150..219P](https://ui.adsabs.harvard.edu/abs/2009JEcon.150..219P). [CiteSeerX](CiteSeerX%20(identifier).md) [10.1.1.511.9750](https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.511.9750). [doi](doi%20(identifier).md):[10.1016/j.jeconom.2008.12.014](https://doi.org/10.1016%2Fj.jeconom.2008.12.014). Archived from [the original](http://www.wise.xmu.edu.cn/Master/Download/..%5C..%5CUploadFiles%5Cpaper-masterdownload%5C2009519932327055475115776.pdf) \(PDF\) on March 7, 2016. Retrieved June 2, 2011. <a id="^ref-22"></a>^ref-22
23. Geary RC\(1936\) The distribution of the "Student's ratio for the non-normal samples". Supplement to the Journal of the Royal Statistical Society 3 \(2\): 178–184 <a id="^ref-23"></a>^ref-23
24. <a id="CITEREFLukacs, Eugene1942"></a> [Lukacs, Eugene](Eugene%20Lukacs.md) \(March 1942\). "A Characterization of the Normal Distribution". _[Annals of Mathematical Statistics](Annals%20of%20Mathematical%20Statistics.md)_. __13__ \(1\): 91–93. [doi](doi%20(identifier).md):[10.1214/AOMS/1177731647](https://doi.org/10.1214%2FAOMS%2F1177731647). [ISSN](ISSN%20(identifier).md) [0003-4851](https://search.worldcat.org/issn/0003-4851). [JSTOR](JSTOR%20(identifier).md#content) [2236166](https://www.jstor.org/stable/2236166). [MR](MR%20(identifier).md) [0006626](https://mathscinet.ams.org/mathscinet-getitem?mr=0006626). [Zbl](Zbl%20(identifier).md) [0060.28509](https://zbmath.org/?format=complete&q=an:0060.28509). [Wikidata](WDQ%20(identifier).md) [Q55897617](../not found/Q55897617.md). <a id="^ref-24"></a>^ref-24
25. [Patel & Read \(1996](#CITEREFPatelRead1996), \[2.1.4\]\) <a id="^ref-25"></a>^ref-25
26. [Fan \(1991](#CITEREFFan1991), p. 1258\) <a id="^ref-26"></a>^ref-26
27. [Patel & Read \(1996](#CITEREFPatelRead1996), \[2.1.8\]\) <a id="^ref-27"></a>^ref-27
28. <a id="CITEREFPapoulis"></a> Papoulis, Athanasios. _Probability, Random Variables and Stochastic Processes_ \(4th ed.\). p. 148. <a id="^ref-28"></a>^ref-28
29. <a id="CITEREFWinkelbauer2012"></a> Winkelbauer, Andreas \(2012\). "Moments and Absolute Moments of the Normal Distribution". [arXiv](ArXiv%20(identifier).md):[1209.4340](https://arxiv.org/abs/1209.4340) \[[math.ST](https://arxiv.org/archive/math.ST)\]. <a id="^ref-29"></a>^ref-29
30. [Bryc \(1995](#CITEREFBryc1995), p. 23\) <a id="^ref-30"></a>^ref-30
31. [Bryc \(1995](#CITEREFBryc1995), p. 24\) <a id="^ref-31"></a>^ref-31
32. <a id="CITEREFWilliams2001"></a> Williams, David \(2001\). [_Weighing the odds : a course in probability and statistics_](https://archive.org/details/weighingoddscour00will) \(Reprinted. ed.\). Cambridge \[u.a.\]: Cambridge Univ. Press. pp. [197](https://archive.org/details/weighingoddscour00will/page/n219)–199. [ISBN](ISBN%20(identifier).md) [978-0-521-00618-7](https://en.wikipedia.org/wiki/Special:BookSources/978-0-521-00618-7). <a id="^ref-32"></a>^ref-32
33. <a id="CITEREFJosé M. BernardoAdrian F. M. Smith2000"></a> José M. Bernardo; Adrian F. M. Smith \(2000\). [_Bayesian theory_](https://archive.org/details/bayesiantheory00bern_963) \(Reprint ed.\). Chichester \[u.a.\]: Wiley. pp. [209](https://archive.org/details/bayesiantheory00bern_963/page/n224), 366. [ISBN](ISBN%20(identifier).md) [978-0-471-49464-5](https://en.wikipedia.org/wiki/Special:BookSources/978-0-471-49464-5). <a id="^ref-33"></a>^ref-33
34. O'Hagan, A. \(1994\) _Kendall's Advanced Theory of statistics, Vol 2B, Bayesian Inference_, Edward Arnold. [ISBN](ISBN%20(identifier).md) [0-340-52922-9](https://en.wikipedia.org/wiki/Special:BookSources/0-340-52922-9) \(Section 5.40\) <a id="^ref-34"></a>^ref-34
35. [Bryc \(1995](#CITEREFBryc1995), p. 35\) <a id="^ref-35"></a>^ref-35
36. [UIUC, Lecture 21. _The Multivariate Normal Distribution_](http://www.math.uiuc.edu/~r-ash/Stat/StatLec21-25.pdf), 21.6:"Individually Gaussian Versus Jointly Gaussian". <a id="^ref-36"></a>^ref-36
37. Edward L. Melnick and Aaron Tenenbein, "Misspecifications of the Normal Distribution", _[The American Statistician](The%20American%20Statistician.md)_, volume 36, number 4 November 1982, pages 372–373 <a id="^ref-37"></a>^ref-37
38. ["Kullback Leibler \(KL\) Distance of Two Normal \(Gaussian\) Probability Distributions"](http://www.allisons.org/ll/MML/KL/Normal/). _Allisons.org_. December 5, 2007. Retrieved March 3, 2017. <a id="^ref-38"></a>^ref-38
39. <a id="CITEREFJordan2010"></a> Jordan, Michael I. \(February 8, 2010\). ["Stat260: Bayesian Modeling and Inference: The Conjugate Prior for the Normal Distribution"](http://www.cs.berkeley.edu/~jordan/courses/260-spring10/lectures/lecture5.pdf) \(PDF\). <a id="^ref-39"></a>^ref-39
40. [Amari & Nagaoka \(2000\)](#CITEREFAmariNagaoka2000) <a id="^ref-40"></a>^ref-40
41. ["Expectation of the maximum of gaussian random variables"](https://math.stackexchange.com/a/89147). _Mathematics Stack Exchange_. Retrieved April 7, 2024. <a id="^ref-41"></a>^ref-41
42. ["Normal Approximation to Poisson Distribution"](http://www.stat.ucla.edu/~dinov/courses_students.dir/Applets.dir/NormalApprox2PoissonApplet.html). _Stat.ucla.edu_. Retrieved March 3, 2017. <a id="^ref-42"></a>^ref-42
43. [Bryc \(1995](#CITEREFBryc1995), p. 27\) <a id="^ref-43"></a>^ref-43
44. <a id="CITEREFWeisstein"></a> Weisstein, Eric W. ["Normal Product Distribution"](http://mathworld.wolfram.com/NormalProductDistribution.html). _MathWorld_. wolfram.com. <a id="^ref-44"></a>^ref-44
45. <a id="CITEREFLukacs1942"></a> Lukacs, Eugene \(1942\). ["A Characterization of the Normal Distribution"](https://doi.org/10.1214%2Faoms%2F1177731647). _[The Annals of Mathematical Statistics](the%20Annals%20of%20Mathematical%20Statistics.md)_. __13__ \(1\): 91–3. [doi](doi%20(identifier).md):[10.1214/aoms/1177731647](https://doi.org/10.1214%2Faoms%2F1177731647). [ISSN](ISSN%20(identifier).md) [0003-4851](https://search.worldcat.org/issn/0003-4851). [JSTOR](JSTOR%20(identifier).md#content) [2236166](https://www.jstor.org/stable/2236166). <a id="^ref-45"></a>^ref-45
46. <a id="CITEREFBasuLaha1954"></a> Basu, D.; Laha, R. G. \(1954\). "On Some Characterizations of the Normal Distribution". _[Sankhyā](Sankhyā%20(journal).md)_. __13__ \(4\): 359–62. [ISSN](ISSN%20(identifier).md) [0036-4452](https://search.worldcat.org/issn/0036-4452). [JSTOR](JSTOR%20(identifier).md#content) [25048183](https://www.jstor.org/stable/25048183). <a id="^ref-46"></a>^ref-46
47. <a id="CITEREFLehmann1997"></a> Lehmann, E. L. \(1997\). _Testing Statistical Hypotheses_ \(2nd ed.\). Springer. p. 199. [ISBN](ISBN%20(identifier).md) [978-0-387-94919-2](https://en.wikipedia.org/wiki/Special:BookSources/978-0-387-94919-2). <a id="^ref-47"></a>^ref-47
48. [Patel & Read \(1996](#CITEREFPatelRead1996), \[2.3.6\]\) <a id="^ref-48"></a>^ref-48
49. [Galambos & Simonelli \(2004](#CITEREFGalambosSimonelli2004), Theorem 3.5\) <a id="^ref-49"></a>^ref-49
50. [Lukacs & King \(1954\)](#CITEREFLukacsKing1954) <a id="^ref-50"></a>^ref-50
51. <a id="CITEREFQuine1993"></a> Quine, M.P. \(1993\). ["On three characterisations of the normal distribution"](http://www.math.uni.wroc.pl/~pms/publicationsArticle.php?nr=14.2&nrA=8&ppB=257&ppE=263). _Probability and Mathematical Statistics_. __14__ \(2\): 257–263. <a id="^ref-51"></a>^ref-51
52. <a id="CITEREFJohn1982"></a> John, S \(1982\). "The three parameter two-piece normal family of distributions and its fitting". _Communications in Statistics – Theory and Methods_. __11__ \(8\): 879–885. [doi](doi%20(identifier).md):[10.1080/03610928208828279](https://doi.org/10.1080%2F03610928208828279). <a id="^ref-52"></a>^ref-52
53. [Krishnamoorthy \(2006](#CITEREFKrishnamoorthy2006), p. 127\) <a id="^ref-53"></a>^ref-53
54. [Krishnamoorthy \(2006](#CITEREFKrishnamoorthy2006), p. 130\) <a id="^ref-54"></a>^ref-54
55. [Krishnamoorthy \(2006](#CITEREFKrishnamoorthy2006), p. 133\) <a id="^ref-55"></a>^ref-55
56. [Maxwell \(1860\)](#CITEREFMaxwell1860), p. 23. <a id="^ref-56"></a>^ref-56
57. [Bryc \(1995\)](#CITEREFBryc1995), p. 1. <a id="^ref-57"></a>^ref-57
58. <a id="CITEREFLarkoski2023"></a> Larkoski, Andrew J. \(2023\). [_Quantum Mechanics: A Mathematical Introduction_](https://books.google.com/books?id=iKmnEAAAQBAJ&dq=normal%20distribution&pg=PA120). United Kingdom: Cambridge University Press. pp. 120–121. [ISBN](ISBN%20(identifier).md) [978-1-009-12222-1](https://en.wikipedia.org/wiki/Special:BookSources/978-1-009-12222-1). Retrieved May 30, 2025. <a id="^ref-58"></a>^ref-58
59. [Huxley \(1932\)](#CITEREFHuxley1932) <a id="^ref-59"></a>^ref-59
60. <a id="CITEREFJaynes2003"></a> Jaynes, Edwin T. \(2003\). [_Probability Theory: The Logic of Science_](https://books.google.com/books?id=tTN4HuUNXjgC&pg=PA592). Cambridge University Press. pp. 592–593. [ISBN](ISBN%20(identifier).md) [9780521592710](https://en.wikipedia.org/wiki/Special:BookSources/9780521592710). <a id="^ref-60"></a>^ref-60
61. <a id="CITEREFOosterbaan1994"></a> Oosterbaan, Roland J. \(1994\). ["Chapter 6: Frequency and Regression Analysis of Hydrologic Data"](http://www.waterlog.info/pdf/freqtxt.pdf) \(PDF\). In Ritzema, Henk P. \(ed.\). _Drainage Principles and Applications, Publication 16_ \(second revised ed.\). Wageningen, The Netherlands: International Institute for Land Reclamation and Improvement \(ILRI\). pp. 175–224. [ISBN](ISBN%20(identifier).md) [978-90-70754-33-4](https://en.wikipedia.org/wiki/Special:BookSources/978-90-70754-33-4). <a id="^ref-61"></a>^ref-61
62. Why Most Published Research Findings Are False, John P. A. Ioannidis, 2005 <a id="^ref-62"></a>^ref-62
63. <a id="CITEREFWichura1988"></a> Wichura, Michael J. \(1988\). "Algorithm AS241: The Percentage Points of the Normal Distribution". _Applied Statistics_. __37__ \(3\): 477–84. [doi](doi%20(identifier).md):[10.2307/2347330](https://doi.org/10.2307%2F2347330). [JSTOR](JSTOR%20(identifier).md#content) [2347330](https://www.jstor.org/stable/2347330). <a id="^ref-63"></a>^ref-63
64. [Johnson, Kotz & Balakrishnan \(1995](#CITEREFJohnsonKotzBalakrishnan1995), Equation \(26.48\)\) <a id="^ref-64"></a>^ref-64
65. [Kinderman & Monahan \(1977\)](#CITEREFKindermanMonahan1977) <a id="^ref-65"></a>^ref-65
66. [Leva \(1992\)](#CITEREFLeva1992) <a id="^ref-66"></a>^ref-66
67. [Marsaglia & Tsang \(2000\)](#CITEREFMarsagliaTsang2000) <a id="^ref-67"></a>^ref-67
68. [Karney \(2016\)](#CITEREFKarney2016) <a id="^ref-68"></a>^ref-68
69. [Du, Fan & Wei \(2022\)](#CITEREFDuFanWei2022) <a id="^ref-69"></a>^ref-69
70. [Monahan \(1985](#CITEREFMonahan1985), section 2\) <a id="^ref-70"></a>^ref-70
71. [Wallace \(1996\)](#CITEREFWallace1996) <a id="^ref-71"></a>^ref-71
72. [Johnson, Kotz & Balakrishnan \(1994](#CITEREFJohnsonKotzBalakrishnan1994), p. 85\) <a id="^ref-72"></a>^ref-72
73. [Le Cam & Lo Yang \(2000](#CITEREFLe%20CamLo%20Yang2000), p. 74\) <a id="^ref-73"></a>^ref-73
74. De Moivre, Abraham \(1733\), Corollary I – see [Walker \(1985](#CITEREFWalker1985), p. 77\) <a id="^ref-74"></a>^ref-74
75. [Stigler \(1986](#CITEREFStigler1986), [p. 76](https://archive.org/details/historyofstatist00stig/page/76/mode/2up?q=%22de+moivre%22)\) <a id="^ref-75"></a>^ref-75
76. [Gauss \(1809](#CITEREFGauss1809), section 177\) <a id="^ref-76"></a>^ref-76
77. [Gauss \(1809](#CITEREFGauss1809), section 179\) <a id="^ref-77"></a>^ref-77
78. [Laplace \(1774](#CITEREFLaplace1774), Problem III\) <a id="^ref-78"></a>^ref-78
79. [Pearson \(1905](#CITEREFPearson1905), p. 189\) <a id="^ref-79"></a>^ref-79
80. [Gauss \(1809](#CITEREFGauss1809), section 177\) <a id="^ref-80"></a>^ref-80
81. [Stigler \(1986](#CITEREFStigler1986), p. 144\) <a id="^ref-81"></a>^ref-81
82. [Stigler \(1978](#CITEREFStigler1978), p. 243\) <a id="^ref-82"></a>^ref-82
83. [Stigler \(1978](#CITEREFStigler1978), p. 244\) <a id="^ref-83"></a>^ref-83
84. Jaynes, Edwin J.; _Probability Theory: The Logic of Science_, [Ch. 7](http://www-biba.inrialpes.fr/Jaynes/cc07s.pdf). <a id="^ref-84"></a>^ref-84
85. Peirce, Charles S. \(c. 1909 MS\), _[Collected Papers](Charles%20Sanders%20Peirce%20bibliography.md#CP)_ v. 6, paragraph 327. <a id="^ref-85"></a>^ref-85
86. [Kruskal & Stigler \(1997\)](#CITEREFKruskalStigler1997). <a id="^ref-86"></a>^ref-86
87. ["Earliest Uses... \(Entry Standard Normal Curve\)"](http://jeff560.tripod.com/s.html). <a id="^ref-87"></a>^ref-87
88. [Hoel \(1947\)](#CITEREFHoel1947) introduces the terms _standard normal curve_ [\(p. 33\)](https://archive.org/details/in.ernet.dli.2015.263186/page/n41/mode/2up?q=%22standard+normal+curve%22) and _standard normal distribution_ [\(p. 69\)](https://archive.org/details/in.ernet.dli.2015.263186/page/n77/mode/2up?q=%22standard+normal+distribution%22). <a id="^ref-88"></a>^ref-88
89. [Mood \(1950\)](#CITEREFMood1950) explicitly defines the _standard normal distribution_ [\(p. 112\)](https://archive.org/details/introductiontoth0000alex/page/112/mode/2up?q=%22standard+normal+distribution%22). <a id="^ref-89"></a>^ref-89
90. <a id="CITEREFSunKongPal2021"></a> Sun, Jingchao; Kong, Maiying; Pal, Subhadip \(June 22, 2021\). ["The Modified-Half-Normal distribution: Properties and an efficient sampling scheme"](https://www.tandfonline.com/doi/abs/10.1080/03610926.2021.1934700?journalCode=lsta20). _Communications in Statistics – Theory and Methods_. __52__ \(5\): 1591–1613. [doi](doi%20(identifier).md):[10.1080/03610926.2021.1934700](https://doi.org/10.1080%2F03610926.2021.1934700). [ISSN](ISSN%20(identifier).md) [0361-0926](https://search.worldcat.org/issn/0361-0926). [S2CID](S2CID%20(identifier).md#S2CID) [237919587](https://api.semanticscholar.org/CorpusID:237919587). <a id="^ref-90"></a>^ref-90

### sources

- <a id="CITEREFAldrichMiller"></a> Aldrich, John; Miller, Jeff. ["Earliest Uses of Symbols in Probability and Statistics"](http://jeff560.tripod.com/stat.html).
- <a id="CITEREFAldrichMiller"></a> Aldrich, John; Miller, Jeff. ["Earliest Known Uses of Some of the Words of Mathematics"](http://jeff560.tripod.com/mathword.html). In particular, the entries for ["bell-shaped and bell curve"](http://jeff560.tripod.com/b.html), ["normal \(distribution\)"](http://jeff560.tripod.com/n.html), ["Gaussian"](http://jeff560.tripod.com/g.html), and ["Error, law of error, theory of errors, etc."](http://jeff560.tripod.com/e.html).
- <a id="CITEREFAmariNagaoka2000"></a> [Amari, Shun'ichi](Shun'ichi%20Amari.md); Nagaoka, Hiroshi \(2000\). _Methods of Information Geometry_. Oxford University Press. [ISBN](ISBN%20(identifier).md) [978-0-8218-0531-2](https://en.wikipedia.org/wiki/Special:BookSources/978-0-8218-0531-2).
- <a id="CITEREFBernardoSmith2000"></a> [Bernardo, José M.](José-Miguel%20Bernardo.md); [Smith, Adrian F. M.](Adrian%20Smith%20(statistician).md) \(2000\). _Bayesian Theory_. Wiley. [ISBN](ISBN%20(identifier).md) [978-0-471-49464-5](https://en.wikipedia.org/wiki/Special:BookSources/978-0-471-49464-5).
- <a id="CITEREFBryc1995"></a> Bryc, Wlodzimierz \(1995\). [_The Normal Distribution: Characterizations with Applications_](https://books.google.com/books?id=tyXjBwAAQBAJ). Springer-Verlag. [ISBN](ISBN%20(identifier).md) [978-0-387-97990-8](https://en.wikipedia.org/wiki/Special:BookSources/978-0-387-97990-8).
- <a id="CITEREFCasellaBerger2001"></a> [Casella, George](George%20Casella.md); [Berger, Roger L.](Roger%20Lee%20Berger.md) \(2001\). _Statistical Inference_ \(2nd ed.\). Duxbury. [ISBN](ISBN%20(identifier).md) [978-0-534-24312-8](https://en.wikipedia.org/wiki/Special:BookSources/978-0-534-24312-8).
- <a id="CITEREFCody1969"></a> Cody, William J. \(1969\). ["Rational Chebyshev Approximations for the Error Function"](error%20function.md#cite%20note-5). _Mathematics of Computation_. __23__ \(107\): 631–638. [Bibcode](bibcode%20(identifier).md):[1969MaCom..23..631C](https://ui.adsabs.harvard.edu/abs/1969MaCom..23..631C). [doi](doi%20(identifier).md):[10.1090/S0025-5718-1969-0247736-4](https://doi.org/10.1090%2FS0025-5718-1969-0247736-4).
- <a id="CITEREFCoverThomas2006"></a> [Cover, Thomas M.](Thomas%20M.%20Cover.md); [Thomas, Joy A.](Joy%20A.%20Thomas.md) \(2006\). [_Elements of Information Theory_](https://books.google.com/books?id=VWq5GG6ycxMC). John Wiley and Sons. [ISBN](ISBN%20(identifier).md) [9780471241959](https://en.wikipedia.org/wiki/Special:BookSources/9780471241959).
- <a id="CITEREFDia2023"></a> Dia, Yaya D. \(2023\). ["Approximate Incomplete Integrals, Application to Complementary Error Function"](https://ssrn.com/abstract=4487559). _SSRN_. [doi](doi%20(identifier).md):[10.2139/ssrn.4487559](https://doi.org/10.2139%2Fssrn.4487559). [S2CID](S2CID%20(identifier).md#S2CID) [259689086](https://api.semanticscholar.org/CorpusID:259689086).
- <a id="CITEREFde Moivre1738"></a> [de Moivre, Abraham](Abraham%20de%20Moivre.md) \(2000\) \[First published 1738\]. [_The Doctrine of Chances_](The%20Doctrine%20of%20Chances.md). American Mathematical Society. [ISBN](ISBN%20(identifier).md) [978-0-8218-2103-9](https://en.wikipedia.org/wiki/Special:BookSources/978-0-8218-2103-9).
- <a id="CITEREFDuFanWei2022"></a> Du, Y.; Fan, B.; Wei, B. \(2022\). "An improved exact sampling algorithm for the standard normal distribution". _Computational Statistics_. __37__ \(2\): 721–737. [arXiv](ArXiv%20(identifier).md):[2008.03855](https://arxiv.org/abs/2008.03855). [doi](doi%20(identifier).md):[10.1007/s00180-021-01136-w](https://doi.org/10.1007%2Fs00180-021-01136-w).
- <a id="CITEREFFan1991"></a> Fan, Jianqing \(1991\). ["On the optimal rates of convergence for nonparametric deconvolution problems"](https://doi.org/10.1214%2Faos%2F1176348248). _The Annals of Statistics_. __19__ \(3\): 1257–1272. [doi](doi%20(identifier).md):[10.1214/aos/1176348248](https://doi.org/10.1214%2Faos%2F1176348248). [JSTOR](JSTOR%20(identifier).md#content) [2241949](https://www.jstor.org/stable/2241949).
- <a id="CITEREFGalton1889"></a> [Galton, Francis](Francis%20Galton.md) \(1889\). [_Natural Inheritance_](http://galton.org/books/natural-inheritance/pdf/galton-nat-inh-1up-clean.pdf) \(PDF\). London, UK: Richard Clay and Sons.
- <a id="CITEREFGalambosSimonelli2004"></a> [Galambos, Janos](Janos%20Galambos.md); Simonelli, Italo \(2004\). [_Products of Random Variables: Applications to Problems of Physics and to Arithmetical Functions_](https://archive.org/details/productsofrandom00gala). Marcel Dekker, Inc. [ISBN](ISBN%20(identifier).md) [978-0-8247-5402-0](https://en.wikipedia.org/wiki/Special:BookSources/978-0-8247-5402-0).
- <a id="CITEREFGauss1809"></a> [Gauss, Carolo Friderico](Carl%20Friedrich%20Gauss.md) \(1809\). [_Theoria motvs corporvm coelestivm in sectionibvs conicis Solem ambientivm_](https://archive.org/details/theoriamotuscor00gausgoog) \[_Theory of the Motion of the Heavenly Bodies Moving about the Sun in Conic Sections_\] \(in Latin\). Hambvrgi, Svmtibvs F. Perthes et I. H. Besser. [English translation](https://books.google.com/books?id=1TIAAAAAQAAJ).
- <a id="CITEREFGould1981"></a> [Gould, Stephen Jay](Stephen%20Jay%20Gould.md) \(1981\). [_The Mismeasure of Man_](The%20Mismeasure%20of%20Man.md) \(first ed.\). W. W. Norton. [ISBN](ISBN%20(identifier).md) [978-0-393-01489-1](https://en.wikipedia.org/wiki/Special:BookSources/978-0-393-01489-1).
- <a id="CITEREFHalperinHartleyHoel1965"></a> Halperin, Max; Hartley, Herman O.; Hoel, Paul G. \(1965\). "Recommended Standards for Statistical Symbols and Notation. COPSS Committee on Symbols and Notation". _The American Statistician_. __19__ \(3\): 12–14. [doi](doi%20(identifier).md):[10.2307/2681417](https://doi.org/10.2307%2F2681417). [JSTOR](JSTOR%20(identifier).md#content) [2681417](https://www.jstor.org/stable/2681417).
- <a id="CITEREFHart1968"></a> Hart, John F.; et al. \(1968\). _Computer Approximations_. New York, NY: John Wiley & Sons, Inc. [ISBN](ISBN%20(identifier).md) [978-0-88275-642-4](https://en.wikipedia.org/wiki/Special:BookSources/978-0-88275-642-4).
- ["Normal Distribution"](https://www.encyclopediaofmath.org/index.php?title=Normal_Distribution), _[Encyclopedia of Mathematics](Encyclopedia%20of%20Mathematics.md)_, [EMS Press](European%20Mathematical%20Society.md), 2001 \[1994\]
- <a id="CITEREFHerrnsteinMurray1994"></a> [Herrnstein, Richard J.](Richard%20J.%20Herrnstein.md); [Murray, Charles](Charles%20Murray%20(political%20scientist).md) \(1994\). [_The Bell Curve: Intelligence and Class Structure in American Life_](The%20Bell%20Curve.md). [Free Press](Free%20Press%20(publisher).md). [ISBN](ISBN%20(identifier).md) [978-0-02-914673-6](https://en.wikipedia.org/wiki/Special:BookSources/978-0-02-914673-6).
- <a id="CITEREFHoel1947"></a> Hoel, Paul G. \(1947\). [_Introduction To Mathematical Statistics_](https://archive.org/details/in.ernet.dli.2015.263186/page/n1/mode/2up). New York: Wiley.
- <a id="CITEREFHuxley1932"></a> [Huxley, Julian S.](Julian%20S.%20Huxley.md) \(1972\) \[First published 1932\]. _Problems of Relative Growth_. London. [ISBN](ISBN%20(identifier).md) [978-0-486-61114-3](https://en.wikipedia.org/wiki/Special:BookSources/978-0-486-61114-3). [OCLC](OCLC%20(identifier).md#OCLC) [476909537](https://search.worldcat.org/oclc/476909537).
- <a id="CITEREFJohnsonKotzBalakrishnan1994"></a> [Johnson, Norman L.](Norman%20Lloyd%20Johnson.md); [Kotz, Samuel](Samuel%20Kotz.md); Balakrishnan, Narayanaswamy \(1994\). _Continuous Univariate Distributions, Volume 1_. Wiley. [ISBN](ISBN%20(identifier).md) [978-0-471-58495-7](https://en.wikipedia.org/wiki/Special:BookSources/978-0-471-58495-7).
- <a id="CITEREFJohnsonKotzBalakrishnan1995"></a> Johnson, Norman L.; Kotz, Samuel; Balakrishnan, Narayanaswamy \(1995\). _Continuous Univariate Distributions, Volume 2_. Wiley. [ISBN](ISBN%20(identifier).md) [978-0-471-58494-0](https://en.wikipedia.org/wiki/Special:BookSources/978-0-471-58494-0).
- <a id="CITEREFKarney2016"></a> Karney, C. F. F. \(2016\). "Sampling exactly from the normal distribution". _ACM Transactions on Mathematical Software_. __42__ \(1\): 3:1–14. [arXiv](ArXiv%20(identifier).md):[1303.6257](https://arxiv.org/abs/1303.6257). [doi](doi%20(identifier).md):[10.1145/2710016](https://doi.org/10.1145%2F2710016). [S2CID](S2CID%20(identifier).md#S2CID) [14252035](https://api.semanticscholar.org/CorpusID:14252035).
- <a id="CITEREFKindermanMonahan1977"></a> Kinderman, Albert J.; Monahan, John F. \(1977\). ["Computer Generation of Random Variables Using the Ratio of Uniform Deviates"](https://doi.org/10.1145%2F355744.355750). _ACM Transactions on Mathematical Software_. __3__ \(3\): 257–260. [doi](doi%20(identifier).md):[10.1145/355744.355750](https://doi.org/10.1145%2F355744.355750). [S2CID](S2CID%20(identifier).md#S2CID) [12884505](https://api.semanticscholar.org/CorpusID:12884505).
- <a id="CITEREFKrishnamoorthy2006"></a> Krishnamoorthy, Kalimuthu \(2006\). _Handbook of Statistical Distributions with Applications_. Chapman & Hall/CRC. [ISBN](ISBN%20(identifier).md) [978-1-58488-635-8](https://en.wikipedia.org/wiki/Special:BookSources/978-1-58488-635-8).
- <a id="CITEREFKruskalStigler1997"></a> [Kruskal, William H.](William%20H.%20Kruskal.md); Stigler, Stephen M. \(1997\). Spencer, Bruce D. \(ed.\). _Normative Terminology: 'Normal' in Statistics and Elsewhere_. Statistics and Public Policy. Oxford University Press. [ISBN](ISBN%20(identifier).md) [978-0-19-852341-3](https://en.wikipedia.org/wiki/Special:BookSources/978-0-19-852341-3).
- <a id="CITEREFLaplace1774"></a> [Laplace, Pierre-Simon de](Pierre-Simon%20Laplace.md) \(1774\). ["Mémoire sur la probabilité des causes par les événements"](http://gallica.bnf.fr/ark:/12148/bpt6k77596b/f32). _Mémoires de l'Académie Royale des Sciences de Paris \(Savants étrangers\), Tome 6_: 621–656. Translated by Stephen M. Stigler in _Statistical Science_ __1__ \(3\), 1986: [JSTOR](JSTOR%20(identifier).md#content) [2245476](https://www.jstor.org/stable/2245476).
- <a id="CITEREFLaplace1812"></a> Laplace, Pierre-Simon \(1812\). [_Théorie analytique des probabilités_](https://archive.org/details/thorieanalytiqu00laplgoog) \[_[Analytical theory of probabilities](analytical%20theory%20of%20probabilities.md#analytic%20theory%20of%20probabilities)_\]. Paris, Ve. Courcier.
- <a id="CITEREFLe CamLo Yang2000"></a> [Le Cam, Lucien](Lucien%20Le%20Cam.md); [Lo Yang, Grace](Grace%20Yang.md) \(2000\). _Asymptotics in Statistics: Some Basic Concepts_ \(second ed.\). Springer. [ISBN](ISBN%20(identifier).md) [978-0-387-95036-5](https://en.wikipedia.org/wiki/Special:BookSources/978-0-387-95036-5).
- <a id="CITEREFLeva1992"></a> Leva, Joseph L. \(1992\). ["A fast normal random number generator"](https://web.archive.org/web/20100716035328/http://saluc.engr.uconn.edu/refs/crypto/rng/leva92afast.pdf) \(PDF\). _ACM Transactions on Mathematical Software_. __18__ \(4\): 449–453. [CiteSeerX](CiteSeerX%20(identifier).md) [10.1.1.544.5806](https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.544.5806). [doi](doi%20(identifier).md):[10.1145/138351.138364](https://doi.org/10.1145%2F138351.138364). [S2CID](S2CID%20(identifier).md#S2CID) [15802663](https://api.semanticscholar.org/CorpusID:15802663). Archived from [the original](http://saluc.engr.uconn.edu/refs/crypto/rng/leva92afast.pdf) \(PDF\) on July 16, 2010.
- <a id="CITEREFLexis1878"></a> [Lexis, Wilhelm](Wilhelm%20Lexis.md) \(1878\). "Sur la durée normale de la vie humaine et sur la théorie de la stabilité des rapports statistiques". _Annales de Démographie Internationale_. __II__. Paris: 447–462.
- <a id="CITEREFLukacsKing1954"></a> Lukacs, Eugene; King, Edgar P. \(1954\). ["A Property of Normal Distribution"](https://doi.org/10.1214%2Faoms%2F1177728796). _The Annals of Mathematical Statistics_. __25__ \(2\): 389–394. [doi](doi%20(identifier).md):[10.1214/aoms/1177728796](https://doi.org/10.1214%2Faoms%2F1177728796). [JSTOR](JSTOR%20(identifier).md#content) [2236741](https://www.jstor.org/stable/2236741).
- <a id="CITEREFMcPherson1990"></a> McPherson, Glen \(1990\). [_Statistics in Scientific Investigation: Its Basis, Application and Interpretation_](https://archive.org/details/statisticsinscie0000mcph). Springer-Verlag. [ISBN](ISBN%20(identifier).md) [978-0-387-97137-7](https://en.wikipedia.org/wiki/Special:BookSources/978-0-387-97137-7).
- <a id="CITEREFMarsagliaTsang2000"></a> [Marsaglia, George](George%20Marsaglia.md); Tsang, Wai Wan \(2000\). ["The Ziggurat Method for Generating Random Variables"](https://doi.org/10.18637%2Fjss.v005.i08). _Journal of Statistical Software_. __5__ \(8\). [doi](doi%20(identifier).md):[10.18637/jss.v005.i08](https://doi.org/10.18637%2Fjss.v005.i08).
- <a id="CITEREFMarsaglia2004"></a> Marsaglia, George \(2004\). ["Evaluating the Normal Distribution"](https://doi.org/10.18637%2Fjss.v011.i04). _Journal of Statistical Software_. __11__ \(4\). [doi](doi%20(identifier).md):[10.18637/jss.v011.i04](https://doi.org/10.18637%2Fjss.v011.i04).
- <a id="CITEREFMaxwell1860"></a> [Maxwell, James Clerk](James%20Clerk%20Maxwell.md) \(1860\). ["V. Illustrations of the dynamical theory of gases. — Part I: On the motions and collisions of perfectly elastic spheres"](https://books.google.com/books?id=-YU7AQAAMAAJ&pg=PA19). _Philosophical Magazine_. Series 4. __19__ \(124\): 19–32. [Bibcode](bibcode%20(identifier).md):[1860LEDPM..19...19M](https://ui.adsabs.harvard.edu/abs/1860LEDPM..19...19M). [doi](doi%20(identifier).md):[10.1080/14786446008642818](https://doi.org/10.1080%2F14786446008642818).
- <a id="CITEREFMonahan1985"></a> Monahan, J. F. \(1985\). ["Accuracy in random number generation"](https://doi.org/10.1090%2FS0025-5718-1985-0804945-X). _Mathematics of Computation_. __45__ \(172\): 559–568. [doi](doi%20(identifier).md):[10.1090/S0025-5718-1985-0804945-X](https://doi.org/10.1090%2FS0025-5718-1985-0804945-X).
- <a id="CITEREFMood1950"></a> [Mood, Alexander McFarlane](Alexander%20M.%20Mood.md) \(1950\). [_Introduction to the Theory of Statistics_](https://archive.org/details/introductiontoth0000alex/page/n5/mode/2up). New York: McGraw-Hill.
- <a id="CITEREFPatelRead1996"></a> Patel, Jagdish K.; Read, Campbell B. \(1996\). _Handbook of the Normal Distribution_ \(2nd ed.\). CRC Press. [ISBN](ISBN%20(identifier).md) [978-0-8247-9342-5](https://en.wikipedia.org/wiki/Special:BookSources/978-0-8247-9342-5).
- <a id="CITEREFPearson1901"></a> [Pearson, Karl](Karl%20Pearson.md) \(1901\). ["On Lines and Planes of Closest Fit to Systems of Points in Space"](http://stat.smmu.edu.cn/history/pearson1901.pdf) \(PDF\). _[Philosophical Magazine](Philosophical%20Magazine.md)_. 6. __2__ \(11\): 559–572. [doi](doi%20(identifier).md):[10.1080/14786440109462720](https://doi.org/10.1080%2F14786440109462720). [S2CID](S2CID%20(identifier).md#S2CID) [125037489](https://api.semanticscholar.org/CorpusID:125037489).
- <a id="CITEREFPearson1905"></a> [Pearson, Karl](Karl%20Pearson.md) \(1905\). ["'Das Fehlergesetz und seine Verallgemeinerungen durch Fechner und Pearson'. A rejoinder"](https://zenodo.org/record/1449456). _Biometrika_. __4__ \(1\): 169–212. [doi](doi%20(identifier).md):[10.2307/2331536](https://doi.org/10.2307%2F2331536). [JSTOR](JSTOR%20(identifier).md#content) [2331536](https://www.jstor.org/stable/2331536).
- <a id="CITEREFPearson1920"></a> Pearson, Karl \(1920\). ["Notes on the History of Correlation"](https://zenodo.org/record/1431597). _Biometrika_. __13__ \(1\): 25–45. [doi](doi%20(identifier).md):[10.1093/biomet/13.1.25](https://doi.org/10.1093%2Fbiomet%2F13.1.25). [JSTOR](JSTOR%20(identifier).md#content) [2331722](https://www.jstor.org/stable/2331722).
- <a id="CITEREFRohrbasserVéron2003"></a> Rohrbasser, Jean-Marc; Véron, Jacques \(2003\). ["Wilhelm Lexis: The Normal Length of Life as an Expression of the "Nature of Things""](http://www.persee.fr/web/revues/home/prescript/article/pop_1634-2941_2003_num_58_3_18444). _Population_. __58__ \(3\): 303–322. [doi](doi%20(identifier).md):[10.3917/pope.303.0303](https://doi.org/10.3917%2Fpope.303.0303).
- <a id="CITEREFShore1982"></a> Shore, H \(1982\). "Simple Approximations for the Inverse Cumulative Function, the Density Function and the Loss Integral of the Normal Distribution". _Journal of the Royal Statistical Society. Series C \(Applied Statistics\)_. __31__ \(2\): 108–114. [doi](doi%20(identifier).md):[10.2307/2347972](https://doi.org/10.2307%2F2347972). [JSTOR](JSTOR%20(identifier).md#content) [2347972](https://www.jstor.org/stable/2347972).
- <a id="CITEREFShore2005"></a> Shore, H \(2005\). "Accurate RMM-Based Approximations for the CDF of the Normal Distribution". _Communications in Statistics – Theory and Methods_. __34__ \(3\): 507–513. [doi](doi%20(identifier).md):[10.1081/sta-200052102](https://doi.org/10.1081%2Fsta-200052102). [S2CID](S2CID%20(identifier).md#S2CID) [122148043](https://api.semanticscholar.org/CorpusID:122148043).
- <a id="CITEREFShore2011"></a> Shore, H \(2011\). "Response Modeling Methodology". _WIREs Comput Stat_. __3__ \(4\): 357–372. [doi](doi%20(identifier).md):[10.1002/wics.151](https://doi.org/10.1002%2Fwics.151). [S2CID](S2CID%20(identifier).md#S2CID) [62021374](https://api.semanticscholar.org/CorpusID:62021374).
- <a id="CITEREFShore2012"></a> Shore, H \(2012\). "Estimating Response Modeling Methodology Models". _WIREs Comput Stat_. __4__ \(3\): 323–333. [doi](doi%20(identifier).md):[10.1002/wics.1199](https://doi.org/10.1002%2Fwics.1199). [S2CID](S2CID%20(identifier).md#S2CID) [122366147](https://api.semanticscholar.org/CorpusID:122366147).
- <a id="CITEREFStigler1978"></a> [Stigler, Stephen M.](Stephen%20Stigler.md) \(1978\). ["Mathematical Statistics in the Early States"](https://doi.org/10.1214%2Faos%2F1176344123). _The Annals of Statistics_. __6__ \(2\): 239–265. [doi](doi%20(identifier).md):[10.1214/aos/1176344123](https://doi.org/10.1214%2Faos%2F1176344123). [JSTOR](JSTOR%20(identifier).md#content) [2958876](https://www.jstor.org/stable/2958876).
- <a id="CITEREFStigler1982"></a> Stigler, Stephen M. \(1982\). "A Modest Proposal: A New Standard for the Normal". _The American Statistician_. __36__ \(2\): 137–138. [doi](doi%20(identifier).md):[10.2307/2684031](https://doi.org/10.2307%2F2684031). [JSTOR](JSTOR%20(identifier).md#content) [2684031](https://www.jstor.org/stable/2684031).
- <a id="CITEREFStigler1986"></a> Stigler, Stephen M. \(1986\). [_The History of Statistics: The Measurement of Uncertainty before 1900_](https://archive.org/details/historyofstatist00stig). Harvard University Press. [ISBN](ISBN%20(identifier).md) [978-0-674-40340-6](https://en.wikipedia.org/wiki/Special:BookSources/978-0-674-40340-6).
- <a id="CITEREFStigler1999"></a> Stigler, Stephen M. \(1999\). _Statistics on the Table_. Harvard University Press. [ISBN](ISBN%20(identifier).md) [978-0-674-83601-3](https://en.wikipedia.org/wiki/Special:BookSources/978-0-674-83601-3).
- <a id="CITEREFWalker1985"></a> Walker, Helen M. \(1985\). ["De Moivre on the Law of Normal Probability"](http://www.york.ac.uk/depts/maths/histstat/demoivre.pdf) \(PDF\). In Smith, David Eugene \(ed.\). _A Source Book in Mathematics_. Dover. [ISBN](ISBN%20(identifier).md) [978-0-486-64690-9](https://en.wikipedia.org/wiki/Special:BookSources/978-0-486-64690-9).
- <a id="CITEREFWallace1996"></a> [Wallace, C. S.](Chris%20Wallace%20(computer%20scientist).md) \(1996\). ["Fast pseudo-random generators for normal and exponential variates"](https://doi.org/10.1145%2F225545.225554). _ACM Transactions on Mathematical Software_. __22__ \(1\): 119–127. [doi](doi%20(identifier).md):[10.1145/225545.225554](https://doi.org/10.1145%2F225545.225554). [S2CID](S2CID%20(identifier).md#S2CID) [18514848](https://api.semanticscholar.org/CorpusID:18514848).
- <a id="CITEREFWeisstein"></a> [Weisstein, Eric W.](Eric%20W.%20Weisstein.md) ["Normal Distribution"](http://mathworld.wolfram.com/NormalDistribution.html). [MathWorld](MathWorld.md).
- <a id="CITEREFWest2009"></a> West, Graeme \(2009\). ["Better Approximations to Cumulative Normal Functions"](https://web.archive.org/web/20120229202051/https://wilmott.com/pdfs/090721_west.pdf) \(PDF\). _Wilmott Magazine_: 70–76. Archived from [the original](https://wilmott.com/pdfs/090721_west.pdf) \(PDF\) on February 29, 2012.
- <a id="CITEREFZelenSevero1964"></a> Zelen, Marvin; Severo, Norman C. \(1972\) \[First published 1964\]. [_Probability Functions \(chapter 26\)_](http://www.math.sfu.ca/~cbm/aands/page_931.htm). _[Handbook of mathematical functions with formulas, graphs, and mathematical tables](Abramowitz%20and%20Stegun.md)_, by [Abramowitz, M.](Milton%20Abramowitz.md); and [Stegun, I. A.](Irene%20A.%20Stegun.md): National Bureau of Standards. New York, NY: Dover. [ISBN](ISBN%20(identifier).md) [978-0-486-61272-0](https://en.wikipedia.org/wiki/Special:BookSources/978-0-486-61272-0).

## external links

> ![Wikimedia Commons logo](../../archives/Wikimedia%20Commons/Commons-logo.svg) Wikimedia Commons has media related to ___[Normal distribution](https://commons.wikimedia.org/wiki/Category%3ANormal%20distribution)___.

- ["Normal distribution"](https://www.encyclopediaofmath.org/index.php?title=Normal_distribution), _[Encyclopedia of Mathematics](Encyclopedia%20of%20Mathematics.md)_, [EMS Press](European%20Mathematical%20Society.md), 2001 \[1994\]
- [Normal distribution calculator](https://www.hackmath.net/en/calculator/normal-distribution)

|                                                                                                 | <!-- - [v](https://en.wikipedia.org/wiki/Template:Probability%20distributions) <br/> - [t](https://en.wikipedia.org/wiki/Template%20talk:Probability%20distributions) <br/> - [e](https://en.wikipedia.org/wiki/Special:EditPage/Template%3AProbability%20distributions) <br/> --> [Probability distributions](probability%20distribution.md) \([list](list%20of%20probability%20distributions.md)\)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| ----------------------------------------------------------------------------------------------: | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
|                                                                   __Discrete <br/> univariate__ | __with finite <br/> support__ <p>  - [Benford](Benford's%20law.md) <br/> - [Bernoulli](Bernoulli%20distribution.md) <br/> - [Beta-binomial](beta-binomial%20distribution.md) <br/> - [Binomial](binomial%20distribution.md) <br/> - [Categorical](categorical%20distribution.md) <br/> - [Hypergeometric](hypergeometric%20distribution.md)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Negative](negative%20hypergeometric%20distribution.md) <br/> - [Poisson binomial](Poisson%20binomial%20distribution.md) <br/> - [Rademacher](Rademacher%20distribution.md) <br/> - [Soliton](soliton%20distribution.md) <br/> - [Discrete uniform](discrete%20uniform%20distribution.md) <br/> - [Zipf](Zipf's%20law.md) <br/> - [Zipf–Mandelbrot](Zipf–Mandelbrot%20law.md)  <p> __with infinite <br/> support__ <p>  - [Beta negative binomial](beta%20negative%20binomial%20distribution.md) <br/> - [Borel](borel%20distribution.md) <br/> - [Conway–Maxwell–Poisson](Conway–Maxwell–Poisson%20distribution.md) <br/> - [Discrete phase-type](discrete%20phase-type%20distribution.md) <br/> - [Delaporte](Delaporte%20distribution.md) <br/> - [Extended negative binomial](extended%20negative%20binomial%20distribution.md) <br/> - [Flory–Schulz](Flory–Schulz%20distribution.md) <br/> - [Gauss–Kuzmin](Gauss–Kuzmin%20distribution.md) <br/> - [Geometric](geometric%20distribution.md) <br/> - [Logarithmic](logarithmic%20distribution.md) <br/> - [Mixed Poisson](mixed%20Poisson%20distribution.md) <br/> - [Negative binomial](negative%20binomial%20distribution.md) <br/> - [Panjer]((a,b,0)%20class%20of%20distributions.md) <br/> - [Parabolic fractal](parabolic%20fractal%20distribution.md) <br/> - [Poisson](Poisson%20distribution.md) <br/> - [Skellam](Skellam%20distribution.md) <br/> - [Yule–Simon](Yule–Simon%20distribution.md) <br/> - [Zeta](zeta%20distribution.md)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
|                                                                 __Continuous <br/> univariate__ | __supported on a <br/> bounded interval__ <p>  - [Arcsine](arcsine%20distribution.md) <br/> - [ARGUS](ARGUS%20distribution.md) <br/> - [Balding–Nichols](Balding–Nichols%20model.md) <br/> - [Bates](Bates%20distribution.md) <br/> - [Beta](beta%20distribution.md)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Generalized](generalized%20beta%20distribution.md) <br/> - [Beta rectangular](beta%20rectangular%20distribution.md) <br/> - [Continuous Bernoulli](continuous%20Bernoulli%20distribution.md) <br/> - [Irwin–Hall](Irwin–Hall%20distribution.md) <br/> - [Kumaraswamy](Kumaraswamy%20distribution.md) <br/> - [Logit-normal](logit-normal%20distribution.md) <br/> - [Noncentral beta](noncentral%20beta%20distribution.md) <br/> - [PERT](PERT%20distribution.md) <br/> - [Power function](power%20function%20distribution.md) <br/> - [Raised cosine](raised%20cosine%20distribution.md) <br/> - [Reciprocal](reciprocal%20distribution.md) <br/> - [Triangular](triangular%20distribution.md) <br/> - [U-quadratic](U-quadratic%20distribution.md) <br/> - [Uniform](continuous%20uniform%20distribution.md) <br/> - [Wigner semicircle](Wigner%20semicircle%20distribution.md)  <p> __supported on a <br/> semi-infinite <br/> interval__ <p>  - [Benini](Benini%20distribution.md) <br/> - [Benktander 1st kind](Benktander%20type%20I%20distribution.md) <br/> - [Benktander 2nd kind](Benktander%20type%20II%20distribution.md) <br/> - [Beta prime](beta%20prime%20distribution.md) <br/> - [Burr](Burr%20distribution.md) <br/> - [Chi](chi%20distribution.md) <br/> - [Chi-squared](chi-squared%20distribution.md)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Noncentral](noncentral%20chi-squared%20distribution.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Inverse](inverse-chi-squared%20distribution.md)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [Scaled](scaled%20inverse%20chi-squared%20distribution.md) <br/> - [Dagum](Dagum%20distribution.md) <br/> - [Davis](Davis%20distribution.md) <br/> - [Erlang](Erlang%20distribution.md)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Hyper](hyper-Erlang%20distribution.md) <br/> - [Exponential](exponential%20distribution.md)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Hyperexponential](hyperexponential%20distribution.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Hypoexponential](hypoexponential%20distribution.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Logarithmic](exponential-logarithmic%20distribution.md) <br/> - [_F_](F-distribution.md)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Noncentral](noncentral%20F-distribution.md) <br/> - [Folded normal](folded%20normal%20distribution.md) <br/> - [Fréchet](Fréchet%20distribution.md) <br/> - [Gamma](gamma%20distribution.md)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Generalized](generalized%20gamma%20distribution.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Inverse](inverse-gamma%20distribution.md) <br/> - [gamma/Gompertz](Gamma_Gompertz%20distribution.md) <br/> - [Gompertz](Gompertz%20distribution.md)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Shifted](shifted%20Gompertz%20distribution.md) <br/> - [Half-logistic](half-logistic%20distribution.md) <br/> - [Half-normal](half-normal%20distribution.md) <br/> - [Hotelling's _T_-squared](Hotelling's%20T-squared%20distribution.md) <br/> - [Hartman–Watson](Hartman–Watson%20distribution.md) <br/> - [Inverse Gaussian](inverse%20Gaussian%20distribution.md)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Generalized](generalized%20inverse%20Gaussian%20distribution.md) <br/> - [Kolmogorov](Kolmogorov–Smirnov%20test.md) <br/> - [Lévy](Lévy%20distribution.md) <br/> - [Log-Cauchy](log-Cauchy%20distribution.md) <br/> - [Log-Laplace](log-Laplace%20distribution.md) <br/> - [Log-logistic](log-logistic%20distribution.md) <br/> - [Log-normal](log-normal%20distribution.md) <br/> - [Log-t](log-t%20distribution.md) <br/> - [Lomax](Lomax%20distribution.md) <br/> - [Matrix-exponential](matrix-exponential%20distribution.md) <br/> - [Maxwell–Boltzmann](Maxwell–Boltzmann%20distribution.md) <br/> - [Maxwell–Jüttner](Maxwell–Jüttner%20distribution.md) <br/> - [Mittag-Leffler](Mittag-Leffler%20distribution.md) <br/> - [Nakagami](Nakagami%20distribution.md) <br/> - [Pareto](Pareto%20distribution.md) <br/> - [Phase-type](phase-type%20distribution.md) <br/> - [Poly-Weibull](Poly-Weibull%20distribution.md) <br/> - [Rayleigh](Rayleigh%20distribution.md) <br/> - [Relativistic Breit–Wigner](relativistic%20Breit–Wigner%20distribution.md) <br/> - [Rice](rice%20distribution.md) <br/> - [Truncated normal](truncated%20normal%20distribution.md) <br/> - [type-2 Gumbel](type-2%20Gumbel%20distribution.md) <br/> - [Weibull](Weibull%20distribution.md)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Discrete](discrete%20Weibull%20distribution.md) <br/> - [Wilks's lambda](Wilks's%20lambda%20distribution.md)  <p> __supported <br/> on the whole <br/> real line__ <p>  - [Cauchy](Cauchy%20distribution.md) <br/> - [Exponential power](generalized%20normal%20distribution.md#version%201) <br/> - [Fisher's _z_](Fisher's%20z-distribution.md) <br/> - [Kaniadakis κ-Gaussian](Kaniadakis%20Gaussian%20distribution.md) <br/> - [Gaussian _q_](Gaussian%20q-distribution.md) <br/> - [Generalized hyperbolic](generalised%20hyperbolic%20distribution.md) <br/> - [Generalized logistic \(logistic-beta\)](generalized%20logistic%20distribution.md) <br/> - [Generalized normal](generalized%20normal%20distribution.md) <br/> - [Geometric stable](geometric%20stable%20distribution.md) <br/> - [Gumbel](Gumbel%20distribution.md) <br/> - [Holtsmark](Holtsmark%20distribution.md) <br/> - [Hyperbolic secant](hyperbolic%20secant%20distribution.md) <br/> - [Johnson's _S<sub>U</sub>_](Johnson's%20SU-distribution.md) <br/> - [Landau](Landau%20distribution.md) <br/> - [Laplace](Laplace%20distribution.md)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Asymmetric](asymmetric%20Laplace%20distribution.md) <br/> - [Logistic](logistic%20distribution.md) <br/> - [Noncentral _t_](noncentral%20t-distribution.md) <br/> - [Normal \(Gaussian\)](normal%20distribution.md) <br/> - [Normal-inverse Gaussian](normal-inverse%20Gaussian%20distribution.md) <br/> - [Skew normal](skew%20normal%20distribution.md) <br/> - [Slash](slash%20distribution.md) <br/> - [Stable](stable%20distribution.md) <br/> - [Student's _t_](Student's%20t-distribution.md) <br/> - [Tracy–Widom](Tracy–Widom%20distribution.md) <br/> - [Variance-gamma](variance-gamma%20distribution.md) <br/> - [Voigt](Voigt%20profile.md)  <p> __with support <br/> whose type varies__ <p>  - [Generalized chi-squared](generalized%20chi-squared%20distribution.md) <br/> - [Generalized extreme value](generalized%20extreme%20value%20distribution.md) <br/> - [Generalized Pareto](generalized%20Pareto%20distribution.md) <br/> - [Marchenko–Pastur](Marchenko–Pastur%20distribution.md) <br/> - [Kaniadakis _κ_-exponential](Kaniadakis%20exponential%20distribution.md) <br/> - [Kaniadakis _κ_-Gamma](Kaniadakis%20gamma%20distribution.md) <br/> - [Kaniadakis _κ_-Weibull](Kaniadakis%20Weibull%20distribution.md) <br/> - [Kaniadakis _κ_-Logistic](Kaniadakis%20logistic%20distribution.md) <br/> - [Kaniadakis _κ_-Erlang](Kaniadakis%20Erlang%20distribution.md) <br/> - [_q_-exponential](q-exponential%20distribution.md) <br/> - [_q_-Gaussian](q-Gaussian%20distribution.md) <br/> - [_q_-Weibull](q-Weibull%20distribution.md) <br/> - [Shifted log-logistic](shifted%20log-logistic%20distribution.md) <br/> - [Tukey lambda](Tukey%20lambda%20distribution.md) |
|                                                                      __Mixed <br/> univariate__ | __continuous- <br/> discrete__ <p>  - [Rectified Gaussian](rectified%20Gaussian%20distribution.md)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
|                       __[Multivariate <br/> \(joint\)](joint%20probability%20distribution.md)__ | - _Discrete:_ <br/> - [Ewens](Ewens's%20sampling%20formula.md) <br/> - [Multinomial](multinomial%20distribution.md)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Dirichlet](Dirichlet-multinomial%20distribution.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Negative](negative%20multinomial%20distribution.md) <br/> - _Continuous:_ <br/> - [Dirichlet](Dirichlet%20distribution.md)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Generalized](generalized%20Dirichlet%20distribution.md) <br/> - [Multivariate Laplace](multivariate%20Laplace%20distribution.md) <br/> - [Multivariate normal](multivariate%20normal%20distribution.md) <br/> - [Multivariate stable](multivariate%20stable%20distribution.md) <br/> - [Multivariate _t_](multivariate%20t-distribution.md) <br/> - [Normal-gamma](normal-gamma%20distribution.md)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Inverse](normal-inverse-gamma%20distribution.md) <br/> - _[Matrix-valued:](random%20matrix.md)_ <br/> - [LKJ](Lewandowski-Kurowicka-Joe%20distribution.md) <br/> - [Matrix beta](matrix%20variate%20beta%20distribution.md) <br/> - [Matrix normal](matrix%20normal%20distribution.md) <br/> - [Matrix _t_](matrix%20t-distribution.md) <br/> - [Matrix gamma](matrix%20gamma%20distribution.md)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Inverse](inverse%20matrix%20gamma%20distribution.md) <br/> - [Wishart](Wishart%20distribution.md)  <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Normal](normal-Wishart%20distribution.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Inverse](inverse-Wishart%20distribution.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Normal-inverse](normal-inverse-Wishart%20distribution.md) <br/> &nbsp;&nbsp;&nbsp;&nbsp;- [Complex](complex%20Wishart%20distribution.md) <br/> - [Uniform distribution on a Stiefel manifold](uniform%20distribution%20on%20a%20Stiefel%20manifold.md)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
|                                                  __[Directional](directional%20statistics.md)__ | ___Univariate \(circular\) [directional](directional%20statistics.md):___ [Circular uniform](circular%20uniform%20distribution.md) <br/> [Univariate von Mises](von%20Mises%20distribution.md) <br/> [Wrapped normal](wrapped%20normal%20distribution.md) <br/> [Wrapped Cauchy](wrapped%20Cauchy%20distribution.md) <br/> [Wrapped exponential](wrapped%20exponential%20distribution.md) <br/> [Wrapped asymmetric Laplace](wrapped%20asymmetric%20Laplace%20distribution.md) <br/> [Wrapped Lévy](wrapped%20Lévy%20distribution.md) <br/> ___Bivariate \(spherical\):___ [Kent](Kent%20distribution.md) <br/> ___Bivariate \(toroidal\):___ [Bivariate von Mises](bivariate%20von%20Mises%20distribution.md) <br/> ___Multivariate:___ [von Mises–Fisher](von%20Mises–Fisher%20distribution.md) <br/> [Bingham](Bingham%20distribution.md)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| __[Degenerate](degenerate%20distribution.md) <br/> and [singular](singular%20distribution.md)__ | ___Degenerate:___ [Dirac delta function](Dirac%20delta%20function.md) <br/> ___Singular:___ [Cantor](Cantor%20distribution.md)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
|                                                                                    __Families__ | - [Circular](circular%20distribution.md) <br/> - [Compound Poisson](compound%20Poisson%20distribution.md) <br/> - [Elliptical](elliptical%20distribution.md) <br/> - [Exponential](exponential%20family.md) <br/> - [Natural exponential](natural%20exponential%20family.md) <br/> - [Location–scale](location–scale%20family.md) <br/> - [Maximum entropy](maximum%20entropy%20probability%20distribution.md) <br/> - [Mixture](mixture%20distribution.md) <br/> - [Pearson](Pearson%20distribution.md) <br/> - [Tweedie](Tweedie%20distribution.md) <br/> - [Wrapped](wrapped%20distribution.md)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
|                                                                                                 | - ![category icon](../../archives/Wikimedia%20Commons/Symbol%20category%20class.svg) [Category](https://en.wikipedia.org/wiki/Category:Probability%20distributions) <br/> - ![Wikimedia Commons logo](../../archives/Wikimedia%20Commons/Commons-logo.svg) [Commons](https://commons.wikimedia.org/wiki/Category%3AProbability%20distributions)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |

|                   | [Authority control databases](https://en.wikipedia.org/wiki/Help:Authority%20control) [![Edit this at Wikidata](../../archives/Wikimedia%20Commons/OOjs%20UI%20icon%20edit-ltr-progressive.svg)](https://www.wikidata.org/wiki/Q133871#identifiers)                                                                                                                                     |
| ----------------: | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| __International__ | - [GND](https://d-nb.info/gnd/4075494-7)                                                                                                                                                                                                                                                                                                                                                |
|      __National__ | - [United States](https://id.loc.gov/authorities/sh85053556) <br/> - [France](https://catalogue.bnf.fr/ark:/12148/cb119421818) <br/> - [BnF data](https://data.bnf.fr/ark:/12148/cb119421818) <br/> - [Czech Republic](https://aleph.nkp.cz/F/?func=find-c&local_base=aut&ccl_term=ica=ph123321&CON_LNG=ENG) <br/> - [Israel](https://www.nli.org.il/en/authorities/987007560462505171) |
|         __Other__ | - [Yale LUX](https://lux.collections.yale.edu/view/concept/d5b5f87b-74e6-4f34-996b-7308b1fe9b73)                                                                                                                                                                                                                                                                                        |

> [Categories](https://en.wikipedia.org/wiki/Help:Category):
>
> - [Normal distribution](https://en.wikipedia.org/wiki/Category:Normal%20distribution)
> - [Continuous distributions](https://en.wikipedia.org/wiki/Category:Continuous%20distributions)
> - [Conjugate prior distributions](https://en.wikipedia.org/wiki/Category:Conjugate%20prior%20distributions)
> - [Exponential family distributions](https://en.wikipedia.org/wiki/Category:Exponential%20family%20distributions)
> - [Stable distributions](https://en.wikipedia.org/wiki/Category:Stable%20distributions)
> - [Location-scale family probability distributions](https://en.wikipedia.org/wiki/Category:Location-scale%20family%20probability%20distributions)
