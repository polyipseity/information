---
aliases:
  - MSE
  - MSEs
  - mean squared error
  - mean squared errors
tags:
  - flashcard/active/general/eng/mean_squared_error
  - language/in/English
---

# mean squared error

- "Mean squared deviation" redirects here. Not to be confused with {@{[Mean squared displacement](mean%20squared%20displacement.md)}@}. <!--SR:!2026-01-12,294,330-->

In {@{[statistics](statistics.md)}@}, {@{the __mean squared error__ \(__MSE__\)<sup>[\[1\]](#^ref-1)</sup> or __mean squared deviation__ \(__MSD__\)}@} of {@{an [estimator](estimator.md) \(of a procedure for estimating an unobserved quantity\) measures the [average](expected%20value.md) of the squares of the [errors](errors%20and%20residuals.md)}@}—that is, {@{the average squared difference between the estimated values and the [true value](statistical%20parameter.md)}@}. MSE is {@{a [risk function](loss%20function.md#expected%20loss), corresponding to the [expected value](expected%20value.md) of the [squared error loss](loss%20function.md#quadratic%20loss%20function)}@}.<sup>[\[2\]](#^ref-2)</sup> The fact that {@{MSE is almost always strictly positive \(and not zero\)}@} is because {@{of [randomness](randomness.md) or because the estimator [does not account for information](omitted-variable%20bias.md) that could produce a more accurate estimate}@}.<sup>[\[3\]](#^ref-3)</sup> In {@{[machine learning](machine%20learning.md), specifically [empirical risk minimization](empirical%20risk%20minimization.md)}@}, MSE may refer to {@{the _empirical_ risk \(the average loss on an observed data set\), as an estimate of the true MSE \(the true risk: the average loss on the actual population distribution\)}@}. <!--SR:!2028-02-12,887,347!2026-02-16,323,347!2027-01-30,578,327!2026-05-04,366,307!2027-11-07,794,330!2027-05-05,653,330!2025-12-09,251,327!2026-03-03,336,347!2025-12-09,251,327-->

The MSE is a measure of {@{the quality of an estimator}@}. As {@{it is derived from the square of [Euclidean distance](Euclidean%20distance.md)}@}, it is {@{always a positive value that decreases as the error approaches zero}@}. <!--SR:!2026-01-12,294,330!2028-04-17,934,347!2028-02-01,876,347-->

The MSE is {@{the second [moment](moment%20(mathematics).md) \(about the origin\) of the error}@}, and thus incorporates {@{both the [variance](variance.md) of the estimator}@} \(how {@{widely spread the estimates are from one [data sample](sampling%20(statistics).md) to another}@}\) and {@{its [bias](bias%20of%20an%20estimator.md)}@} \(how {@{far off the average estimated value is from the true value}@}\).<!-- <sup>\[_[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation%20needed)_\]</sup> --> For {@{an [unbiased estimator](bias%20of%20an%20estimator.md)}@}, the MSE is {@{the variance of the estimator}@}. Like {@{the variance}@}, MSE has {@{the same units of measurement as the square of the quantity being estimated}@}. In {@{an analogy to [standard deviation](standard%20deviation.md)}@}, {@{taking the square root of MSE yields the _root-mean-square error_ or _[root-mean-square deviation](root%20mean%20square%20deviation.md)_ \(RMSE or RMSD\)}@}, which has {@{the same units as the quantity being estimated}@}; for {@{an unbiased estimator}@}, the RMSE is {@{the square root of the [variance](variance.md), known as the [standard error](standard%20error.md)}@}. <!--SR:!2026-02-22,329,347!2028-09-04,1026,347!2028-04-04,925,347!2026-03-03,336,347!2025-12-14,271,330!2028-03-10,908,347!2028-03-29,919,347!2026-09-29,486,327!2028-04-06,925,347!2026-02-27,332,347!2026-02-20,327,347!2025-12-11,25,372!2025-12-11,25,372!2025-12-11,25,372-->

## definition and basic properties

The MSE either {@{assesses the quality of a _[predictor](prediction.md#statistics)_ \(i.e., a function mapping arbitrary inputs to a sample of values of some [random variable](random%20variable.md)\)}@}, or of {@{an _[estimator](estimator.md)_ \(i.e., a [mathematical function](function%20(mathematics).md) mapping a [sample](sampling%20(statistics).md) of data to an estimate of a [parameter](statistical%20parameter.md) of the [population](statistical%20population.md) from which the data is sampled\)}@}. In {@{the context of prediction}@}, understanding {@{the [prediction interval](prediction%20interval.md) can also be useful}@} as it {@{provides a range within which a future observation will fall, with a certain probability}@}. The definition of an MSE differs {@{according to whether one is describing a predictor or an estimator}@}. <!--SR:!2027-10-20,795,347!2026-03-28,308,290!2028-08-24,1036,350!2027-09-21,760,330!2026-03-09,341,347!2027-03-29,625,330-->

### predictor

If {@{a vector of $n$ predictions is generated from a sample of $n$ data points on all variables}@}, and $Y$ is {@{the vector of observed values of the variable being predicted}@}, with ${\hat {Y} }$ being {@{the predicted values \(e.g. as from a [least-squares fit](least%20squares.md)\)}@}, then {@{the within-sample MSE of the predictor}@} is computed as {@{$$\operatorname {MSE} ={\frac {1}{n} }\sum _{i=1}^{n}\left(Y_{i}-{\hat {Y_{i} } }\right)^{2}$$}@} In other words, the MSE is {@{the _mean_ $\left({\frac {1}{n} }\sum _{i=1}^{n}\right)$ of the _squares of the errors_ $\left(Y_{i}-{\hat {Y_{i} } }\right)^{2}$}@}. This is {@{an easily computable quantity for a particular sample}@} \(and hence is {@{sample-dependent}@}\). <!--SR:!2026-11-15,512,310!2026-02-28,333,347!2026-03-06,339,347!2027-10-06,786,347!2026-03-01,334,347!2026-01-10,292,330!2027-10-22,782,330!2026-02-25,330,347-->

In [matrix](matrix%20multiplication.md) notation, {@{$$\operatorname {MSE} ={\frac {1}{n} }\sum _{i=1}^{n}(e_{i})^{2}={\frac {1}{n} }\mathbf {e} ^{\mathsf {T} }\mathbf {e}$$}@} where {@{$e_{i}$ is $(Y_{i}-{\hat {Y_{i} } })$ and $\mathbf {e}$ is a $n\times 1$ column vector}@}. <!--SR:!2025-12-09,251,327!2026-03-04,337,347-->

The MSE can also be computed on {@{_q_ data points that were not used in estimating the model}@}, either because {@{they were held back for this purpose (annotation: validation/test dataset), or because these data have been newly obtained (annotation: new dataset)}@}. Within this process, known as {@{[cross-validation](cross-validation%20(statistics).md)}@}, the MSE is often called {@{the [test MSE](test%20MSE.md)}@},<sup>[\[4\]](#^ref-4)</sup> and is computed as {@{$$\operatorname {MSE} ={\frac {1}{q} }\sum _{i=n+1}^{n+q}\left(Y_{i}-{\hat {Y_{i} } }\right)^{2}$$}@} <!--SR:!2026-01-10,292,330!2026-01-12,294,330!2028-05-08,947,347!2026-03-09,341,347!2026-03-08,341,347-->

### estimator

{@{The MSE of an estimator ${\hat {\theta } }$ with respect to an unknown parameter $\theta$}@} is defined as<sup>[\[1\]](#^ref-1)</sup> {@{$$\operatorname {MSE} ({\hat {\theta } })=\operatorname {E} _{\theta }\left[({\hat {\theta } }-\theta )^{2}\right].$$}@} This definition depends on {@{the unknown parameter}@}, but the MSE is {@{_a priori_ a property of an estimator (annotation: the MSE of an estimator does not depend on the actual samples of a population; it makes sense to talk about the MSE of an estimator even without any samples)}@}. The MSE could be {@{a function of unknown parameters (annotation: e.g. $\operatorname{MSE}(\hat \theta, \theta)$; that is, actual values of the unknown parameters affect the MSE)}@}, in which case {@{any _estimator_ of the MSE based on estimates of these parameters would be a function of the data \(and thus a random variable\)}@}. If {@{the estimator ${\hat {\theta } }$ is derived as a sample statistic}@} and is used to {@{estimate some population parameter}@}, then the expectation is {@{with respect to the sampling distribution of the sample statistic (annotation: the sampling distribution is a function of the unknown parameters)}@}. <!--SR:!2028-12-17,1129,350!2026-01-02,285,330!2026-03-10,342,347!2026-11-11,500,310!2026-09-13,456,310!2027-03-18,577,327!2028-08-31,1042,350!2026-03-01,335,347!2026-02-14,271,287-->

The MSE can be written as {@{the sum of the [variance](variance.md) of the estimator and the squared [bias](bias%20of%20an%20estimator.md) of the estimator}@}, providing {@{a useful way to calculate the MSE}@} and implying that {@{in the case of unbiased estimators, the MSE and variance are equivalent}@}.<sup>[\[5\]](#^ref-5)</sup> {@{$$\operatorname {MSE} ({\hat {\theta } })=\operatorname {Var} _{\theta }({\hat {\theta } })+\operatorname {Bias} ({\hat {\theta } },\theta )^{2}.$$}@} <!--SR:!2027-11-03,806,347!2026-03-12,344,347!2026-01-07,290,330!2029-02-07,1169,350-->

#### proof of variance and bias relationship

(annotation: proof of variance and bias relationship) ::@:: $${\begin{aligned}\operatorname {MSE} ({\hat {\theta } })&=\operatorname {E} _{\theta }\left[({\hat {\theta } }-\theta )^{2}\right]\\&=\operatorname {E} _{\theta }\left[\left({\hat {\theta } }-\operatorname {E} _{\theta }[{\hat {\theta } }]+\operatorname {E} _{\theta }[{\hat {\theta } }]-\theta \right)^{2}\right]\\&=\operatorname {E} _{\theta }\left[\left({\hat {\theta } }-\operatorname {E} _{\theta }[{\hat {\theta } }]\right)^{2}+2\left({\hat {\theta } }-\operatorname {E} _{\theta }[{\hat {\theta } }]\right)\left(\operatorname {E} _{\theta }[{\hat {\theta } }]-\theta \right)+\left(\operatorname {E} _{\theta }[{\hat {\theta } }]-\theta \right)^{2}\right]\\&=\operatorname {E} _{\theta }\left[\left({\hat {\theta } }-\operatorname {E} _{\theta }[{\hat {\theta } }]\right)^{2}\right]+\operatorname {E} _{\theta }\left[2\left({\hat {\theta } }-\operatorname {E} _{\theta }[{\hat {\theta } }]\right)\left(\operatorname {E} _{\theta }[{\hat {\theta } }]-\theta \right)\right]+\operatorname {E} _{\theta }\left[\left(\operatorname {E} _{\theta }[{\hat {\theta } }]-\theta \right)^{2}\right]\\&=\operatorname {E} _{\theta }\left[\left({\hat {\theta } }-\operatorname {E} _{\theta }[{\hat {\theta } }]\right)^{2}\right]+2\left(\operatorname {E} _{\theta }[{\hat {\theta } }]-\theta \right)\operatorname {E} _{\theta }\left[{\hat {\theta } }-\operatorname {E} _{\theta }[{\hat {\theta } }]\right]+\left(\operatorname {E} _{\theta }[{\hat {\theta } }]-\theta \right)^{2}&&\operatorname {E} _{\theta }[{\hat {\theta } }]-\theta ={\text{const.} }\\&=\operatorname {E} _{\theta }\left[\left({\hat {\theta } }-\operatorname {E} _{\theta }[{\hat {\theta } }]\right)^{2}\right]+2\left(\operatorname {E} _{\theta }[{\hat {\theta } }]-\theta \right)\left(\operatorname {E} _{\theta }[{\hat {\theta } }]-\operatorname {E} _{\theta }[{\hat {\theta } }]\right)+\left(\operatorname {E} _{\theta }[{\hat {\theta } }]-\theta \right)^{2}&&\operatorname {E} _{\theta }[{\hat {\theta } }]={\text{const.} }\\&=\operatorname {E} _{\theta }\left[\left({\hat {\theta } }-\operatorname {E} _{\theta }[{\hat {\theta } }]\right)^{2}\right]+\left(\operatorname {E} _{\theta }[{\hat {\theta } }]-\theta \right)^{2}\\&=\operatorname {Var} _{\theta }({\hat {\theta } })+\operatorname {Bias} _{\theta }({\hat {\theta } },\theta )^{2}\end{aligned} }$$ <!--SR:!2026-04-07,334,290!2026-03-02,335,347-->

{@{An even shorter proof can be achieved}@} using {@{the well-known formula that for a random variable $X$, $\mathbb {E} (X^{2})=\operatorname {Var} (X)+(\mathbb {E} (X))^{2}$}@}. By {@{substituting $X$ with, ${\hat {\theta } }-\theta$}@}, we have {@{$${\begin{aligned}\operatorname {MSE} ({\hat {\theta } })&=\mathbb {E} [({\hat {\theta } }-\theta )^{2}]\\&=\operatorname {Var} ({\hat {\theta } }-\theta )+(\mathbb {E} [{\hat {\theta } }-\theta ])^{2}\\&=\operatorname {Var} ({\hat {\theta } })+\operatorname {Bias} ^{2}({\hat {\theta } },\theta )\end{aligned} }$$}@} <!--SR:!2026-01-10,292,330!2026-02-21,328,347!2026-03-11,343,347!2029-01-06,1145,350-->

But in {@{real modeling case}@}, MSE could be described as {@{the addition of model variance, model bias, and irreducible uncertainty \(see [Bias–variance tradeoff](bias–variance%20tradeoff.md)\)}@}. According to the relationship, {@{the MSE of the estimators}@} could be simply used for {@{the [efficiency](efficiency%20(statistics).md) comparison}@}, which includes {@{the information of estimator variance and bias}@}. This is called {@{MSE criterion}@}. <!--SR:!2026-01-06,289,330!2026-10-10,483,310!2026-03-13,345,347!2026-01-19,108,307!2026-02-20,327,347!2026-03-04,125,391-->

## in regression

- Further information: [Reduced chi-squared statistic](reduced%20chi-squared%20statistic.md)

In {@{[regression analysis](regression%20analysis.md)}@}, {@{plotting}@} is {@{a more natural way to view the overall trend of the whole data}@}. {@{The mean of the distance from each point to the predicted regression model}@} can be calculated, and shown as {@{the mean squared error}@}. The squaring is {@{critical to reduce the complexity with negative signs}@}. To {@{minimize MSE}@}, the model could be {@{more accurate, which would mean the model is closer to actual data}@}. One example of {@{a linear regression}@} using this method is {@{the [least squares method](least%20squares.md)}@}—which {@{evaluates appropriateness of linear regression model to model [bivariate dataset](bivariate%20data.md)}@},<sup>[\[6\]](#^ref-6)</sup> but whose limitation is {@{related to known distribution of the data}@}. <!--SR:!2026-03-07,340,347!2027-07-30,720,330!2026-03-09,341,347!2027-06-14,686,330!2026-03-12,344,347!2027-10-17,794,347!2029-03-24,1204,350!2026-09-30,473,310!2026-01-01,284,330!2027-09-27,777,347!2027-09-12,755,330!2027-01-10,564,327-->

The term {@{_mean squared error_}@} is sometimes used to refer to {@{the unbiased estimate of error variance}@}: {@{the [residual sum of squares](residual%20sum%20of%20squares.md) divided by the number of [degrees of freedom](degrees%20of%20freedom%20(statistics).md)}@}. {@{This definition for a known, computed quantity}@} {@{differs from the above definition for the computed MSE of a predictor, in that a different denominator is used}@}. The denominator is {@{the sample size reduced by the number of model parameters estimated from the same data}@}, {@{\(_n_<!-- markdown separator -->−<!-- markdown separator -->_p_\) for _p_ [regressors](dependent%20and%20independent%20variables.md#statistics%20synonyms) or \(_n_<!-- markdown separator -->−<!-- markdown separator -->_p_<!-- markdown separator -->−1\) if an intercept is used (annotation: an intercept can be treated as a regressor having the constant 1 for all observations)}@} \(see {@{[errors and residuals in statistics](errors%20and%20residuals.md)}@} for more details\).<sup>[\[7\]](#^ref-7)</sup> Although {@{the MSE \(as defined in this article\) is not an unbiased estimator of the error variance}@}, it is {@{[consistent](consistent%20estimator.md), given the consistency of the predictor}@}. <!--SR:!2026-03-13,345,347!2026-02-27,333,347!2027-04-14,638,330!2026-02-24,329,347!2026-08-27,451,310!2028-12-31,1122,347!2026-03-08,341,347!2026-03-05,338,347!2026-12-05,529,327!2026-03-10,342,347-->

In regression analysis, {@{"mean squared error", often referred to as [mean squared prediction error](mean%20squared%20prediction%20error.md) or "out-of-sample mean squared error"}@}, can also refer to {@{the mean value of the [squared deviations](squared%20deviations%20from%20the%20mean.md) of the predictions from the true values}@}, over {@{an out-of-sample [test space](training,%20validation,%20and%20test%20data%20sets.md) (annotation: validation/test dataset)}@}, generated by {@{a model estimated over a [particular sample space](training,%20validation,%20and%20test%20data%20sets.md) (annotation: training dataset)}@}. This also is {@{a known, computed quantity}@}, and it varies {@{by sample and by out-of-sample test space}@}. <!--SR:!2028-04-29,945,347!2028-04-22,941,347!2025-12-09,251,327!2026-12-03,521,310!2027-05-29,673,330!2027-04-13,637,330-->

In the context of {@{gradient descent algorithms}@}, it is common to {@{introduce a factor of $1/2$ to the MSE for ease of computation after taking the derivative}@}. So {@{a value which is technically half the mean of squared errors}@} may be called {@{the MSE}@}. <!--SR:!2026-03-05,338,347!2026-03-06,339,347!2027-03-02,594,327!2025-12-15,272,330-->

## examples

### mean

Suppose {@{we have a random sample of size $n$ from a population, $X_{1},\dots ,X_{n}$}@}. Suppose {@{the sample units were chosen [with replacement](simple%20random%20sample.md)}@}. That is, {@{the $n$ units are selected one at a time, and previously selected units are still eligible for selection for all $n$ draws}@}. {@{The usual estimator for the $\mu$}@} is {@{the sample average $${\overline {X} }={\frac {1}{n} }\sum _{i=1}^{n}X_{i}$$}@} which has an expected value {@{equal to the true mean $\mu$ \(so it is unbiased\)}@} and a mean squared error of {@{$$\operatorname {MSE} \left({\overline {X} }\right)=\operatorname {E} \left[\left({\overline {X} }-\mu \right)^{2}\right]=\left({\frac {\sigma }{\sqrt {n} } }\right)^{2}={\frac {\sigma ^{2} }{n} }$$}@} where {@{$\sigma ^{2}$ is the [population variance](variance.md#population%20variance)}@}. <!--SR:!2026-01-09,291,330!2026-03-11,343,347!2026-03-13,345,347!2026-02-26,331,347!2026-03-02,335,347!2028-04-11,929,347!2026-02-19,326,347!2026-03-07,340,347-->

For {@{a [Gaussian distribution](normal%20distribution.md)}@}, this is {@{the [best unbiased estimator](minimum-variance%20unbiased%20estimator.md) \(i.e., one with the lowest MSE among all unbiased estimators\)}@}, but {@{not, say, for a [uniform distribution](continuous%20uniform%20distribution.md)}@}. <!--SR:!2029-02-01,1166,350!2027-08-28,742,330!2027-09-08,751,330-->

### variance

- Further information: [Sample variance](variance.md#sample%20variance)

{@{The usual estimator for the variance}@} is {@{the _corrected [sample variance](variance.md#sample%20variance)_}@}: {@{$$S_{n-1}^{2}={\frac {1}{n-1} }\sum _{i=1}^{n}\left(X_{i}-{\overline {X} }\right)^{2}={\frac {1}{n-1} }\left(\sum _{i=1}^{n}X_{i}^{2}-n{\overline {X} }^{2}\right).$$}@} This is {@{unbiased \(its expected value is $\sigma ^{2}$\), hence also called the _unbiased sample variance_}@}, and its MSE is<sup>[\[8\]](#^ref-8)</sup> {@{$$\operatorname {MSE} (S_{n-1}^{2})={\frac {1}{n} }\left(\mu _{4}-{\frac {n-3}{n-1} }\sigma ^{4}\right)={\frac {1}{n} }\left(\gamma _{2}+{\frac {2n}{n-1} }\right)\sigma ^{4},$$}@} where {@{$\mu _{4}$ is the fourth [central moment](central%20moment.md) of the distribution or population}@}, and {@{$\gamma _{2}=\mu _{4}/\sigma ^{4}-3$ is the [excess kurtosis](kurtosis.md#excess%20kurtosis)}@}. <!--SR:!2028-02-22,894,347!2026-02-21,328,347!2027-02-16,490,267!2026-12-01,518,310!2026-01-10,80,167!2026-08-07,388,290!2027-01-22,556,310-->

However, one can {@{use other estimators for $\sigma ^{2}$ which are proportional to $S_{n-1}^{2}$}@}, and {@{an appropriate choice can always give a lower mean squared error}@}. If we define {@{$$S_{a}^{2}={\frac {n-1}{a} }S_{n-1}^{2}={\frac {1}{a} }\sum _{i=1}^{n}\left(X_{i}-{\overline {X} }\,\right)^{2}$$}@} then we calculate: {@{$${\begin{aligned}\operatorname {MSE} (S_{a}^{2})&=\operatorname {E} \left[\left({\frac {n-1}{a} }S_{n-1}^{2}-\sigma ^{2}\right)^{2}\right]\\&=\operatorname {E} \left[{\frac {(n-1)^{2} }{a^{2} } }S_{n-1}^{4}-2\left({\frac {n-1}{a} }S_{n-1}^{2}\right)\sigma ^{2}+\sigma ^{4}\right]\\&={\frac {(n-1)^{2} }{a^{2} } }\operatorname {E} \left[S_{n-1}^{4}\right]-2\left({\frac {n-1}{a} }\right)\operatorname {E} \left[S_{n-1}^{2}\right]\sigma ^{2}+\sigma ^{4}\\&={\frac {(n-1)^{2} }{a^{2} } }\operatorname {E} \left[S_{n-1}^{4}\right]-2\left({\frac {n-1}{a} }\right)\sigma ^{4}+\sigma ^{4}&&\operatorname {E} \left[S_{n-1}^{2}\right]=\sigma ^{2}\\&={\frac {(n-1)^{2} }{a^{2} } }\left({\frac {\gamma _{2} }{n} }+{\frac {n+1}{n-1} }\right)\sigma ^{4}-2\left({\frac {n-1}{a} }\right)\sigma ^{4}+\sigma ^{4}&&\operatorname {E} \left[S_{n-1}^{4}\right]=\operatorname {MSE} (S_{n-1}^{2})+\sigma ^{4}\\&={\frac {n-1}{na^{2} } }\left((n-1)\gamma _{2}+n^{2}+n\right)\sigma ^{4}-2\left({\frac {n-1}{a} }\right)\sigma ^{4}+\sigma ^{4}\end{aligned} }$$}@} This is {@{minimized}@} when {@{$$a={\frac {(n-1)\gamma _{2}+n^{2}+n}{n} }=n+1+{\frac {n-1}{n} }\gamma _{2}.$$}@} <!--SR:!2026-12-23,544,327!2028-11-12,1101,350!2027-09-01,746,330!2026-06-09,297,247!2026-01-11,293,330!2026-06-18,250,210-->

For {@{a [Gaussian distribution](normal%20distribution.md), where $\gamma _{2}=0$}@}, this means that {@{the MSE is minimized when dividing the sum by $a=n+1$}@}. {@{The minimum excess kurtosis is $\gamma _{2}=-2$}@},<sup>[\[a\]](#^ref-a)</sup> which is {@{achieved by a [Bernoulli distribution](Bernoulli%20distribution.md) with _p_<!-- markdown separator --> = 1/2 \(a coin flip\)}@}, and {@{the MSE is minimized for $a=n-1+{\tfrac {2}{n} }.$}@} Hence {@{regardless of the kurtosis}@}, we {@{get a "better" estimate \(in the sense of having a lower MSE\) by scaling down the unbiased estimator a little bit}@}; this is a simple example of {@{a [shrinkage estimator](shrinkage%20(statistics).md): one "shrinks" the estimator towards zero \(scales down the unbiased estimator\)}@}. <!--SR:!2028-10-26,1086,350!2029-03-19,1201,350!2027-12-05,820,330!2026-02-23,328,347!2027-01-31,522,270!2027-10-01,769,330!2026-03-06,339,347!2026-02-28,334,347-->

Further, while {@{the corrected sample variance is the [best unbiased estimator](minimum-variance%20unbiased%20estimator.md) \(minimum mean squared error among unbiased estimators\) of variance for Gaussian distributions}@}, if {@{the distribution is not Gaussian}@}, then {@{even among unbiased estimators, the best unbiased estimator of the variance may not be $S_{n-1}^{2}.$}@} <!--SR:!2026-06-27,399,307!2026-03-07,340,347!2027-04-10,579,310-->

### Gaussian distribution

The following table gives {@{several estimators of the true parameters of the population, μ and σ<sup>2</sup>, for the Gaussian case}@}.<sup>[\[9\]](#^ref-9)</sup> <!--SR:!2027-08-25,728,330-->

| True value                    | Estimator                                                                                                                                                                                                                             | Mean squared error                                                                                                                               |
| ----------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ |
| {@{$$\theta =\mu$$}@}         | {@{${\hat {\theta } }$ = the unbiased estimator of the [population mean](statistical%20population.md#mean), ${\overline {X} }={\frac {1}{n} }\sum _{i=1}^{n}(X_{i})$}@}                                                               | {@{$$\operatorname {MSE} ({\overline {X} })=\operatorname {E} (({\overline {X} }-\mu )^{2})=\left({\frac {\sigma }{\sqrt {n} } }\right)^{2}$$}@} |
| {@{$$\theta =\sigma ^{2}$$}@} | {@{${\hat {\theta } }$ = the unbiased estimator of the [population variance](variance.md#population%20variance%20and%20sample%20variance), $S_{n-1}^{2}={\frac {1}{n-1} }\sum _{i=1}^{n}\left(X_{i}-{\overline {X} }\,\right)^{2}$}@} | {@{$$\operatorname {MSE} (S_{n-1}^{2})=\operatorname {E} ((S_{n-1}^{2}-\sigma ^{2})^{2})={\frac {2}{n-1} }\sigma ^{4}$$}@}                       |
| {@{$$\theta =\sigma ^{2}$$}@} | {@{${\hat {\theta } }$ = the biased estimator of the [population variance](variance.md#population%20variance%20and%20sample%20variance), $S_{n}^{2}={\frac {1}{n} }\sum _{i=1}^{n}\left(X_{i}-{\overline {X} }\,\right)^{2}$}@}       | {@{$$\operatorname {MSE} (S_{n}^{2})=\operatorname {E} ((S_{n}^{2}-\sigma ^{2})^{2})={\frac {2n-1}{n^{2} } }\sigma ^{4}$$}@}                     |
| {@{$$\theta =\sigma ^{2}$$}@} | {@{${\hat {\theta } }$ = the biased estimator of the [population variance](variance.md#population%20variance%20and%20sample%20variance), $S_{n+1}^{2}={\frac {1}{n+1} }\sum _{i=1}^{n}\left(X_{i}-{\overline {X} }\,\right)^{2}$}@}   | {@{$$\operatorname {MSE} (S_{n+1}^{2})=\operatorname {E} ((S_{n+1}^{2}-\sigma ^{2})^{2})={\frac {2}{n+1} }\sigma ^{4}$$}@}                       | <!--SR:!2026-03-06,339,347!2027-10-13,791,347!2027-01-22,519,310!2026-03-12,344,347!2026-09-07,450,310!2026-06-19,391,307!2026-12-19,500,330!2028-12-22,1114,347!2026-10-07,366,230!2025-12-18,274,330!2028-12-30,1121,347!2026-05-15,366,307-->

## interpretation

{@{An MSE of zero}@}, meaning that {@{the estimator ${\hat {\theta } }$ predicts observations of the parameter $\theta$ with perfect accuracy}@}, is {@{ideal \(but typically not possible\)}@}. <!--SR:!2026-03-02,336,347!2026-02-26,331,347!2026-02-25,332,347-->

Values of MSE may be used for {@{comparative purposes}@}. {@{Two or more [statistical models](statistical%20model.md)}@} may be {@{compared using their MSEs—as a measure of how well they explain a given set of observations}@}: {@{An unbiased estimator \(estimated from a statistical model\) with the smallest variance among all unbiased estimators}@} is {@{the _best unbiased estimator_ or MVUE \([Minimum-Variance Unbiased Estimator](minimum-variance%20unbiased%20estimator.md)\)}@}. <!--SR:!2026-03-12,344,347!2026-03-01,334,347!2027-11-05,793,330!2027-01-17,550,310!2026-04-22,343,290-->

Both {@{[analysis of variance](analysis%20of%20variance.md) and [linear regression](linear%20regression.md) techniques}@} estimate {@{the MSE as part of the analysis}@} and use {@{the estimated MSE to determine the [statistical significance](statistical%20significance.md)}@} of {@{the factors or predictors under study}@}. {@{The goal of [experimental design](design%20of%20experiments.md)}@} is to construct {@{experiments in such a way}@} that when {@{the observations are analyzed, the MSE is close to zero}@} relative to {@{the magnitude of at least one of the estimated treatment effects}@}. <!--SR:!2026-02-15,322,347!2025-12-10,268,330!2028-03-01,816,327!2028-12-19,1130,350!2026-02-25,305,290-->

In {@{[one-way analysis of variance](one-way%20analysis%20of%20variance.md)}@}, MSE can be calculated by {@{the division of the sum of squared errors by the degree of freedom}@}. Also, {@{the f-value}@} is {@{the ratio of the mean squared treatment and the MSE}@}. <!--SR:!2026-03-08,341,347!2028-04-27,938,347!2026-02-17,324,347!2028-04-08,928,347-->

MSE is also used in {@{several [stepwise regression](stepwise%20regression.md) techniques}@} as part of {@{the determination as to how many predictors from a candidate set to include in a model for a given set of observations}@}. <!--SR:!2027-11-20,821,347!2027-01-14,567,327-->

## applications

<!-- | ![](../../archives/Wikimedia%20Commons/Edit-clear.svg) | This article __is in [list](https://en.wikipedia.org/wiki/Wikipedia:Manual%20of%20Style/Lists) format but may read better as [prose](https://en.wikipedia.org/wiki/Wikipedia:Manual%20of%20Style/Lists#Use%20prose%20where%20understood%20easily)__. You can help by [converting this article](https://en.wikipedia.org/w/index.php?title=Mean_squared_error&action=edit), if appropriate. [Editing help](https://en.wikipedia.org/wiki/Help:Editing) is available. _\(April 2021\)_ | -->

- Minimizing MSE is {@{a key criterion in selecting estimators: see [minimum mean-square error](minimum%20mean%20square%20error.md)}@}. Among {@{unbiased estimators}@}, {@{minimizing the MSE is equivalent to minimizing the variance}@}, and the estimator that {@{does this is the [minimum variance unbiased estimator](minimum-variance%20unbiased%20estimator.md)}@}. However, {@{a biased estimator may have lower MSE; see [estimator bias](bias%20of%20an%20estimator.md)}@}.
- In {@{[statistical modelling](statistical%20model.md)}@} {@{the MSE}@} can represent {@{the difference between the actual observations and the observation values predicted by the model}@}. In this context, it is used to {@{determine the extent to which the model fits the data}@} as well as {@{whether removing some explanatory variables is possible without significantly harming the model's predictive ability}@}.
- In {@{[forecasting](forecasting.md) and [prediction](prediction.md)}@}, {@{the [Brier score](Brier%20score.md)}@} is {@{a measure of [forecast skill](forecast%20skill.md) based on MSE}@}. <!--SR:!2026-02-19,326,347!2026-02-23,330,347!2028-09-29,1065,350!2026-03-04,337,347!2028-03-31,920,347!2026-01-11,293,330!2028-11-16,1085,347!2026-03-03,336,347!2026-12-29,536,310!2027-05-24,668,330!2026-02-28,333,347!2026-03-04,337,347-->

## loss function

{@{Squared error loss}@} is {@{one of the most widely used [loss functions](loss%20function.md) in statistics}@}, though its widespread use {@{stems more from mathematical convenience than considerations of actual loss in applications}@}. {@{[Carl Friedrich Gauss](Carl%20Friedrich%20Gauss.md)}@}, who {@{introduced the use of mean squared error}@}, was {@{aware of its arbitrariness and was in agreement with objections to it on these grounds}@}.<sup>[\[3\]](#^ref-3)</sup> {@{The mathematical benefits of mean squared error}@} are {@{particularly evident in its use at analyzing the performance of [linear regression](linear%20regression.md)}@}, as it {@{allows one to partition the variation in a dataset into variation explained by the model and variation explained by randomness}@}. <!--SR:!2026-02-18,325,347!2026-03-11,343,347!2029-03-10,1193,350!2027-01-08,563,327!2026-02-26,332,347!2028-02-25,896,347!2028-04-09,929,347!2026-01-09,291,330!2026-02-27,332,347-->

### criticism

{@{The use of mean squared error without question}@} has been criticized by {@{the [decision theorist](decision%20theory.md) [James Berger](James%20O.%20Berger.md)}@}. Mean squared error is {@{the negative of the expected value of one specific [utility function](utility.md#functions), the quadratic utility function}@}, which {@{may not be the appropriate utility function to use under a given set of circumstances}@}. There are, however, some scenarios where {@{mean squared error can serve as a good approximation to a loss function occurring naturally in an application}@}.<sup>[\[10\]](#^ref-10)</sup> <!--SR:!2029-01-13,1150,350!2026-01-11,293,330!2026-11-16,505,310!2027-08-30,742,330!2027-05-17,596,310-->

Like {@{[variance](variance.md)}@}, mean squared error has {@{the disadvantage of heavily weighting [outliers](outlier.md)}@}.<sup>[\[11\]](#^ref-11)</sup> This is {@{a result of the squaring of each term}@}, which {@{effectively weights large errors more heavily than small ones}@}. {@{This property, undesirable in many applications}@}, has led {@{researchers to use alternatives such as the [mean absolute error](mean%20absolute%20error.md), or those based on the [median](median.md)}@}. <!--SR:!2029-01-03,1123,347!2026-03-05,338,347!2026-03-10,342,347!2029-02-19,1178,350!2026-03-04,337,347!2027-01-22,572,327-->

## see also

- [Bias–variance tradeoff](bias–variance%20tradeoff.md)
- [Hodges' estimator](Hodges'%20estimator.md)
- [James–Stein estimator](James–Stein%20estimator.md)
- [Mean percentage error](mean%20percentage%20error.md)
- [Mean square quantization error](mean%20square%20quantization%20error.md)
- [Mean square weighted deviation](reduced%20chi-squared%20statistic.md)
- [Mean squared displacement](mean%20squared%20displacement.md)
- [Mean squared prediction error](mean%20squared%20prediction%20error.md)
- [Minimum mean square error](minimum%20mean%20square%20error.md)
- [Minimum mean squared error estimator](minimum%20mean%20square%20error.md)
- [Overfitting](overfitting.md)
- [Peak signal-to-noise ratio](peak%20signal-to-noise%20ratio.md)

| <!-- hide <p> - [v](https://en.wikipedia.org/wiki/Template:Machine%20learning%20evaluation%20metrics) <br/> - [t](https://en.wikipedia.org/wiki/Template%20talk:Machine%20learning%20evaluation%20metrics) <br/> - [e](https://en.wikipedia.org/wiki/Special:EditPage/Template%3AMachine%20learning%20evaluation%20metrics) <p>  <p>  <br/> --> [Machine learning](machine%20learning.md) evaluation metrics |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| __[Regression](regression%20analysis.md)__                                                                                                                                                                                                                                                                                                                                                                   | - __MSE__ <br/> - [MAE](mean%20absolute%20error.md) <br/> - [sMAPE](symmetric%20mean%20absolute%20percentage%20error.md) <br/> - [MAPE](mean%20absolute%20percentage%20error.md) <br/> - [MASE](mean%20absolute%20scaled%20error.md) <br/> - [MSPE](mean%20squared%20prediction%20error.md) <br/> - [RMS](root%20mean%20square.md) <br/> - [RMSE/RMSD](root%20mean%20square%20deviation.md) <br/> - [R<sup>2</sup>](coefficient%20of%20determination.md) <br/> - [MDA](mean%20directional%20accuracy.md) <br/> - [MAD](median%20absolute%20deviation.md)                                                    |
| __[Classification](statistical%20classification.md)__                                                                                                                                                                                                                                                                                                                                                        | - [F-score](F-score.md) <br/> - [P4](P4-metric.md) <br/> - [Accuracy](accuracy%20and%20precision.md) <br/> - [Precision](precision%20and%20recall.md) <br/> - [Recall](precision%20and%20recall.md) <br/> - [Kappa](Cohen's%20kappa.md) <br/> - [MCC](phi%20coefficient.md) <br/> - [AUC](receiver%20operating%20characteristic.md#area%20under%20the%20curve) <br/> - [ROC](receiver%20operating%20characteristic.md) <br/> - [Sensitivity and specificity](sensitivity%20and%20specificity.md) <br/> - [Logarithmic Loss](cross-entropy.md#cross-entropy%20loss%20function%20and%20logistic%20regression) |
| __[Clustering](cluster%20analysis.md)__                                                                                                                                                                                                                                                                                                                                                                      | - [Silhouette](silhouette%20(clustering).md) <br/> - [Calinski-Harabasz index](Calinski–Harabasz%20index.md) <br/> - [Davies-Bouldin](Davies–Bouldin%20index.md) <br/> - [Dunn index](Dunn%20index.md) <br/> - [Hopkins statistic](Hopkins%20statistic.md) <br/> - [Jaccard index](Jaccard%20index.md) <br/> - [Rand index](Rand%20index.md) <br/> - [Similarity measure](similarity%20measure.md) <br/> - [SMC](simple%20matching%20coefficient.md) <br/> - [SimHash](SimHash.md)                                                                                                                          |
| __[Ranking](ranking%20(information%20retrieval).md)__                                                                                                                                                                                                                                                                                                                                                        | - [MRR](mean%20reciprocal%20rank.md) <br/> - [NDCG](discounted%20cumulative%20gain.md) <br/> - [AP](evaluation%20measures%20(information%20retrieval).md#average%20precision)                                                                                                                                                                                                                                                                                                                                                                                                                               |
| __[Computer Vision](computer%20vision.md)__                                                                                                                                                                                                                                                                                                                                                                  | - [PSNR](peak%20signal-to-noise%20ratio.md) <br/> - [SSIM](structural%20similarity%20index%20measure.md) <br/> - [IoU](Jaccard%20index.md)                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| __[NLP](natural%20language%20processing.md)__                                                                                                                                                                                                                                                                                                                                                                | - [Perplexity](perplexity.md) <br/> - [BLEU](BLEU.md)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| __Deep Learning Related Metrics__                                                                                                                                                                                                                                                                                                                                                                            | - [Inception score](inception%20score.md) <br/> - [FID](Fréchet%20inception%20distance.md)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| __[Recommender system](recommender%20system.md)__                                                                                                                                                                                                                                                                                                                                                            | - [Coverage](coverage%20probability.md) <br/> - [Intra-list Similarity](intra-list%20similarity.md)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| __Similarity__                                                                                                                                                                                                                                                                                                                                                                                               | - [Cosine similarity](cosine%20similarity.md) <br/> - [Euclidean distance](Euclidean%20distance.md) <br/> - [Pearson correlation coefficient](Pearson%20correlation%20coefficient.md)                                                                                                                                                                                                                                                                                                                                                                                                                       |
| - [Confusion matrix](confusion%20matrix.md)                                                                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |

## notes

1. This can be proved by {@{[Jensen's inequality](Jensen's%20inequality.md)}@} as follows. {@{The fourth [central moment](central%20moment.md)}@} is {@{an upper bound for the square of variance (annotation: $\operatorname E\left[X^4\right] \ge \left(\operatorname E\left[X^2\right]\right)^2$)}@}, so that {@{the least value for their ratio is one}@}, therefore, {@{the least value for the [excess kurtosis](kurtosis.md#excess%20kurtosis) is −2}@}, achieved, for instance, by {@{a Bernoulli with _p_=1/2}@}. <a id="^ref-1"></a>^ref-1 <!--SR:!2026-06-22,361,290!2026-03-05,338,347!2028-04-15,934,347!2028-04-03,924,347!2026-11-25,514,310!2026-02-24,331,347-->

## references

This text incorporates [content](https://en.wikipedia.org/wiki/mean_squared_error) from [Wikipedia](Wikipedia.md) available under the [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) license.

1. ["Mean Squared Error \(MSE\)"](https://www.probabilitycourse.com/chapter9/9_1_5_mean_squared_error_MSE.php). _<www.probabilitycourse.com>_. Retrieved 2020-09-12. <a id="^ref-1"></a>^ref-1
2. <a id="CITEREFBickelDoksum2015"></a> [Bickel, Peter J.](Peter%20J.%20Bickel.md); Doksum, Kjell A. \(2015\). _Mathematical Statistics: Basic Ideas and Selected Topics_. Vol. I \(Second ed.\). p. 20. If we use quadratic loss, our risk function is called the _mean squared error_ \(MSE\) ... <a id="^ref-2"></a>^ref-2
3. <a id="CITEREFLehmannCasella1998"></a> Lehmann, E. L.; Casella, George \(1998\). _Theory of Point Estimation_ \(2nd ed.\). New York: Springer. [ISBN](ISBN.md) [978-0-387-98502-2](https://en.wikipedia.org/wiki/Special:BookSources/978-0-387-98502-2). [MR](Mathematical%20Reviews.md) [1639875](https://mathscinet.ams.org/mathscinet-getitem?mr=1639875). <a id="^ref-3"></a>^ref-3
4. <a id="CITEREFGarethWittenHastieTibshirani2021"></a> Gareth, James; Witten, Daniela; Hastie, Trevor; Tibshirani, Rob \(2021\). [_An Introduction to Statistical Learning: with Applications in R_](https://www.statlearning.com/). Springer. [ISBN](ISBN.md) [978-1071614174](https://en.wikipedia.org/wiki/Special:BookSources/978-1071614174). <a id="^ref-4"></a>^ref-4
5. <a id="CITEREFWackerlyMendenhallScheaffer2008"></a> Wackerly, Dennis; Mendenhall, William; Scheaffer, Richard L. \(2008\). _Mathematical Statistics with Applications_ \(7 ed.\). Belmont, CA, USA: Thomson Higher Education. [ISBN](ISBN.md) [978-0-495-38508-0](https://en.wikipedia.org/wiki/Special:BookSources/978-0-495-38508-0). <a id="^ref-5"></a>^ref-5
6. _A modern introduction to probability and statistics : understanding why and how_. Dekking, Michel, 1946-. London: Springer. 2005. [ISBN](ISBN.md) [978-1-85233-896-1](https://en.wikipedia.org/wiki/Special:BookSources/978-1-85233-896-1). [OCLC](OCLC.md#OCLC) [262680588](https://search.worldcat.org/oclc/262680588). <a id="^ref-6"></a>^ref-6
7. Steel, R.G.D, and Torrie, J. H., _Principles and Procedures of Statistics with Special Reference to the Biological Sciences._, [McGraw Hill](McGraw%20Hill%20Education.md), 1960, page 288. <a id="^ref-7"></a>^ref-7
8. <a id="CITEREFMoodGraybillBoes1974"></a> Mood, A.; Graybill, F.; Boes, D. \(1974\). [_Introduction to the Theory of Statistics_](https://archive.org/details/introductiontoth00mood_706) \(3rd ed.\). McGraw-Hill. p. [229](https://archive.org/details/introductiontoth00mood_706/page/n241). <a id="^ref-8"></a>^ref-8
9. <a id="degroot"></a> [DeGroot, Morris H.](Morris%20H.%20DeGroot.md) \(1980\). _Probability and Statistics_ \(2nd ed.\). Addison-Wesley. <a id="^ref-9"></a>^ref-9
10. <a id="CITEREFBerger1985"></a> [Berger, James O.](James%20O.%20Berger.md) \(1985\). "2.4.2 Certain Standard Loss Functions". [_Statistical Decision Theory and Bayesian Analysis_](https://archive.org/details/statisticaldecis00berg) \(2nd ed.\). New York: Springer-Verlag. p. [60](https://archive.org/details/statisticaldecis00berg/page/n72). [ISBN](ISBN.md) [978-0-387-96098-2](https://en.wikipedia.org/wiki/Special:BookSources/978-0-387-96098-2). [MR](Mathematical%20Reviews.md) [0804611](https://mathscinet.ams.org/mathscinet-getitem?mr=0804611). <a id="^ref-10"></a>^ref-10
11. <a id="CITEREFBermejoCabestany2001"></a> Bermejo, Sergio; Cabestany, Joan \(2001\). "Oriented principal component analysis for large margin classifiers". _Neural Networks_. __14__ \(10\): 1447–1461. [doi](digital%20object%20identifier.md):[10.1016/S0893-6080\(01\)00106-X](https://doi.org/10.1016%2FS0893-6080%2801%2900106-X). [PMID](PubMed.md#PubMed%20identifier) [11771723](https://pubmed.ncbi.nlm.nih.gov/11771723). <a id="^ref-11"></a>^ref-11

> [Categories](https://en.wikipedia.org/wiki/Help:Category):
>
> - [Point estimation performance](https://en.wikipedia.org/wiki/Category:Point%20estimation%20performance)
> - [Statistical deviation and dispersion](https://en.wikipedia.org/wiki/Category:Statistical%20deviation%20and%20dispersion)
> - [Loss functions](https://en.wikipedia.org/wiki/Category:Loss%20functions)
> - [Least squares](https://en.wikipedia.org/wiki/Category:Least%20squares)
