---
aliases:
  - RKHS
  - feature map
  - feature maps
  - reproducing kernel Hilbert space
tags:
  - flashcard/active/general/eng/reproducing_kernel_Hilbert_space
  - language/in/English
---

# reproducing kernel Hilbert space

> {@{![Figure illustrates related but varying approaches to viewing RKHS](../../archives/Wikimedia%20Commons/Different%20Views%20on%20RKHS.png)}@}
>
> Figure illustrates {@{related but varying approaches to viewing RKHS}@} <!--SR:!2026-04-14,62,310!2026-04-23,67,310-->

In {@{[functional analysis](functional%20analysis.md)}@}, {@{a __reproducing kernel Hilbert space__ \(__RKHS__\)}@} is {@{a [Hilbert space](Hilbert%20space.md) of functions in which point evaluation is a continuous [linear functional](linear%20functional.md)}@}. Specifically, {@{a Hilbert space $H$ of functions from a set $X$ \(to $\mathbb {R}$ or $\mathbb {C}$\) is an RKHS}@} if {@{the point-evaluation functional $L_{x}:H\to \mathbb {C}$, $L_{x}(f)=f(x)$, is continuous for every $x\in X$}@}. Equivalently, {@{$H$ is an RKHS}@} if there exists {@{a function $K_{x}\in H$ such that, for all $f\in H$, $$\langle f,K_{x}\rangle =f(x).$$}@} {@{The function $K_{x}$}@} is then called {@{the _reproducing kernel_}@}, and it {@{reproduces the value of $f$ at $x$ via the inner product}@}. <!--SR:!2026-04-22,66,310!2026-04-30,76,329!2026-04-23,67,310!2026-05-03,76,329!2026-03-31,44,290!2026-04-30,76,329!2026-04-22,66,310!2026-04-04,57,310!2026-04-20,67,310!2026-04-02,56,310-->

{@{An immediate consequence of this property}@} is that {@{convergence in norm}@} implies {@{[uniform convergence](uniform%20convergence.md) on any subset of $X$ on which $\|K_{x}\|$ is bounded}@}. (annotation: In {@{an RKHS}@}, {@{point evaluation}@} satisfies {@{$|f(x)| = |\langle f, K_x\rangle| \le \|f\| \cdot \|K_x\|$}@}. So if {@{$f_n \to f$ in norm}@}, then {@{$|f_n(x) - f(x)| \le \|f_n - f\| \cdot \|K_x\|$}@}. On {@{any subset where $\|K_x\|$ is bounded by $M$}@}, we have {@{$|f_n(x) - f(x)| \le M \cdot \|f_n - f\| \to 0$}@}, which means {@{convergence is uniform there}@}.) However, {@{the converse does not necessarily hold}@}. Often the set $X$ carries {@{a topology}@}, and $\|K_{x}\|$ depends {@{continuously on $x\in X$}@}, in which case: {@{convergence in norm}@} implies {@{uniform convergence on compact subsets of $X$}@}. (annotation: If {@{$X$ has a topology and $\|K_x\|$ depends continuously on $x \in X$}@}, then on {@{any compact subset $C \subset X$}@}, $\|K_x\|$ is {@{bounded by continuity and compactness}@}.) <!--SR:!2026-04-12,64,310!2026-04-22,66,310!2026-04-15,63,310!2026-03-26,50,309!2026-04-01,55,310!2026-05-02,75,329!2026-04-13,61,310!2026-04-30,76,329!2026-04-18,63,310!2026-03-19,44,290!2026-04-03,55,310!2026-04-13,61,310!2026-04-19,71,329!2026-03-31,54,310!2026-04-13,65,310!2026-04-12,64,310!2026-03-31,54,310!2026-04-23,67,310!2026-04-15,63,310-->

It is not {@{entirely straightforward to construct natural examples of a Hilbert space}@} which are not {@{an RKHS in a non-trivial fashion}@}.<sup>[\[1\]](#^ref-1)</sup> Some examples, {@{however, have been found}@}.<sup>[\[2\]](#^ref-2)</sup><sup>[\[3\]](#^ref-3)</sup> <!--SR:!2026-04-13,61,310!2026-04-22,66,310!2026-04-22,66,310-->

While, {@{formally, [_L_<sup>2</sup> spaces](square-integrable%20function.md)}@} are defined as {@{Hilbert spaces of equivalence classes of functions}@}, this definition can {@{trivially be extended to a Hilbert space of functions}@} by choosing {@{a \(total\) function as a representative for each equivalence class}@}. However, {@{no choice of representatives}@} can make {@{this space an RKHS}@} \($K_{0}$ would need to be {@{the non-existent Dirac delta function}@}\). However, there are {@{RKHSs in which the norm is an _L_<sup>2</sup>-norm}@}, such as {@{the space of band-limited functions}@} \(see the example below\). <!--SR:!2026-04-22,66,310!2026-04-23,67,310!2026-04-12,63,310!2026-04-22,66,310!2026-04-22,66,310!2026-04-23,67,310!2026-04-14,62,310!2026-04-23,67,310!2026-04-19,71,329-->

An RKHS is associated with {@{a kernel that reproduces every function in the space}@} in the sense that for every {@{$x$ in the set on which the functions are defined}@}, {@{"evaluation at $x$" can be performed}@} by taking {@{an inner product with a function determined by the kernel}@}. {@{Such a _reproducing kernel_ exists}@} {@{[if and only if](if%20and%20only%20if.md) every evaluation functional is continuous}@}. <!--SR:!2026-04-13,65,310!2026-05-02,75,329!2026-05-02,75,329!2026-04-20,67,310!2026-04-02,56,310!2026-04-19,71,329-->

{@{The reproducing kernel}@} was first introduced in {@{the 1907 work of [Stanisław Zaremba](Stanisław%20Zaremba%20(mathematician).md)}@}<sup>\[_[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation%20needed)_\]</sup> concerning {@{[boundary value problems](boundary%20value%20problem.md) for [harmonic](harmonic%20function.md) and [biharmonic functions](biharmonic%20equation.md)}@}. {@{[James Mercer](James%20Mercer%20(mathematician).md) simultaneously examined [functions](positive-definite%20kernel.md)}@} which satisfy {@{the reproducing property in the theory of [integral equations](integral%20equation.md)}@}. The idea of the reproducing kernel remained {@{untouched for nearly twenty years}@} until it appeared in {@{the dissertations of [Gábor Szegő](Gábor%20Szegő.md), [Stefan Bergman](Stefan%20Bergman.md), and [Salomon Bochner](Salomon%20Bochner.md)}@}. The subject was eventually {@{systematically developed in the early 1950s by [Nachman Aronszajn](Nachman%20Aronszajn.md) and Stefan Bergman}@}.<sup>[\[4\]](#^ref-4)</sup> <!--SR:!2026-04-14,62,310!2026-04-20,67,310!2026-04-19,71,329!2026-04-29,75,329!2026-04-12,60,310!2026-04-22,66,310!2026-04-22,66,310!2026-04-20,67,310-->

These spaces have {@{wide applications}@}, including {@{[complex analysis](complex%20analysis.md), [harmonic analysis](harmonic%20analysis.md), and [quantum mechanics](quantum%20mechanics.md)}@}. Reproducing kernel Hilbert spaces are particularly important in the field of {@{[statistical learning theory](statistical%20learning%20theory.md)}@} because of {@{the celebrated [representer theorem](representer%20theorem.md)}@} which states that {@{every function in an RKHS that minimises an empirical risk functional}@} can be written as {@{a [linear combination](linear%20combination.md) of the kernel function evaluated at the training points}@}. This is {@{a practically useful result}@} as it effectively {@{simplifies the [empirical risk minimization](empirical%20risk%20minimization.md) problem}@} from {@{an infinite dimensional to a finite dimensional optimization problem}@}. <!--SR:!2026-04-20,67,310!2026-04-24,75,329!2026-04-23,67,310!2026-04-14,67,329!2026-05-03,76,329!2026-04-20,67,310!2026-04-05,58,310!2026-04-25,76,329!2026-04-24,68,329-->

For {@{ease of understanding}@}, we provide {@{the framework for real-valued Hilbert spaces}@}. The theory can be easily {@{extended to spaces of complex-valued functions}@} and hence include {@{the many important examples of reproducing kernel Hilbert spaces}@} that are {@{spaces of [analytic functions](analytic%20functions.md)}@}.<sup>[\[5\]](#^ref-5)</sup> <!--SR:!2026-04-22,66,310!2026-04-04,57,310!2026-05-02,75,329!2026-04-23,67,310!2026-04-23,67,310-->

## definition

Let $X$ be {@{an arbitrary [set](set%20(mathematics).md)}@} and $H$ {@{a [Hilbert space](Hilbert%20space.md) of [real-valued functions](real-valued%20function.md) on $X$}@}, equipped with {@{pointwise addition and pointwise [scalar multiplication](scalar%20multiplication.md)}@}. {@{The [evaluation](Cartesian%20closed%20category.md#evaluation) functional over the Hilbert space of functions $H$}@} is {@{a linear functional that evaluates each function at a point $x$}@}, {@{$$L_{x}:f\mapsto f(x){\text{   } }\forall f\in H.$$}@} We say that {@{_H_ is a __reproducing kernel Hilbert space__}@} if, for {@{all $x$ in $X$}@}, $L_{x}$ is {@{[continuous](continuous%20function%20(topology).md#continuous%20functions%20between%20topological%20spaces) at every $f$ in $H$}@} or, equivalently, if $L_{x}$ is {@{a [bounded operator](bounded%20operator.md) on $H$}@}, i.e. there exists {@{some $M_{x}>0$ such that $$L_{x}(f):=f(x)\leq M_{x}\,\|f\|_{H}\qquad \forall f\in H. \, \tag{1}$$}@} __<a id="math 1" style="display: none;">(1)</a>__ Although {@{$M_{x}<\infty$ is assumed for all $x\in X$}@}, it might still be the case that {@{$\sup _{x}M_{x}=\infty$}@}. <!--SR:!2026-04-23,67,310!2026-04-01,55,310!2026-04-13,61,310!2026-04-30,76,329!2026-04-23,67,310!2026-04-22,66,310!2026-04-01,45,290!2026-04-06,59,310!2026-04-22,66,310!2026-04-04,57,310!2026-03-19,44,290!2026-04-25,76,329!2026-04-23,67,310-->

While property \(__[1](#math%201)__\) is {@{the weakest condition}@} that ensures {@{both the existence of an inner product and the evaluation of every function in $H$ at every point in the domain}@}, it does not {@{lend itself to easy application in practice}@}. {@{A more intuitive definition}@} of the RKHS can be obtained by observing that this property guarantees that {@{the evaluation functional can be represented by taking the inner product of $f$ with a function $K_{x}$ in $H$}@}. This function is the so-called {@{__reproducing kernel__<sup>\[_[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation%20needed)_\]</sup> for the Hilbert space $H$}@} from which {@{the RKHS takes its name}@}. More formally, {@{the [Riesz representation theorem](Riesz%20representation%20theorem.md)}@} implies that for {@{all $x$ in $X$}@} there exists {@{a unique element $K_{x}$ of $H$ with the reproducing property}@}, {@{$$f(x)=L_{x}(f)=\langle f,\ K_{x}\rangle _{H}\quad \forall f\in H. \tag{2}$$}@} __<a id="math 2" style="display: none;">(2)</a>__  Since $K_{x}$ is itself {@{a function defined on $X$ with values in the field $\mathbb {R}$}@} \(or {@{$\mathbb {C}$ in the case of complex Hilbert spaces}@}\) and as {@{$K_{x}$ is in $H$}@} we have that {@{$$K_{x}(y)=L_{y}(K_{x})=\langle K_{x},\ K_{y}\rangle _{H},$$}@} where $K_{y}\in H$ is {@{the element in $H$ associated to $L_{y}$}@}. <!--SR:!2026-04-22,66,310!2026-03-31,54,310!2026-04-15,63,310!2026-05-02,75,329!2026-04-23,67,310!2026-05-03,76,329!2026-05-03,76,329!2026-04-23,67,310!2026-04-23,67,310!2026-05-02,75,329!2026-04-23,67,310!2026-04-23,67,310!2026-04-20,67,310!2026-04-14,62,310!2026-04-11,54,309!2026-04-25,76,329-->

This allows us to define {@{the reproducing kernel of $H$}@} as {@{a function $K:X\times X\to \mathbb {R}$ \(or $\mathbb {C}$ in the complex case\)}@} by {@{$$K(x,y)=\langle K_{x},\ K_{y}\rangle _{H}.$$}@} From this definition it is easy to see that {@{$K:X\times X\to \mathbb {R}$ \(or $\mathbb {C}$ in the complex case\)}@} is both {@{symmetric \(resp. conjugate symmetric\) and [positive definite](positive%20definite.md)}@}, i.e. {@{$$\sum _{i,j=1}^{n}c_{i}c_{j}K(x_{i},x_{j})=\sum _{i=1}^{n}c_{i}\left\langle K_{x_{i} },\sum _{j=1}^{n}c_{j}K_{x_{j} }\right\rangle _{H}=\left\langle \sum _{i=1}^{n}c_{i}K_{x_{i} },\sum _{j=1}^{n}c_{j}K_{x_{j} }\right\rangle _{H}=\left\|\sum _{i=1}^{n}c_{i}K_{x_{i} }\right\|_{H}^{2}\geq 0$$}@} for every {@{$n\in \mathbb {N} ,x_{1},\dots ,x_{n}\in X,{\text{ and } }c_{1},\dots ,c_{n}\in \mathbb {R}$}@}.<sup>[\[6\]](#^ref-6)</sup> {@{The Moore–Aronszajn theorem}@} \(see below\) is {@{a sort of converse to this}@}: if {@{a function $K$ satisfies these conditions}@} then there is {@{a Hilbert space of functions on $X$ for which it is a reproducing kernel}@}. <!--SR:!2026-04-18,63,310!2026-04-23,67,310!2026-04-23,67,310!2026-04-02,56,310!2026-04-20,67,310!2026-03-07,32,270!2026-04-23,67,310!2026-04-22,66,310!2026-04-13,66,329!2026-04-20,67,310!2026-05-03,76,329-->

## examples

{@{The simplest example}@} of a reproducing kernel Hilbert space is {@{the space $L^{2}(X,\mu )$}@} where {@{$X$ is a set and $\mu$ is the [counting measure](counting%20measure.md) on $X$}@}. For {@{$x\in X$}@}, {@{the reproducing kernel $K_{x}$}@} is {@{the [indicator function](indicator%20function.md) of the one point set $\{x\}\subset X$}@}. <!--SR:!2026-04-23,67,310!2026-04-20,67,310!2026-04-01,55,310!2026-04-15,67,310!2026-04-22,66,310!2026-04-20,67,310-->

{@{Nontrivial reproducing kernel Hilbert spaces}@} often involve {@{[analytic functions](analytic%20function.md)}@}, as we now illustrate by example. Consider {@{the Hilbert space of [bandlimited](bandlimiting.md) [continuous functions](continuous%20function.md) $H$}@}. Fix some {@{[cutoff frequency](cutoff%20frequency.md) $0<a<\infty$}@} and define the Hilbert space {@{$$H=\{f\in L^{2}(\mathbb {R} )\mid \operatorname {supp} (F)\subset [-a,a]\}$$}@} where $L^{2}(\mathbb {R} )$ is {@{the set of square integrable functions}@}, and {@{$F(\omega )=\int _{-\infty }^{\infty }f(t)e^{-i\omega t}\,dt$}@} is {@{the [Fourier transform](Fourier%20transform.md) of $f$}@}. As {@{the inner product}@}, we use {@{$$\langle f,g\rangle _{L^{2} }=\int _{-\infty }^{\infty }f(x)\cdot {\overline {g(x)} }\,dx.$$}@} Since this is {@{a closed subspace of $L^{2}(\mathbb {R} )$}@}, it is {@{a Hilbert space}@}. Moreover, {@{the elements of $H$}@} are {@{smooth functions on $\mathbb {R}$ that tend to zero at infinity}@}, essentially by {@{the [Riemann-Lebesgue lemma](Riemann-Lebesgue%20lemma.md)}@}. In fact, {@{the elements of $H$}@} are {@{the restrictions to $\mathbb {R}$ of entire [holomorphic functions](holomorphic%20function.md), by the [Paley–Wiener theorem](Paley–Wiener%20theorem.md)}@}. <!--SR:!2026-04-22,66,310!2026-03-31,54,310!2026-04-22,66,310!2026-04-01,55,310!2026-04-24,68,329!2026-04-22,66,310!2026-04-23,67,310!2026-04-02,56,310!2026-04-23,67,310!2026-04-23,67,310!2026-04-29,75,329!2026-05-03,76,329!2026-04-15,63,310!2026-04-14,62,310!2026-03-31,54,310!2026-04-23,67,310!2026-04-23,67,310-->

From {@{the [Fourier inversion theorem](Fourier%20inversion%20theorem.md)}@}, we have {@{$$f(x)={\frac {1}{2\pi } }\int _{-a}^{a}F(\omega )e^{ix\omega }\,d\omega .$$}@} It then follows by {@{the [Cauchy–Schwarz inequality](Cauchy–Schwarz%20inequality.md) and [Plancherel's theorem](Plancherel's%20theorem.md)}@} (annotation: {@{the inequality is applied}@} with {@{$f(\omega )={\frac {1}{\sqrt {2\pi } }}F(\omega ){\text{ and }}g(\omega )={\frac {1}{\sqrt {2\pi } }}e^{ix\omega }\chi _{[-a,a]}(\omega)$}@}, where $\chi _{[-a,a]}$ is {@{the [indicator function](indicator%20function.md) of the interval $[-a,a]$}@}) that, for {@{all $x$}@}, {@{$$|f(x)|\leq {\frac {1}{2\pi } }{\sqrt {2a\int _{-a}^{a}|F(\omega )|^{2}\,d\omega } }={\frac {\sqrt {2a} }{2\pi } }{\sqrt {\int _{-\infty }^{\infty }|F(\omega )|^{2}\,d\omega } }={\sqrt {\frac {a}{\pi } } }\|f\|_{L^{2} }.$$}@} This inequality shows that {@{the evaluation functional is bounded}@}, proving that {@{$H$ is indeed a RKHS}@}. <!--SR:!2026-04-27,76,329!2026-04-23,67,310!2026-04-15,63,310!2026-04-14,62,310!2026-05-30,89,290!2026-03-31,54,310!2026-04-02,56,310!2026-05-13,77,354!2026-05-13,77,354!2026-05-12,76,354-->

{@{The kernel function $K_{x}$}@} in this case is given by {@{$$K_{x}(y)={\frac {a}{\pi } }\operatorname {sinc} \left({\frac {a}{\pi } }(y-x)\right)={\frac {\sin(a(y-x))}{\pi (y-x)} }.$$}@} {@{The Fourier transform of $K_{x}(y)$}@} defined above is given by {@{$$\int _{-\infty }^{\infty }K_{x}(y)e^{-i\omega y}\,dy={\begin{cases}e^{-i\omega x}&{\text{if } }\omega \in [-a,a],\\0&{\textrm {otherwise} },\end{cases} }$$}@} which is a consequence of {@{the [time-shifting property of the Fourier transform](Fourier%20transform.md#basic%20properties)}@}. (annotation: That is, find {@{a function with a Fourier transform of $\operatorname{rect}(x / 2a)$}@} to {@{cover the entire bandwidth}@}, then apply {@{the time-shifting property}@}.) Consequently, using {@{[Plancherel's theorem](Plancherel's%20theorem.md)}@}, we have {@{$$\langle f,K_{x}\rangle _{L^{2} }=\int _{-\infty }^{\infty }f(y)\cdot {\overline {K_{x}(y)} }\,dy={\frac {1}{2\pi } }\int _{-a}^{a}F(\omega )\cdot e^{i\omega x}\,d\omega =f(x).$$}@} Thus we obtain {@{the reproducing property of the kernel}@}. <!--SR:!2026-05-02,75,329!2026-04-16,50,250!2026-04-13,61,310!2026-03-05,27,270!2026-04-14,62,310!2026-04-23,67,310!2026-03-16,38,290!2026-04-22,66,310!2026-04-19,71,329!2026-03-17,39,290!2026-05-03,76,329-->

{@{$K_{x}$}@} in this case is {@{the "bandlimited version" of the [Dirac delta function](Dirac%20delta%20function.md)}@}, and that {@{$K_{x}(y)$}@} converges to {@{$\delta (y-x)$ in the weak sense}@} as {@{the cutoff frequency $a$ tends to infinity}@}. <!--SR:!2026-04-23,67,310!2026-04-22,66,310!2026-05-03,76,329!2026-04-19,66,310!2026-04-23,67,310-->

## Moore–Aronszajn theorem

We have seen how {@{a reproducing kernel Hilbert space}@} defines {@{a reproducing kernel function that is both symmetric and [positive definite](positive%20definite%20kernel.md)}@}. {@{The Moore–Aronszajn theorem}@} goes {@{in the other direction}@}; it states that {@{every symmetric, positive definite kernel defines a unique reproducing kernel Hilbert space}@}. The theorem first appeared in {@{Aronszajn's _Theory of Reproducing Kernels_, although he attributes it to [E. H. Moore](E.%20H.%20Moore.md)}@}. <!--SR:!2026-04-23,67,310!2026-04-25,76,329!2026-04-12,63,310!2026-05-02,75,329!2026-04-25,76,329!2026-04-22,66,310-->

&emsp; __Theorem__. Suppose _K_ is {@{a symmetric, [positive definite kernel](positive%20definite%20kernel.md) on a set _X_}@}. Then there is {@{a unique Hilbert space of functions on _X_}@} for which {@{_K_ is a reproducing kernel}@}. (annotation: {@{"Unique" here}@} means {@{any two Hilbert spaces with reproducing kernel $K$}@} differ {@{only up to a canonical isometry}@}.) <!--SR:!2026-04-22,66,310!2026-04-14,62,310!2026-05-03,76,329!2026-04-23,67,310!2026-04-22,66,310!2026-04-15,67,310-->

__Proof__. For {@{all _x_ in _X_}@}, define {@{_K<sub>x</sub>_ = _K_\(_x_, ⋅ \)}@}. Let _H_<sub>0</sub> be {@{the [linear span](linear%20span.md) (annotation: _finite_ linear combinations) of {_K<sub>x</sub>_ : _x_ ∈ _X_}<!-- flashcard separator -->}@}. Define {@{an inner product on _H_<sub>0</sub>}@} by {@{$$\left\langle \sum _{j=1}^{n}b_{j}K_{y_{j} },\sum _{i=1}^{m}a_{i}K_{x_{i} }\right\rangle _{H_{0} }=\sum _{i=1}^{m}\sum _{j=1}^{n}{a_{i} }b_{j}K(y_{j},x_{i}),$$}@} which implies {@{$K(x,y)=\left\langle K_{x},K_{y}\right\rangle _{H_{0} }$}@}. {@{The symmetry of this inner product}@} follows from {@{the symmetry of _K_}@} and the {@{non-degeneracy}@} follows from the fact that {@{_K_ is positive definite}@}. <!--SR:!2026-04-23,67,310!2026-03-19,44,290!2026-04-20,67,310!2026-04-23,67,310!2026-03-15,37,290!2026-03-16,38,290!2026-04-20,67,310!2026-04-22,66,310!2026-04-22,66,310!2026-04-22,73,329-->

Let _H_ be {@{the [completion](completion%20(metric%20space).md#completion) of _H_<sub>0</sub> with respect to this inner product}@}. Then _H_ consists of {@{functions of the form}@} {@{$$f(x)=\sum _{i=1}^{\infty }a_{i}K_{x_{i} }(x)\quad {\text{where} }\quad \lim _{n\to \infty }\sup _{p\geq 0}\left\|\sum _{i=n}^{n+p}a_{i}K_{x_{i} }\right\|_{H_{0} }=0.$$}@} Now we can check {@{the reproducing property \(__[2](#math%202)__\)}@}: {@{$$\langle f,K_{x}\rangle _{H}=\sum _{i=1}^{\infty }a_{i}\left\langle K_{x_{i} },K_{x}\right\rangle _{H_{0} }=\sum _{i=1}^{\infty }a_{i}K(x_{i},x)=f(x).$$}@} <!--SR:!2026-04-23,67,310!2026-04-23,67,310!2026-03-31,51,309!2026-03-16,38,290!2026-03-14,31,270-->

To {@{prove uniqueness}@}, let _G_ be {@{another Hilbert space of functions for which _K_ is a reproducing kernel}@}. For every {@{_x_ and _y_ in _X_}@}, \(__[2](#math%202)__\) implies that {@{$$\langle K_{x},K_{y}\rangle _{H}=K(x,y)=\langle K_{x},K_{y}\rangle _{G}.$$}@} By linearity, {@{$\langle \cdot ,\cdot \rangle _{H}=\langle \cdot ,\cdot \rangle _{G}$}@} on {@{the span of $\{K_{x}:x\in X\}$ (annotation: was defined to be _H_<sub>0</sub>)}@}. Then {@{$H\subset G$}@} because _G_ is {@{complete and contains _H_<sub>0</sub> and hence contains its completion}@}. <!--SR:!2026-04-01,55,310!2026-04-23,67,310!2026-04-10,53,309!2026-04-20,67,310!2026-04-22,66,310!2026-04-22,66,310!2026-03-20,43,290!2026-04-22,66,310-->

Now we need to prove that {@{every element of _G_ is in _H_}@}. Let $f$ be {@{an element of _G_}@}. Since _H_ is {@{a closed subspace of _G_}@}, we can write {@{$f=f_{H}+f_{H^{\bot } }$}@} where {@{$f_{H}\in H$ and $f_{H^{\bot } }\in H^{\bot }$}@}. Now if {@{$x\in X$}@} then, since _K_ is {@{a reproducing kernel of _G_ and _H_}@}: {@{$$f(x)=\langle K_{x},f\rangle _{G}=\langle K_{x},f_{H}\rangle _{G}+\langle K_{x},f_{H^{\bot } }\rangle _{G}=\langle K_{x},f_{H}\rangle _{G}=\langle K_{x},f_{H}\rangle _{H}=f_{H}(x),$$}@} where we have used the fact that {@{$K_{x}$ belongs to _H_}@} so that its {@{inner product with $f_{H^{\bot } }$ in _G_ is zero}@}. This shows that {@{$f=f_{H}$ in _G_ and concludes the proof}@}. <!--SR:!2026-04-22,66,310!2026-04-13,65,310!2026-04-20,67,310!2026-04-22,66,310!2026-04-14,62,310!2026-03-04,26,270!2026-04-22,66,310!2026-03-26,43,290!2026-04-22,66,310!2026-05-02,75,329!2026-04-19,66,310-->

(annotation: {@{Moore–Aronszajn theorem}@} gives {@{an _abstract_ RKHS construction that is unique for a given $K$}@}: start with {@{the span of kernel sections $K_x$}@}​ and complete {@{it under the induced inner product}@}. This works for {@{any positive definite kernel on any set $X$}@}, without {@{extra structure}@}. This is in contrast to {@{construction via integral operators and Mercer's theorem}@}, which requires {@{more assumptions on $X$ and $K$}@}, and gives {@{the _same_ RKHS construction (given that $K$ is the same)}@} but in {@{a more _concrete_ form}@}.) <!--SR:!2026-04-23,67,310!2026-04-22,66,310!2026-04-23,74,329!2026-04-23,67,310!2026-04-13,61,310!2026-04-20,67,310!2026-05-02,75,329!2026-04-23,67,310!2026-04-13,61,310!2026-04-20,67,310-->

## integral operators and Mercer's theorem

We may characterize {@{a symmetric positive definite kernel $K$}@} via {@{the integral operator using [Mercer's theorem](mercer's%20theorem.md)}@} and obtain {@{an additional view of the RKHS}@}. Let $X$ be {@{a compact space equipped with a strictly positive finite [Borel measure](Borel%20measure.md) $\mu$}@} and $K:X\times X\to \mathbb {R}$ {@{a continuous, symmetric, and positive definite function}@}. Define {@{the integral operator $T_{K}:L_{2}(X)\to L_{2}(X)$}@} as {@{$$[T_{K}f](\cdot )=\int _{X}K({}\cdot {},t)f(t)\,d\mu (t)$$}@} where $L_{2}(X)$ is {@{the space of square integrable functions with respect to $\mu$}@}. <!--SR:!2026-05-03,76,329!2026-04-01,55,310!2026-04-13,66,329!2026-04-01,45,290!2026-04-30,76,329!2026-03-22,44,309!2026-04-22,66,310!2026-04-01,55,310-->

{@{Mercer's theorem}@} states that {@{the spectral decomposition of the integral operator $T_{K}$ of $K$}@} yields {@{a series representation of $K$}@} in terms of {@{the eigenvalues and eigenfunctions of $T_{K}$}@}. This then implies that $K$ is {@{a reproducing kernel}@} so that the corresponding RKHS can be {@{defined in terms of these eigenvalues and eigenfunctions}@}. We provide the details below. <!--SR:!2026-04-13,61,310!2026-04-01,45,290!2026-04-18,63,310!2026-04-22,66,310!2026-04-25,76,329!2026-04-22,66,310-->

Under these assumptions $T_{K}$ is {@{a compact (annotation: It is an integral operator with a continuous kernel on a compact space, which ensures that it maps bounded sets in $L_2(X)$ to relatively compact sets.)}@}, {@{continuous (annotation: It is a bounded linear operator on $L_2(X)$; specifically, $\|T_K f\|_{L_2} \le C \|f\|_{L_2}$ for some constant $C$ depending on $K$ and the _finite_ Borel measure $\mu$.)}@}, {@{self-adjoint (annotation: $K(x,t)$ is symmetric, so $\langle T_K f, g\rangle = \langle f, T_K g\rangle$ for all $f,g \in L_2(X)$.)}@}, and {@{[positive operator](positive%20operator.md) (annotation: $K$ is positive definite, meaning $\langle T_K f, f\rangle \ge 0$ for all $f \in L_2(X)$.)}@}. {@{The [spectral theorem](spectral%20theorem.md) for self-adjoint operators}@} implies that there is {@{an at most countable decreasing sequence $(\sigma _{i})_{i\geq 0}$}@} such that {@{$\lim _{i\to \infty }\sigma _{i}=0$ and $T_{K}\varphi _{i}(x)=\sigma _{i}\varphi _{i}(x)$}@}, where the $\{\varphi _{i}\}$ form {@{an orthonormal basis of $L_{2}(X)$ (annotation: remember _orthonormal_ for later)}@}. By {@{the positivity of $T_{K}$}@}, {@{$\sigma _{i}>0$ for all $i$}@}. One can also show that $T_{K}$ maps {@{continuously into the space of continuous functions $C(X)$ (annotation: the integral of a continuous kernel over a compact domain depends continuously on $x$)}@} and therefore we may choose {@{continuous functions as the eigenvectors}@}, that is, {@{$\varphi _{i}\in C(X)$ for all $i$}@}. Then by {@{Mercer's theorem}@} (annotation: applies when {@{$X$ is compact, $\mu$ is a finite positive Borel measure, and $K$ is continuous, symmetric, and positive definite}@}) $K$ may be {@{written in terms of the eigenvalues and continuous eigenfunctions}@} as {@{$$K(x,y)=\sum _{j=1}^{\infty }\sigma _{j}\,\varphi _{j}(x)\,\varphi _{j}(y)$$}@} for {@{all $x,y\in X$}@} such that {@{$$\lim _{n\to \infty }\sup _{u,v}\left|K(u,v)-\sum _{j=1}^{n}\sigma _{j}\,\varphi _{j}(u)\,\varphi _{j}(v)\right|=0.$$}@} (annotation: {@{Uniform convergence of the Mercer series}@} ensures the kernel can be {@{represented by eigenfunctions and eigenvalues}@} in a way that defines {@{a complete Hilbert space of functions}@}, which is essential for {@{constructing the RKHS}@}.) {@{This above series representation}@} is referred to as {@{a Mercer kernel or Mercer representation of $K$}@}. <!--SR:!2026-03-26,43,290!2026-03-07,32,270!2026-04-01,45,290!2026-03-20,43,290!2026-04-19,66,310!2026-04-23,67,310!2026-03-14,31,270!2026-04-23,67,310!2026-04-30,76,329!2026-04-22,66,310!2026-03-19,44,290!2026-04-22,66,310!2026-04-02,56,310!2026-04-15,63,310!2026-03-05,27,270!2026-03-27,47,309!2026-03-16,38,290!2026-04-01,55,310!2026-03-14,31,270!2026-04-23,67,310!2026-04-22,66,310!2026-04-23,67,310!2026-03-19,43,290!2026-04-22,66,310!2026-04-23,67,310-->

Furthermore, it can be shown that {@{the RKHS $H$ of $K$}@} is given by {@{$$H=\left\{f\in L_{2}(X)\,{\Bigg \vert }\,\sum _{i=1}^{\infty }{\frac {\left\langle f,\varphi _{i}\right\rangle _{L_{2} }^{2} }{\sigma _{i} } }<\infty \right\}$$}@} (annotation: It consists of {@{square-integrable functions on $X$}@} whose {@{expansion in the eigenfunction basis has coefficients}@} that decay {@{fast enough so that their weighted sum by $1/\sigma_i$ is finite}@}, ensuring {@{completeness and the RKHS structure}@}. {@{The division by $\sigma_i$}@} comes from {@{the inner product definition}@} below.) where {@{the inner product of $H$}@} given by {@{$$\left\langle f,g\right\rangle _{H}=\sum _{i=1}^{\infty }{\frac {\left\langle f,\varphi _{i}\right\rangle _{L_{2} }\left\langle g,\varphi _{i}\right\rangle _{L_{2} } }{\sigma _{i} } }.$$}@} (annotation: {@{The division by $\sigma_i$}@} is needed to remove {@{the extra factor $\sigma_i$ from evaluating $\langle K_x, \varphi_i \rangle_{L_2}$ for each $i$}@}.) {@{This representation of the RKHS}@} has application in {@{probability and statistics}@}, for example to {@{the [Karhunen–Loève representation](Karhunen–Loève%20theorem.md)}@} for stochastic processes and {@{[kernel PCA](kernel%20PCA.md)}@}. <!--SR:!2026-03-20,43,290!2026-04-14,66,310!2026-04-23,67,310!2026-04-23,67,310!2026-04-20,67,310!2026-05-03,76,329!2026-04-14,66,310!2026-03-31,54,310!2026-04-30,76,329!2026-03-26,43,290!2026-05-02,75,329!2026-05-03,76,329!2026-04-22,66,310!2026-04-02,56,310!2026-04-15,63,310!2026-04-19,66,310-->

(annotation: Compared to {@{Moore–Aronszajn theorem}@}, {@{construction via integral operators and Mercer's theorem}@} _additionally_ requires $X$ to be {@{a compact topological space with a finite Borel measure}@} and $K$ is _additionally_ {@{continuous}@}, and gives {@{the _same_ RKHS construction (given $K$ is the same)}@} but in {@{a more _concrete_ form}@}, as it also provides {@{an explicit orthonormal basis and inner product formula}@} for the RKHS in terms of {@{the eigenfunctions $\varphi_i$}@}.) <!--SR:!2026-04-20,67,310!2026-05-02,75,329!2026-04-02,56,310!2026-04-05,51,309!2026-04-18,63,310!2026-04-20,67,310!2026-04-22,66,310!2026-04-13,61,310-->

## feature maps

{@{A __feature map__}@} is {@{a map $\varphi \colon X\rightarrow F$}@}, where $F$ is {@{a Hilbert space which we will call the feature space}@}. The first sections presented the connection between {@{bounded/continuous evaluation functions, positive definite functions, and integral operators}@} and in this section we provide {@{another representation of the RKHS in terms of feature maps}@}. <!--SR:!2026-04-23,67,310!2026-04-22,66,310!2026-04-22,66,310!2026-04-02,56,310!2026-04-23,67,310-->

{@{Every feature map}@} {@{defines a kernel}@} via {@{$$K(x,y)=\langle \varphi (x),\varphi (y)\rangle _{F}. \tag{3}$$}@} __<a id="math 3" style="display: none;">3</a>__ Clearly $K$ is {@{symmetric and positive definiteness}@} follows from {@{the properties of inner product in $F$}@}. Conversely, {@{every positive definite function and corresponding reproducing kernel Hilbert space}@} has {@{infinitely many associated feature maps}@} such that {@{\(__[3](#math%203)__\) (annotation: defining a kernel) holds}@}. <!--SR:!2026-04-22,66,310!2026-04-23,67,310!2026-03-21,46,309!2026-05-03,76,329!2026-04-22,66,310!2026-04-22,66,310!2026-04-14,62,310!2026-04-20,67,310-->

For example, we can trivially take {@{$F=H$ and $\varphi (x)=K_{x}$ for all $x\in X$}@}. Then {@{\(__[3](#math%203)__\) (annotation: defining a kernel)}@} is satisfied by {@{the reproducing property}@}. {@{Another classical example}@} of a feature map relates to {@{the previous section regarding integral operators}@} by taking {@{$F=\ell ^{2}$}@} (annotation: space of {@{square-integrable functions on $X$}@} whose {@{expansion in the eigenfunction basis has coefficients}@} that decay {@{fast enough so that their weighted sum by $1/\sigma_i$ is finite}@}; see above) and {@{$\varphi (x)=({\sqrt {\sigma _{i} } }\varphi _{i}(x))_{i}$}@}. <!--SR:!2026-04-20,67,310!2026-03-31,44,290!2026-04-23,67,310!2026-04-02,56,310!2026-04-20,67,310!2026-05-02,75,329!2026-04-22,66,310!2026-04-15,63,310!2026-04-22,66,310!2026-04-21,55,334-->

This connection between {@{kernels and feature maps}@} provides us with {@{a new way to understand positive definite functions}@} and hence {@{reproducing kernels as inner products in $H$}@}. Moreover, {@{every feature map}@} can naturally define {@{a RKHS by means of the definition of a positive definite function}@}. <!--SR:!2026-04-25,76,329!2026-04-23,67,310!2026-04-19,66,310!2026-04-23,67,310!2026-04-22,66,310-->

Lastly, feature maps allow us to construct {@{function spaces that reveal another perspective on the RKHS}@}. Consider {@{the linear space}@} {@{$$H_{\varphi }=\{f:X\to \mathbb {R} \mid \exists w\in F,f(x)=\langle w,\varphi (x)\rangle _{F},\forall {\text{  } }x\in X\}.$$}@} (annotation: Intuitively, $H_\varphi$ is {@{the set of functions that can be represented by inner product with some weight vector $w$ in the feature space $F$}@}, i.e. written as {@{$f(x) = \langle w, \varphi(x) \rangle_F$}@}, where $\varphi(x)$ maps {@{each input into a feature space}@} and $w$ is {@{a weight vector in that space}@}. These functions are {@{linear in the feature space}@}, even if they appear {@{nonlinear in the original space}@}, which is {@{the core idea behind kernel methods}@}.) We can define {@{a norm on $H_{\varphi }$}@} by {@{$$\|f\|_{\varphi }=\inf\{\|w\|_{F}:w\in F,f(x)=\langle w,\varphi (x)\rangle _{F},\forall {\text{  } }x\in X\}.$$}@} (annotation: Intuitively, this means {@{the norm of a function}@} is {@{the smallest possible norm}@} of {@{any weight vector $w$ in the feature space that represents $f$}@}. It measures {@{the "simplest" or most efficient representation of $f$ in terms of $w$}@}.) It can be shown that {@{$H_{\varphi }$}@} is {@{a RKHS with kernel defined by $K(x,y)=\langle \varphi (x),\varphi (y)\rangle _{F}$}@}. This representation implies that {@{the elements of the RKHS}@} are {@{inner products of elements in the feature space}@} and can accordingly be seen as {@{hyperplanes}@}. {@{This view of the RKHS}@} is related to {@{the [kernel trick](kernel%20trick.md#mathematics%20the%20kernel%20trick) in machine learning}@}.<sup>[\[7\]](#^ref-7)</sup> <!--SR:!2026-04-30,76,329!2026-04-22,66,310!2026-03-15,32,270!2026-03-19,44,290!2026-04-22,66,310!2026-03-31,54,310!2026-04-29,75,329!2026-04-18,63,310!2026-04-23,67,310!2026-04-19,71,329!2026-03-17,39,290!2026-04-19,66,310!2026-04-27,76,329!2026-04-13,61,310!2026-04-22,66,310!2026-04-22,66,310!2026-03-19,41,309!2026-04-22,66,310!2026-04-20,67,310!2026-05-02,75,329!2026-04-23,67,310!2026-04-22,66,310!2026-05-18,81,354-->

## properties

Useful properties of RKHSs:

- Let {@{$(X_{i})_{i=1}^{p}$}@} be {@{a sequence of sets}@} and {@{$(K_{i})_{i=1}^{p}$}@} be {@{a collection of corresponding positive definite functions on $(X_{i})_{i=1}^{p}$}@}. It then follows that {@{$$K((x_{1},\ldots ,x_{p}),(y_{1},\ldots ,y_{p}))=K_{1}(x_{1},y_{1})\cdots K_{p}(x_{p},y_{p})$$}@} is {@{a kernel on $X=X_{1}\times \dots \times X_{p}$}@}.
- Let {@{$X_{0}\subset X$}@}, then {@{the restriction of $K$ to $X_{0}\times X_{0}$}@} is also {@{a reproducing kernel}@}.
- Consider {@{a normalized kernel $K$}@} such that {@{$K(x,x)=1$ for all $x\in X$}@}. Define {@{a pseudo-metric on X}@} (annotation: {@{not a metric}@} as {@{different points in X can map to the same feature vector, giving distance zero even when $x \ne y$}@}) as {@{$$d_{K}(x,y)=\|K_{x}-K_{y}\|_{H}^{2}=2(1-K(x,y))\qquad \forall x\in X.$$}@} (annotation: Derived from {@{$\lVert K_x - K_y \rVert_H^2$}@} by applying {@{$\lVert K_x - K_y \rVert_H^2 = \lVert K_x \rVert_H^2 + \lVert K_y \rVert_H^2 - 2 \langle K_x, K_y \rangle_H$}@}.) By {@{the [Cauchy–Schwarz inequality](Cauchy–Schwarz%20inequality.md)}@}, {@{$$K(x,y)^{2}\leq K(x,x)K(y,y)=1\qquad \forall x,y\in X.$$}@} (annotation: Indeed, this inequality holds for {@{any positive definite matrix}@}.) This inequality allows us to view $K$ as {@{a [measure of similarity](similarity%20measure.md) between inputs}@}. If {@{$x,y\in X$ are similar}@} then $K(x,y)$ will be {@{closer to 1}@} while if {@{$x,y\in X$ are dissimilar then $K(x,y)$ will be closer to 0}@}. (annotation: Thus {@{$d_K(x, y)$}@} can be interpreted as {@{a distance measure induced by the kernel $K$}@}, where {@{more similar points have smaller distances}@}.)
- {@{The closure of the span of $\{K_{x}\mid x\in X\}$}@} coincides {@{with $H$}@}.<sup>[\[8\]](#^ref-8)</sup> <!--SR:!2026-04-22,66,310!2026-04-14,62,310!2026-04-20,67,310!2026-04-30,76,329!2026-04-22,66,310!2026-03-31,54,310!2026-03-19,44,290!2026-04-01,55,310!2026-04-22,66,310!2026-04-23,67,310!2026-04-04,57,310!2026-04-19,66,310!2026-04-03,50,309!2026-03-17,39,290!2026-04-23,67,310!2026-04-25,76,329!2026-04-01,45,290!2026-04-01,55,310!2026-04-14,62,310!2026-03-31,54,310!2026-04-22,66,310!2026-04-22,66,310!2026-04-01,52,309!2026-04-22,66,310!2026-05-12,76,354!2026-05-12,76,354!2026-05-13,77,354!2026-04-19,53,334!2026-03-17,19,351-->

## common examples

### bilinear kernels

{@{$$K(x,y)=\langle x,y\rangle$$}@} {@{The RKHS $H$ corresponding to this kernel}@} is {@{the dual space}@}, consisting of {@{functions $f(x)=\langle x,\beta \rangle$ satisfying $\|f\|_{H}^{2}=\|\beta \|^{2}$}@}. (annotation: Simply apply {@{the Moore–Aronszajn theorem}@} to see that the RKHS consists of {@{linear functionals on the input space}@}, with {@{the norm induced by the inner product}@}, so that {@{the RKHS norm of $f(x)$ equals the norm of its defining vector $\beta$}@}.) <!--SR:!2026-04-19,71,329!2026-04-15,63,310!2026-03-27,44,290!2026-03-06,31,270!2026-04-15,63,310!2026-04-04,57,310!2026-04-23,67,310!2026-05-24,86,354-->

### polynomial kernels

{@{$$K(x,y)=(\alpha \langle x,y\rangle +1)^{d},\qquad \alpha \in \mathbb {R} ,d\in \mathbb {N}$$}@} (annotation: To construct the RKHS, expand {@{the polynomial using the binomial theorem}@} and apply {@{the Moore–Aronszajn theorem}@}.) <!--SR:!2026-03-31,44,290!2026-04-30,76,329!2026-04-22,66,310-->

### [radial basis function kernels](radial%20basis%20function%20kernel.md)

These are {@{another common class of kernels}@} which satisfy {@{$K(x,y)=K(\|x-y\|)$ (annotation: hence "radial")}@}. Some examples include: (annotation: 2 items: {@{Gaussian or squared exponential, Laplacian}@}) <!--SR:!2026-04-25,76,329!2026-03-31,54,310!2026-05-02,75,329-->

- __Gaussian__ or __squared exponential kernel__: ::@:: $$K(x,y)=e^{-{\frac {\|x-y\|^{2} }{2\sigma ^{2} } } },\qquad \sigma >0$$ <!--SR:!2026-04-19,71,329!2026-04-25,76,329-->
- __Laplacian kernel__: ::@:: $$K(x,y)=e^{-{\frac {\|x-y\|}{\sigma } } },\qquad \sigma >0$$ The squared norm of a function $f$ in the RKHS $H$ with this kernel is:<sup>[\[9\]](#^ref-9)</sup><sup>[\[10\]](#^ref-10)</sup> $$\|f\|_{H}^{2}=\int _{\mathbb {R} }{\Big (}{\frac {1}{\sigma } }f(x)^{2}+\sigma f'(x)^{2}{\Big )}\mathrm {d} x.$$ <!--SR:!2026-04-01,43,250!2026-03-19,44,290-->

### [Bergman kernels](Bergman%20kernel.md)

We also provide examples of {@{[Bergman kernels](Bergman%20kernel.md)}@}. Let _X_ be {@{finite}@} and let _H_ consist of {@{all complex-valued functions on _X_}@}. Then {@{an element of _H_}@} can be represented as {@{an array of complex numbers}@}. If {@{the usual [inner product](inner%20product.md) is used}@}, then _K<sub>x</sub>_ is {@{the function whose value is 1 at _x_ and 0 everywhere else}@}, and {@{$K(x,y)$}@} can be thought of as {@{an identity matrix}@} since {@{$$K(x,y)={\begin{cases}1&x=y\\0&x\neq y\end{cases} }$$}@} In this case, _H_ is {@{isomorphic to $\mathbb {C} ^{n}$}@}. <!--SR:!2026-04-22,66,310!2026-04-15,63,310!2026-04-02,56,310!2026-04-20,67,310!2026-04-08,61,310!2026-04-22,66,310!2026-04-22,66,310!2026-04-23,67,310!2026-04-02,54,310!2026-04-13,61,310!2026-03-31,44,290-->

The case of {@{$X=\mathbb {D}$ \(where $\mathbb {D}$ denotes the [unit disc](unit%20disc.md)\)}@} is {@{more sophisticated}@}. Here {@{the [Bergman space](Bergman%20space.md) $A^{2}(\mathbb {D} )$}@} is the space of {@{[square-integrable](square-integrable%20function.md) [holomorphic functions](holomorphic%20function.md) on $\mathbb {D}$}@}. It can be shown that {@{the reproducing kernel for $A^{2}(\mathbb {D} )$}@} is {@{$$K(x,y)={\frac {1}{\pi } }{\frac {1}{(1-x{\overline {y} })^{2} } }.$$}@} <!--SR:!2026-04-22,66,310!2026-04-23,67,310!2026-04-23,67,310!2026-04-04,57,310!2026-04-20,67,310!2026-03-19,38,289-->

Lastly, the space of {@{band limited functions in $L^{2}(\mathbb {R} )$ with bandwidth $2a$}@} is {@{a RKHS with reproducing kernel}@} {@{$$K(x,y)={\frac {\sin a(x-y)}{\pi (x-y)} }.$$}@} (annotation: Same as {@{the example near the start of the article}@}.) <!--SR:!2026-04-17,67,310!2026-04-22,66,310!2026-04-19,71,329!2026-04-23,67,310-->

## extension to vector-valued functions

In this section we extend {@{the definition of the RKHS}@} to spaces of {@{vector-valued functions}@} as this extension is particularly {@{important in [multi-task learning](multi-task%20learning.md) and [manifold regularization](manifold%20regularization.md)}@}. {@{The main difference}@} is that {@{the reproducing kernel $\Gamma$}@} is {@{a symmetric function that is now a positive semi-definite _matrix_ for every $x,y$ in $X$}@} (annotation: if it is unclear, $\Gamma(x, y)$ outputs {@{a matrix instead of a scalar}@}). More formally, we define {@{a vector-valued RKHS \(vvRKHS\)}@} as {@{a Hilbert space of functions $f:X\to \mathbb {R} ^{T}$}@} such that for {@{all $c\in \mathbb {R} ^{T}$ and $x\in X$}@} {@{$$\Gamma _{x}c(y)=\Gamma (x,y)c\in H{\text{ for } }y\in X$$}@} and {@{$$\langle f,\Gamma _{x}c\rangle _{H}=f(x)^{\intercal }c.$$}@} (annotation: {@{These two properties}@} are similar to {@{the scalar-valued case}@}, with {@{a testing vector $c$ that iterates over all $c \in \mathbb R^T$}@} added.) This second property parallels {@{the reproducing property for the scalar-valued case}@}. This definition can also be connected to {@{integral operators, bounded evaluation functions, and feature maps}@} as we {@{saw for the scalar-valued RKHS}@}. We can equivalently define the vvRKHS as {@{a vector-valued Hilbert space with a bounded evaluation functional}@} and show that this implies {@{the existence of a unique reproducing kernel}@} by {@{the Riesz Representation theorem}@}. {@{Mercer's theorem}@} can also be {@{extended to address the vector-valued setting}@} and we can therefore obtain {@{a feature map view of the vvRKHS}@}. Lastly, it can also be shown that {@{the closure of the span of $\{\Gamma _{x}c:x\in X,c\in \mathbb {R} ^{T}\}$}@} coincides {@{with $H$}@}, another property {@{similar to the scalar-valued case}@}. <!--SR:!2026-04-22,66,310!2026-04-30,76,329!2026-04-22,66,310!2026-04-22,66,310!2026-04-30,76,329!2026-04-23,67,310!2026-04-23,67,310!2026-04-23,67,310!2026-04-14,67,329!2026-03-19,44,290!2026-04-23,67,310!2026-03-04,26,270!2026-03-31,54,310!2026-04-17,67,310!2026-04-12,65,329!2026-04-30,76,329!2026-04-23,67,310!2026-04-29,75,329!2026-05-03,76,329!2026-04-23,67,310!2026-03-25,42,290!2026-04-22,66,310!2026-04-22,66,310!2026-04-22,66,310!2026-04-23,67,310!2026-04-23,67,310!2026-04-20,67,310-->

We can gain {@{intuition for the vvRKHS}@} by taking {@{a component-wise perspective on these spaces}@}. In particular, we find that every vvRKHS is {@{isometrically [isomorphic](isomorphic.md) to a scalar-valued RKHS on a particular input space}@}. Let {@{$\Lambda =\{1,\dots ,T\}$}@}. Consider {@{the space $X\times \Lambda$ and the corresponding reproducing kernel}@} {@{$$\gamma :X\times \Lambda \times X\times \Lambda \to \mathbb {R} . \tag{4}$$}@} __<a id="math 4" style="display: none;">4</a>__ As noted above, {@{the RKHS associated to this reproducing kernel}@} is given by {@{the closure of the span of $\{\gamma _{(x,t)}:x\in X,t\in \Lambda \}$}@} where {@{$\gamma _{(x,t)}(y,s)=\gamma ((x,t),(y,s))$}@} for {@{every set of pairs $(x,t),(y,s)\in X\times \Lambda$}@}. <!--SR:!2026-04-04,57,310!2026-04-23,67,310!2026-04-20,67,310!2026-04-01,55,310!2026-05-02,75,329!2026-03-19,44,290!2026-04-25,76,329!2026-04-22,66,310!2026-03-21,42,309!2026-03-20,43,290-->

The connection to {@{the scalar-valued RKHS}@} can then be made by the fact that {@{every matrix-valued kernel (annotation: $\Gamma$)}@} can be identified with {@{a kernel of the form of \(__[4](#math%204)__\) (annotation: $\gamma: X \times \Lambda \times X \times \Lambda \to \mathbb {R}$)}@} via {@{$$\Gamma (x,y)_{(t,s)}=\gamma ((x,t),(y,s)).$$}@} Moreover, {@{every kernel with the form of \(__[4](#math%204)__\)}@} defines {@{a matrix-valued kernel with the above (annotation: same) expression}@}. Now letting {@{the map $D:H_{\Gamma }\to H_{\gamma }$}@} be defined as {@{$$(Df)(x,t)=\langle f(x),e_{t}\rangle _{\mathbb {R} ^{T} }$$}@} where $e_{t}$ is {@{the $t^{\text{th} }$ component of the canonical basis for $\mathbb {R} ^{T}$}@}, one can show that $D$ is {@{bijective and an isometry between $H_{\Gamma }$ and $H_{\gamma }$}@}. <!--SR:!2026-04-15,67,310!2026-04-23,67,310!2026-03-15,37,290!2026-03-17,39,290!2026-04-23,67,310!2026-04-22,66,310!2026-03-27,44,290!2026-04-22,66,310!2026-04-23,67,310!2026-04-20,67,310-->

While {@{this view of the vvRKHS}@} can be useful in {@{multi-task learning}@}, this isometry does not {@{reduce the study of the vector-valued case to that of the scalar-valued case}@}. In fact, this isometry procedure can make {@{both the scalar-valued kernel and the input space}@} too {@{difficult to work with in practice}@} as {@{properties of the original kernels are often lost}@}.<sup>[\[11\]](#^ref-11)</sup><sup>[\[12\]](#^ref-12)</sup><sup>[\[13\]](#^ref-13)</sup> <!--SR:!2026-04-04,57,310!2026-04-18,63,310!2026-05-03,76,329!2026-04-02,56,310!2026-04-02,56,310!2026-04-23,67,310-->

{@{An important class of matrix-valued reproducing kernels}@} are {@{_separable_ kernels}@} which can factorized as {@{the product of a scalar valued kernel and a $T$-dimensional symmetric positive semi-definite matrix}@}. In light of {@{our previous discussion}@} these kernels are of the form {@{$$\gamma ((x,t),(y,s))=K(x,y)K_{T}(t,s)$$}@} for {@{all $x,y$ in $X$ and $t,s$ in $T$}@}. As {@{the scalar-valued kernel}@} encodes {@{dependencies between the inputs}@}, we can observe that {@{the matrix-valued kernel}@} encodes {@{dependencies among both the inputs and the outputs}@}. <!--SR:!2026-04-12,65,329!2026-04-30,76,329!2026-04-22,66,310!2026-04-20,67,310!2026-03-31,44,290!2026-04-04,57,310!2026-05-03,76,329!2026-04-23,67,310!2026-04-01,55,310!2026-03-28,44,290-->

We lastly remark that {@{the above theory can be further extended}@} to spaces of {@{functions with values in function spaces}@} but obtaining {@{kernels for these spaces is a more difficult task}@}.<sup>[\[14\]](#^ref-14)</sup> <!--SR:!2026-04-01,55,310!2026-04-13,61,310!2026-04-30,76,329-->

## connection between RKHSs and the ReLU function

{@{The [ReLU function](rectifier%20(neural%20networks).md)}@} is commonly defined as {@{$f(x)=\max\{0,x\}$}@} and is a mainstay in {@{the architecture of neural networks where it is used as an activation function}@}. One can construct {@{a ReLU-like nonlinear function}@} using {@{the theory of reproducing kernel Hilbert spaces}@}. Below, we derive {@{this construction}@} and show how it implies {@{the representation power of neural networks with ReLU activations}@}. <!--SR:!2026-04-20,67,310!2026-03-31,54,310!2026-04-14,67,329!2026-04-01,55,310!2026-04-23,67,310!2026-04-22,66,310!2026-04-23,67,310-->

We will work with {@{the Hilbert space ${\mathcal {H} }=L_{2}^{1}(0)[0,\infty )$ (annotation: weird notation...)}@} of {@{absolutely continuous functions with $f(0)=0$ and square integrable \(i.e. $L_{2}$\) derivative}@}. It has {@{the inner product $$\langle f,g\rangle _{\mathcal {H} }=\int _{0}^{\infty }f'(x)g'(x)\,dx.$$}@} To construct {@{the reproducing kernel}@} it suffices to consider {@{a dense subspace}@}, so let {@{$f\in C^{1}[0,\infty )$ and $f(0)=0$}@}. {@{The Fundamental Theorem of Calculus}@} then gives {@{$$f(y)=\int _{0}^{y}f'(x)\,dx=\int _{0}^{\infty }G(x,y)f'(x)\,dx=\langle K_{y},f\rangle$$}@} where {@{$$G(x,y)={\begin{cases}1,&x<y\\0,&{\text{otherwise} }\end{cases} }$$}@} and {@{$K_{y}'(x)=G(x,y),\ K_{y}(0)=0$}@} i.e. {@{$$K(x,y)=K_{y}(x)=\int _{0}^{x}G(z,y)\,dz={\begin{cases}x,&0\leq x<y\\y,&{\text{otherwise.} }\end{cases} }=\min(x,y)$$}@} This implies {@{$K_{y}=K(\cdot ,y)$ reproduces $f$}@}. <!--SR:!2026-03-19,44,290!2026-04-22,66,310!2026-05-02,75,329!2026-04-22,66,310!2026-04-07,60,310!2026-04-20,67,310!2026-04-23,67,310!2026-05-31,90,290!2026-04-23,67,310!2026-04-30,76,329!2026-03-15,37,290!2026-04-23,67,310-->

Moreover {@{the minimum function on $X\times X=[0,\infty )\times [0,\infty )$}@} has {@{the following representations with the ReLu function}@}: {@{$$\min(x,y)=x-\operatorname {ReLU} (x-y)=y-\operatorname {ReLU} (y-x).$$}@} Using {@{this formulation}@}, we can apply {@{the [representer theorem](representer%20theorem.md) to the RKHS}@}, letting one prove {@{the optimality of using ReLU activations in neural network settings}@}.<sup>\[_[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation%20needed)_\]</sup> <!--SR:!2026-04-22,66,310!2026-04-23,67,310!2026-03-27,44,290!2026-04-23,67,310!2026-04-23,67,310!2026-04-14,66,310-->

## see also

- [Positive definite kernel](positive%20definite%20kernel.md)
- [Mercer's theorem](Mercer's%20theorem.md)
- [Kernel trick](kernel%20trick.md#mathematics%20the%20kernel%20trick)
- [Kernel embedding of distributions](kernel%20embedding%20of%20distributions.md)
- [Representer theorem](representer%20theorem.md)

## notes

1. Alpay, D., and T. M. Mills. "A family of Hilbert spaces which are not reproducing kernel Hilbert spaces." J. Anal. Appl. 1.2 \(2003\): 107–111. <a id="^ref-1"></a>^ref-1
2. Z. Pasternak-Winiarski, "On weights which admit reproducing kernel of Bergman type", _International Journal of Mathematics and Mathematical Sciences_, vol. 15, Issue 1, 1992. <a id="^ref-2"></a>^ref-2
3. T. Ł. Żynda, "On weights which admit reproducing kernel of Szegő type", _Journal of Contemporary Mathematical Analysis_ \(Armenian Academy of Sciences\), 55, 2020. <a id="^ref-3"></a>^ref-3
4. Okutmustur <a id="^ref-4"></a>^ref-4
5. Paulson <a id="^ref-5"></a>^ref-5
6. Durrett <a id="^ref-6"></a>^ref-6
7. Rosasco <a id="^ref-7"></a>^ref-7
8. Rosasco <a id="^ref-8"></a>^ref-8
9. Berlinet, Alain and Thomas, Christine. _[Reproducing kernel Hilbert spaces in Probability and Statistics](https://books.google.com/books?id=bX3TBwAAQBAJ&dq=%22Reproducing+kernel+Hilbert+spaces+in+Probability+and+Statistics%22&pg=PP11)_, Kluwer Academic Publishers, 2004 <a id="^ref-9"></a>^ref-9
10. Thomas-Agnan C . Computing a family of reproducing kernels for statistical applications. Numerical Algorithms, 13, pp. 21-32 \(1996\) <a id="^ref-10"></a>^ref-10
11. De Vito <a id="^ref-11"></a>^ref-11
12. Zhang <a id="^ref-12"></a>^ref-12
13. Alvarez <a id="^ref-13"></a>^ref-13
14. Rosasco <a id="^ref-14"></a>^ref-14

## references

This text incorporates [content](https://en.wikipedia.org/wiki/reproducing_kernel_Hilbert_space) from [Wikipedia](Wikipedia.md) available under the [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) license.

- Alvarez, Mauricio, Rosasco, Lorenzo and Lawrence, Neil, “Kernels for Vector-Valued Functions: a Review,” [https://arxiv.org/abs/1106.6251](https://arxiv.org/abs/1106.6251), June 2011.
- <a id="CITEREFAronszajn1950"></a> [Aronszajn, Nachman](Nachman%20Aronszajn.md) \(1950\). ["Theory of Reproducing Kernels"](https://doi.org/10.1090%2FS0002-9947-1950-0051437-7). _[Transactions of the American Mathematical Society](Transactions%20of%20the%20American%20Mathematical%20Society.md)_. __68__ \(3\): 337–404. [Bibcode](bibcode%20(identifier).md):[1950TAMS...68..337A](https://ui.adsabs.harvard.edu/abs/1950TAMS...68..337A). [doi](doi%20(identifier).md):[10.1090/S0002-9947-1950-0051437-7](https://doi.org/10.1090%2FS0002-9947-1950-0051437-7). [JSTOR](JSTOR%20(identifier).md#content) [1990404](https://www.jstor.org/stable/1990404). [MR](MR%20(identifier).md) [0051437](https://mathscinet.ams.org/mathscinet-getitem?mr=0051437).
- Berlinet, Alain and Thomas, Christine. _Reproducing kernel Hilbert spaces in Probability and Statistics_, Kluwer Academic Publishers, 2004.
- <a id="CITEREFCuckerSmale2002"></a> Cucker, Felipe; Smale, Steve \(2002\). ["On the Mathematical Foundations of Learning"](https://doi.org/10.1090%2FS0273-0979-01-00923-5). _[Bulletin of the American Mathematical Society](Bulletin%20of%20the%20American%20Mathematical%20Society.md)_. __39__ \(1\): 1–49. [doi](doi%20(identifier).md):[10.1090/S0273-0979-01-00923-5](https://doi.org/10.1090%2FS0273-0979-01-00923-5). [MR](MR%20(identifier).md) [1864085](https://mathscinet.ams.org/mathscinet-getitem?mr=1864085).
- De Vito, Ernest, Umanita, Veronica, and Villa, Silvia. "An extension of Mercer theorem to vector-valued measurable kernels," [arXiv](ArXiv%20(identifier).md):[1110.4017](https://arxiv.org/abs/1110.4017), June 2013.
- Durrett, Greg. 9.520 Course Notes, Massachusetts Institute of Technology, [https://www.mit.edu/~9.520/scribe-notes/class03\_gdurett.pdf](https://www.mit.edu/~9.520/scribe-notes/class03_gdurett.pdf), February 2010.
- <a id="CITEREFKimeldorfWahba1971"></a> Kimeldorf, George; [Wahba, Grace](Grace%20Wahba.md) \(1971\). ["Some results on Tchebycheffian Spline Functions"](http://www.stat.wisc.edu/~wahba/ftp1/oldie/kw71.pdf) \(PDF\). _Journal of Mathematical Analysis and Applications_. __33__ \(1\): 82–95. [Bibcode](bibcode%20(identifier).md):[1971JMAA...33...82K](https://ui.adsabs.harvard.edu/abs/1971JMAA...33...82K). [doi](doi%20(identifier).md):[10.1016/0022-247X\(71\)90184-3](https://doi.org/10.1016%2F0022-247X%2871%2990184-3). [MR](MR%20(identifier).md) [0290013](https://mathscinet.ams.org/mathscinet-getitem?mr=0290013).
- Okutmustur, Baver. “Reproducing Kernel Hilbert Spaces,” M.S. dissertation, Bilkent University, [https://users.metu.edu.tr/baver/MS.Thesis.pdf](https://users.metu.edu.tr/baver/MS.Thesis.pdf), August 2005.
- Paulsen, Vern. “An introduction to the theory of reproducing kernel Hilbert spaces,” [https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=440218056738e05b5ab43679f932a9f33fccee87](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=440218056738e05b5ab43679f932a9f33fccee87).
- <a id="CITEREFSteinwartScovel2012"></a> Steinwart, Ingo; Scovel, Clint \(2012\). "Mercer's theorem on general domains: On the interaction between measures, kernels, and RKHSs". _Constr. Approx_. __35__ \(3\): 363–417. [doi](doi%20(identifier).md):[10.1007/s00365-012-9153-3](https://doi.org/10.1007%2Fs00365-012-9153-3). [MR](MR%20(identifier).md) [2914365](https://mathscinet.ams.org/mathscinet-getitem?mr=2914365). [S2CID](S2CID%20(identifier).md#S2CID) [253885172](https://api.semanticscholar.org/CorpusID:253885172).
- Rosasco, Lorenzo and Poggio, Thomas. "A Regularization Tour of Machine Learning – MIT 9.520 Lecture Notes" Manuscript, Dec. 2014.
- [Wahba, Grace](Grace%20Wahba.md), _Spline Models for Observational Data_, [SIAM](http://www.siam.org/books/), 1990.
- <a id="CITEREFZhangXuZhang2012"></a> Zhang, Haizhang; Xu, Yuesheng; Zhang, Qinghui \(2012\). ["Refinement of Operator-valued Reproducing Kernels"](http://www.jmlr.org/papers/volume13/zhang12a/zhang12a.pdf) \(PDF\). _Journal of Machine Learning Research_. __13__: 91–136.

> [Category](https://en.wikipedia.org/wiki/Help:Category):
>
> - [Hilbert spaces](https://en.wikipedia.org/wiki/Category:Hilbert%20spaces)
