---
aliases:
  - variance
  - variances
tags:
  - flashcard/active/general/eng/variance
  - language/in/English
---

# variance

In [probability theory](probability%20theory.md) and [statistics](statistics.md), __variance__ is {@{the [expected value](expected%20value.md) of the [squared deviation from the mean](squared%20deviations%20from%20the%20mean.md) of a [random variable](random%20variable.md)}@}. {@{The [standard deviation](standard%20deviation.md) (SD)}@} is {@{obtained as the square root of the variance}@}. Variance is {@{a measure of [dispersion](statistical%20dispersion.md), meaning it is a measure of how far a set of numbers is spread out from their average value}@}. It is {@{the second [central moment](central%20moment.md) of a [distribution](probability%20distribution.md)}@}, and {@{the [covariance](covariance.md) of the random variable with itself}@}, and it is often represented by {@{$\sigma ^{2}$, $s^{2}$, $\operatorname {Var} (X)$, $V(X)$, or $\mathbb {V} (X)$}@}. <!--SR:!2026-09-09,546,318!2025-08-30,281,338!2025-08-03,261,338!2025-05-21,194,318!2025-04-05,161,318!2025-05-29,193,318!2025-09-26,300,338-->

## population variance and sample variance

- see: [unbiased estimation of standard deviation](unbiased%20estimation%20of%20standard%20deviation.md)

Real-world observations such as the measurements of yesterday's rain throughout the day typically {@{cannot be complete sets of all possible observations that could be made}@}. As such, the variance calculated from the finite set will {@{in general not match the variance that would have been calculated from the full population of possible observations}@}. This means that one {@{[estimates](estimation%20theory.md) the mean and variance from a limited set of observations by using an [estimator](estimator.md) equation}@}. The estimator is {@{a function of the [sample](sampling%20(statistics).md) of _n_ [observations](observation.md) drawn without observational bias from the whole [population](statistical%20population.md) of potential observations}@}. In this example, the sample would be {@{the set of actual measurements of yesterday's rainfall from available rain gauges within the geography of interest}@}. <!--SR:!2026-07-02,503,318!2025-07-22,251,338!2025-07-23,252,338!2025-07-16,201,278!2027-01-05,655,338-->

The simplest estimators for population mean and population variance are {@{simply the mean and variance of the sample, the __sample mean__ and __(uncorrected) sample variance__}@} – these are {@{[consistent estimators](consistent%20estimator.md) (they converge to the value of the whole population as the number of samples increases) but can be improved}@}. Most simply, the sample variance is {@{computed as the sum of [squared deviations](squared%20deviations%20from%20the%20mean.md) about the (sample) mean, divided by _n_ as the number of samples}@}. However, {@{using values other than _n_}@} improves the estimator in various ways. Four common values for the denominator are {@{_n_, _n_ − 1, _n_ + 1, and _n_ − 1.5}@}: _n_ is {@{the simplest (the variance of the sample)}@}, _n_ − 1 {@{eliminates bias}@}, _n_ + 1 {@{minimizes [mean squared error](mean%20squared%20error.md) for the normal distribution}@}, and _n_ − 1.5 {@{mostly eliminates bias in [unbiased estimation of standard deviation](unbiased%20estimation%20of%20standard%20deviation.md) (_standard deviation_ instead of _variance_) for the normal distribution}@}. <!--SR:!2025-05-23,198,318!2025-08-11,263,330!2025-09-22,298,338!2026-06-19,490,310!2025-03-23,140,298!2025-05-03,182,310!2025-10-16,298,298!2025-09-27,280,298!2025-05-19,182,315-->

Firstly, if {@{the true population mean is unknown}@}, then {@{the sample variance (which uses the sample mean in place of the true mean) is a [biased estimator](bias%20of%20an%20estimator.md)}@}: it {@{underestimates the variance by a factor of (_n_ − 1) / _n_}@}; correcting this factor, resulting in {@{the sum of squared deviations about the sample mean divided by _n_ - 1 instead of _n_, is called _[Bessel's correction](Bessel's%20correction.md)_}@}. The resulting estimator is {@{unbiased and is called the __(corrected) sample variance__ or __unbiased sample variance__}@}. If {@{the mean is determined in some other way than from the same samples used to estimate the variance}@}, then {@{this bias does not arise, and the variance can safely be estimated as that of the samples about the (independently known) mean}@}. <!--SR:!2025-08-27,277,338!2025-04-29,182,318!2025-08-21,275,338!2025-08-25,278,338!2025-05-15,189,310!2025-08-22,275,338!2025-05-29,206,330-->

Secondly, the sample variance {@{does not generally minimize [mean squared error](mean%20squared%20error.md) between sample variance and population variance}@}. Correcting for bias {@{often makes this worse: one can always choose a scale factor that performs better than the corrected sample variance}@}, though the optimal scale factor {@{depends on the [excess kurtosis](kurtosis.md#excess%20kurtosis) of the population (see [mean squared error: variance](mean%20squared%20error.md#variance)) and introduces bias}@}. This always consists of {@{scaling down the unbiased estimator (dividing by a number larger than _n_ − 1)}@} and is a simple example of {@{a [shrinkage estimator](shrinkage%20(statistics).md): one "shrinks" the unbiased estimator towards zero}@}. For the normal distribution, {@{dividing by _n_ + 1 (instead of _n_ − 1 or _n_) minimizes mean squared error}@}. The resulting estimator is {@{biased, however}@}, and is known as {@{the __biased sample variation__ (_variation_ instead of _variance_)}@}. <!--SR:!2025-05-09,188,318!2025-10-28,272,278!2026-02-10,353,298!2025-06-17,216,318!2025-08-02,240,318!2025-09-11,268,298!2025-10-06,309,338!2025-09-12,293,338-->

### population variance

In general, {@{the ___population variance___ of a _finite_ [population](statistical%20population.md) of size _N_ with values _x_<sub>_i_</sub>}@} is given by {@{$${\begin{aligned}\sigma ^{2}&={\frac {1}{N} }\sum _{i=1}^{N}\left(x_{i}-\mu \right)^{2}={\frac {1}{N} }\sum _{i=1}^{N}\left(x_{i}^{2}-2\mu x_{i}+\mu ^{2}\right)\\[5pt]&=\left({\frac {1}{N} }\sum _{i=1}^{N}x_{i}^{2}\right)-2\mu \left({\frac {1}{N} }\sum _{i=1}^{N}x_{i}\right)+\mu ^{2}\\[5pt]&=\operatorname {E} [x_{i}^{2}]-\mu ^{2}\end{aligned} }$$}@} where the population mean is {@{$\mu =\operatorname {E} [x_{i}]={\frac {1}{N} }\sum _{i=1}^{N}x_{i}$}@} and {@{$\operatorname {E} [x_{i}^{2}]=\left({\frac {1}{N} }\sum _{i=1}^{N}x_{i}^{2}\right)$}@}, where $\operatorname {E}$ is {@{the [expectation value](expected%20value.md) operator}@}. <!--SR:!2025-08-18,273,338!2025-09-12,267,290!2025-10-15,316,330!2025-09-02,282,338!2025-08-17,272,338-->

The population variance can also be computed using {@{$$\sigma ^{2}={\frac {1}{N^{2} } }\sum _{i<j}\left(x_{i}-x_{j}\right)^{2}={\frac {1}{2N^{2} } }\sum _{i,j=1}^{N}\left(x_{i}-x_{j}\right)^{2}$$}@}. (The right side has {@{duplicate terms and zero terms in the sum while the middle side has only unique nonzero terms to sum}@}.) This is true because {@{$${\begin{aligned}&{\frac {1}{2N^{2} } }\sum _{i,j=1}^{N}\left(x_{i}-x_{j}\right)^{2}\\[5pt]={}&{\frac {1}{2N^{2} } }\sum _{i,j=1}^{N}\left(x_{i}^{2}-2x_{i}x_{j}+x_{j}^{2}\right)\\[5pt]={}&{\frac {1}{2N} }\sum _{j=1}^{N}\left({\frac {1}{N} }\sum _{i=1}^{N}x_{i}^{2}\right)-\left({\frac {1}{N} }\sum _{i=1}^{N}x_{i}\right)\left({\frac {1}{N} }\sum _{j=1}^{N}x_{j}\right)+{\frac {1}{2N} }\sum _{i=1}^{N}\left({\frac {1}{N} }\sum _{j=1}^{N}x_{j}^{2}\right)\\[5pt]={}&{\frac {1}{2} }\left(\sigma ^{2}+\mu ^{2}\right)-\mu ^{2}+{\frac {1}{2} }\left(\sigma ^{2}+\mu ^{2}\right)\\[5pt]={}&\sigma ^{2}.\end{aligned} }$$}@}. <!--SR:!2025-05-13,194,318!2025-08-19,273,338!2025-10-28,220,210-->

The population variance matches {@{the variance of the generating probability distribution}@}. In this sense, the concept of population can be {@{extended to continuous random variables with infinite populations}@}. <!--SR:!2025-05-19,197,318!2025-06-01,203,318-->

> [!tip] tips
>
> - strategy for proving equivalence between multiple definitions of population variance ::@:: Expand the squared binomial under the summation sign. Then decompose it into multiple summation signs. Finally replace summation signs with population mean $\mu$ or population standard deviation $\sigma$. <!--SR:!2025-05-03,179,310!2025-04-14,153,298-->

### sample variance

- see: [sample standard deviation](standard%20deviation.md#sample%20standard%20deviation)_

#### biased sample variance

In many practical situations, the true variance of a population is {@{not known _a priori_ and must be computed somehow}@}. When {@{dealing with extremely large populations}@}, it is {@{not possible to count every object in the population}@}, so {@{the computation must be performed on a [sample](sampling%20(statistics).md) of the population}@}. This is generally referred to as {@{__sample variance__ or __empirical variance__}@}. Sample variance can also be applied to {@{the estimation of the variance of a continuous distribution from a sample of that distribution}@}. <!--SR:!2025-06-29,226,318!2025-09-15,293,338!2026-02-08,394,318!2025-09-11,292,338!2025-06-25,229,338!2025-04-02,146,298-->

We take {@{a [sample with replacement](sampling%20(statistics).md) of _n_ values _Y_<sub>1</sub>, ..., _Y_<sub>_n_</sub> from the population of size $N$, where _n_ < _N_}@}, and {@{estimate the variance on the basis of this sample}@}. Directly taking the variance of the sample data gives {@{the average of the [squared deviations](squared%20deviations%20from%20the%20mean.md)}@}: {@{$${\tilde {S} }_{Y}^{2}={\frac {1}{n} }\sum _{i=1}^{n}\left(Y_{i}-{\overline {Y} }\right)^{2}=\left({\frac {1}{n} }\sum _{i=1}^{n}Y_{i}^{2}\right)-{\overline {Y} }^{2}={\frac {1}{n^{2} } }\sum _{i,j\,:\,i<j}\left(Y_{i}-Y_{j}\right)^{2}$$}@}. (See the section [population variance](#population%20variance) for the derivation of this formula.) Here, ${\overline {Y} }$ denotes {@{the [sample mean](sample%20mean%20and%20covariance.md): $${\overline {Y} }={\frac {1}{n} }\sum _{i=1}^{n}Y_{i}$$}@}. <!--SR:!2025-07-20,249,338!2025-06-22,210,318!2025-07-26,214,278!2025-04-12,156,298!2025-05-22,202,318-->

Since {@{the _Y_<sub>_i_</sub> are selected randomly}@}, both ${\overline {Y} }$ and ${\tilde {S} }_{Y}^{2}$ are {@{[random variables](random%20variable.md)}@}. Their expected values can be evaluated by {@{averaging over the ensemble of all possible samples {_Y_<sub>_i_</sub>} of size _n_ from the population}@}. For ${\tilde {S} }_{Y}^{2}$ this gives: $${\begin{aligned}\operatorname {E} [{\tilde {S} }_{Y}^{2}]&=\operatorname {E} \left[{\frac {1}{n} }\sum _{i=1}^{n}\left(Y_{i}-{\frac {1}{n} }\sum _{j=1}^{n}Y_{j}\right)^{2}\right]\\[5pt]&={\frac {1}{n} }\sum _{i=1}^{n}\operatorname {E} \left[Y_{i}^{2}-{\frac {2}{n} }Y_{i}\sum _{j=1}^{n}Y_{j}+{\frac {1}{n^{2} } }\sum _{j=1}^{n}Y_{j}\sum _{k=1}^{n}Y_{k}\right]\\[5pt]&={\frac {1}{n} }\sum _{i=1}^{n}\left(\operatorname {E} \left[Y_{i}^{2}\right]-{\frac {2}{n} }\left(\sum _{j\neq i}\operatorname {E} \left[Y_{i}Y_{j}\right]+\operatorname {E} \left[Y_{i}^{2}\right]\right)+{\frac {1}{n^{2} } }\sum _{j=1}^{n}\sum _{k\neq j}^{n}\operatorname {E} \left[Y_{j}Y_{k}\right]+{\frac {1}{n^{2} } }\sum _{j=1}^{n}\operatorname {E} \left[Y_{j}^{2}\right]\right)\\[5pt]&={\frac {1}{n} }\sum _{i=1}^{n}\left({\frac {n-2}{n} }\operatorname {E} \left[Y_{i}^{2}\right]-{\frac {2}{n} }\sum _{j\neq i}\operatorname {E} \left[Y_{i}Y_{j}\right]+{\frac {1}{n^{2} } }\sum _{j=1}^{n}\sum _{k\neq j}^{n}\operatorname {E} \left[Y_{j}Y_{k}\right]+{\frac {1}{n^{2} } }\sum _{j=1}^{n}\operatorname {E} \left[Y_{j}^{2}\right]\right)\\[5pt]&={\frac {1}{n} }\sum _{i=1}^{n}\left[{\frac {n-2}{n} }\left(\sigma ^{2}+\mu ^{2}\right)-{\frac {2}{n} }(n-1)\mu ^{2}+{\frac {1}{n^{2} } }n(n-1)\mu ^{2}+{\frac {1}{n} }\left(\sigma ^{2}+\mu ^{2}\right)\right]\\[5pt]&={\frac {n-1}{n} }\sigma ^{2}.\end{aligned} }$$ Here {@{$\sigma ^{2}=\operatorname {E} [Y_{i}^{2}]-\mu ^{2}$}@} derived in the section [population variance](#population%20variance) and {@{$\operatorname {E} [Y_{i}Y_{j}]=\operatorname {E} [Y_{i}]\operatorname {E} [Y_{j}]=\mu ^{2}$ due to independency of $Y_{i}$ and $Y_{j}$}@} are used. <!--SR:!2025-08-24,278,338!2025-05-25,198,318!2025-05-20,187,318!2025-07-28,256,338!2025-07-16,246,338-->

Hence ${\tilde {S} }_{Y}^{2}$ gives {@{an estimate of the population variance that is biased by a factor of ${\frac {n-1}{n} }$}@} as {@{the expectation value of ${\tilde {S} }_{Y}^{2}$ is smaller than the population variance (true variance) by that factor}@}. For this reason, {@{${\tilde {S} }_{Y}^{2}$ is referred to as the _biased sample variance_}@}. <!--SR:!2025-10-22,322,338!2025-05-21,196,318!2026-07-09,508,318-->

> [!tip] tips
>
> - strategy for deriving Bessel's correction ::@:: Assume we are sampling $n$ individuals $Y_1, \ldots Y_n$ with replacement from a population of more than $n$ individuals. Since we are sampling with replacement, each sampled individual $Y_i$ is a random variable. Its expected value is $$\operatorname{E}[Y_i] = \mu$$. Further, remember the well-known identity $$\operatorname{E}\left[Y_i^2\right] = \mu^2 + \sigma^2$$. As for others, note that the expected value operator is linear, i.e. $$\operatorname{E}[aA + bB] = a \operatorname{E}[A] + b \operatorname{E}[B]$$ for two random variables $A$ and $B$ and two constants $a$ and $b$. Finally, to make it easier to simplify, extract $\operatorname{E}\left[Y_i^2\right]$ as much as possible before replacing expressions with $\mu$ and $\sigma$. <!--SR:!2025-07-26,219,342!2025-10-23,308,362-->

#### unbiased sample variance

Correcting for this bias yields {@{the _unbiased sample variance_, denoted $S^{2}$}@}: {@{$$S^{2}={\frac {n}{n-1} }{\tilde {S} }_{Y}^{2}={\frac {n}{n-1} }\left[{\frac {1}{n} }\sum _{i=1}^{n}\left(Y_{i}-{\overline {Y} }\right)^{2}\right]={\frac {1}{n-1} }\sum _{i=1}^{n}\left(Y_{i}-{\overline {Y} }\right)^{2}$$}@}. Either estimator may be {@{simply referred to as the _sample variance_ when the version can be determined by context}@}. The same proof is {@{also applicable for samples taken from a continuous probability distribution}@}. <!--SR:!2025-06-25,228,330!2026-08-19,530,318!2026-06-01,466,310!2025-04-13,152,298-->

The use of {@{the term _n_ − 1}@} is called {@{[Bessel's correction](Bessel's%20correction.md)}@}, and it is {@{also used in [sample covariance](sample%20mean%20and%20covariance.md) and the [sample standard deviation](standard%20deviation.md#sample%20standard%20deviation) (the square root of variance)}@}. The square root is {@{a [concave function](concave%20function.md) and thus introduces negative bias (by [Jensen's inequality](Jensen's%20inequality.md))}@}, (annotation: Applying Jensen's inequality: {@{$\sqrt{\operatorname E\left[S_{n - 1}^2\right]} = \sqrt{\sigma^2} = \sigma \ge \operatorname E\left[\sqrt{S_{n - 1}^2} \right]$, so it is an underestimate}@}.) which {@{depends on the distribution, and thus the corrected sample standard deviation (using Bessel's correction) is biased}@}. The [unbiased estimation of standard deviation](unbiased%20estimation%20of%20standard%20deviation.md) is {@{a technically involved problem, though for the normal distribution using the term _n_ − 1.5 yields an almost unbiased estimator}@}. <!--SR:!2025-07-29,257,338!2025-06-29,232,338!2025-04-05,146,298!2025-05-11,135,258!2026-03-16,382,290!2025-05-22,188,318!2025-08-08,158,337-->

The unbiased sample variance is {@{a [U-statistic](u-statistic.md) for the function _ƒ_(_y_<sub>1</sub>, _y_<sub>2</sub>) = (_y_<sub>1</sub> − _y_<sub>2</sub>)<sup>2</sup>/2}@}, meaning that {@{it is obtained by averaging a 2-sample statistic (i.e. the above function) over all possible 2-element subsets of the population}@}. <!--SR:!2025-10-02,305,330!2025-07-21,250,338-->

## references

This text incorporates [content](https://en.wikipedia.org/wiki/variance) from [Wikipedia](Wikipedia.md) available under the [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) license.
