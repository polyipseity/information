---
aliases:
  - outer product
  - outer products
tags:
  - flashcard/active/general/eng/outer_product
  - language/in/English
---

# outer product

## definition

Given {@{two [column vectors](row%20and%20column%20vectors.md) $\mathbf u$ and $\mathbf v$ of size $m \times 1$ and $n \times 1$ respectively $$\mathbf u = \begin{bmatrix} u_1 \\ \vdots \\ u_m \end{bmatrix}, \quad \mathbf v = \begin{bmatrix} v_1 \\ \vdots \\ v_m \end{bmatrix}$$}@}, their outer product, denoted {@{$\mathbf u \otimes \mathbf v$}@}, is defined as {@{the $m \times n$ matrix $\mathbf A$ obtained by multiplying each element of $\mathbf u$ by the [complex conjugate](complex%20conjugate.md) of each element of $\mathbf v$}@}: {@{$$\mathbf u \otimes \mathbf v = \mathbf A = \begin{aligned} & \quad \begin{matrix} \phantom{u_0} v_1^* & \phantom{u_0} v_2^* & \cdots & \phantom{u_0} v_n^* \end{matrix} \\ \begin{matrix} u_1 \\ u_2 \\ \vdots \\ u_m \end{matrix} & \begin{bmatrix} u_1 v_1^* & u_1 v_2^* & \cdots & u_1 v_n^* \\ u_2 v_1^* & u_2 v_2^* & \cdots & u_2 v_n^* \\ \vdots & \vdots & \ddots & \vdots \\ u_m v_1^* & u_m v_2^* & \cdots & u_m v_n^* \end{bmatrix} \end{aligned}$$ (the variables surrounding the matrix are for visualization only)}@}. Or, in index notation: {@{$$(\mathbf u \otimes \mathbf v)_{ij} = \mathbf A_{ij} = u_i v_j^*$$}@}. The above can also be written as {@{[matrix multiplication](matrix%20multiplication.md): $$\mathbf u \otimes \mathbf v = \mathbf A = \mathbf u \mathbf v^{\mathrm H} = \mathbf u \left(\mathbf v^\intercal \right)^*$$, where $\mathbf v^{\mathrm H} = \left(\mathbf v^\intercal \right)^*$ is the [conjugate transpose](conjugate%20transpose.md) of $\mathbf v$}@}. <!--SR:!2027-10-31,933,350!2028-10-16,1204,350!2027-09-22,875,330!2028-10-28,1214,350!2028-06-20,1030,330!2028-03-16,1037,350-->

If {@{the vectors $\mathbf u$ and $\mathbf v$ have the same dimension and the dimension is bigger than $1 \times 1$}@}, then {@{$\det(\mathbf u \otimes \mathbf v) = 0$}@}. <!--SR:!2028-07-17,960,290!2027-04-19,759,330-->

### contrast with Euclidean inner product

If {@{$m = n$}@}, then one can {@{take the [matrix multiplication](matrix%20multiplication.md) the other way, yielding a [scalar](scalar%20(mathematics).md) (or $1 \times 1$ [matrix](matrix%20(mathematics).md))}@}: {@{$$\mathbf u \otimes \mathbf v = \mathbf u \mathbf v^{\mathrm H} \xrightarrow{\text{swap} } \mathbf v^{\mathrm H} \mathbf u = \mathbf v \cdot \mathbf u = \langle \mathbf v, \mathbf u \rangle$$}@}, which is {@{the standard [inner product](inner%20product%20space.md) for complex vectors, better known as the [complex dot product](dot%20product.md#complex%20vectors)}@}. (Note that the inner product here is {@{[antilinear](antilinear%20map.md) in the first argument, which is the convention in [physics](physics.md) and other fields but not [mathematics](mathematics.md); in most mathematical context, the inner product is [linear](linear%20map.md) in the first argument instead}@}. The convention used here will be used thereafter.) <!--SR:!2028-06-09,1106,350!2028-02-01,1005,350!2027-08-22,862,330!2028-07-08,1129,350!2026-06-18,535,330-->

Obviously, {@{the dot product}@} equals {@{the [trace](trace%20(linear%20algebra).md) of the outer product with the order swapped}@}: {@{$\operatorname{tr}(\mathbf u \otimes \mathbf v) = \sum_{i = 1}^n u_i v_i^* = \mathbf v \cdot \mathbf u$}@}. However, unlike the dot product, the outer product is {@{not [commutative](commutative%20property.md)}@}: {@{$\mathbf u \otimes \mathbf v \ne \mathbf v \otimes \mathbf u$ in general}@}. <!--SR:!2028-10-13,1075,310!2027-12-24,974,350!2026-02-19,91,376!2026-02-19,91,376!2026-02-18,90,376-->

Using above, we can rewrite {@{the left or right multiplication of a vector by an outer product}@}. Denoting the [dot product](dot%20product.md) {@{(that is [antilinear](antilinear%20map.md) first argument) by $\cdot$}@}, if given {@{an $n \times 1$ vector $\mathbf w$, then $$(\mathbf u \otimes \mathbf v) \mathbf w = \mathbf u \mathbf v^{\mathrm H} \mathbf w = \mathbf u (\mathbf v \cdot \mathbf w)$$}@}. If instead given {@{an $1 \times m$ vector $\mathbf x$, then $$\mathbf x (\mathbf u \otimes \mathbf v) = \mathbf x \mathbf u \mathbf v^{\mathrm H} = \left(\mathbf x^{\mathrm H}\right)^{\mathrm H} \mathbf u \mathbf v^{\mathrm H} = \left( \mathbf x^{\mathrm H} \cdot \mathbf u \right) \mathbf v^{\mathrm H}$$}@}, where $\phantom{}^{\mathrm H}$ is the [conjugate transpose](conjugate%20transpose.md). <!--SR:!2027-03-01,717,330!2027-02-05,702,330!2029-03-21,1241,310!2026-11-07,535,270-->

### connection with the matrix product

Given {@{two [matrices](matrix%20(mathematics).md) $\mathbf A$ of size $m \times p$ and $\mathbf B$ of size $p \times n$}@}, consider {@{the [matrix product](matrix%20multiplication.md) $\mathbf C = \mathbf A \mathbf B$ defined as usual as a matrix of size $m \times n$}@}. Recall that matrix multiplication, using index notation, is: {@{$$\mathbf C_{ij} = \sum_{k = 1}^p \mathbf A_{ik} \mathbf B_{kj}$$}@}. <!--SR:!2027-06-07,793,330!2028-11-03,1219,350!2029-07-08,1415,350-->

We can {@{duplicate the above equation in index equation $m \times n$ times}@} to get the entire matrix. Then, move {@{the summation sign outside to get: $$\mathbf C = \sum_{k = 1}^p \begin{bmatrix} \mathbf A_{1k} \mathbf B_{k1} & \mathbf A_{1k} \mathbf B_{k2} & \cdots & \mathbf A_{1k} \mathbf B_{kn} \\ \mathbf A_{2k} \mathbf B_{k1} & \mathbf A_{2k} \mathbf B_{k2} & \cdots & \mathbf A_2{k} \mathbf B_{kn} \\ \vdots & \vdots & \ddots & \vdots \\ \mathbf A_{mk} \mathbf B_{k1} & \mathbf A_{mk} \mathbf B_{k2} & \cdots & \mathbf A_{mk} \mathbf B_{kn} \end{bmatrix} = \sum_{k = 1}^p \begin{bmatrix} \mathbf A_{1k} \\ \mathbf A_{2k} \\ \vdots \\ \mathbf A_{mk} \end{bmatrix} \begin{bmatrix} \mathbf B_{k1} & \mathbf B_{k2} & \cdots & \mathbf B_{kn} \end{bmatrix} = \sum_{k = 1}^p \mathbf A^{\text{col} }_k \mathbf B^{\text{row} }_k = \sum_{k = 1}^p \mathbf A^{\text{col} }_k \otimes \left(\mathbf B^{\text{row} }_k\right)^{\mathrm H}$$}@}, showing that the matrix product $\mathbf C$ can be written as {@{a sum of column-by-row outer products}@}. This expression has {@{duality with the more common expression that expresses each entry of $\mathbf C$ by row-by-column [inner products](inner%20product%20space.md) ([antilinear](antilinear%20map.md) first argument) or [dot products](dot%20product.md)}@}: {@{$$\mathbf C_{ij} = \sum_{i = 1}^p \mathbf A_{ip} \mathbf B_{pj} = \begin{bmatrix} \mathbf A_{i1} & \mathbf A_{i2} & \cdots & \mathbf A_{in} \end{bmatrix} \begin{bmatrix} \mathbf B_{1j} \\ \mathbf B_{2j} \\ \vdots \\ \mathbf B_{mj} \end{bmatrix} = \mathbf A^{\text{row} }_i \mathbf B^{\text{col} }_j = \left(\mathbf A^{\text{row} }_i\right)^{\mathrm H} \cdot \mathbf B^{\text{col} }_j$$}@}. Compare the expressions. <!--SR:!2028-06-20,1116,350!2026-12-01,547,270!2028-06-29,1121,350!2028-09-23,1190,350!2026-04-22,427,290-->

This relation is relevant {@{in the application of [singular value decomposition](singular%20value%20decomposition.md) (SVD) (and [eigendecomposition](eigendecomposition%20of%20a%20matrix.md) as a special case)}@}. In particular, the decomposition can be interpreted as {@{the sum of outer products of each left $\mathbf u_k$ and right $\mathbf v_k$ singular vectors (compact SVD), scaled by the corresponding nonzero singular value $\sigma_k$ (ordered in descending values)}@}: {@{$$\mathbf A = \mathbf U \mathbf \Sigma \mathbf V^{\mathrm H} = \begin{bmatrix} \mathbf u_1 & \cdots & \mathbf u_{\operatorname{rank}(\mathbf A)} \end{bmatrix} \begin{bmatrix} \sigma_1 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & \sigma_{\operatorname{rank}(\mathbf A)} \end{bmatrix} \begin{bmatrix} \mathbf v_1^{\mathrm H} \\ \vdots \\ \mathbf v_{\operatorname{rank}(\mathbf A)}^{\mathrm H} \end{bmatrix} = \sum_{k = 1}^{\operatorname{rank}(\mathbf A)} \sigma_k \mathbf u_k \mathbf v_k^{\mathrm H} = \sum_{k = 1}^{\operatorname{rank}(\mathbf A)} \sigma_k (\mathbf u_k \otimes \mathbf v_k)$$}@}. This results implies that {@{$\mathbf A$ can be expressed as a sum of rank-1 matrices with [spectral norm](matrix%20norm.md#spectral%20norm(p%20=%202)) $\sigma_k$ in decreasing order}@}. This also explains why, in general, {@{the last terms contribute less}@}, which {@{motivates removing some last terms to get a [truncated SVD](singular%20value%20decomposition.md#truncated%20SVD) as an approximation}@}. In fact, the first term is {@{the least square fit of a matrix to an outer product of vectors}@}. <!--SR:!2028-01-06,983,350!2026-01-16,365,290!2026-12-29,565,270!2026-06-18,515,310!2028-01-10,987,350!2026-06-12,478,310!2026-10-10,623,330-->

## references

This text incorporates [content](https://en.wikipedia.org/wiki/outer_product) from [Wikipedia](Wikipedia.md) available under the [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) license.
