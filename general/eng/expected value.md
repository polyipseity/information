---
aliases:
  - expected value
  - expected values
tags:
  - flashcard/active/general/eng/expected_value
  - language/in/English
---

# expected value

- This article is about {@{the term used in probability theory and statistics}@}. For other uses, see [Expected value \(disambiguation\)](expected%20value%20(disambiguation).md).
- "E\(X\)" redirects here. For {@{the $e^{x}$ function}@}, see {@{[Exponential function](exponential%20function.md)}@}. <!--SR:!2025-12-30,282,330!2025-12-27,280,330!2025-12-17,272,330-->

| Part of a series on [statistics](statistics.md) <br/> [__Probability theory__](probability%20theory.md)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|
| ![standard deviation](../../archives/Wikimedia%20Commons/Standard%20deviation%20diagram%20micro.svg)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| - [Probability](probability.md)  <br/>     - [Axioms](probability%20axioms.md) <p>  <p> - [Determinism](determinism.md)  <br/>     - [System](deterministic%20system.md) <p>  <p> - [Indeterminism](indeterminism.md) <br/> - [Randomness](randomness.md)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| - [Probability space](probability%20space.md) <br/> - [Sample space](sample%20space.md) <br/> - [Event](event%20(probability%20theory).md)  <br/>     - [Collectively exhaustive events](collectively%20exhaustive%20events.md) <br/>     - [Elementary event](elementary%20event.md) <br/>     - [Mutual exclusivity](mutual%20exclusivity.md) <br/>     - [Outcome](outcome%20(probability).md) <br/>     - [Singleton](singleton%20(mathematics).md) <p>  <p> - [Experiment](experiment%20(probability%20theory).md)  <br/>     - [Bernoulli trial](Bernoulli%20trial.md) <p>  <p> - [Probability distribution](probability%20distribution.md)  <br/>     - [Bernoulli distribution](Bernoulli%20distribution.md) <br/>     - [Binomial distribution](binomial%20distribution.md) <br/>     - [Exponential distribution](exponential%20distribution.md) <br/>     - [Normal distribution](normal%20distribution.md) <br/>     - [Pareto distribution](Pareto%20distribution.md) <br/>     - [Poisson distribution](Poisson%20distribution.md) <p>  <p> - [Probability measure](probability%20measure.md) <br/> - [Random variable](random%20variable.md)  <br/>     - [Bernoulli process](Bernoulli%20process.md) <br/>     - [Continuous or discrete](continuous%20or%20discrete%20variable.md) <br/>     - __Expected value__ <br/>     - [Variance](variance.md) <br/>     - [Markov chain](Markov%20chain.md) <br/>     - [Observed value](realization%20(probability).md) <br/>     - [Random walk](random%20walk.md) <br/>     - [Stochastic process](stochastic%20process.md) |
| - [Complementary event](complementary%20event.md) <br/> - [Joint probability](joint%20probability%20distribution.md) <br/> - [Marginal probability](marginal%20distribution.md) <br/> - [Conditional probability](conditional%20probability.md)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| - [Independence](independence%20(probability%20theory).md) <br/> - [Conditional independence](conditional%20independence.md) <br/> - [Law of total probability](law%20of%20total%20probability.md) <br/> - [Law of large numbers](law%20of%20large%20numbers.md) <br/> - [Bayes' theorem](Bayes'%20theorem.md) <br/> - [Boole's inequality](Boole's%20inequality.md)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| - [Venn diagram](Venn%20diagram.md) <br/> - [Tree diagram](tree%20diagram%20(probability%20theory).md)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
<!-- -->
<!-- | - [v](https://en.wikipedia.org/wiki/Template:Probability%20fundamentals) <br/> - [t](https://en.wikipedia.org/wiki/Template%20talk:Probability%20fundamentals) <br/> - [e](https://en.wikipedia.org/wiki/Special:EditPage/Template%3AProbability%20fundamentals) | -->

In {@{[probability theory](probability%20theory.md)}@}, {@{the __expected value__ \(also called __expectation__, __expectancy__, __expectation operator__, __mathematical expectation__, __mean__, __expectation value__, or __first [moment](moment%20(mathematics).md)__\)}@} is {@{a generalization of the [weighted average](weighted%20arithmetic%20mean.md)}@}. Informally, the expected value is {@{the [mean](arithmetic%20mean.md) of the possible values a [random variable](random%20variable.md) can take, weighted by the probability of those outcomes}@}. Since {@{it is obtained through arithmetic}@}, the expected value sometimes {@{may not even be included in the sample data set}@}; it is not {@{the value you would "expect" to get in reality}@}. <!--SR:!2025-10-10,201,310!2025-08-28,178,310!2025-05-23,106,290!2025-08-17,158,307!2025-08-29,179,310!2026-01-11,292,330!2025-05-23,106,290-->

The expected value of {@{a random variable with a finite number of outcomes}@} is {@{a weighted average of all possible outcomes}@}. In the case of {@{a continuum of possible outcomes}@}, the expectation is {@{defined by [integration](integral.md)}@}. In {@{the axiomatic foundation for probability provided by [measure theory](measure%20(mathematics).md)}@}, the expectation is {@{given by [Lebesgue integration](Lebesgue%20integral.md)}@}. <!--SR:!2026-01-09,290,330!2025-10-30,233,330!2025-12-13,268,330!2026-01-10,291,330!2026-01-12,293,330!2026-01-04,285,330-->

The expected value of a random variable _X_ is often denoted by {@{E\(_X_\), E\[_X_\], or E<!-- markdown separator -->_X_, with E also often stylized as $\mathbb {E}$ or _E_}@}.<sup>[\[1\]](#^ref-1)</sup><sup>[\[2\]](#^ref-2)</sup><sup>[\[3\]](#^ref-3)</sup> <!--SR:!2026-01-12,293,330-->

## history

{@{The idea of the expected value}@} originated in {@{the middle of the 17th century from the study of the so-called [problem of points](problem%20of%20points.md)}@}, which {@{seeks to divide the stakes _in a fair way_ between two players}@}, who have to {@{end their game before it is properly finished}@}.<sup>[\[4\]](#^ref-4)</sup> This problem {@{had been debated for centuries}@}. {@{Many conflicting proposals and solutions had been suggested over the years}@} when it was posed to {@{[Blaise Pascal](Blaise%20Pascal.md) by French writer and amateur mathematician [Chevalier de Méré](Antoine%20Gombaud.md) in 1654}@}. Méré claimed that {@{this problem could not be solved and that it showed just how flawed mathematics was when it came to its application to the real world}@}. Pascal, {@{being a mathematician, was provoked and determined to solve the problem once and for all}@}. <!--SR:!2025-11-06,240,330!2025-06-23,119,290!2025-12-24,277,330!2025-12-22,276,330!2025-10-23,227,330!2025-12-28,281,330!2025-10-06,198,287!2026-01-09,290,330!2025-12-11,266,330-->

He {@{began to discuss the problem}@} in {@{the famous series of letters to [Pierre de Fermat](Pierre%20de%20Fermat.md)}@}. Soon enough, they {@{both independently came up with a solution}@}. They {@{solved the problem in different computational ways}@}, but {@{their results were identical because their computations were based on the same fundamental principle}@}. The principle is that {@{the value of a future gain should be directly proportional to the chance of getting it}@}. This principle seemed to {@{have come naturally to both of them}@}. They were {@{very pleased by the fact that they had found essentially the same solution}@}, and this in turn {@{made them absolutely convinced that they had solved the problem conclusively}@}; however, they {@{did not publish their findings}@}. They {@{only informed a small circle of mutual scientific friends in Paris about it}@}.<sup>[\[5\]](#^ref-5)</sup> <!--SR:!2026-03-06,337,347!2025-12-21,276,330!2025-11-21,252,330!2025-10-26,230,330!2025-06-09,122,307!2026-03-08,339,347!2025-12-18,273,330!2025-09-07,190,310!2025-09-05,188,310!2025-12-19,274,330!2025-11-07,240,330-->

In {@{Dutch mathematician [Christiaan Huygens'](Christiaan%20Huygens.md) book}@}, he {@{considered the problem of points}@}, and {@{presented a solution based on the same principle as the solutions of Pascal and Fermat}@}. Huygens published his treatise {@{in 1657, \(see [Huygens \(1657\)](#CITEREFHuygens1657)\) "_De ratiociniis in ludo aleæ_"}@} on {@{probability theory just after visiting Paris}@}. The book {@{extended the concept of expectation}@} by {@{adding rules for how to calculate expectations in more complicated situations than the original problem \(e.g., for three or more players\)}@}, and can be seen as {@{the first successful attempt at laying down the foundations of the [theory of probability](probability%20theory.md)}@}. <!--SR:!2026-01-09,290,330!2026-01-08,290,330!2026-03-01,306,290!2025-11-01,198,250!2025-11-21,252,330!2025-09-05,188,310!2025-09-03,184,310!2025-12-21,276,330-->

In {@{the foreword to his treatise}@}, Huygens wrote: <!--SR:!2025-12-25,277,330-->

> It should be said, also, that for {@{some time some of the best mathematicians of France have occupied themselves with this kind of calculus}@} so that {@{no one should attribute to me the honour of the first invention. This does not belong to me}@}. But these savants, although {@{they put each other to the test by proposing to each other many questions difficult to solve, have hidden their methods}@}. I have had therefore {@{to examine and go deeply for myself into this matter by beginning with the elements}@}, and it is {@{impossible for me for this reason to affirm that I have even started from the same principle}@}. But finally {@{I have found that my answers in many cases do not differ from theirs}@}.
>
> —&hairsp;Edwards \(2002\) <!--SR:!2026-01-08,290,330!2026-03-12,342,347!2025-12-17,271,330!2025-09-07,190,310!2025-12-29,281,330!2026-01-08,290,330-->

In {@{the mid-nineteenth century, [Pafnuty Chebyshev](Pafnuty%20Chebyshev.md)}@} became {@{the first person to think systematically in terms of the expectations of [random variables](random%20variable.md)}@}.<sup>[\[6\]](#^ref-6)</sup> <!--SR:!2025-06-20,126,290!2026-01-03,285,330-->

### etymology

{@{Neither Pascal nor Huygens}@} used {@{the term "expectation" in its modern sense}@}. In particular, Huygens writes:<sup>[\[7\]](#^ref-7)</sup> <!--SR:!2026-01-11,292,330!2026-01-07,289,330-->

> That any one Chance or Expectation to win any thing is worth just such a Sum, as wou'd procure in the same Chance and Expectation at a fair Lay. ... If I expect a or b, and have an equal chance of gaining them, my Expectation is worth \(a+b\)/2.

More than {@{a hundred years later, in 1814, [Pierre-Simon Laplace](Pierre-Simon%20Laplace.md)}@} published {@{his tract "_Théorie analytique des probabilités_"}@}, where {@{the concept of expected value was defined explicitly}@}:<sup>[\[8\]](#^ref-8)</sup> <!--SR:!2025-09-21,184,270!2026-03-15,345,347!2025-08-25,177,310-->

> ... this advantage in the theory of chance is the product of the sum hoped for by the probability of obtaining it; it is the partial sum which ought to result when we do not wish to run the risks of the event in supposing that the division is made proportional to the probabilities. This division is the only equitable one when all strange circumstances are eliminated; because an equal degree of probability gives an equal right for the sum hoped for. We will call this advantage _mathematical hope._

## notations

{@{The use of the letter E to denote "expected value"}@} goes back to {@{[W. A. Whitworth](William%20Allen%20Whitworth.md) in 1901}@}.<sup>[\[9\]](#^ref-9)</sup> The symbol has since {@{become popular for English writers}@}. In {@{German, E stands for _Erwartungswert_}@}, in {@{Spanish for _esperanza matemática_}@}, and in {@{French for _espérance mathématique_}@}.<sup>[\[10\]](#^ref-10)</sup> <!--SR:!2025-12-25,278,330!2025-09-20,183,270!2025-11-22,253,330!2025-06-22,127,290!2025-09-22,185,270!2025-12-21,276,330-->

When {@{"E" is used to denote "expected value"}@}, authors use {@{a variety of stylizations}@}: the expectation operator can be {@{stylized as E \(upright\), _E_ \(italic\), or $\mathbb {E}$ \(in [blackboard bold](blackboard%20bold.md)\)}@}, while a variety of {@{bracket notations \(such as E\(_X_\), E\[_X_\], and E<!-- markdown separator -->_X_\) are all used}@}. <!--SR:!2025-12-19,274,330!2025-12-13,268,330!2026-01-06,287,330!2026-01-07,288,330-->

Another popular notation is {@{μ<sub>_X_</sub>}@}. {@{⟨<!-- markdown separator -->_X_<!-- markdown separator -->⟩, ⟨<!-- markdown separator -->_X_<!-- markdown separator -->⟩<sub>av</sub>, and ${\overline {X} }$}@} are commonly used in {@{physics}@}.<sup>[\[11\]](#^ref-11)</sup> {@{M\(_X_\)}@} is used in {@{Russian-language literature}@}. <!--SR:!2026-03-15,345,347!2026-01-12,293,330!2025-12-07,248,327!2026-01-13,294,330!2026-01-06,288,330-->

## definition

As discussed above, there are {@{several context-dependent ways of defining the expected value}@}. The simplest and original definition deals with {@{the case of finitely many possible outcomes, such as in the flip of a coin}@}. With the {@{theory of infinite series}@}, this can be {@{extended to the case of countably many possible outcomes}@}. It is also very common to {@{consider the distinct case of random variables dictated by \(piecewise-\)continuous [probability density functions](probability%20density%20function.md)}@}, as {@{these arise in many natural contexts}@}. All of these specific definitions may be viewed as {@{special cases of the general definition based upon the mathematical tools of [measure theory](measure%20(mathematics).md) and [Lebesgue integration](Lebesgue%20integral.md)}@}, which {@{provide these different contexts with an axiomatic foundation and common language}@}. <!--SR:!2025-11-19,251,330!2025-08-30,180,310!2025-12-31,283,330!2026-03-14,344,347!2025-08-30,180,310!2026-02-26,330,347!2025-10-16,205,310!2025-11-17,248,330-->

Any definition of expected value may be {@{extended to define an expected value of a multidimensional random variable, i.e. a [random vector](multivariate%20random%20variable.md) _X_}@}. It is defined {@{component by component, as E\[_X_\]<sub>_i_</sub> = E\[_X_<sub>_i_</sub>\]}@}. Similarly, one may define the expected value of {@{a [random matrix](random%20matrix.md) _X_ with components _X_<sub>_ij_</sub> by E\[_X_\]<sub>_ij_</sub> = E\[_X_<sub>_ij_</sub>\]}@}. <!--SR:!2026-01-11,292,330!2026-03-02,333,347!2025-12-18,273,330-->

### random variables with finitely many outcomes

Consider {@{a random variable _X_ with a _finite_ list _x_<sub>1</sub>, ..., _x_<sub>_k_</sub> of possible outcomes}@}, each of which {@{\(respectively\) has probability _p_<sub>1</sub>, ..., _p_<sub>_k_</sub> of occurring}@}. The expectation of _X_ is defined as<sup>[\[12\]](#^ref-12)</sup> {@{$$\operatorname {E} [X]=x_{1}p_{1}+x_{2}p_{2}+\cdots +x_{k}p_{k}.$$}@} <!--SR:!2025-11-20,251,330!2026-02-17,298,290!2025-12-24,277,330-->

Since {@{the probabilities must satisfy _p_<sub>1</sub> + ⋅⋅⋅ + _p_<sub>_k_</sub> = 1}@}, it is natural to {@{interpret E\[_X_\] as a [weighted average](weighted%20arithmetic%20mean.md) of the _x_<sub>_i_</sub> values}@}, with {@{weights given by their probabilities _p_<sub>_i_</sub>}@}. <!--SR:!2025-12-22,276,330!2026-01-07,288,330!2026-03-09,340,347-->

In the special case that {@{all possible outcomes are [equiprobable](equiprobability.md) \(that is, _p_<sub>1</sub> = ⋅⋅⋅ = _p_<sub>_k_</sub>\)}@}, the weighted average is given by {@{the standard [average](arithmetic%20mean.md)}@}. In the general case, the expected value takes into account the fact that {@{some outcomes are more likely than others}@}. <!--SR:!2026-02-28,331,347!2026-01-05,286,330!2026-01-12,293,330-->

#### examples

> {@{![An illustration of the convergence of sequence averages of rolls of a dice to the expected value of 3.5 as the number of rolls \(trials\) grows](../../archives/Wikimedia%20Commons/Largenumbers.svg)}@}
>
> {@{An illustration of the convergence of sequence averages of rolls of a dice to the expected value of 3.5 as the number of rolls \(trials\) grows}@} <!--SR:!2025-11-14,246,330!2025-12-22,276,330-->

- Let {@{$X$ represent the outcome of a roll of a fair six-sided die}@}. More specifically, $X$ will be {@{the number of [pips](pip%20(counting).md) showing on the top face of the die after the toss}@}. The possible values for $X$ are {@{1, 2, 3, 4, 5, and 6, all of which are equally likely with a probability of ⁠1/6⁠}@}. The expectation of $X$ is {@{$$\operatorname {E} [X]=1\cdot {\frac {1}{6} }+2\cdot {\frac {1}{6} }+3\cdot {\frac {1}{6} }+4\cdot {\frac {1}{6} }+5\cdot {\frac {1}{6} }+6\cdot {\frac {1}{6} }=3.5.$$}@} If {@{one rolls the die $n$ times and computes the average \([arithmetic mean](arithmetic%20mean.md)\) of the results}@}, then {@{as $n$ grows, the average will [almost surely](almost%20surely.md) [converge](limit%20of%20a%20sequence.md) to the expected value}@}, a fact known as {@{the [strong law of large numbers](law%20of%20large%20numbers.md#strong%20law)}@}.
- {@{The [roulette](roulette.md) game}@} consists of {@{a small ball and a wheel with 38 numbered pockets around the edge}@}. As {@{the wheel is spun, the ball bounces around randomly until it settles down in one of the pockets}@}. Suppose random variable $X$ represents {@{the \(monetary\) outcome of a \$1 bet on a single number \("straight up" bet\)}@}. If {@{the bet wins \(which happens with probability ⁠1/38⁠ in American roulette\)}@}, {@{the payoff is \$35; otherwise the player loses the bet}@}. The expected profit from such a bet will be {@{$$\operatorname {E} [\,{\text{gain from } }\$1{\text{ bet} }\,]=-\$1\cdot {\frac {37}{38} }+\$35\cdot {\frac {1}{38} }=-\${\frac {1}{19} }.$$}@} That is, {@{the expected value to be won from a \$1 bet is −\$⁠1/19⁠}@}. Thus, {@{in 190 bets, the net loss will probably be about \$10}@}. <!--SR:!2026-03-03,308,290!2025-10-31,234,330!2025-12-23,276,330!2025-12-17,272,330!2025-11-11,243,330!2025-08-31,181,310!2025-11-24,255,330!2026-01-13,294,330!2025-08-09,171,327!2025-07-12,131,290!2026-03-10,341,347!2026-02-27,330,347!2025-12-22,276,330!2025-12-18,273,330!2025-12-17,272,330!2025-09-10,193,310-->

### random variables with countably infinitely many outcomes

Informally, the expectation of {@{a random variable with a [countably infinite set](countable%20set.md) of possible outcomes}@} is defined {@{analogously as the weighted average of all possible outcomes}@}, where {@{the weights are given by the probabilities of realizing each given value}@}. This is to say that {@{$$\operatorname {E} [X]=\sum _{i=1}^{\infty }x_{i}\,p_{i},$$}@} where {@{_x_<sub>1</sub>, _x_<sub>2</sub>, ... are the possible outcomes of the random variable _X_ and _p_<sub>1</sub>, _p_<sub>2</sub>, ... are their corresponding probabilities}@}. In {@{many non-mathematical textbooks}@}, this is {@{presented as the full definition of expected values in this context}@}.<sup>[\[13\]](#^ref-13)</sup> <!--SR:!2025-11-24,255,330!2025-09-09,192,310!2025-10-05,209,327!2026-01-13,294,330!2026-01-13,294,330!2025-12-13,268,330!2026-01-13,294,330-->

However, there are {@{some subtleties with infinite summation}@}, so {@{the above formula is not suitable as a mathematical definition}@}. In particular, {@{the [Riemann series theorem](Riemann%20series%20theorem.md) of [mathematical analysis](mathematical%20analysis.md)}@} illustrates that {@{the value of certain infinite sums involving positive and negative summands depends on the order in which the summands are given}@}. Since {@{the outcomes of a random variable have no naturally given order}@}, this {@{creates a difficulty in defining expected value precisely}@}. <!--SR:!2025-11-08,241,330!2025-08-26,176,310!2025-10-09,200,310!2025-11-24,255,330!2025-08-19,175,310!2025-12-22,276,330-->

For this reason, {@{many mathematical textbooks only consider the case that the infinite sum given above [converges absolutely](absolute%20convergence.md)}@}, which implies that {@{the infinite sum is a finite number independent of the ordering of summands}@}.<sup>[\[14\]](#^ref-14)</sup> In {@{the alternative case that the infinite sum does not converge absolutely}@}, one says {@{the random variable _does not have finite expectation_}@}.<sup>[\[14\]](#^ref-14)</sup> <!--SR:!2025-12-19,274,330!2025-11-22,253,330!2026-01-13,294,330!2025-11-07,241,330-->

<!-- markdownlint-disable-next-line MD024 -->
#### examples

- Suppose {@{$x_{i}=i$ and $p_{i}={\tfrac {c}{i\cdot 2^{i} } }$ for $i=1,2,3,\ldots ,$}@} where {@{$c={\tfrac {1}{\ln 2} }$ is the scaling factor which makes the probabilities sum to 1}@}. Then we have {@{$$\operatorname {E} [X]\,=\sum _{i}x_{i}p_{i}=1({\tfrac {c}{2} })+2({\tfrac {c}{8} })+3({\tfrac {c}{24} })+\cdots \,=\,{\tfrac {c}{2} }+{\tfrac {c}{4} }+{\tfrac {c}{8} }+\cdots \,=\,c\,=\,{\tfrac {1}{\ln 2} }.$$}@} <!--SR:!2025-09-04,171,270!2025-05-20,104,290!2025-06-08,121,307-->

### random variables with density

Now consider {@{a random variable _X_ which has a [probability density function](probability%20density%20function.md) given by a function _f_ on the [real number line](number%20line.md)}@}. This means that {@{the probability of _X_ taking on a value in any given [open interval](interval%20(mathematics).md#open%20interval)}@} is {@{given by the [integral](integral.md) of _f_ over that interval}@}. The expectation of _X_ is then {@{given by the integral<sup>[\[15\]](#^ref-15)</sup> $$\operatorname {E} [X]=\int _{-\infty }^{\infty }xf(x)\,dx.$$}@} <!--SR:!2026-03-02,307,290!2026-03-11,341,347!2026-01-08,290,330!2025-12-18,273,330-->

{@{A general and mathematically precise formulation of this definition}@} uses {@{[measure theory](measure%20(mathematics).md) and [Lebesgue integration](Lebesgue%20integral.md)}@}, and the corresponding {@{theory of _absolutely continuous random variables_}@} is described in the next section. {@{The density functions of many common distributions}@} are {@{[piecewise continuous](piecewise%20function.md)}@}, and as such the theory is often {@{developed in this restricted setting}@}.<sup>[\[16\]](#^ref-16)</sup> For such functions, it is {@{sufficient to only consider the standard [Riemann integration](Riemann%20integral.md)}@}. Sometimes {@{_continuous random variables_}@} are {@{defined as those corresponding to this special class of densities}@}, although {@{the term is used differently by various authors}@}. <!--SR:!2026-01-08,290,330!2025-11-13,244,330!2025-12-16,271,330!2025-10-05,198,310!2025-08-24,174,310!2026-03-05,336,347!2026-01-08,289,330!2025-11-04,237,330!2026-01-09,290,330!2026-01-12,293,330-->

Analogously to the {@{countably-infinite case above}@}, there are {@{subtleties with this expression due to the infinite region of integration}@}. Such subtleties can be seen concretely if {@{the distribution of _X_ is given by the [Cauchy distribution](Cauchy%20distribution.md) Cauchy\(0, π\)}@}, so that {@{_f_\(_x_\) = \(_x_<sup>2</sup> + π<sup>2</sup>\)<sup>−1</sup>}@}. It is straightforward to compute in this case that {@{$$\int _{a}^{b}xf(x)\,dx=\int _{a}^{b}{\frac {x}{x^{2}+\pi ^{2} } }\,dx={\frac {1}{2} }\ln {\frac {b^{2}+\pi ^{2} }{a^{2}+\pi ^{2} } }.$$}@} {@{The limit of this expression as _a_ → −∞ and _b_ → ∞ does not exist}@}: if {@{the limits are taken so that _a_ = −<!-- markdown separator -->_b_, then the limit is zero}@}, while if {@{the constraint 2<!-- markdown separator -->_a_ = −<!-- markdown separator -->_b_ is taken, then the limit is ln\(2\)}@}. <!--SR:!2025-12-19,274,330!2026-01-04,286,330!2025-11-01,235,330!2025-10-20,221,327!2025-06-22,127,290!2025-12-21,276,330!2025-08-22,182,327!2026-03-03,334,347-->

To {@{avoid such ambiguities}@}, in mathematical textbooks it is common to {@{require that the given integral [converges absolutely](absolute%20convergence.md), with E\[_X_\] left undefined otherwise}@}.<sup>[\[17\]](#^ref-17)</sup> However, {@{measure-theoretic notions as given below}@} can be {@{used to give a systematic definition of E\[_X_\] for more general random variables _X_}@}. <!--SR:!2026-03-09,339,347!2025-11-19,250,330!2026-03-13,343,347!2025-12-27,280,330-->

### arbitrary real-valued random variables

{@{All definitions of the expected value}@} may be {@{expressed in the language of [measure theory](measure%20(mathematics).md)}@}. In general, if {@{_X_ is a real-valued [random variable](random%20variable.md) defined on a [probability space](probability%20space.md) \(Ω, Σ, P\)}@}, then the expected value of _X_, denoted by E\[_X_\], is defined as {@{the [Lebesgue integral](Lebesgue%20integral.md)<sup>[\[18\]](#^ref-18)</sup> $$\operatorname {E} [X]=\int _{\Omega }X\,d\operatorname {P} .$$}@} Despite {@{the newly abstract situation}@}, this definition is {@{extremely similar in nature to the very simplest definition of expected values, given above, as certain weighted averages}@}. This is because, in measure theory, the value of the Lebesgue integral of _X_ is {@{defined via weighted averages of _approximations_ of _X_ which take on finitely many values}@}.<sup>[\[19\]](#^ref-19)</sup> Moreover, if {@{given a random variable with finitely or countably many possible values}@}, the Lebesgue theory of expectation is {@{identical to the summation formulas given above}@}. However, the Lebesgue theory {@{clarifies the scope of the theory of probability density functions}@}. A random variable _X_ is said to be {@{_absolutely continuous_ if any of the following conditions are satisfied}@}: <!--SR:!2025-12-26,279,330!2025-12-26,279,330!2025-12-18,273,330!2025-07-03,126,290!2026-01-13,294,330!2025-12-16,270,330!2025-09-06,189,310!2026-01-09,290,330!2025-12-28,280,330!2026-03-06,337,347!2025-06-19,125,290-->

- (annotation: absolutely continuous random variable / [probability density function](probability%20density%20function.md)) ::@:: there is a nonnegative [measurable function](measurable%20function.md) _f_ on the real line such that$$\operatorname {P} (X\in A)=\int _{A}f(x)\,dx,$$ for any [Borel set](Borel%20set.md) _A_, in which the integral is Lebesgue. <!--SR:!2025-12-24,261,290!2025-06-06,115,290-->
- (annotation: absolutely continuous random variable / [cumulative distribution function](cumulative%20distribution%20function.md)) ::@:: the [cumulative distribution function](cumulative%20distribution%20function.md) of _X_ is [absolutely continuous](absolute%20continuity.md). <!--SR:!2025-08-25,175,310!2026-01-10,291,330-->
- (annotation: absolutely continuous random variable / zero probability) ::@:: for any Borel set _A_ of real numbers with [Lebesgue measure](Lebesgue%20measure.md) equal to zero, the probability of _X_ being valued in _A_ is also equal to zero <!--SR:!2025-09-01,182,310!2026-01-13,294,330-->
- (annotation: absolutely continuous random variable / epsilon probability) ::@:: for any positive number ε there is a positive number δ such that: if _A_ is a Borel set with Lebesgue measure less than δ, then the probability of _X_ being valued in _A_ is less than ε. <!--SR:!2025-12-26,279,330!2025-11-27,212,270-->

These conditions are {@{all equivalent, although this is nontrivial to establish}@}.<sup>[\[20\]](#^ref-20)</sup> In this definition, _f_ is called {@{the _probability density function_ of _X_ \(relative to Lebesgue measure\)}@}. According to {@{the change-of-variables formula for Lebesgue integration}@},<sup>[\[21\]](#^ref-21)</sup> combined with {@{the [law of the unconscious statistician](law%20of%20the%20unconscious%20statistician.md)}@},<sup>[\[22\]](#^ref-22)</sup> it follows that {@{$$\operatorname {E} [X]\equiv \int _{\Omega }X\,d\operatorname {P} =\int _{\mathbb {R} }xf(x)\,dx$$ for any absolutely continuous random variable _X_}@}. {@{The above discussion of continuous random variables}@} is {@{thus a special case of the general Lebesgue theory}@}, due to the fact that {@{every piecewise-continuous function is measurable}@}. <!--SR:!2025-11-12,245,330!2025-12-19,274,330!2025-12-19,274,330!2025-12-21,276,330!2025-07-12,131,290!2025-12-21,276,330!2026-01-08,290,330!2025-12-20,275,330-->

{@{![Expected value _µ_ and median _m_ on the graph of the cumulative distribution function _F_](../../archives/Wikimedia%20Commons/Roland%20Uhl%202023%20Charakterisierung%20des%20Erwartungswertes%20Bild1.svg) <a id="Uhl2023Bild1"></a>}@} (annotation: graph before this) <p> The expected value of {@{any real-valued random variable $X$}@} can also be {@{defined on the graph of its [cumulative distribution function](cumulative%20distribution%20function.md) $F$ by a nearby equality of areas}@}. In fact, {@{$\operatorname {E} [X]=\mu$ with a real number $\mu$}@} {@{if and only if the two surfaces in the $x$-$y$-plane, described by$$x\leq \mu ,\;\,0\leq y\leq F(x)\quad {\text{or} }\quad x\geq \mu ,\;\,F(x)\leq y\leq 1$$ respectively, have the same finite area}@}, i.e. iff {@{$$\int _{-\infty }^{\mu }F(x)\,dx=\int _{\mu }^{\infty }{\big (}1-F(x){\big )}\,dx$$ and both [improper Riemann integrals](improper%20integral.md) converge}@}. (annotation: If you {@{swap _x_- and _y_-axis in the CDF graph, you should intutively see why this is true}@}.) Finally, this is {@{equivalent to the representation $$\operatorname {E} [X]=\int _{0}^{\infty }{\bigl (}1-F(x){\bigr )}\,dx-\int _{-\infty }^{0}F(x)\,dx,$$ also with convergent integrals}@}.<sup>[\[23\]](#^ref-23)</sup>  (annotation: Similar to above, if you {@{swap _x_- and _y_-axis in the CDF graph, you should intutively see why this is true}@}.) <!--SR:!2025-09-06,189,310!2025-08-26,176,310!2025-07-02,125,290!2025-11-24,255,330!2025-11-16,248,330!2026-03-21,319,290!2025-12-20,275,330!2025-10-01,195,310!2025-12-14,269,330-->

### infinite expected values

Expected values as defined above are {@{automatically finite numbers}@}. However, in many cases it is {@{fundamental to be able to consider expected values of ±∞}@}. This is {@{intuitive, for example, in the case of the [St. Petersburg paradox](St.%20Petersburg%20paradox.md)}@}, in which one {@{considers a random variable with possible outcomes _x_<sub>_i_</sub> = 2<sup>_i_</sup>, with associated probabilities _p_<sub>_i_</sub> = 2<sup>−<!-- markdown separator -->_i_</sup>, for _i_ ranging over all positive integers}@}. According to {@{the summation formula in the case of random variables with countably many outcomes}@}, one has {@{$$\operatorname {E} [X]=\sum _{i=1}^{\infty }x_{i}\,p_{i}=2\cdot {\frac {1}{2} }+4\cdot {\frac {1}{4} }+8\cdot {\frac {1}{8} }+16\cdot {\frac {1}{16} }+\cdots =1+1+1+1+\cdots .$$}@} It is {@{natural to say that the expected value equals +∞}@}. <!--SR:!2025-12-17,272,330!2026-01-10,291,330!2025-12-16,271,330!2026-01-11,292,330!2025-11-21,252,330!2025-11-04,238,330!2025-11-16,247,330-->

There is {@{a rigorous mathematical theory underlying such ideas}@}, which is often {@{taken as part of the definition of the Lebesgue integral}@}.<sup>[\[19\]](#^ref-19)</sup> The first fundamental observation is that, {@{whichever of the above definitions are followed}@}, {@{any _nonnegative_ random variable whatsoever can be given an unambiguous expected value}@}; whenever {@{absolute convergence fails}@}, then {@{the expected value can be defined as +∞}@}. The second fundamental observation is that {@{any random variable can be written as the difference of two nonnegative random variables}@}. Given a random variable _X_, one {@{defines the [positive and negative parts](positive%20and%20negative%20parts.md) by _X_<sup> +</sup> = max\(_X_, 0\) and _X_<sup> −</sup> = −min\(_X_, 0\)}@}. These are {@{nonnegative random variables, and it can be directly checked that _X_ = _X_<sup> +</sup> − _X_<sup> −</sup>}@}. Since {@{E\[_X_<sup> +</sup>\] and E\[_X_<sup> −</sup>\] are both then defined as either nonnegative numbers or +∞}@}, it is then {@{natural to define: $$\operatorname {E} [X]={\begin{cases}\operatorname {E} [X^{+}]-\operatorname {E} [X^{-}]&{\text{if } }\operatorname {E} [X^{+}]<\infty {\text{ and } }\operatorname {E} [X^{-}]<\infty ;\\+\infty &{\text{if } }\operatorname {E} [X^{+}]=\infty {\text{ and } }\operatorname {E} [X^{-}]<\infty ;\\-\infty &{\text{if } }\operatorname {E} [X^{+}]<\infty {\text{ and } }\operatorname {E} [X^{-}]=\infty ;\\{\text{undefined} }&{\text{if } }\operatorname {E} [X^{+}]=\infty {\text{ and } }\operatorname {E} [X^{-}]=\infty .\end{cases} }$$}@} According to this definition, {@{E\[_X_\] exists and is finite if and only if E\[_X_<sup> +</sup>\] and E\[_X_<sup> −</sup>\] are both finite}@}. Due {@{to the formula \|_X_\| = _X_<sup> +</sup> + _X_<sup> −</sup>}@}, this is the case {@{if and only if E\|_X_\| is finite}@}, and this is equivalent to {@{the absolute convergence conditions in the definitions above}@}. As such, the present considerations {@{do not define finite expected values in any cases not previously considered}@}; they are {@{only useful for infinite expectations}@}. <!--SR:!2025-11-22,253,330!2026-02-20,297,290!2025-10-27,231,330!2025-10-28,232,330!2025-10-25,229,330!2025-11-15,246,330!2025-12-20,275,330!2025-12-28,281,330!2025-12-20,275,330!2025-11-18,249,330!2025-11-18,249,330!2026-03-14,344,347!2025-10-24,228,330!2025-10-03,197,310!2025-10-23,227,330!2026-01-10,291,330!2025-05-17,102,290-->

- In the case of {@{the St. Petersburg paradox}@}, one has {@{_X_<sup> −</sup> = 0 and so E\[_X_\] = +∞ as desired}@}.
- Suppose {@{the random variable _X_ takes values 1, −2, 3, −4, ...}@} with {@{respective probabilities 6π<sup>−2</sup>, 6\(2π\)<sup>−2</sup>, 6\(3π\)<sup>−2</sup>, 6\(4π\)<sup>−2</sup>, ... (annotation: $\sum_{k = 1}^\infty \frac 1 {k^2} = \frac {\pi^2} 6$)}@}. Then it follows that {@{_X_<sup> +</sup> takes value 2<!-- markdown separator -->_k_<!-- markdown separator -->−1 with probability 6\(\(2<!-- markdown separator -->_k_<!-- markdown separator -->−1\)π\)<sup>−2</sup> for each positive integer _k_, and takes value 0 with remaining probability}@}. Similarly, {@{_X_<sup> −</sup> takes value 2<!-- markdown separator -->_k_ with probability 6\(2<!-- markdown separator -->_k_<!-- markdown separator -->π\)<sup>−2</sup> for each positive integer _k_ and takes value 0 with remaining probability}@}. Using {@{the definition for non-negative random variables}@}, one can show that {@{both E\[_X_<sup> +</sup>\] = ∞ and E\[_X_<sup> −</sup>\] = ∞ \(see [Harmonic series](harmonic%20series%20(mathematics).md)\)}@}. Hence, in this case the expectation of _X_ is undefined.
- Similarly, {@{the Cauchy distribution}@}, as discussed above, has {@{undefined expectation}@}. <!--SR:!2026-01-10,291,330!2025-12-20,275,330!2025-08-27,177,310!2025-10-15,204,310!2025-12-26,278,330!2025-06-29,122,290!2025-11-09,242,330!2025-11-11,244,330!2025-12-14,269,330!2025-11-14,245,330-->

## expected values of common distributions

The following table gives {@{the expected values of some commonly occurring [probability distributions](probability%20distribution.md)}@}. The third column gives {@{the expected values both in the form immediately given by the definition, as well as in the simplified form obtained by computation therefrom}@}. The details of these computations, which are {@{not always straightforward}@}, can be found in the indicated references. <!--SR:!2025-08-12,167,310!2025-08-15,144,267!2025-08-27,177,310-->

| Distribution                                                                                                    | Notation                                          | Mean E\(X\)                                                                                                                                                                           |
| --------------------------------------------------------------------------------------------------------------- | ------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| {@{[Bernoulli](Bernoulli%20distribution.md)}@}<sup>[\[24\]](#^ref-24)</sup>                                     | {@{$X\sim ~b(1,p)$}@}                             | {@{$0\cdot (1-p)+1\cdot p=p$}@}                                                                                                                                                       |
| {@{[Binomial](binomial%20distribution.md)}@}<sup>[\[25\]](#^ref-25)</sup>                                       | {@{$X\sim B(n,p)$}@}                              | {@{$\sum _{i=0}^{n}i{n \choose i}p^{i}(1-p)^{n-i}=np$}@}                                                                                                                              |
| {@{[Poisson](Poisson%20distribution.md)}@}<sup>[\[26\]](#^ref-26)</sup>                                         | {@{$X\sim \mathrm {Po} (\lambda )$}@}             | {@{$\sum _{i=0}^{\infty }{\frac {ie^{-\lambda }\lambda ^{i} }{i!} }=\lambda$}@}                                                                                                       |
| {@{[Geometric](geometric%20distribution.md)}@}<sup>[\[27\]](#^ref-27)</sup>                                     | {@{$X\sim \mathrm {Geometric} (p)$ \(shifted\)}@} | {@{$\sum _{i=1}^{\infty }ip(1-p)^{i-1}={\frac {1}{p} }$ \(unshifted: $\frac {1 - p} p$\)}@}                                                                                           |
| {@{[Uniform](continuous%20uniform%20distribution.md)}@}<sup>[\[28\]](#^ref-28)</sup>                            | {@{$X\sim U(a,b)$}@}                              | {@{$\int _{a}^{b}{\frac {x}{b-a} }\,dx={\frac {a+b}{2} }$}@}                                                                                                                          |
| {@{[Exponential](exponential%20distribution.md)}@}<sup>[\[29\]](#^ref-29)</sup>                                 | {@{$X\sim \exp(\lambda )$}@}                      | {@{$\int _{0}^{\infty }\lambda xe^{-\lambda x}\,dx={\frac {1}{\lambda } }$}@}                                                                                                         |
| {@{[Normal](normal%20distribution.md)}@}<sup>[\[30\]](#^ref-30)</sup>                                           | {@{$X\sim N(\mu ,\sigma ^{2})$}@}                 | {@{${\frac {1}{\sqrt {2\pi \sigma ^{2} } } }\int _{-\infty }^{\infty }x\,e^{-{\frac {1}{2} }\left({\frac {x-\mu }{\sigma } }\right)^{2} }\,dx=\mu$}@}                                 |
| {@{[Standard Normal](normal%20distribution.md#standard%20normal%20distribution)}@}<sup>[\[31\]](#^ref-31)</sup> | {@{$X\sim N(0,1)$}@}                              | {@{${\frac {1}{\sqrt {2\pi } } }\int _{-\infty }^{\infty }xe^{-x^{2}/2}\,dx=0$}@}                                                                                                     |
| {@{[Pareto](Pareto%20distribution.md)}@}<sup>[\[32\]](#^ref-32)</sup>                                           | {@{$X\sim \mathrm {Par} (\alpha ,k)$}@}           | {@{$\int _{k}^{\infty }\alpha k^{\alpha }x^{-\alpha }\,dx={\begin{cases}{\frac {\alpha k}{\alpha -1} }&{\text{if  } }\alpha >1\\\infty &{\text{if } }0<\alpha \leq 1\end{cases} }$}@} |
| {@{[Cauchy](Cauchy%20distribution.md)}@}<sup>[\[33\]](#^ref-33)</sup>                                           | {@{$X\sim \mathrm {Cauchy} (x_{0},\gamma )$}@}    | {@{${\frac {1}{\pi } }\int _{-\infty }^{\infty }{\frac {\gamma x}{(x-x_{0})^{2}+\gamma ^{2} } }\,dx$ is [undefined](indeterminate%20form.md)}@}                                       | <!--SR:!2025-05-19,103,290!2025-12-23,276,330!2025-12-22,276,330!2025-11-23,254,330!2025-11-24,255,330!2025-10-09,185,270!2025-11-15,247,330!2025-12-20,275,330!2026-02-14,296,290!2025-12-20,275,330!2025-07-15,136,290!2025-06-29,99,230!2025-09-10,193,310!2025-12-14,269,330!2026-01-11,292,330!2025-12-12,267,330!2026-01-08,290,330!2025-07-29,130,250!2025-12-27,279,330!2025-12-15,270,330!2025-10-25,194,270!2025-12-15,270,330!2025-09-03,186,310!2026-02-07,291,290!2025-12-21,276,330!2025-05-18,92,270!2025-09-09,126,210!2025-11-02,236,330!2025-11-19,251,330!2025-07-22,126,250-->

## properties

{@{The basic properties below \(and their names in bold\)}@} {@{replicate or follow immediately from those of [Lebesgue integral](Lebesgue%20integral.md)}@}. Note that {@{the letters "a.s." stand for "[almost surely](almost%20surely.md)"}@}—{@{a central property of the Lebesgue integral}@}. Basically, one says that {@{an inequality like $X\geq 0$ is true almost surely}@}, when {@{the probability measure attributes zero-mass to the complementary event $\left\{X<0\right\}.$}@} <!--SR:!2025-12-16,271,330!2025-12-17,272,330!2026-01-12,293,330!2026-03-15,345,347!2025-08-28,178,310!2026-01-11,292,330-->

- Non-negativity: ::@:: If $X\geq 0$ \(a.s.\), then $\operatorname {E} [X]\geq 0.$ <!--SR:!2025-11-24,255,330!2026-01-05,287,330-->
- Linearity of expectation:<sup>[\[34\]](#^ref-34)</sup> {@{The expected value operator \(or _expectation operator_\) $\operatorname {E} [\cdot ]$ is [linear](linear%20map.md)}@} in the sense that, {@{for any random variables $X$ and $Y$ (annotation: independence is _not_ required), and a constant $a$, $${\begin{aligned}\operatorname {E} [X+Y]&=\operatorname {E} [X]+\operatorname {E} [Y],\\\operatorname {E} [aX]&=a\operatorname {E} [X],\end{aligned} }$$}@} whenever {@{the right-hand side is well-defined}@}. By {@{[induction](mathematical%20induction.md)}@}, this means that {@{the expected value of the sum of any finite number of random variables is the sum of the expected values of the individual random variables}@}, and {@{the expected value scales linearly with a multiplicative constant}@}. Symbolically, for {@{$N$ random variables $X_{i}$ and constants $a_{i}(1\leq i\leq N)$}@}, we have {@{$\operatorname {E} \left[\sum _{i=1}^{N}a_{i}X_{i}\right]=\sum _{i=1}^{N}a_{i}\operatorname {E} [X_{i}]$}@}. If we {@{think of the set of random variables with finite expected value as forming a vector space}@}, then {@{the linearity of expectation implies that the expected value is a [linear form](linear%20form.md) on this vector space}@}.
- Monotonicity: ::@:: If $X\leq Y$ [\(a.s.\)](almost%20surely.md), and both $\operatorname {E} [X]$ and $\operatorname {E} [Y]$ exist, then $\operatorname {E} [X]\leq \operatorname {E} [Y].$Proof follows from the linearity and the non-negativity property for $Z=Y-X,$ since $Z\geq 0$ \(a.s.\). <!--SR:!2025-12-18,273,330!2025-10-02,196,310-->
- Non-degeneracy: ::@:: If $\operatorname {E} [|X|]=0,$ then $X=0$ \(a.s.\). <!--SR:!2025-12-21,276,330!2025-12-17,272,330-->
- (annotation: equality) ::@:: If $X=Y$ [\(a.s.\)](almost%20surely.md), then $\operatorname {E} [X]=\operatorname {E} [Y].$ In other words, if X and Y are random variables that take different values with probability zero, then the expectation of X will equal the expectation of Y. <!--SR:!2026-03-04,335,347!2025-12-17,272,330-->
- (annotation: idempotency) ::@:: If $X=c$ [\(a.s.\)](almost%20surely.md) for some real number _c_, then $\operatorname {E} [X]=c.$ In particular, for a random variable $X$ with well-defined expectation, $\operatorname {E} [\operatorname {E} [X]]=\operatorname {E} [X].$ A well defined expectation implies that there is one number, or rather, one constant that defines the expected value. Thus follows that the expectation of this constant is just the original expected value. <!--SR:!2026-03-15,345,347!2025-12-22,276,330-->
- (annotation: inequality) ::@:: As a consequence of the formula \|_X_\| = _X_<sup>+</sup> + _X_<sup>−</sup> as discussed above, together with the [triangle inequality](triangle%20inequality.md), it follows that for any random variable $X$ with well-defined expectation, one has $|\operatorname {E} [X]|\leq \operatorname {E} |X|.$ <!--SR:!2026-03-15,345,347!2026-02-09,292,290-->
- (annotation: [indicator function](../../../../general/indicator%20function.md)) ::@:: Let __1__<sub>_A_</sub> denote the [indicator function](indicator%20function.md) of an [event](event%20(probability%20theory).md) _A_, then E\[__1__<sub>_A_</sub>\] is given by the probability of _A_. This is nothing but a different way of stating the expectation of a [Bernoulli random variable](Bernoulli%20distribution.md), as calculated in the table above. <!--SR:!2025-08-17,175,310!2025-09-02,186,310-->
- Formulas in terms of CDF: If {@{$F(x)$ is the [cumulative distribution function](cumulative%20distribution%20function.md) of a random variable _X_}@}, then {@{$$\operatorname {E} [X]=\int _{-\infty }^{\infty }x\,dF(x),$$ where the values on both sides are well defined or not well defined simultaneously}@}, and the integral is taken in {@{the sense of [Lebesgue-Stieltjes](Lebesgue–Stieltjes%20integration.md)}@}. As a consequence of {@{[integration by parts](integration%20by%20parts.md) as applied to this representation of E\[_X_\]}@}, it can be proved that {@{$$\operatorname {E} [X]=\int _{0}^{\infty }(1-F(x))\,dx-\int _{-\infty }^{0}F(x)\,dx,$$ with the integrals taken in the sense of Lebesgue}@}.<sup>[\[35\]](#^ref-35)</sup> As a special case, for {@{any random variable _X_ valued in the nonnegative integers {0, 1, 2, 3, ...}<!-- flashcard separator -->}@}, one has {@{$$\operatorname {E} [X]=\sum _{n=0}^{\infty }\Pr(X>n),$$ where _P_ denotes the underlying probability measure}@}.
- Non-multiplicativity: ::@:: In general, the expected value is not multiplicative, i.e. $\operatorname {E} [XY]$ is not necessarily equal to $\operatorname {E} [X]\cdot \operatorname {E} [Y].$ If $X$ and $Y$ are [independent](independence%20(probability%20theory).md#independent%20random%20variables), then one can show that $\operatorname {E} [XY]=\operatorname {E} [X]\operatorname {E} [Y].$ If the random variables are [dependent](dependent%20and%20independent%20variables.md), then generally $\operatorname {E} [XY]\neq \operatorname {E} [X]\operatorname {E} [Y],$ although in special cases of dependency the equality may hold (annotation: see [uncorrelatedness](uncorrelatedness%20(probability%20theory).md)). <!--SR:!2025-12-15,270,330!2025-12-31,283,330-->
- [Law of the unconscious statistician](law%20of%20the%20unconscious%20statistician.md): ::@:: The expected value of a measurable function of $X,$ $g(X),$ given that $X$ has a probability density function $f(x),$ is given by the [inner product](inner%20product%20space.md) of $f$ and $g$:<sup>[\[34\]](#^ref-34)</sup> $$\operatorname {E} [g(X)]=\int _{\mathbb {R} }g(x)f(x)\,dx.$$ This formula also holds in multidimensional case, when $g$ is a function of several random variables, and $f$ is their [joint density](probability%20density%20function.md#densities%20associated%20with%20multiple%20variables).<sup>[\[34\]](#^ref-34)</sup><sup>[\[36\]](#^ref-36)</sup> <!--SR:!2025-05-21,105,290!2025-11-12,244,330-->

### inequalities

{@{[Concentration inequalities](concentration%20inequality.md)}@} {@{control the likelihood of a random variable taking on large values}@}. {@{[Markov's inequality](Markov's%20inequality.md)}@} is {@{among the best-known and simplest to prove}@}: for {@{a _nonnegative_ random variable _X_ and any positive number _a_, it states that<sup>[\[37\]](#^ref-37)</sup> $$\operatorname {P} (X\geq a)\leq {\frac {\operatorname {E} [X]}{a} }.$$}@} (annotation: Proof is by {@{rearranging the inequality to get $a \operatorname P(X \geq a) \le \operatorname E[X]$ and noting that $X \ge 0$}@}. Then {@{the [probability mass function](probability%20mass%20function.md) $p_X(x) = \begin{cases} \operatorname E[X] / a, & x = a \\ 0, & \text{otherwise} \end{cases}$ gives the extreme case (equality) and it becomes obvious why the inequality holds}@}.) If {@{_X_ is any random variable with finite expectation}@}, then {@{Markov's inequality may be applied to the random variable \|_X_<!-- markdown separator -->−E\[_X_\]\|<sup>2</sup>}@} to {@{obtain [Chebyshev's inequality](Chebyshev's%20inequality.md) $$\operatorname {P} (|X-{\text{E} }[X]|\geq a)\leq {\frac {\operatorname {Var} [X]}{a^{2} } },$$ where Var is the [variance](variance.md)}@}.<sup>[\[37\]](#^ref-37)</sup> These inequalities are {@{significant for their nearly complete lack of conditional assumptions}@}. For example, for {@{any random variable with finite expectation}@}, the Chebyshev inequality implies that {@{there is at least a 75% probability of an outcome being within two [standard deviations](standard%20deviation.md) of the expected value}@}. However, in {@{special cases the Markov and Chebyshev inequalities often give much weaker information than is otherwise available}@}. For example, in {@{the case of an unweighted dice}@}, Chebyshev's inequality says {@{that odds of rolling between 1 and 6 is at least 53%}@}; in reality, {@{the odds are of course 100%}@}.<sup>[\[38\]](#^ref-38)</sup> {@{The [Kolmogorov inequality](Kolmogorov's%20inequality.md)}@} {@{extends the Chebyshev inequality to the context of sums of random variables}@}.<sup>[\[39\]](#^ref-39)</sup> <!--SR:!2026-03-14,344,347!2026-03-12,342,347!2025-12-16,271,330!2026-03-07,338,347!2025-09-19,182,270!2025-07-25,157,310!2026-01-10,291,330!2025-06-14,126,307!2025-12-18,273,330!2025-09-02,185,310!2026-01-11,292,330!2026-03-01,332,347!2025-07-11,130,290!2025-12-19,274,330!2025-11-10,243,330!2026-01-13,294,330!2025-05-24,107,290!2025-10-19,218,330!2026-01-16,272,290-->

{@{The following three inequalities}@} are {@{of fundamental importance in the field of [mathematical analysis](mathematical%20analysis.md) and its applications to probability theory}@}. (annotation: They are: {@{[Jensen's inequality](Jensen's%20inequality.md), [Hölder's inequality](Hölder's%20inequality.md), [Minkowski inequality](Minkowski%20inequality.md)}@}.) <!--SR:!2026-03-13,343,347!2026-02-22,299,290!2025-12-19,274,330-->

- [Jensen's inequality](Jensen's%20inequality.md): Let {@{_f_: __R__ → __R__ be a [convex function](convex%20function.md) and _X_ a random variable with finite expectation}@}. Then {@{<sup>[\[40\]](#^ref-40)</sup> $$f(\operatorname {E} (X))\leq \operatorname {E} (f(X)).$$}@} Part of the assertion is that {@{the [negative part](positive%20and%20negative%20parts.md) of _f_\(_X_\) has finite expectation, so that the right-hand side is well-defined \(possibly infinite\)}@}. {@{Convexity of _f_}@} can be phrased as saying that {@{the output of the weighted average of _two_ inputs under-estimates the same weighted average of the two outputs}@}; Jensen's inequality {@{extends this to the setting of completely general weighted averages, as represented by the expectation}@}. In the special case that {@{_f_\(_x_\) = \|_x_\|<sup>_t_/_s_</sup> for positive numbers _s_ \< _t_}@}, one obtains {@{the Lyapunov inequality<sup>[\[41\]](#^ref-41)</sup> $$\left(\operatorname {E} |X|^{s}\right)^{1/s}\leq \left(\operatorname {E} |X|^{t}\right)^{1/t}.$$}@} This can also be proved by {@{the Hölder inequality}@}.<sup>[\[40\]](#^ref-40)</sup> In {@{measure theory}@}, this is particularly notable for {@{proving the inclusion L<sup>_s_</sup> ⊂ L<sup>_t_</sup> of [L<sup>_p_</sup> spaces](Lp%20space.md), in the special case of [probability spaces](probability%20space.md)}@}.
- [Hölder's inequality](Hölder's%20inequality.md): ::@:: if _p_ \> 1 and _q_ \> 1 are numbers satisfying _p_<sup> −1</sup> + _q_<sup> −1</sup> = 1, then$$\operatorname {E} |XY|\leq (\operatorname {E} |X|^{p})^{1/p}(\operatorname {E} |Y|^{q})^{1/q}.$$ for any random variables _X_ and _Y_.<sup>[\[40\]](#^ref-40)</sup> The special case of _p_ = _q_ = 2 is called the [Cauchy–Schwarz inequality](Cauchy–Schwarz%20inequality.md), and is particularly well-known.<sup>[\[40\]](#^ref-40)</sup> (annotation: generalization of the [Cauchy–Schwarz inequality](Cauchy–Schwarz%20inequality.md)) <!--SR:!2025-07-20,124,250!2025-06-26,91,250-->
- [Minkowski inequality](Minkowski%20inequality.md): ::@:: given any number _p_ ≥ 1, for any random variables _X_ and _Y_ with E\|_X_\|<sup>_p_</sup> and E\|_Y_\|<sup>_p_</sup> both finite, it follows that E\|_X_ + _Y_\|<sup>_p_</sup> is also finite and<sup>[\[42\]](#^ref-42)</sup> $${\Bigl (}\operatorname {E} |X+Y|^{p}{\Bigr )}^{1/p}\leq {\Bigl (}\operatorname {E} |X|^{p}{\Bigr )}^{1/p}+{\Bigl (}\operatorname {E} |Y|^{p}{\Bigr )}^{1/p}.$$ (annotation: generalization of the [triangle inequality](triangle%20inequality.md)) <!--SR:!2025-07-28,129,250!2025-06-25,72,230-->

{@{The Hölder and Minkowski inequalities}@} can be {@{extended to general [measure spaces](measure%20space.md), and are often given in that context}@}. By contrast, {@{the Jensen inequality is special to the case of probability spaces}@}. <!--SR:!2025-11-06,239,330!2025-12-14,268,330!2025-12-16,271,330-->

### expectations under convergence of random variables

In general, it is {@{not the case that $\operatorname {E} [X_{n}]\to \operatorname {E} [X]$ even if $X_{n}\to X$ pointwise}@}. Thus, one {@{cannot interchange limits and expectation, without additional conditions on the random variables}@}. To see this, let {@{$U$ be a random variable distributed uniformly on $[0,1]$}@}. For $n\geq 1,$ define {@{a sequence of random variables $$X_{n}=n\cdot \mathbf {1} \left\{U\in \left(0,{\tfrac {1}{n} }\right)\right\},$$}@} with {@{$\mathbf {1} \{A\}$ being the indicator function of the event $A.$}@} Then, it {@{follows that $X_{n}\to 0$ pointwise}@}. But, {@{$\operatorname {E} [X_{n}]=n\cdot \Pr \left(U\in \left[0,{\tfrac {1}{n} }\right]\right)=n\cdot {\tfrac {1}{n} }=1$ for each $n$}@}. Hence, {@{$\lim _{n\to \infty }\operatorname {E} [X_{n}]=1\neq 0=\operatorname {E} \left[\lim _{n\to \infty }X_{n}\right]$}@}. <!--SR:!2026-01-13,294,330!2026-01-12,293,330!2026-01-03,285,330!2025-12-27,280,330!2026-01-08,290,330!2025-12-16,271,330!2025-12-20,275,330!2025-11-20,251,330-->

Analogously, for {@{general sequence of random variables $\{Y_{n}:n\geq 0\}$}@}, the expected value operator is {@{not $\sigma$-additive, i.e.$$\operatorname {E} \left[\sum _{n=0}^{\infty }Y_{n}\right]\neq \sum _{n=0}^{\infty }\operatorname {E} [Y_{n}].$$}@} An example is easily obtained by {@{setting $Y_{0}=X_{1}$ and $Y_{n}=X_{n+1}-X_{n}$ for $n\geq 1$}@}, where {@{$X_{n}$ is as in the previous example}@}. (annotation: The left hand side {@{contains a telescoping series of $X_n$ that cancels out to yield $\operatorname E\left[\lim_{n \to \infty} X_n\right] = 0$}@}, while the right hand side {@{has $\operatorname E[Y_n] = \operatorname E[X_{n + 1}] - \operatorname E[X_n] = 1 - 1 = 0$ for $n \ge 1$ and thus yields $\operatorname E[Y_0] = \operatorname E[X_1] = 1$}@}.) <!--SR:!2025-10-27,227,327!2025-12-12,267,330!2025-11-18,250,330!2026-01-13,294,330!2025-07-11,130,290!2026-03-17,344,350-->

{@{A number of convergence results}@} specify {@{exact conditions which allow one to interchange limits and expectations}@}, as specified below. (annotation: Some of them are: {@{[monotone convergence theorem](monotone%20convergence%20theorem.md), [Fatou's lemma](Fatou's%20lemma.md), [dominated convergence theorem](dominated%20convergence%20theorem.md), [uniform integrability](uniform%20integrability.md)}@}.) <!--SR:!2026-03-14,344,347!2025-08-21,164,310!2025-08-27,142,270-->

- [Monotone convergence theorem](monotone%20convergence%20theorem.md): Let {@{$\{X_{n}:n\geq 0\}$ be a (annotation: nondecreasing) sequence of (annotation: non-negative) random variables}@}, with {@{$0\leq X_{n}\leq X_{n+1}$ \(a.s\) for each $n\geq 0$}@}. Furthermore, let {@{$X_{n}\to X$ pointwise}@}. Then, the monotone convergence theorem states that {@{$\lim _{n}\operatorname {E} [X_{n}]=\operatorname {E} [X]$}@}. <p> Using the monotone convergence theorem, one can show that {@{expectation indeed satisfies countable additivity for non-negative random variables}@}. In particular, let {@{$\{X_{i}\}_{i=0}^{\infty }$ be non-negative random variables}@}. It follows from the [monotone convergence theorem](monotone%20convergence%20theorem.md) that {@{$$\operatorname {E} \left[\sum _{i=0}^{\infty }X_{i}\right]=\sum _{i=0}^{\infty }\operatorname {E} [X_{i}].$$}@}
- [Fatou's lemma](Fatou's%20lemma.md): Let {@{$\{X_{n}\geq 0:n\geq 0\}$ be a sequence of non-negative random variables}@}. Fatou's lemma states that {@{$$\operatorname {E} [\liminf _{n}X_{n}]\leq \liminf _{n}\operatorname {E} [X_{n}].$$}@} <p> __Corollary.__ Let {@{$X_{n}\geq 0$ with $\operatorname {E} [X_{n}]\leq C$ for all $n\geq 0$. If $X_{n}\to X$ \(a.s\), then $\operatorname {E} [X]\leq C$}@}. <p> __Proof__ is by {@{observing that $X=\liminf _{n}X_{n}$ \(a.s.\) and applying Fatou's lemma}@}.
- [Dominated convergence theorem](dominated%20convergence%20theorem.md): Let {@{$\{X_{n}:n\geq 0\}$ be a sequence of random variables}@}. If {@{$X_{n}\to X$ [pointwise](pointwise%20convergence.md) \(a.s.\), $|X_{n}|\leq Y\leq +\infty$ \(a.s.\), and $\operatorname {E} [Y]<\infty$}@}. Then, according to the dominated convergence theorem,
  - (annotation: [dominated convergence theorem](dominated%20convergence%20theorem.md) / dominance) ::@:: $\operatorname {E} |X|\leq \operatorname {E} [Y]<\infty$; <!--SR:!2025-11-13,246,330!2025-12-21,276,330-->
  - (annotation: [dominated convergence theorem](dominated%20convergence%20theorem.md) / convergence) ::@:: $$\lim _{n}\operatorname {E} [X_{n}]=\operatorname {E} [X]$$ <!--SR:!2025-07-09,128,290!2025-11-17,249,330-->
  - (annotation: [dominated convergence theorem](dominated%20convergence%20theorem.md) / convergence of difference) ::@:: $$\lim _{n}\operatorname {E} |X_{n}-X|=0.$$ <!--SR:!2026-01-06,287,330!2025-11-19,251,330-->
- [Uniform integrability](uniform%20integrability.md): ::@:: In some cases, the equality $\lim _{n}\operatorname {E} [X_{n}]=\operatorname {E} [\lim _{n}X_{n}]$ holds when the sequence $\{X_{n}\}$ is _uniformly integrable._ <!--SR:!2025-05-13,87,270!2025-12-06,247,327-->

### relationship with characteristic function

{@{The probability density function $f_{X}$ of a scalar random variable $X$}@} is related to {@{its [characteristic function](characteristic%20function%20(probability%20theory).md) $\varphi _{X}$}@} by {@{the inversion formula: $$f_{X}(x)={\frac {1}{2\pi } }\int _{\mathbb {R} }e^{-itx}\varphi _{X}(t)\,dt.$$}@} For {@{the expected value of $g(X)$ \(where $g:{\mathbb {R} }\to {\mathbb {R} }$ is a [Borel function](measurable%20function.md)\)}@}, we can use this inversion formula to {@{obtain $$\operatorname {E} [g(X)]={\frac {1}{2\pi } }\int _{\mathbb {R} }g(x)\left[\int _{\mathbb {R} }e^{-itx}\varphi _{X}(t)\,dt\right]dx.$$}@} If {@{$\operatorname {E} [g(X)]$ is finite}@}, {@{changing the order of integration, we get, in accordance with [Fubini–Tonelli theorem](Fubini's%20theorem.md)}@}, {@{$$\operatorname {E} [g(X)]={\frac {1}{2\pi } }\int _{\mathbb {R} }G(t)\varphi _{X}(t)\,dt,$$}@} where {@{$$G(t)=\int _{\mathbb {R} }g(x)e^{-itx}\,dx$$ is the [Fourier transform](Fourier%20transform.md) of $g(x)$}@}.  The expression for $\operatorname {E} [g(X)]$ also {@{follows directly from the [Plancherel theorem](Plancherel%20theorem.md)}@}. <!--SR:!2026-03-14,344,347!2025-11-03,237,330!2025-05-21,39,190!2025-12-21,276,330!2025-07-21,125,250!2026-01-13,294,330!2025-10-29,233,330!2025-09-05,172,270!2025-10-06,182,270!2025-05-14,99,290-->

## uses and applications

- See also: [Estimation theory](estimation%20theory.md)

{@{The expectation of a random variable}@} plays {@{an important role in a variety of contexts}@}. <!--SR:!2026-01-09,290,330!2025-12-20,275,330-->

In {@{[statistics](statistics.md)}@}, where {@{one seeks [estimates](estimator.md) for unknown [parameters](statistical%20parameter.md) based on available data gained from [samples](sampling%20(statistics).md)}@}, {@{the [sample mean](sample%20mean%20and%20covariance.md)}@} serves as {@{an estimate for the expectation, and is itself a random variable}@}. In such settings, the sample mean is considered to {@{meet the desirable criterion for a "good" estimator in being [unbiased](bias%20of%20an%20estimator.md)}@}; that is, {@{the expected value of the estimate is equal to the [true value](statistical%20parameter.md) of the underlying parameter}@}. <!--SR:!2025-08-17,171,310!2025-11-24,255,330!2026-01-11,292,330!2026-01-02,285,330!2025-11-19,250,330!2025-11-17,248,330-->

For a different example, in {@{[decision theory](decision%20theory.md)}@}, {@{an agent making an optimal choice in the context of incomplete information}@} is often {@{assumed to maximize the expected value of their [utility function](expected%20utility%20hypothesis.md#von%20Neumann–Morgenstern%20utility%20theorem)}@}. <!--SR:!2025-12-29,281,330!2025-08-31,181,310!2025-12-18,272,330-->

It is possible to {@{construct an expected value equal to the probability of an event}@} by {@{taking the expectation of an [indicator function](indicator%20function.md) that is one if the event has occurred and zero otherwise}@}. This relationship can be used to {@{translate properties of expected values into properties of probabilities}@}, e.g. using {@{the [law of large numbers](law%20of%20large%20numbers.md) to justify estimating probabilities by [frequencies](frequency%20(statistics).md)}@}. <!--SR:!2025-11-23,254,330!2026-02-07,291,290!2025-09-02,183,310!2025-12-23,276,330-->

{@{The expected values of the powers of _X_}@} are called {@{the [moments](moment%20(mathematics).md) of _X_}@}; {@{the [moments about the mean](central%20moment.md) of _X_}@} are {@{expected values of powers of _X_ − E\[_X_\]}@}. {@{The moments of some random variables}@} can be {@{used to specify their distributions, via their [moment generating functions](moment-generating%20function.md)}@}. <!--SR:!2025-12-16,271,330!2025-11-13,245,330!2026-01-13,294,330!2025-12-12,267,330!2025-11-23,254,330!2025-11-03,236,330-->

To {@{empirically estimate the expected value of a random variable}@}, one {@{repeatedly measures observations of the variable and computes the [arithmetic mean](arithmetic%20mean.md) of the results}@}. If {@{the expected value exists}@}, this procedure {@{estimates the true expected value in an unbiased manner}@} and has {@{the property of minimizing the sum of the squares of the [residuals](errors%20and%20residuals.md) \(the sum of the squared differences between the observations and the estimate\)}@}. {@{The law of large numbers demonstrates \(under fairly mild conditions\)}@} that, as {@{the [size](sample%20size%20determination.md) of the sample gets larger, the [variance](variance.md) of this estimate gets smaller}@}. <!--SR:!2025-12-30,282,330!2026-01-08,290,330!2025-12-22,276,330!2026-03-07,337,347!2025-08-24,176,310!2026-01-13,294,330!2026-03-13,343,347-->

This property is often {@{exploited in a wide variety of applications}@}, including {@{general problems of [statistical estimation](estimation%20theory.md) and [machine learning](machine%20learning.md)}@}, to {@{estimate \(probabilistic\) quantities of interest via [Monte Carlo methods](Monte%20Carlo%20method.md)}@}, since {@{most quantities of interest can be written in terms of expectation}@}, e.g. {@{$\operatorname {P} ({X\in {\mathcal {A} } })=\operatorname {E} [{\mathbf {1} }_{\mathcal {A} }]$, where ${\mathbf {1} }_{\mathcal {A} }$ is the indicator function of the set ${\mathcal {A} }$.}@} <!--SR:!2026-03-15,345,347!2026-01-09,290,330!2025-06-30,123,290!2025-12-27,279,330!2025-12-21,276,330-->

> {@{![The mass of probability distribution is balanced at the expected value, here a Beta\(α,β\) distribution with expected value α/\(α+β\).](../../archives/Wikimedia%20Commons/Beta%20first%20moment.svg)}@}
>
> {@{The mass of probability distribution is balanced at the expected value}@}, here {@{a Beta\(α,β\) distribution with expected value α/\(α+β\)}@}. <!--SR:!2025-06-27,122,290!2025-12-28,281,330!2025-09-18,181,270-->

In {@{[classical mechanics](classical%20mechanics.md)}@}, {@{the [center of mass](center%20of%20mass.md) is an analogous concept to expectation}@}. For example, suppose {@{_X_ is a discrete random variable with values _x<sub>i</sub>_ and corresponding probabilities _p<sub>i</sub>_}@}. Now consider {@{a weightless rod on which are placed weights}@}, at {@{locations _x<sub>i</sub>_ along the rod and having masses _p<sub>i</sub>_ \(whose sum is one\)}@}. {@{The point at which the rod balances}@} is E\[_X_\]. <!--SR:!2025-12-18,273,330!2025-12-22,276,330!2025-11-20,251,330!2025-12-22,276,330!2026-01-01,284,330!2025-12-22,276,330-->

Expected values can also be used to {@{compute the variance}@}, by {@{means of the computational formula for the variance $$\operatorname {Var} (X)=\operatorname {E} [X^{2}]-(\operatorname {E} [X])^{2}.$$}@} <!--SR:!2025-11-05,239,330!2026-01-13,294,330-->

A very important application of the expectation value is in the field of {@{[quantum mechanics](quantum%20mechanics.md)}@}. {@{The [expectation value of a quantum mechanical operator](expectation%20value%20(quantum%20mechanics).md) ${\hat {A} }$ operating on a [quantum state](quantum%20state.md) vector $|\psi \rangle$}@} is written as {@{$\langle {\hat {A} }\rangle =\langle \psi |{\hat {A} }|\psi \rangle$}@}. {@{The [uncertainty](uncertainty%20principle.md) in ${\hat {A} }$}@} can be {@{calculated by the formula $(\Delta A)^{2}=\langle {\hat {A} }^{2}\rangle -\langle {\hat {A} }\rangle ^{2}$}@}. (annotation: this suspicously {@{looks like variance}@}...) <!--SR:!2025-12-28,280,330!2025-05-12,86,270!2026-02-21,298,290!2026-01-10,291,330!2025-07-18,123,250!2026-03-08,338,347-->

## see also

- [Central tendency](central%20tendency.md)
- [Conditional expectation](conditional%20expectation.md)
- [Expectation \(epistemic\)](expectation%20(philosophy).md)
- [Expectile](expectile.md) – ::@:: related to expectations in a way analogous to that in which quantiles are related to medians <!--SR:!2025-12-19,274,330!2025-12-25,278,330-->
- [Law of total expectation](law%20of%20total%20expectation.md) – ::@:: the expected value of the conditional expected value of _X_ given _Y_ is the same as the expected value of _X_ <!--SR:!2025-09-09,192,310!2025-09-08,191,310-->
- [Median](median.md#probability%20distributions) – ::@:: indicated by $m$ in a [drawing above](#Uhl2023Bild1) <!--SR:!2026-01-05,286,330!2026-01-13,294,330-->
- [Nonlinear expectation](nonlinear%20expectation.md) – ::@:: a generalization of the expected value <!--SR:!2025-09-04,187,310!2026-01-12,293,330-->
- [Population mean](statistical%20population.md#mean)
- [Predicted value](simple%20linear%20regression.md#variance%20of%20the%20mean%20response)
- [Wald's equation](Wald's%20equation.md) – ::@:: an equation for calculating the expected value of a random number of random variables <!--SR:!2025-11-02,236,330!2026-01-08,290,330-->

## references

This text incorporates [content](https://en.wikipedia.org/wiki/expected_value) from [Wikipedia](Wikipedia.md) available under the [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) license.

1. ["Expectation \| Mean \| Average"](https://www.probabilitycourse.com/chapter3/3_2_2_expectation.php). _<www.probabilitycourse.com>_. Retrieved 2020-09-11. <a id="^ref-1"></a>^ref-1
2. <a id="CITEREFHansen"></a> Hansen, Bruce. ["PROBABILITY AND STATISTICS FOR ECONOMISTS"](https://web.archive.org/web/20220119041716/https://ssc.wisc.edu/~bhansen/probability/Probability.pdf) \(PDF\). Archived from [the original](https://ssc.wisc.edu/~bhansen/probability/Probability.pdf) \(PDF\) on 2022-01-19. Retrieved 2021-07-20. <a id="^ref-2"></a>^ref-2
3. <a id="CITEREFWasserman2010"></a> Wasserman, Larry \(December 2010\). _All of Statistics: a concise course in statistical inference_. Springer texts in statistics. p. 47. [ISBN](ISBN.md) [9781441923226](https://en.wikipedia.org/wiki/Special:BookSources/9781441923226). <a id="^ref-3"></a>^ref-3
4. _History of Probability and Statistics and Their Applications before 1750_. Wiley Series in Probability and Statistics. 1990. [doi](digital%20object%20identifier.md):[10.1002/0471725161](https://doi.org/10.1002%2F0471725161). [ISBN](ISBN.md) [9780471725169](https://en.wikipedia.org/wiki/Special:BookSources/9780471725169). <a id="^ref-4"></a>^ref-4
5. <a id="CITEREFOre1960"></a> Ore, Oystein \(1960\). "Ore, Pascal and the Invention of Probability Theory". _The American Mathematical Monthly_. __67__ \(5\): 409–419. [doi](digital%20object%20identifier.md):[10.2307/2309286](https://doi.org/10.2307%2F2309286). [JSTOR](JSTOR.md#content) [2309286](https://www.jstor.org/stable/2309286). <a id="^ref-5"></a>^ref-5
6. <a id="CITEREFGeorge Mackey1980"></a> George Mackey \(July 1980\). "HARMONIC ANALYSIS AS THE EXPLOITATION OF SYMMETRY - A HISTORICAL SURVEY". _Bulletin of the American Mathematical Society_. New Series. __3__ \(1\): 549. <a id="^ref-6"></a>^ref-6
7. <a id="CITEREFHuygens"></a> Huygens, Christian. ["The Value of Chances in Games of Fortune. English Translation"](https://math.dartmouth.edu/~doyle/docs/huygens/huygens.pdf) \(PDF\). <a id="^ref-7"></a>^ref-7
8. <a id="CITEREFLaplace, Pierre Simon, marquis de, 1749-1827.1952"></a> Laplace, Pierre Simon, marquis de, 1749-1827. \(1952\) \[1951\]. _A philosophical essay on probabilities_. Dover Publications. [OCLC](OCLC.md#OCLC) [475539](https://search.worldcat.org/oclc/475539). <a id="^ref-8"></a>^ref-8
9. Whitworth, W.A. \(1901\) _Choice and Chance with One Thousand Exercises._ Fifth edition. Deighton Bell, Cambridge. \[Reprinted by Hafner Publishing Co., New York, 1959.\] <a id="^ref-9"></a>^ref-9
10. ["Earliest uses of symbols in probability and statistics"](http://jeff560.tripod.com/stat.html). <a id="^ref-10"></a>^ref-10
11. [Feller 1968](#CITEREFFeller1968), p. 221. <a id="^ref-11"></a>^ref-11
12. [Billingsley 1995](#CITEREFBillingsley1995), p. 76. <a id="^ref-12"></a>^ref-12
13. [Ross 2019](#CITEREFRoss2019), Section 2.4.1. <a id="^ref-13"></a>^ref-13
14. [Feller 1968](#CITEREFFeller1968), Section IX.2. <a id="^ref-14"></a>^ref-14
15. [Papoulis & Pillai 2002](#CITEREFPapoulisPillai2002), Section 5-3; [Ross 2019](#CITEREFRoss2019), Section 2.4.2. <a id="^ref-15"></a>^ref-15
16. [Feller 1971](#CITEREFFeller1971), Section I.2. <a id="^ref-16"></a>^ref-16
17. [Feller 1971](#CITEREFFeller1971), p. 5. <a id="^ref-17"></a>^ref-17
18. [Billingsley 1995](#CITEREFBillingsley1995), p. 273. <a id="^ref-18"></a>^ref-18
19. [Billingsley 1995](#CITEREFBillingsley1995), Section 15. <a id="^ref-19"></a>^ref-19
20. [Billingsley 1995](#CITEREFBillingsley1995), Theorems 31.7 and 31.8 and p. 422. <a id="^ref-20"></a>^ref-20
21. [Billingsley 1995](#CITEREFBillingsley1995), Theorem 16.13. <a id="^ref-21"></a>^ref-21
22. [Billingsley 1995](#CITEREFBillingsley1995), Theorem 16.11. <a id="^ref-22"></a>^ref-22
23. <a id="CITEREFUhl2023"></a> Uhl, Roland \(2023\). [_Charakterisierung des Erwartungswertes am Graphen der Verteilungsfunktion_](https://opus4.kobv.de/opus4-fhbrb/files/2986/Uhl2023.pdf) \[_Characterization of the expected value on the graph of the cumulative distribution function_\] \(PDF\). Technische Hochschule Brandenburg. [doi](digital%20object%20identifier.md):[10.25933/opus4-2986](https://doi.org/10.25933%2Fopus4-2986). pp. 2–4. <a id="^ref-23"></a>^ref-23
24. [Casella & Berger 2001](#CITEREFCasellaBerger2001), p. 89; [Ross 2019](#CITEREFRoss2019), Example 2.16. <a id="^ref-24"></a>^ref-24
25. [Casella & Berger 2001](#CITEREFCasellaBerger2001), Example 2.2.3; [Ross 2019](#CITEREFRoss2019), Example 2.17. <a id="^ref-25"></a>^ref-25
26. [Billingsley 1995](#CITEREFBillingsley1995), Example 21.4; [Casella & Berger 2001](#CITEREFCasellaBerger2001), p. 92; [Ross 2019](#CITEREFRoss2019), Example 2.19. <a id="^ref-26"></a>^ref-26
27. [Casella & Berger 2001](#CITEREFCasellaBerger2001), p. 97; [Ross 2019](#CITEREFRoss2019), Example 2.18. <a id="^ref-27"></a>^ref-27
28. [Casella & Berger 2001](#CITEREFCasellaBerger2001), p. 99; [Ross 2019](#CITEREFRoss2019), Example 2.20. <a id="^ref-28"></a>^ref-28
29. [Billingsley 1995](#CITEREFBillingsley1995), Example 21.3; [Casella & Berger 2001](#CITEREFCasellaBerger2001), Example 2.2.2; [Ross 2019](#CITEREFRoss2019), Example 2.21. <a id="^ref-29"></a>^ref-29
30. [Casella & Berger 2001](#CITEREFCasellaBerger2001), p. 103; [Ross 2019](#CITEREFRoss2019), Example 2.22. <a id="^ref-30"></a>^ref-30
31. [Billingsley 1995](#CITEREFBillingsley1995), Example 21.1; [Casella & Berger 2001](#CITEREFCasellaBerger2001), p. 103. <a id="^ref-31"></a>^ref-31
32. [Johnson, Kotz & Balakrishnan 1994](#CITEREFJohnsonKotzBalakrishnan1994), Chapter 20. <a id="^ref-32"></a>^ref-32
33. [Feller 1971](#CITEREFFeller1971), Section II.4. <a id="^ref-33"></a>^ref-33
34. <a id="CITEREFWeisstein"></a> Weisstein, Eric W. ["Expectation Value"](https://mathworld.wolfram.com/ExpectationValue.html). _mathworld.wolfram.com_. Retrieved 2020-09-11. <a id="^ref-34"></a>^ref-34
35. [Feller 1971](#CITEREFFeller1971), Section V.6. <a id="^ref-35"></a>^ref-35
36. [Papoulis & Pillai 2002](#CITEREFPapoulisPillai2002), Section 6-4. <a id="^ref-36"></a>^ref-36
37. [Feller 1968](#CITEREFFeller1968), Section IX.6; [Feller 1971](#CITEREFFeller1971), Section V.7; [Papoulis & Pillai 2002](#CITEREFPapoulisPillai2002), Section 5-4; [Ross 2019](#CITEREFRoss2019), Section 2.8. <a id="^ref-37"></a>^ref-37
38. [Feller 1968](#CITEREFFeller1968), Section IX.6. <a id="^ref-38"></a>^ref-38
39. [Feller 1968](#CITEREFFeller1968), Section IX.7. <a id="^ref-39"></a>^ref-39
40. [Feller 1971](#CITEREFFeller1971), Section V.8. <a id="^ref-40"></a>^ref-40
41. [Billingsley 1995](#CITEREFBillingsley1995), pp. 81, 277. <a id="^ref-41"></a>^ref-41
42. [Billingsley 1995](#CITEREFBillingsley1995), Section 19. <a id="^ref-42"></a>^ref-42

## bibliography

- <a id="CITEREFEdwards2002"></a> Edwards, A.W.F \(2002\). _Pascal's arithmetical triangle: the story of a mathematical idea_ \(2nd ed.\). JHU Press. [ISBN](ISBN.md) [0-8018-6946-3](https://en.wikipedia.org/wiki/Special:BookSources/0-8018-6946-3).
- <a id="CITEREFHuygens1657"></a> Huygens, Christiaan \(1657\). [_De ratiociniis in ludo aleæ_](http://www.york.ac.uk/depts/maths/histstat/huygens.pdf) \(English translation, published in 1714\).
- <a id="CITEREFBillingsley1995"></a> [Billingsley, Patrick](Patrick%20Billingsley.md) \(1995\). _Probability and measure_. Wiley Series in Probability and Mathematical Statistics \(Third edition of 1979 original ed.\). New York: John Wiley & Sons, Inc. [ISBN](ISBN.md) [0-471-00710-2](https://en.wikipedia.org/wiki/Special:BookSources/0-471-00710-2). [MR](Mathematical%20Reviews.md) [1324786](https://mathscinet.ams.org/mathscinet-getitem?mr=1324786).
- <a id="CITEREFCasellaBerger2001"></a> [Casella, George](George%20Casella.md); [Berger, Roger L.](Roger%20Lee%20Berger.md) \(2001\). _Statistical inference_. Duxbury Advanced Series \(Second edition of 1990 original ed.\). Pacific Grove, CA: Duxbury. [ISBN](ISBN.md) [0-534-11958-1](https://en.wikipedia.org/wiki/Special:BookSources/0-534-11958-1).
- <a id="CITEREFFeller1968"></a> [Feller, William](William%20Feller.md) \(1968\). _An introduction to probability theory and its applications. Volume I_ \(Third edition of 1950 original ed.\). New York–London–Sydney: John Wiley & Sons, Inc. [MR](Mathematical%20Reviews.md) [0228020](https://mathscinet.ams.org/mathscinet-getitem?mr=0228020).
- <a id="CITEREFFeller1971"></a> [Feller, William](William%20Feller.md) \(1971\). _An introduction to probability theory and its applications. Volume II_ \(Second edition of 1966 original ed.\). New York–London–Sydney: John Wiley & Sons, Inc. [MR](Mathematical%20Reviews.md) [0270403](https://mathscinet.ams.org/mathscinet-getitem?mr=0270403).
- <a id="CITEREFJohnsonKotzBalakrishnan1994"></a> [Johnson, Norman L.](Norman%20Johnson%20(mathematician).md); [Kotz, Samuel](Samuel%20Kotz.md); Balakrishnan, N. \(1994\). _Continuous univariate distributions. Volume 1_. Wiley Series in Probability and Mathematical Statistics \(Second edition of 1970 original ed.\). New York: John Wiley & Sons, Inc. [ISBN](ISBN.md) [0-471-58495-9](https://en.wikipedia.org/wiki/Special:BookSources/0-471-58495-9). [MR](Mathematical%20Reviews.md) [1299979](https://mathscinet.ams.org/mathscinet-getitem?mr=1299979).
- <a id="CITEREFPapoulisPillai2002"></a> [Papoulis, Athanasios](Athanasios%20Papoulis.md); [Pillai, S. Unnikrishna](Unnikrishna%20Pillai.md) \(2002\). _Probability, random variables, and stochastic processes_ \(Fourth edition of 1965 original ed.\). New York: McGraw-Hill. [ISBN](ISBN.md) [0-07-366011-6](https://en.wikipedia.org/wiki/Special:BookSources/0-07-366011-6). __\(Erratum:  [\[1\]](https://www.mhhe.com/engcs/electrical/papoulis/graphics/eratta.pdf)\)__
- <a id="CITEREFRoss2019"></a> [Ross, Sheldon M.](Sheldon%20M.%20Ross.md) \(2019\). _Introduction to probability models_ \(Twelfth edition of 1972 original ed.\). London: Academic Press. [doi](digital%20object%20identifier.md):[10.1016/C2017-0-01324-1](https://doi.org/10.1016%2FC2017-0-01324-1). [ISBN](ISBN.md) [978-0-12-814346-9](https://en.wikipedia.org/wiki/Special:BookSources/978-0-12-814346-9). [MR](Mathematical%20Reviews.md) [3931305](https://mathscinet.ams.org/mathscinet-getitem?mr=3931305).

| <!-- hide <p> - [v](https://en.wikipedia.org/wiki/Template:Theory%20of%20probability%20distributions) <br/> - [t](https://en.wikipedia.org/wiki/Template%20talk:Theory%20of%20probability%20distributions) <br/> - [e](https://en.wikipedia.org/wiki/Special:EditPage/Template%3ATheory%20of%20probability%20distributions) <p>  <p>  <br/> --> Theory of [probability distributions](probability%20distribution.md) |                                                                                                                          |
|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:| ------------------------------------------------------------------------------------------------------------------------:|
| - [probability mass function](probability%20mass%20function.md) \(pmf\) <br/> - [probability density function](probability%20density%20function.md) \(pdf\) <br/> - [cumulative distribution function](cumulative%20distribution%20function.md) \(cdf\) <br/> - [quantile function](quantile%20function.md)                                                                                                          | ![log-logistic density function plot, without labels](../../archives/Wikimedia%20Commons/Loglogisticpdf%20no-labels.svg) |
| - [raw moment](raw%20moment.md) <br/> - [central moment](central%20moment.md) <br/> - [mean](expected%20value.md) <br/> - [variance](variance.md) <br/> - [standard deviation](standard%20deviation.md) <br/> - [skewness](skewness.md) <br/> - [kurtosis](kurtosis.md) <br/> - [L-moment](L-moment.md)                                                                                                              |                                                                                                                          |
| - [moment-generating function](moment-generating%20function.md) \(mgf\) <br/> - [characteristic function](characteristic%20function%20(probability%20theory).md) <br/> - [probability-generating function](probability-generating%20function.md) \(pgf\) <br/> - [cumulant](cumulant.md) <br/> - [combinant](combinant.md)                                                                                           |                                                                                                                          |

| <!-- hide -->[Authority control databases](https://en.wikipedia.org/wiki/Help:Authority%20control) [![Edit this at Wikidata](../../archives/Wikimedia%20Commons/OOjs%20UI%20icon%20edit-ltr-progressive.svg)](https://www.wikidata.org/wiki/Q200125#identifiers) |                                                                                    |
| -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:| ---------------------------------------------------------------------------------- |
| __National__                                                                                                                                                                                                                                    | - [Germany](https://d-nb.info/gnd/4152930-3)                                       |
| __Other__                                                                                                                                                                                                                                       | - [Encyclopedia of Modern Ukraine](http://esu.com.ua/search_articles.php?id=67449) |

> [Categories](https://en.wikipedia.org/wiki/Help:Category):
>
> - [Theory of probability distributions](https://en.wikipedia.org/wiki/Category:Theory%20of%20probability%20distributions)
> - [Gambling terminology](https://en.wikipedia.org/wiki/Category:Gambling%20terminology)
