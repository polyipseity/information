---
aliases:
  - Bellman equation
  - Bellman equations
tags:
  - flashcard/active/general/eng/Bellman_equation
  - language/in/English
---

# Bellman equation

<!-- | ![](../../archives/Wikimedia%20Commons/Text%20document%20with%20red%20question%20mark.svg) | This article includes a list of [general references](https://en.wikipedia.org/wiki/Wikipedia:Citing%20sources#General%20references), but __it lacks sufficient corresponding [inline citations](https://en.wikipedia.org/wiki/Wikipedia:Citing%20sources#Inline%20citations)__. Please help to [improve](https://en.wikipedia.org/wiki/Wikipedia:WikiProject%20Reliability) this article by [introducing](https://en.wikipedia.org/wiki/Wikipedia:When%20to%20cite) more precise citations. _\(April 2018\)__\([Learn how and when to remove this message](https://en.wikipedia.org/wiki/Help:Maintenance%20template%20removal)\)_ | -->

> {@{![Bellman flow chart.](../../archives/Wikimedia%20Commons/Bellman%20flow%20chart.png)}@}
>
> {@{Bellman flow chart}@}. <!--SR:!2027-01-03,552,310!2025-11-15,263,330-->

{@{A __Bellman equation__}@}, named after {@{[Richard E. Bellman](Richard%20E.%20Bellman.md)}@}, is {@{a [necessary condition](necessity%20and%20sufficiency.md#necessity) for optimality associated with the mathematical [optimization](mathematical%20optimization.md) method known as [dynamic programming](dynamic%20programming.md)}@}.<sup>[\[1\]](#^ref-1)</sup> It writes {@{the "value" of a decision problem at a certain point in time}@} in terms of {@{the payoff from some initial choices and the "value" of the remaining decision problem that results from those initial choices}@}.<sup>[\[2\]](#^ref-2)</sup> This breaks {@{a dynamic optimization problem into a [sequence](sequence.md) of simpler subproblems}@}, as {@{Bellman's "principle of optimality" prescribes}@}.<sup>[\[3\]](#^ref-3)</sup> The equation applies to {@{algebraic structures with a total ordering}@}; for {@{algebraic structures with a partial ordering}@}, {@{the generic Bellman's equation can be used}@}.<sup>[\[4\]](#^ref-4)</sup> <!--SR:!2025-11-30,276,330!2027-08-08,738,330!2025-11-06,216,270!2025-11-16,264,330!2025-11-11,260,330!2025-11-12,261,330!2025-10-18,241,330!2025-11-25,272,330!2025-11-29,275,330!2025-10-23,246,330-->

The Bellman equation was first applied to {@{engineering [control theory](control%20theory.md) and to other topics in applied mathematics}@}, and subsequently {@{became an important tool in [economic theory](economics.md#theoretical%20research)}@}; though {@{the basic concepts of dynamic programming}@} are prefigured in {@{[John von Neumann](John%20von%20Neumann.md) and [Oskar Morgenstern](Oskar%20Morgenstern.md)'s}@} {@{_[Theory of Games and Economic Behavior](Theory%20of%20Games%20and%20Economic%20Behavior.md)_}@} and {@{[Abraham Wald](Abraham%20Wald.md)'s _[sequential analysis](sequential%20analysis.md)_}@}.<!-- <sup>\[_[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation%20needed)_\]</sup> --> {@{The term "Bellman equation"}@} usually refers to {@{the dynamic programming equation \(DPE\) associated with [discrete-time](discrete%20time%20and%20continuous%20time.md) optimization problems}@}.<sup>[\[5\]](#^ref-5)</sup> In {@{continuous-time optimization problems}@}, the analogous equation is {@{a [partial differential equation](partial%20differential%20equation.md) that is called the [Hamilton–Jacobi–Bellman equation](Hamilton–Jacobi–Bellman%20equation.md)}@}.<sup>[\[6\]](#^ref-6)</sup><sup>[\[7\]](#^ref-7)</sup> <!--SR:!2025-12-19,289,330!2025-10-25,247,330!2025-11-26,272,330!2026-03-30,342,290!2027-08-29,759,330!2027-09-04,764,330!2025-08-25,193,310!2025-11-30,276,330!2025-11-30,276,330!2025-12-07,232,270-->

In {@{discrete time any multi-stage optimization problem can be solved}@} by {@{analyzing the appropriate Bellman equation}@}. {@{The appropriate Bellman equation}@} can be found by {@{introducing new state variables \(state augmentation\)}@}.<sup>[\[8\]](#^ref-8)</sup> However, {@{the resulting augmented-state multi-stage optimization problem}@} has {@{a higher dimensional state space than the original multi-stage optimization problem}@} - an issue that {@{can potentially render the augmented problem intractable due to the "[curse of dimensionality](curse%20of%20dimensionality.md)"}@}. Alternatively, it has been shown that if {@{the cost function of the multi-stage optimization problem satisfies a "backward separable" structure}@}, then {@{the appropriate Bellman equation can be found without state augmentation}@}.<sup>[\[9\]](#^ref-9)</sup> <!--SR:!2025-12-22,292,330!2025-10-26,248,330!2025-12-07,281,330!2025-11-20,268,330!2025-08-24,192,310!2025-09-06,193,310!2025-10-22,243,330!2026-03-24,335,290!2025-11-10,259,330-->

## analytical concepts in dynamic programming

To {@{understand the Bellman equation}@}, {@{several underlying concepts must be understood}@}. First, {@{any optimization problem has some objective}@}: {@{minimizing travel time, minimizing cost, maximizing profits, maximizing utility, etc.}@} {@{The mathematical function that describes this objective}@} is {@{called the _[objective function](loss%20function.md)_}@}.<!-- <sup>\[_[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation%20needed)_\]</sup> --> <!--SR:!2025-12-18,288,330!2025-10-17,240,330!2025-11-13,262,330!2026-08-20,455,310!2025-10-27,248,330!2025-11-26,272,330-->

Dynamic programming breaks {@{a multi-period planning problem into simpler steps at different points in time}@}. Therefore, it requires {@{keeping track of how the decision situation is evolving over time}@}. {@{The information about the current situation that is needed to make a correct decision}@} is {@{called the "state"}@}.<sup>[\[10\]](#^ref-10)</sup><sup>[\[11\]](#^ref-11)</sup> For example, to {@{decide how much to consume and spend at each point in time}@}, people would need to {@{know \(among other things\) their initial wealth}@}. Therefore, {@{wealth $(W)$ would be one of their _[state variables](state%20variable.md)_}@}, but {@{there would probably be others}@}. <!--SR:!2025-09-25,204,310!2025-11-20,267,330!2025-12-23,293,330!2025-10-22,245,330!2025-11-24,271,330!2025-10-29,250,330!2025-11-01,253,330!2025-12-21,291,330-->

{@{The variables chosen at any given point in time}@} are often called {@{the _[control variables](control%20flow.md)_}@}. For instance, given {@{their current wealth}@}, people might {@{decide how much to consume now}@}. {@{Choosing the control variables now}@} may be {@{equivalent to choosing the next state}@}; more generally, {@{the next state is affected by other factors in addition to the current control}@}. For example, in the simplest case, {@{today's wealth \(the state\) and consumption \(the control\)}@} might {@{exactly determine tomorrow's wealth \(the new state\)}@}, though {@{typically other factors will affect tomorrow's wealth too}@}.<!-- <sup>\[_[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation%20needed)_\]</sup> --> <!--SR:!2025-12-01,276,330!2025-10-15,238,330!2025-09-21,200,310!2025-12-24,294,330!2025-12-18,288,330!2025-11-09,258,330!2025-11-30,276,330!2025-12-20,290,330!2025-11-15,263,330!2025-11-02,254,330-->

{@{The dynamic programming approach}@} describes {@{the optimal plan by finding a rule that tells what the controls should be}@}, given {@{any possible value of the state}@}. For example, if {@{consumption \(_c_\) depends _only_ on wealth \(_W_\)}@}, we would {@{seek a rule $c(W)$ that gives consumption as a function of wealth}@}. Such a rule, determining {@{the controls as a function of the states}@}, is called {@{a _policy function_}@}.<sup>[\[12\]](#^ref-12)</sup><sup>[\[10\]](#^ref-10)</sup> <!--SR:!2025-10-16,240,330!2025-10-09,233,330!2025-12-13,283,330!2025-10-25,246,330!2025-11-23,270,330!2025-12-21,291,330!2025-11-12,261,330-->

Finally, by definition, {@{the optimal decision rule}@} is {@{the one that achieves the best possible value of the objective}@}. For example, if {@{someone chooses consumption, given wealth, in order to maximize happiness}@} \(assuming {@{happiness _H_ can be represented by a mathematical function, such as a [utility](utility.md) function and is something defined by wealth}@}\), then {@{each level of wealth will be associated with some highest possible level of happiness, $H(W)$}@}. The {@{best possible value of the objective, written as a function of the state}@}, is called {@{the _value function_}@}.<!-- <sup>\[_[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation%20needed)_\]</sup> --> <!--SR:!2025-11-21,268,330!2025-10-27,249,330!2025-12-20,290,330!2025-09-05,192,310!2025-08-20,180,310!2025-11-29,275,330!2025-12-24,294,330-->

Bellman showed that {@{a dynamic [optimization](mathematical%20optimization.md) problem in [discrete time](discrete%20time%20and%20continuous%20time.md) can be stated in a [recursive](recursion.md), step-by-step form known as [backward induction](backward%20induction.md)}@} by {@{writing down the relationship between the value function in one period and the value function in the next period}@}. {@{The relationship between these two value functions}@} is {@{called the "Bellman equation"}@}. In this approach, {@{the optimal policy in the last time period}@} is {@{specified in advance as a function of the state variable's value at that time}@}, and {@{the resulting optimal value of the objective function}@} is thus {@{expressed in terms of that value of the state variable}@}. Next, {@{the next-to-last period's optimization}@} involves {@{maximizing the sum of that period's period-specific objective function and the optimal value of the future objective function}@}, giving {@{that period's optimal policy}@} {@{contingent upon the value of the state variable as of the next-to-last period decision}@}.<!-- <sup>\[_[clarification needed](https://en.wikipedia.org/wiki/Wikipedia:Please%20clarify)_\]</sup> --> This logic {@{continues recursively back in time}@}, until {@{the first period decision rule is derived}@}, as {@{a function of the initial state variable value}@}, by {@{optimizing the sum of the first-period-specific objective function and the value of the second period's value function, which gives the value for all the future periods}@}. Thus, {@{each period's decision is made}@} by {@{explicitly acknowledging that all future decisions will be optimally made}@}.<!-- <sup>\[_[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation%20needed)_\]</sup> --> <!--SR:!2026-09-29,483,310!2025-11-11,260,330!2025-10-10,234,330!2025-12-16,286,330!2027-07-21,726,330!2025-10-21,243,330!2025-10-12,236,330!2025-11-21,268,330!2025-11-17,265,330!2026-09-24,488,310!2025-10-17,239,330!2025-09-04,191,310!2025-12-22,292,330!2025-11-18,265,330!2025-10-10,234,330!2025-12-03,230,270!2025-08-21,189,310!2025-08-27,195,310-->

## derivation

### a dynamic decision problem

Let {@{$x_{t}$ be the state at time $t$}@}. For {@{a decision that begins at time 0}@}, we {@{take as given the initial state $x_{0}$}@}. At any time, {@{the set of possible actions depends on the current state}@}; we express this {@{as $a_{t}\in \Gamma (x_{t})$}@}, where a particular action $a_{t}$ represents {@{particular values for one or more control variables}@}, and $\Gamma (x_{t})$ is {@{the set of actions available to be taken at state $x_{t}$}@}. It is also assumed that the state {@{changes from $x$ to a new state $T(x,a)$ when action $a$ is taken}@}, and that {@{the current payoff from taking action $a$ in state $x$}@} is $F(x,a)$. Finally, we {@{assume impatience}@}, represented by {@{a [discount factor](discounting.md#discount%20factor) $0<\beta <1$}@}. <!--SR:!2025-12-15,285,330!2025-10-14,237,330!2025-10-17,241,330!2025-08-06,170,310!2025-12-16,286,330!2025-11-13,262,330!2025-11-29,275,330!2025-08-24,192,310!2027-05-20,678,330!2025-12-01,276,330!2025-11-18,266,330-->

Under these assumptions, {@{an infinite-horizon decision problem}@} takes the following form: {@{$$V(x_{0})\;=\;\max _{\left\{a_{t}\right\}_{t=0}^{\infty } }\sum _{t=0}^{\infty }\beta ^{t}F(x_{t},a_{t}),$$}@} subject to the constraints {@{$$a_{t}\in \Gamma (x_{t}),\;x_{t+1}=T(x_{t},a_{t}),\;\forall t=0,1,2,\dots$$}@} Notice that we have defined notation $V(x_{0})$ to {@{denote the optimal value that can be obtained by maximizing this objective function subject to the assumed constraints}@}. This function is {@{the _value function_}@}. It is {@{a function of the initial state variable $x_{0}$}@}, since {@{the best value obtainable depends on the initial situation}@}. <!--SR:!2025-12-07,281,330!2026-03-28,343,290!2025-10-20,200,270!2025-11-22,269,330!2025-10-24,246,330!2025-11-27,273,330!2025-10-22,244,330-->

### Bellman's principle of optimality

{@{The dynamic programming method}@} breaks {@{this decision problem into smaller subproblems}@}. {@{Bellman's _principle of optimality_}@} describes how to do this: <!--SR:!2025-11-28,274,330!2025-12-06,280,330!2025-11-28,274,330-->

{@{Principle of Optimality}@}: An optimal policy has the property that {@{whatever the initial state and initial decision are}@}, {@{the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision}@}. \(See Bellman, 1957, Chap. III.3.\)<sup>[\[10\]](#^ref-10)</sup><sup>[\[11\]](#^ref-11)</sup><sup>[\[13\]](#^ref-13)</sup> <!--SR:!2025-11-22,269,330!2025-12-11,281,330!2025-10-16,239,330-->

In {@{computer science}@}, {@{a problem that can be broken apart like this}@} is said to {@{have [optimal substructure](optimal%20substructure.md)}@}. In the context of {@{dynamic [game theory](game%20theory.md)}@}, this principle is {@{analogous to the concept of [subgame perfect equilibrium](subgame%20perfect%20equilibrium.md)}@}, although {@{what constitutes an optimal policy in this case}@} is {@{conditioned on the decision-maker's opponents choosing similarly optimal policies from their points of view}@}. <!--SR:!2025-10-13,237,330!2025-11-11,260,330!2025-08-21,189,310!2025-12-15,285,330!2025-10-18,240,330!2025-12-14,284,330!2025-10-29,251,330-->

As suggested by the _principle of optimality_, we will {@{consider the first decision separately, setting aside all future decisions}@} \(we will start {@{afresh from time 1 with the new state $x_{1}$}@}\). Collecting {@{the future decisions in brackets on the right}@}, the above infinite-horizon decision problem is equivalent to:<!-- <sup>\[_[clarification needed](https://en.wikipedia.org/wiki/Wikipedia:Please%20clarify)_\]</sup> --> {@{$$\max _{a_{0} }\left\{F(x_{0},a_{0})+\beta \left[\max _{\left\{a_{t}\right\}_{t=1}^{\infty } }\sum _{t=1}^{\infty }\beta ^{t-1}F(x_{t},a_{t}):a_{t}\in \Gamma (x_{t}),\;x_{t+1}=T(x_{t},a_{t}),\;\forall t\geq 1\right]\right\}$$}@} subject to the constraints {@{$$a_{0}\in \Gamma (x_{0}),\;x_{1}=T(x_{0},a_{0}).$$}@} Here we are {@{choosing $a_{0}$}@}, knowing that {@{our choice will cause the time 1 state to be $x_{1}=T(x_{0},a_{0})$}@}. That new state will then {@{affect the decision problem from time 1 on}@}. {@{The whole future decision problem}@} appears {@{inside the square brackets on the right}@}.<!-- <sup>\[_[clarification needed](https://en.wikipedia.org/wiki/Wikipedia:Please%20clarify)_\]</sup><sup>\[_[further explanation needed](https://en.wikipedia.org/wiki/Wikipedia:Please%20clarify)_\]</sup> --> <!--SR:!2025-11-12,261,330!2025-12-14,284,330!2025-12-23,293,330!2026-04-29,334,290!2025-11-28,274,330!2025-12-24,294,330!2025-12-23,293,330!2025-10-30,251,330!2025-11-23,270,330!2025-12-18,288,330-->

### the Bellman equation

So far it seems {@{we have only made the problem uglier by separating today's decision from future decisions}@}. But we can {@{simplify}@} by noticing that {@{what is inside the square brackets on the right is _the value_ of the time 1 decision problem, starting from state $x_{1}=T(x_{0},a_{0})$}@}. <!--SR:!2025-10-24,245,330!2025-10-31,252,330!2025-12-17,287,330-->

Therefore, the problem can be rewritten as {@{a [recursive](recursion.md) definition of the value function}@}: {@{$V(x_{0})=\max _{a_{0} }\{F(x_{0},a_{0})+\beta V(x_{1})\}$, subject to the constraints: $a_{0}\in \Gamma (x_{0}),\;x_{1}=T(x_{0},a_{0}).$}@} This is {@{the Bellman equation}@}. It may be {@{simplified even further}@} if {@{the time subscripts are dropped and the value of the next state is plugged in}@}: {@{$$V(x)=\max _{a\in \Gamma (x)}\{F(x,a)+\beta V(T(x,a))\}.$$}@} The Bellman equation is classified as {@{a [functional equation](functional%20equation.md)}@}, because {@{solving it means finding the unknown function $V$}@}, which is {@{the _value function_}@}. Recall that {@{the value function describes the best possible value of the objective}@}, as {@{a function of the state $x$}@}. By {@{calculating the value function}@}, we will also {@{find the function $a(x)$ that describes the optimal action as a function of the state}@}; this is called {@{the _policy function_}@}. <!--SR:!2025-08-26,194,310!2026-01-29,297,290!2027-06-12,696,330!2025-12-20,290,330!2027-08-25,755,330!2026-03-13,328,290!2025-08-23,191,310!2025-11-27,273,330!2025-12-22,292,330!2025-08-29,197,310!2025-10-11,235,330!2025-12-01,276,330!2025-08-22,190,310!2025-10-31,252,330-->

### in a stochastic problem

- See also: [Markov decision process](Markov%20decision%20process.md)

In {@{the deterministic setting}@}, {@{other techniques besides dynamic programming}@} can be used to {@{tackle the above [optimal control](optimal%20control.md) problem}@}. However, the Bellman Equation is often {@{the most convenient method of solving _stochastic_ optimal control problems}@}. <!--SR:!2025-10-16,239,330!2025-11-08,257,330!2025-12-05,279,330!2025-10-18,241,330-->

For {@{a specific example from economics}@}, consider {@{an infinitely-lived consumer with initial wealth endowment ${\color {Red}a_{0} }$ at period $0$}@}. They have {@{an instantaneous [utility function](utility.md#functions) $u(c)$}@} where {@{$c$ denotes consumption and discounts the next period utility at a rate of $0<\beta <1$}@}. Assume that {@{what is not consumed in period $t$ carries over to the next period with interest rate $r$}@}. Then {@{the consumer's utility maximization problem}@} is {@{to choose a consumption plan $\{ {\color {OliveGreen}c_{t} }\}$}@} that solves {@{$$\max \sum _{t=0}^{\infty }\beta ^{t}u({\color {OliveGreen}c_{t} })$$}@} subject to {@{$${\color {Red}a_{t+1} }=(1+r)({\color {Red}a_{t} }-{\color {OliveGreen}c_{t} }),\;{\color {OliveGreen}c_{t} }\geq 0,$$ and $$\lim _{t\rightarrow \infty }{\color {Red}a_{t} }\geq 0.$$}@} <!--SR:!2027-07-14,721,330!2026-08-31,462,310!2025-11-03,255,330!2025-10-21,244,330!2025-08-21,181,310!2025-11-22,269,330!2027-08-03,738,330!2025-11-25,272,330!2025-08-22,182,310-->

{@{The first constraint}@} is {@{the capital accumulation/law of motion specified by the problem}@}, while {@{the second constraint}@} is {@{a [transversality condition](transversality%20(mathematics).md) that the consumer does not carry debt at the end of their life}@}. The Bellman equation is {@{$$V(a)=\max _{0\leq c\leq a}\{u(c)+\beta V((1+r)(a-c))\},$$}@} Alternatively, one can treat the sequence problem directly using, for example, {@{the [Hamiltonian equations](Hamiltonian%20(control%20theory).md)}@}. <!--SR:!2025-10-14,238,330!2025-08-26,194,310!2025-11-20,268,330!2027-05-29,686,330!2025-10-14,213,270!2025-08-25,182,310-->

Now, if {@{the interest rate varies from period to period}@}, {@{the consumer is faced with a stochastic optimization problem}@}. Let the interest _r_ follow {@{a [Markov process](Markov%20chain.md) with probability transition function $Q(r,d\mu _{r})$}@} where {@{$d\mu _{r}$ denotes the [probability measure](probability%20measure.md) governing the distribution of interest rate next period if current interest rate is $r$}@}. In this model {@{the consumer decides their current period consumption}@} after {@{the current period interest rate is announced}@}. <!--SR:!2025-12-07,281,330!2027-08-12,742,330!2025-09-06,193,310!2026-11-15,520,310!2025-12-17,287,330!2025-12-16,286,330-->

Rather than {@{simply choosing a single sequence $\{ {\color {OliveGreen}c_{t} }\}$}@}, the consumer now must {@{choose a sequence $\{ {\color {OliveGreen}c_{t} }\}$ for each possible realization of a $\{r_{t}\}$}@} in such a way that {@{their lifetime expected utility is maximized}@}: {@{$$\max _{\left\{c_{t}\right\}_{t=0}^{\infty } }\mathbb {E} {\bigg (}\sum _{t=0}^{\infty }\beta ^{t}u({\color {OliveGreen}c_{t} }){\bigg )}.$$}@} <!--SR:!2025-11-27,273,330!2025-08-26,183,310!2025-12-12,282,330!2025-09-20,195,270-->

{@{The expectation $\mathbb {E}$}@} is taken {@{with respect to the appropriate probability measure given by _Q_ on the sequences of _r_'s}@}. Because {@{_r_ is governed by a Markov process}@}, {@{dynamic programming simplifies the problem significantly}@}. Then the Bellman equation is simply: {@{$$V(a,r)=\max _{0\leq c\leq a}\{u(c)+\beta \int V((1+r)(a-c),r')Q(r,d\mu _{r})\}.$$}@} Under {@{some reasonable assumption}@}, {@{the resulting optimal policy function _g_\(_a_,_r_\) is [measurable](measure%20(mathematics).md)}@}. <!--SR:!2025-10-17,240,330!2025-08-24,192,310!2025-12-01,276,330!2025-10-25,246,330!2026-07-16,355,250!2025-10-23,244,330!2026-06-19,373,290-->

For {@{a general stochastic sequential optimization problem with Markovian shocks}@} and where {@{the agent is faced with their decision _[ex-post](list%20of%20Latin%20phrases%20(E).md#ex%20post)_}@} (annotation: For example, the consumer {@{decides their current period consumption before the current period interest rate is announced}@}.), the Bellman equation takes a very similar form {@{$$V(x,z)=\max _{c\in \Gamma (x,z)}\{F(x,c,z)+\beta \int V(T(x,c),z')d\mu _{z}(z')\}.$$}@} (annotation: Note that {@{the notations have different meanings from above, since this is for a general optimization problem instead of the example above}@}.) <!--SR:!2025-11-14,263,330!2025-10-28,250,330!2025-11-30,276,330!2025-08-04,146,250!2025-11-30,276,330-->

## solution methods

- The [method of undetermined coefficients](method%20of%20undetermined%20coefficients.md), also known as 'guess and verify', ::@:: can be used to solve some infinite-horizon, [autonomous](autonomous%20system%20(mathematics).md) Bellman equations.<sup>[\[14\]](#^ref-14)</sup> <!--SR:!2025-10-25,247,330!2025-08-25,193,310-->
- The Bellman equation can be solved by {@{[backwards induction](backward%20induction.md)}@}, either {@{[analytically](closed-form%20expression.md) in a few special cases, or [numerically](numerical%20analysis.md) on a computer}@}. Numerical backwards induction is {@{applicable to a wide variety of problems}@}, but {@{may be infeasible when there are many state variables, due to the [curse of dimensionality](curse%20of%20dimensionality.md)}@}. {@{Approximate dynamic programming}@} has been introduced by {@{[D. P. Bertsekas](Dimitri%20Bertsekas.md) and [J. N. Tsitsiklis](John%20Tsitsiklis.md)}@} with {@{the use of [artificial neural networks](neural%20network%20(machine%20learning).md) \([multilayer perceptrons](multilayer%20perceptron.md)\)}@} for {@{approximating the Bellman function}@}.<sup>[\[15\]](#^ref-15)</sup> This is {@{an effective mitigation strategy for reducing the impact of dimensionality}@} by {@{replacing the memorization of the complete function mapping for the whole space domain with the memorization of the sole neural network parameters}@}. In particular, for {@{continuous-time systems}@}, {@{an approximate dynamic programming approach that combines both policy iterations with neural networks}@} was introduced.<sup>[\[16\]](#^ref-16)</sup> In {@{discrete-time}@}, an approach to {@{solve the HJB equation combining value iterations and neural networks}@} was introduced.<sup>[\[17\]](#^ref-17)</sup>
- By {@{calculating the first-order conditions associated with the Bellman equation}@}, and then {@{using the [envelope theorem](envelope%20theorem.md) to eliminate the derivatives of the value function}@}, it is possible to {@{obtain a system of [difference equations](recurrence%20relation.md#difference%20equation) or [differential equations](differential%20equation.md) called the '[Euler equations](Euler–Lagrange%20equation.md)'}@}.<sup>[\[18\]](#^ref-18)</sup> Standard techniques for {@{the solution of difference or differential equations}@} can then be used to {@{calculate the dynamics of the state variables and the control variables of the optimization problem}@}. <!--SR:!2025-10-20,243,330!2025-12-19,289,330!2025-12-19,289,330!2025-11-19,266,330!2025-10-15,238,330!2025-09-11,190,270!2026-07-21,429,310!2025-11-29,275,330!2025-09-06,193,310!2025-08-18,155,250!2026-08-30,469,310!2026-08-27,466,310!2025-08-23,183,310!2025-10-16,215,270!2025-11-17,265,330!2026-11-13,517,310!2026-08-22,455,310!2025-11-24,271,330!2025-10-26,247,330-->

## applications in economics

{@{The first known application of a Bellman equation}@} in economics is due to {@{[Martin Beckmann](Martin%20J.%20Beckmann.md) and [Richard Muth](Richard%20Muth.md)}@}.<sup>[\[19\]](#^ref-19)</sup> {@{Martin Beckmann}@} also {@{wrote extensively on consumption theory}@} using {@{the Bellman equation in 1959}@}. His work influenced {@{[Edmund S. Phelps](Edmund%20Phelps.md), among others}@}. <!--SR:!2025-12-24,294,330!2025-09-07,193,310!2027-06-10,695,330!2025-10-19,242,330!2026-01-24,292,290!2026-08-11,446,310-->

{@{A celebrated economic application}@} of a Bellman equation is {@{[Robert C. Merton](Robert%20C.%20Merton.md)'s seminal 1973 article}@} on {@{the [intertemporal capital asset pricing model](intertemporal%20CAPM.md)}@}.<sup>[\[20\]](#^ref-20)</sup> \(See also {@{[Merton's portfolio problem](Merton's%20portfolio%20problem.md)}@}\). {@{The solution to Merton's theoretical model, one in which investors chose between income today and future income or capital gains}@}, is a form of Bellman's equation. Because {@{economic applications of dynamic programming}@} usually {@{result in a Bellman equation that is a [difference equation](recurrence%20relation.md#difference%20equation)}@}, economists refer to {@{dynamic programming as a "recursive method"}@} and {@{a subfield of [recursive economics](recursive%20economics.md)}@} is now recognized within economics. <!--SR:!2025-11-23,270,330!2027-01-17,563,310!2026-02-27,319,290!2025-10-20,242,330!2026-09-15,481,310!2025-11-14,263,330!2025-11-10,259,330!2025-10-19,242,330!2025-12-17,287,330-->

{@{[Nancy Stokey](Nancy%20Stokey.md), [Robert E. Lucas](Robert%20Lucas%20Jr..md), and [Edward Prescott](Edward%20C.%20Prescott.md)}@} describe {@{stochastic and nonstochastic dynamic programming in considerable detail}@}, and develop {@{theorems for the existence of solutions to problems meeting certain conditions}@}. They also describe {@{many examples of modeling theoretical problems in economics using recursive methods}@}.<sup>[\[21\]](#^ref-21)</sup> This book led to {@{dynamic programming being employed to solve a wide range of theoretical problems in economics}@}, including optimal [economic growth](economic%20growth.md), [resource extraction](natural%20resource.md#extraction), [principal–agent problems](principal–agent%20problem.md), [public finance](public%20finance.md), business [investment](investment.md), [asset pricing](asset%20pricing.md), [factor](factors%20of%20production.md) supply, and [industrial organization](industrial%20organization.md). {@{[Lars Ljungqvist](Lars%20Ljungqvist.md) and [Thomas Sargent](Thomas%20J.%20Sargent.md)}@} apply {@{dynamic programming to study a variety of theoretical questions}@} in [monetary policy](monetary%20policy.md), [fiscal policy](fiscal%20policy.md), [taxation](tax.md), [economic growth](economic%20growth.md), [search theory](search%20theory.md), and [labor economics](labour%20economics.md).<sup>[\[22\]](#^ref-22)</sup> {@{[Avinash Dixit](Avinash%20Dixit.md) and [Robert Pindyck](Robert%20Pindyck.md)}@} showed the value of the method for {@{thinking about [capital budgeting](capital%20budgeting.md)}@}.<sup>[\[23\]](#^ref-23)</sup> {@{Anderson}@} adapted the technique to {@{business valuation, including privately held businesses}@}.<sup>[\[24\]](#^ref-24)</sup> <!--SR:!2026-01-24,300,290!2025-11-21,268,330!2026-11-25,528,310!2025-11-26,272,330!2025-10-30,251,330!2026-03-02,322,290!2025-10-28,249,330!2025-09-05,180,290!2027-08-11,741,330!2025-11-24,271,330!2027-01-08,556,310-->

{@{Using dynamic programming to solve concrete problems}@} is {@{complicated by informational difficulties}@}, such as {@{choosing the unobservable discount rate}@}. There are also {@{computational issues}@}, the main one being {@{the [curse of dimensionality](curse%20of%20dimensionality.md)}@} arising from {@{the vast number of possible actions and potential state variables that must be considered before an optimal strategy can be selected}@}. For {@{an extensive discussion of computational issues}@}, see Miranda and Fackler,<sup>[\[25\]](#^ref-25)</sup> and Meyn 2007.<sup>[\[26\]](#^ref-26)</sup> <!--SR:!2025-11-20,268,330!2025-11-16,264,330!2025-11-19,267,330!2025-11-19,267,330!2025-12-11,281,330!2025-08-28,196,310!2025-11-14,263,330-->

## example

In {@{[Markov decision processes](Markov%20decision%20process.md)}@}, a Bellman equation is {@{a [recursion](recursion.md) for expected rewards}@}. For example, {@{the expected reward for being in a particular state _s_ and following some fixed policy $\pi$}@} has the Bellman equation: {@{$$V^{\pi }(s)=R(s,\pi (s))+\gamma \sum _{s'}P(s'|s,\pi (s))V^{\pi }(s').\ {}$$}@} This equation describes {@{the expected reward for taking the action prescribed by some policy $\pi$}@}. {@{The equation for the optimal policy}@} is {@{referred to as the _Bellman optimality equation_}@}: {@{$$V^{\pi *}(s)=\max _{a}\left\{ {R(s,a)+\gamma \sum _{s'}P(s'|s,a)V^{\pi *}(s')}\right\}.\ {}$$}@} where {@{${\pi *}$ is the optimal policy and $V^{\pi *}$ refers to the value function of the optimal policy}@}. The equation above describes {@{the reward for taking the action giving the highest expected return}@}. <!--SR:!2027-03-23,636,330!2025-11-25,272,330!2025-12-21,291,330!2025-09-30,204,270!2025-10-29,251,330!2025-10-24,245,330!2025-10-23,245,330!2025-10-09,193,270!2027-07-22,729,330!2025-11-30,276,330-->

## see also

- [Bellman pseudospectral method](Bellman%20pseudospectral%20method.md)
- [Dynamic programming](dynamic%20programming.md) – Problem optimization method
- [Hamilton–Jacobi–Bellman equation](Hamilton–Jacobi–Bellman%20equation.md) – An optimality condition in optimal control theory
- [Markov decision process](Markov%20decision%20process.md) – Mathematical model for sequential decision making under uncertainty
- [Optimal control theory](optimal%20control.md) – Mathematical way of attaining a desired output from a dynamic system
- [Optimal substructure](optimal%20substructure.md) – Property of a computational problem
- [Recursive competitive equilibrium](recursive%20competitive%20equilibrium.md) – economic equilibrium concept associated with a dynamic program
- [Stochastic dynamic programming](stochastic%20dynamic%20programming.md) – 1957 technique for modelling problems of decision making under uncertainty

## references

This text incorporates [content](https://en.wikipedia.org/wiki/Bellman_equation) from [Wikipedia](Wikipedia.md) available under the [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) license.

1. <a id="CITEREFDixit1990"></a> Dixit, Avinash K. \(1990\). [_Optimization in Economic Theory_](https://books.google.com/books?id=dHrsHz0VocUC&pg=PA164) \(2nd ed.\). Oxford University Press. p. 164. [ISBN](ISBN.md) [0-19-877211-4](https://en.wikipedia.org/wiki/Special:BookSources/0-19-877211-4). <a id="^ref-1"></a>^ref-1
2. ["Bellman's principle of optimality"](https://www.ques10.com/p/8343/bellmans-principle-of-optimality/). _<www.ques10.com>_. Retrieved 2023-08-17. <a id="^ref-2"></a>^ref-2
3. <a id="CITEREFKirk1970"></a> Kirk, Donald E. \(1970\). [_Optimal Control Theory: An Introduction_](https://books.google.com/books?id=fCh2SAtWIdwC&pg=PA55). Prentice-Hall. p. 55. [ISBN](ISBN.md) [0-13-638098-0](https://en.wikipedia.org/wiki/Special:BookSources/0-13-638098-0). <a id="^ref-3"></a>^ref-3
4. <a id="CITEREFSzcześniakWoźna-Szcześniak2023"></a> Szcześniak, Ireneusz; Woźna-Szcześniak, Bożena \(2023\), "Generic Dijkstra: Correctness and tractability", _NOMS 2023-2023 IEEE/IFIP Network Operations and Management Symposium_, pp. 1–7, [arXiv](ArXiv.md):[2204.13547](https://arxiv.org/abs/2204.13547), [doi](digital%20object%20identifier.md):[10.1109/NOMS56928.2023.10154322](https://doi.org/10.1109%2FNOMS56928.2023.10154322), [ISBN](ISBN.md) [978-1-6654-7716-1](https://en.wikipedia.org/wiki/Special:BookSources/978-1-6654-7716-1), [S2CID](Semantic%20Scholar.md#S2CID) [248427020](https://api.semanticscholar.org/CorpusID:248427020) <a id="^ref-4"></a>^ref-4
5. [Kirk 1970](#CITEREFKirk1970), p. [70](https://books.google.com/books?id=fCh2SAtWIdwC&pg=PA70) <a id="^ref-5"></a>^ref-5
6. <a id="CITEREFKamienSchwartz1991"></a> [Kamien, Morton I.](Morton%20Kamien.md); Schwartz, Nancy L. \(1991\). [_Dynamic Optimization: The Calculus of Variations and Optimal Control in Economics and Management_](https://books.google.com/books?id=liLCAgAAQBAJ&pg=PA261) \(Second ed.\). Amsterdam: Elsevier. p. 261. [ISBN](ISBN.md) [0-444-01609-0](https://en.wikipedia.org/wiki/Special:BookSources/0-444-01609-0). <a id="^ref-6"></a>^ref-6
7. [Kirk 1970](#CITEREFKirk1970), p. [88](https://books.google.com/books?id=fCh2SAtWIdwC&pg=PA88) <a id="^ref-7"></a>^ref-7
8. <a id="CITEREFJonesPeet2020"></a> Jones, Morgan; Peet, Matthew M. \(2020\). "Extensions of the Dynamic Programming Framework: Battery Scheduling, Demand Charges, and Renewable Integration". _IEEE Transactions on Automatic Control_. __66__ \(4\): 1602–1617. [arXiv](ArXiv.md):[1812.00792](https://arxiv.org/abs/1812.00792). [doi](digital%20object%20identifier.md):[10.1109/TAC.2020.3002235](https://doi.org/10.1109%2FTAC.2020.3002235). [S2CID](Semantic%20Scholar.md#S2CID) [119622206](https://api.semanticscholar.org/CorpusID:119622206). <a id="^ref-8"></a>^ref-8
9. <a id="CITEREFJonesPeet2021"></a> Jones, Morgan; Peet, Matthew M. \(2021\). ["A Generalization of Bellman's Equation with Application to Path Planning, Obstacle Avoidance and Invariant Set Estimation"](https://www.sciencedirect.com/science/article/pii/S0005109821000303). _Automatica_. __127__: 109510. [arXiv](ArXiv.md):[2006.08175](https://arxiv.org/abs/2006.08175). [doi](digital%20object%20identifier.md):[10.1016/j.automatica.2021.109510](https://doi.org/10.1016%2Fj.automatica.2021.109510). [S2CID](Semantic%20Scholar.md#S2CID) [222350370](https://api.semanticscholar.org/CorpusID:222350370). <a id="^ref-9"></a>^ref-9
10. <a id="CITEREFBellman2003"></a> Bellman, R.E. \(2003\) \[1957\]. _Dynamic Programming_. Dover. [ISBN](ISBN.md) [0-486-42809-5](https://en.wikipedia.org/wiki/Special:BookSources/0-486-42809-5). <a id="^ref-10"></a>^ref-10
11. <a id="CITEREFDreyfus2002"></a> Dreyfus, S. \(2002\). "Richard Bellman on the birth of dynamic programming". _Operations Research_. __50__ \(1\): 48–51. [doi](digital%20object%20identifier.md):[10.1287/opre.50.1.48.17791](https://doi.org/10.1287%2Fopre.50.1.48.17791). <a id="^ref-11"></a>^ref-11
12. Bellman, 1957, Ch. III.2. <a id="^ref-12"></a>^ref-12
13. <a id="CITEREFBellman1952"></a> Bellman, R \(August 1952\). ["On the Theory of Dynamic Programming"](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1063639). _Proc Natl Acad Sci U S A_. __38__ \(8\): 716–9. [Bibcode](bibcode.md):[1952PNAS...38..716B](https://ui.adsabs.harvard.edu/abs/1952PNAS...38..716B). [doi](digital%20object%20identifier.md):[10.1073/pnas.38.8.716](https://doi.org/10.1073%2Fpnas.38.8.716). [PMC](PubMed%20Central.md#PMCID) [1063639](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1063639). [PMID](PubMed.md#PubMed%20identifier) [16589166](https://pubmed.ncbi.nlm.nih.gov/16589166). <a id="^ref-13"></a>^ref-13
14. <a id="CITEREFLjungqvistSargent2004"></a> Ljungqvist, Lars; Sargent, Thomas J. \(2004\). [_Recursive Macroeconomic Theory_](https://archive.org/details/recursivemacroec02edljun) \(2nd ed.\). MIT Press. pp. [88](https://archive.org/details/recursivemacroec02edljun/page/88)–90. [ISBN](ISBN.md) [0-262-12274-X](https://en.wikipedia.org/wiki/Special:BookSources/0-262-12274-X). <a id="^ref-14"></a>^ref-14
15. <a id="CITEREFBertsekasTsitsiklis1996"></a> Bertsekas, Dimitri P.; Tsitsiklis, John N. \(1996\). _Neuro-dynamic Programming_. Athena Scientific. [ISBN](ISBN.md) [978-1-886529-10-6](https://en.wikipedia.org/wiki/Special:BookSources/978-1-886529-10-6). <a id="^ref-15"></a>^ref-15
16. <a id="CITEREFAbu-KhalafLewis2005"></a> Abu-Khalaf, Murad; Lewis, Frank L. \(2005\). "Nearly optimal control laws for nonlinear systems with saturating actuators using a neural network HJB approach". _Automatica_. __41__ \(5\): 779–791. [doi](digital%20object%20identifier.md):[10.1016/j.automatica.2004.11.034](https://doi.org/10.1016%2Fj.automatica.2004.11.034). [S2CID](Semantic%20Scholar.md#S2CID) [14757582](https://api.semanticscholar.org/CorpusID:14757582). <a id="^ref-16"></a>^ref-16
17. <a id="CITEREFAl-TamimiLewisAbu-Khalaf2008"></a> Al-Tamimi, Asma; Lewis, Frank L.; Abu-Khalaf, Murad \(2008\). "Discrete-Time Nonlinear HJB Solution Using Approximate Dynamic Programming: Convergence Proof". _IEEE Transactions on Systems, Man, and Cybernetics - Part B: Cybernetics_. __38__ \(4\): 943–949. [doi](digital%20object%20identifier.md):[10.1109/TSMCB.2008.926614](https://doi.org/10.1109%2FTSMCB.2008.926614). [PMID](PubMed.md#PubMed%20identifier) [18632382](https://pubmed.ncbi.nlm.nih.gov/18632382). [S2CID](Semantic%20Scholar.md#S2CID) [14202785](https://api.semanticscholar.org/CorpusID:14202785). <a id="^ref-17"></a>^ref-17
18. <a id="CITEREFMiao2014"></a> Miao, Jianjun \(2014\). [_Economic Dynamics in Discrete Time_](https://books.google.com/books?id=dh2EBAAAQBAJ&pg=PA134). MIT Press. p. 134. [ISBN](ISBN.md) [978-0-262-32560-8](https://en.wikipedia.org/wiki/Special:BookSources/978-0-262-32560-8). <a id="^ref-18"></a>^ref-18
19. <a id="CITEREFBeckmannMuth1954"></a> Beckmann, Martin; Muth, Richard \(1954\). ["On the Solution to the 'Fundamental Equation' of inventory theory"](http://cowles.yale.edu/sites/default/files/files/pub/cdp/e-2116.pdf) \(PDF\). _Cowles Commission Discussion Paper 2116_. <a id="^ref-19"></a>^ref-19
20. <a id="CITEREFMerton1973"></a> Merton, Robert C. \(1973\). "An Intertemporal Capital Asset Pricing Model". _[Econometrica](econometrica.md)_. __41__ \(5\): 867–887. [doi](digital%20object%20identifier.md):[10.2307/1913811](https://doi.org/10.2307%2F1913811). [JSTOR](JSTOR.md) [1913811](https://www.jstor.org/stable/1913811). <a id="^ref-20"></a>^ref-20
21. <a id="CITEREFStokeyLucasPrescott1989"></a> Stokey, Nancy; Lucas, Robert E.; Prescott, Edward \(1989\). _Recursive Methods in Economic Dynamics_. Harvard University Press. [ISBN](ISBN.md) [0-674-75096-9](https://en.wikipedia.org/wiki/Special:BookSources/0-674-75096-9). <a id="^ref-21"></a>^ref-21
22. <a id="CITEREFLjungqvistSargent2012"></a> Ljungqvist, Lars; Sargent, Thomas \(2012\). _Recursive Macroeconomic Theory_ \(3rd ed.\). MIT Press. [ISBN](ISBN.md) [978-0-262-01874-6](https://en.wikipedia.org/wiki/Special:BookSources/978-0-262-01874-6). <a id="^ref-22"></a>^ref-22
23. <a id="CITEREFDixitPindyck1994"></a> Dixit, Avinash; Pindyck, Robert \(1994\). [_Investment Under Uncertainty_](https://archive.org/details/investmentunderu00dixi_0). Princeton University Press. [ISBN](ISBN.md) [0-691-03410-9](https://en.wikipedia.org/wiki/Special:BookSources/0-691-03410-9). <a id="^ref-23"></a>^ref-23
24. <a id="CITEREFAnderson2004"></a> Anderson, Patrick L. \(2004\). "Ch. 10". _Business Economics & Finance_. CRC Press. [ISBN](ISBN.md) [1-58488-348-0](https://en.wikipedia.org/wiki/Special:BookSources/1-58488-348-0).<a id="CITEREFAnderson2009"></a> — \(2009\). "The Value of Private Businesses in the United States". _Business Economics_. __44__ \(2\): 87–108. [doi](digital%20object%20identifier.md):[10.1057/be.2009.4](https://doi.org/10.1057%2Fbe.2009.4). [S2CID](Semantic%20Scholar.md#S2CID) [154743445](https://api.semanticscholar.org/CorpusID:154743445).<a id="CITEREFAnderson2013"></a> — \(2013\). _Economics of Business Valuation_. Stanford University Press. [ISBN](ISBN.md) [9780804758307](https://en.wikipedia.org/wiki/Special:BookSources/9780804758307). [Stanford Press](http://www.sup.org/book.cgi?id=11400) [Archived](https://web.archive.org/web/20130808132733/http://www.sup.org/book.cgi?id=11400) 2013-08-08 at the [Wayback Machine](Wayback%20Machine.md) <a id="^ref-24"></a>^ref-24
25. <a id="CITEREFMirandaFackler2004"></a> Miranda, Mario J.; Fackler, Paul L. \(2004\). [_Applied Computational Economics and Finance_](https://archive.org/details/appliedcomputati0000mira). MIT Press. [ISBN](ISBN.md) [978-0-262-29175-0](https://en.wikipedia.org/wiki/Special:BookSources/978-0-262-29175-0). <a id="^ref-25"></a>^ref-25
26. <a id="CITEREFMeyn2008"></a> Meyn, Sean \(2008\). [_Control Techniques for Complex Networks_](https://books.google.com/books?id=0OdSX2BZ4WIC). Cambridge University Press. [ISBN](ISBN.md) [978-0-521-88441-9](https://en.wikipedia.org/wiki/Special:BookSources/978-0-521-88441-9). Appendix contains abridged [Meyn & Tweedie](http://decision.csl.uiuc.edu/~meyn/pages/book.html) [Archived](https://web.archive.org/web/20071012194420/http://decision.csl.uiuc.edu/~meyn/pages/book.html) 2007-10-12 at the [Wayback Machine](Wayback%20Machine.md). <a id="^ref-26"></a>^ref-26
