---
aliases:
  - outer product
  - outer products
tags:
  - flashcard/active/general/outer_product
  - language/in/English
---

# outer product

## definition

Given {{two [column vectors](row%20and%20column%20vectors.md) $\mathbf u$ and $\mathbf v$ of size $m \times 1$ and $n \times 1$ respectively $$\mathbf u = \begin{bmatrix} u_1 \\ \vdots \\ u_m \end{bmatrix}, \quad \mathbf v = \begin{bmatrix} v_1 \\ \vdots \\ v_m \end{bmatrix}$$}}, their outer product, denoted {{$\mathbf u \otimes \mathbf v$}}, is defined as {{the $m \times n$ matrix $\mathbf A$ obtained by multiplying each element of $\mathbf u$ by the [complex conjugate](complex%20conjugate.md) of each element of $\mathbf v$}}: {{$$\mathbf u \otimes \mathbf v = \mathbf A = \begin{aligned} & \quad \begin{matrix} \phantom{u_0} v_1^* & \phantom{u_0} v_2^* & \cdots & \phantom{u_0} v_n^* \end{matrix} \\ \begin{matrix} u_1 \\ u_2 \\ \vdots \\ u_m \end{matrix} & \begin{bmatrix} u_1 v_1^* & u_1 v_2^* & \cdots & u_1 v_n^* \\ u_2 v_1^* & u_2 v_2^* & \cdots & u_2 v_n^* \\ \vdots & \vdots & \ddots & \vdots \\ u_m v_1^* & u_m v_2^* & \cdots & u_m v_n^* \end{bmatrix} \end{aligned}$$ (the variables surrounding the matrix are for visualization only)}}. Or, in index notation: {{$$(\mathbf u \otimes \mathbf v)_{ij} = \mathbf A_{ij} = u_i v_j^*$$}}. The above can also be written as {{[matrix multiplication](matrix%20multiplication.md): $$\mathbf u \otimes \mathbf v = \mathbf A = \mathbf u \mathbf v^{\mathrm H} = \mathbf u \left(\mathbf v^\intercal \right)^*$$, where $\mathbf v^{\mathrm H} = \left(\mathbf v^\intercal \right)^*$ is the [conjugate transpose](conjugate%20transpose.md) of $\mathbf v$}}. <!--SR:!2024-09-18,48,310!2024-10-05,62,310!2024-10-05,63,310!2024-10-06,63,310!2024-10-16,72,310!2024-09-24,54,310-->

If {{the vectors $\mathbf u$ and $\mathbf v$ have the same dimension and the dimension is bigger than $1 \times 1$}}, then {{$\det(\mathbf u \otimes \mathbf v) = 0$}}. <!--SR:!2024-09-12,40,290!2024-09-26,56,310-->

### contrast with Euclidean inner product

If {{$m = n$}}, then one can {{take the [matrix multiplication](matrix%20multiplication.md) the other way, yielding a [scalar](scalar%20(mathematics).md) (or $1 \times 1$ [matrix](matrix%20(mathematics).md))}}: {{$$\mathbf u \otimes \mathbf v = \mathbf u \mathbf v^{\mathrm H} \xrightarrow{\text{swap} } \mathbf v^{\mathrm H} \mathbf u = \mathbf v \cdot \mathbf u = \langle \mathbf v, \mathbf u \rangle$$}}, which is {{the standard [inner product](inner%20product%20space.md) for complex vectors, better known as the [complex dot product](dot%20product.md#complex%20vectors)}}. (Note that the inner product here is {{[antilinear](antilinear%20map.md) in the first argument, which is the convention in [physics](physics.md) and other fields but not [mathematics](mathematics.md); in most mathematical context, the inner product is [linear](linear%20map.md) in the first argument instead}}. The convention used here will be used thereafter.) <!--SR:!2024-09-29,57,310!2024-09-22,52,310!2024-09-23,50,290!2024-09-30,58,310!2024-12-30,124,310-->

Obviously, the dot product equals {{the [trace](trace%20(linear%20algebra).md) of the outer product with the order swapped: $\operatorname{tr}(\mathbf u \otimes \mathbf v) = \sum_{i = 1}^n u_i v_i^* = \mathbf v \cdot \mathbf u$}}. However, unlike the dot product, the outer product is {{not [commutative](commutative%20property.md): $\mathbf u \otimes \mathbf v \ne \mathbf v \otimes \mathbf u$ in general}}. <!--SR:!2024-11-21,86,290!2024-09-20,50,310-->

Using above, we can rewrite {{the left or right multiplication of a vector by a outer product}}. Denoting the [dot product](dot%20product.md) {{(that is [antilinear](antilinear%20map.md) first argument) by $\cdot$}}, if given {{an $n \times 1$ vector $\mathbf w$, then $$(\mathbf u \otimes \mathbf v) \mathbf w = \mathbf u \mathbf v^{\mathrm H} \mathbf w = \mathbf u (\mathbf v \cdot \mathbf w)$$}}. If instead given {{an $1 \times m$ vector $\mathbf x$, then $$\mathbf x (\mathbf u \otimes \mathbf v) = \mathbf x \mathbf u \mathbf v^{\mathrm H} = \left(\mathbf x^{\mathrm H}\right)^{\mathrm H} \mathbf u \mathbf v^{\mathrm H} = \left( \mathbf x^{\mathrm H} \cdot \mathbf u \right) \mathbf v^{\mathrm H}$$}}, where $\phantom{}^{\mathrm H}$ is the [conjugate transpose](conjugate%20transpose.md). <!--SR:!2024-09-28,56,310!2024-09-24,52,310!2024-12-24,106,290!2024-11-03,71,270-->

### connection with the matrix product

Given {{two [matrices](matrix%20(mathematics).md) $\mathbf A$ of size $m \times p$ and $\mathbf B$ of size $p \times n$}}, consider {{the [matrix product](matrix%20multiplication.md) $\mathbf C = \mathbf A \mathbf B$ defined as usual as a matrix of size $m \times n$}}. Recall that matrix multiplication, using index notation, is: {{$$\mathbf C_{ij} = \sum_{k = 1}^p \mathbf A_{ik} \mathbf B_{kj}$$}}. <!--SR:!2024-10-02,60,310!2024-10-05,63,310!2024-10-17,73,310-->

We can {{duplicate the above equation in index equation $m \times n$ times}} to get the entire matrix. Then, move {{the summation sign outside to get: $$\mathbf C = \sum_{k = 1}^p \begin{bmatrix} \mathbf A_{1k} \mathbf B_{k1} & \mathbf A_{1k} \mathbf B_{k2} & \cdots & \mathbf A_{1k} \mathbf B_{kn} \\ \mathbf A_{2k} \mathbf B_{k1} & \mathbf A_{2k} \mathbf B_{k2} & \cdots & \mathbf A_2{k} \mathbf B_{kn} \\ \vdots & \vdots & \ddots & \vdots \\ \mathbf A_{mk} \mathbf B_{k1} & \mathbf A_{mk} \mathbf B_{k2} & \cdots & \mathbf A_{mk} \mathbf B_{kn} \end{bmatrix} = \sum_{k = 1}^p \begin{bmatrix} \mathbf A_{1k} \\ \mathbf A_{2k} \\ \vdots \\ \mathbf A_{mk} \end{bmatrix} \begin{bmatrix} \mathbf B_{k1} & \mathbf B_{k2} & \cdots & \mathbf B_{kn} \end{bmatrix} = \sum_{k = 1}^p \mathbf A^{\text{col} }_k \mathbf B^{\text{row} }_k = \sum_{k = 1}^p \mathbf A^{\text{col} }_k \otimes \left(\mathbf B^{\text{row} }_k\right)^{\mathrm H}$$}}, showing that the matrix product $\mathbf C$ can be written as {{a sum of column-by-row outer products}}. This expression has {{duality with the more common expression that expresses each entry of $\mathbf C$ by row-by-column [inner products](inner%20product%20space.md) ([antilinear](antilinear%20map.md) first argument) or [dot products](dot%20product.md)}}: {{$$\mathbf C_{ij} = \sum_{i = 1}^p \mathbf A_{ip} \mathbf B_{pj} = \begin{bmatrix} \mathbf A_{i1} & \mathbf A_{i2} & \cdots & \mathbf A_{in} \end{bmatrix} \begin{bmatrix} \mathbf B_{1j} \\ \mathbf B_{2j} \\ \vdots \\ \mathbf B_{mj} \end{bmatrix} = \mathbf A^{\text{row} }_i \mathbf B^{\text{col} }_j = \left(\mathbf A^{\text{row} }_i\right)^{\mathrm H} \cdot \mathbf B^{\text{col} }_j$$}}. Compare the expressions. <!--SR:!2024-09-29,57,310!2024-11-11,76,270!2024-09-30,58,310!2024-10-03,61,310!2024-09-25,51,290-->

This relation is relevant {{in the application of [singular value decomposition](singular%20value%20decomposition.md) (SVD) (and [eigendecomposition](eigendecomposition%20of%20a%20matrix.md) as a special case)}}. In particular, the decomposition can be interpreted as {{the sum of outer products of each left $\mathbf u_k$ and right $\mathbf v_k$ singular vectors (compact SVD), scaled by the corresponding nonzero singular value $\sigma_k$ (ordered in descending values)}}: {{$$\mathbf A = \mathbf U \mathbf \Sigma \mathbf V^{\mathrm H} = \begin{bmatrix} \mathbf u_1 & \cdots & \mathbf u_{\operatorname{rank}(\mathbf A)} \end{bmatrix} \begin{bmatrix} \sigma_1 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & \sigma_{\operatorname{rank}(\mathbf A)} \end{bmatrix} \begin{bmatrix} \mathbf v_1^{\mathrm H} \\ \vdots \\ \mathbf v_{\operatorname{rank}(\mathbf A)}^{\mathrm H} \end{bmatrix} = \sum_{k = 1}^{\operatorname{rank}(\mathbf A)} \sigma_k \mathbf u_k \mathbf v_k^{\mathrm H} = \sum_{k = 1}^{\operatorname{rank}(\mathbf A)} \sigma_k (\mathbf u_k \otimes \mathbf v_k)$$}}. This results implies that {{$\mathbf A$ can be expressed as a sum of rank-1 matrices with [spectral norm](matrix%20norm.md#spectral%20norm(p%20=%202)) $\sigma_k$ in decreasing order}}. This also explains why, in general, {{the last terms contribute less}}, which {{motivates removing some last terms to get a [truncated SVD](singular%20value%20decomposition.md#truncated%20SVD) as an approximation}}. In fact, the first term is {{the least square fit of a matrix to an outer product of vectors}}. <!--SR:!2024-09-21,51,310!2024-09-13,43,290!2024-11-13,77,270!2024-09-14,44,290!2024-09-21,51,310!2024-09-19,49,310!2025-01-25,144,310-->

## references

This text incorporates [content](https://en.wikipedia.org/wiki/outer_product) from [Wikipedia](Wikipedia.md) available under the [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) license.
