---
aliases:
  - eigenvalue
  - eigenvalue and eigenvector
  - eigenvalues
  - eigenvalues and eigenvectors
  - eigenvector
  - eigenvector and eigenvalue
  - eigenvectors
  - eigenvectors and eigenvalues
tags:
  - flashcard/general/eigenvalues_and_eigenvectors
  - language/in/English
---

# eigenvalues and eigenvectors

In [linear algebra](linear%20algebra.md), it is often important to {{know which [vectors](vector%20space.md) have their directions unchanged by a given [linear transformation](linear%20map.md)}}. An {{__eigenvector__ (/ˈaɪɡən-/ EYE-gən-) or __characteristic vector__}} is such a vector. More precisely, an eigenvector $\mathbf v$ of a linear transformation $T$ is {{a nonzero vector that is [scaled by a constant factor](scalar%20multiplication.md) $\lambda$ when the linear transformation is applied to it: $T \mathbf v = \lambda \mathbf v \quad \mathbf v \ne \mathbf 0$}}. The multiplying factor $\lambda$ is {{the corresponding __eigenvalue__, __characteristic value__, or __characteristic root__}}. <!--SR:!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286-->

## definition

Consider {{a [linear transformation](linear%20map.md) $T$ and a nonzero [vector](vector%20space.md) $\mathbf v$}}. If {{applying $T$ simply scales $\mathbf v$ by a factor of $\lambda$, where $\lambda$ is a [scalar](scalar%20(mathematics).md)}}, then {{$\mathbf v$ is an eigenvector of $T$, and $\lambda$ is the corresponding eigenvalue}}. This relationship can be expressed as: {{$$T \mathbf v = \lambda \mathbf v$$}}. <!--SR:!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286-->

For {{finite-[dimensional](dimension%20(vector%20space).md) [vector spaces](vector%20space.md)}}, there is {{a direct correspondence between _n_-by-_n_ [square matrices](square%20matrix.md) and linear transformations from an [_n_-dimensional](dimension.md) vector space into itself}}, given {{any [basis](basis%20(linear%20algebra).md) of the vector space}}. Hence, it is {{equivalent to define eigenvalues and eigenvectors using either the language of [matrices](matrix%20(mathematics).md), or the language of linear transformations}}. In this case, the above equation can be rewritten as: {{$$A \mathbf u = \lambda \mathbf u$$, where $A$ is the matrix representation of $T$ and $\mathbf u$ is the [coordinate vector](coordinate%20vector.md) of $\mathbf v$}}. <!--SR:!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286-->

## overview

Eigenvalues and eigenvectors give rise to {{many closely related mathematical concepts}}, and {{the prefix _eigen-_ is applied liberally when naming them}}: <!--SR:!2024-07-11,4,286!2024-07-11,4,286-->

- __eigensystem__ of a linear transformation ::: the set of all eigenvectors of the linear transformation, each paired with its corresponding eigenvalue <!--SR:!2024-07-11,4,286!2024-07-11,4,286-->
- __eigenspace__ or __characteristic space__ of a eigenvalue of a linear transformation ::: the set of all eigenvectors of the linear transformation corresponding to the same eigenvalue, together with the zero vector <!--SR:!2024-07-11,4,286!2024-07-11,4,286-->
- __eigenbasis__ :::  the set of eigenvectors of a linear transformation that also forms a [basis](basis%20(linear%20algebra.md)) of the domain of the same linear transformation <!--SR:!2024-07-10,3,266!2024-07-11,4,286-->

## eigenvalues and eigenvectors of matrices

Eigenvalues and eigenvectors are often introduced to students in {{the context of [linear algebra](linear%20algebra.md) courses focused on [matrices](matrix%20(mathematics).md)}}. Furthermore, linear transformations over {{a finite-[dimensional](dimnsion%20(vector%20space).md) [vector space](vector%20space.md)}} can be represented {{using matrices, which is especially common in numerical and computational applications}}. <!--SR:!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286-->

Consider _n_-dimensional vectors formed of {{a vertical list of _n_ scalars}}, such as the 2-dimensional vectors $$\mathbf x = \begin{bmatrix} 10 \\ -20 \end{bmatrix} \quad \text{ and } \quad \mathbf y = \begin{bmatrix} -1 \\ 2 \end{bmatrix}$$. These vectors are {{[scalar multiples](scalar%20multiplication.md) of each other, also called [parallel](parallel%20(geometry).md) or [collinear](collinearity.md)}}, because {{there is a [scalar](scalar%20(mathematics).md) $\lambda$ such that $$\mathbf x = \lambda \mathbf y$$}}. In this case, {{$\lambda = -10$}}. <!--SR:!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286-->

Now consider the linear transformation of _n_-dimensional vectors defined by {{an _n_-by-_n_ matrix $A$}}, such as {{the scaling-by-negative-10 matrix}}: {{$$A = \begin{bmatrix} -10 & 0 \\ 0 & -10 \end{bmatrix}$$}}. Applying the linear transformation on a nonzero vector $\mathbf v$ to make a new vector $\mathbf w$ is represented by {{$$A \mathbf v = \mathbf w$$ or $$\begin{bmatrix} A_{11} & A_{12} & \cdots & A_{1n} \\ A_{21} & A_{22} & \cdots & A_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ A_{n1} & A_{n2} & \cdots & A_{nn} \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix} = \begin{bmatrix} w_1 \\ w_2 \\ \vdots \\ w_n \end{bmatrix}$$}}, where for each row of $\mathbf w$: {{$$w_i = A_{i1} v_1 + A_{i2} v_2 + \cdots + A_{in} v_n = \sum_{k = 1}^n A_{ik} v_k$$}}. This operation is known as {{[matrix multiplication](matrix%20(mathematics).md#matrix%20multiplication)}}. For example, applying $A$ on $\mathbf y = \begin{bmatrix} -1 \\ 2 \end{bmatrix}$ is: {{$$\begin{aligned} A \mathbf y & = \mathbf w \\ \begin{bmatrix} -10 & 0 \\ 0 & -10 \end{bmatrix} \begin{bmatrix} -1 \\ 2 \end{bmatrix} & = \begin{bmatrix} 10 \\ -20 \end{bmatrix} \end{aligned}$$}}, so the result is {{$\mathbf w = \begin{bmatrix} 10 \\ - 20 \end{bmatrix} = \mathbf x$}}. <!--SR:!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286-->

If it happens that the original {{nonzero}} $\mathbf v$ and the resulting $\mathbf w$ are {{scalar multiples}}, that is: {{$$A \mathbf{v} = \mathbf{w} = \lambda \mathbf{v} \quad \mathbf{v} \ne \mathbf{0}$$}}, then {{$\mathbf v$ is the __eigenvector__ of the linear transformation $A$ and the scale factor $\lambda$ is the __eigenvalue__ corresponding to that eigenvector}}. The above equation is {{the __eigenvalue equation__ for the linear transformation $A$}}. For the example above after applying $A$ on $\mathbf y$ to get $\mathbf x = -10 \mathbf y$, {{$\mathbf y$ is an eigenvector of the matrix $A$ and $\lambda = -10$ is the corresponding eigenvalue}}. <!--SR:!2024-07-10,3,266!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286-->

The eigenvalue equation can also be equivalently stated as {{$$(A - \lambda I) \mathbf v = \mathbf 0 \quad \text{ or } \quad (\lambda I - A) \mathbf v = \mathbf 0$$}}, where {{$I$ is the _n_-by-_n_ [identity matrix](identity%20matrix.md) and $\mathbf 0$ is the zero vector}}. <!--SR:!2024-07-11,4,286!2024-07-11,4,286-->

From the eigenvector equation, given a eigenvector with its corresponding eigenvalue, a new eigenvector {{associated with the same eigenvalue can be obtained by scaling the original eigenvector by an arbitrary nonzero scalar}}. The resulting eigenvector still respects the eigenvector equation as {{this is equivalent to multiplying both sides by the nonzero scalar}}. <!--SR:!2024-07-11,4,286!2024-07-11,4,286-->

### eigenvalues and the characteristic polynomial

- see: [characteristic polynomial](charateristic%20polynomial.md)

The rewritten eigenvalue equation with $\lambda I$ in front: {{$$(\lambda I - A) \mathbf v = \mathbf 0$$}} has {{a nonzero vector solution $\mathbf v$ [iff](if%20and%20only%20if.md) the [determinant](determinant.md) of the matrix $(\lambda I - A)$ is zero}}. Therefore, the eigenvalues $\lambda$ are {{that satisfy the equation $\det(\lambda I - A) = 0$}}. This equation is called {{the __characteristic equation__ or the __secular equation__ of $A$}}. <!--SR:!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286!2024-07-10,3,266-->

Using {{the [Leibniz formula for determinants](Leibniz%20formula%20for%20determinants.md)}}, the left-hand side of equation is {{a [polynomial](polynomial.md) function of the variable $\lambda$ and the [degree](degree%20of%20a%20polynomial.md) of this polynomial is $n$, the order of the matrix $A$}}. Its coefficients depend on {{the entries of $A$, except that its term of degree of $n$ is always $\lambda^n$, making the polynomial a [monic polynomial](monic%20polynomial.md)}}. If {{the eigenvalue equation with $A$ in front is used instead}}, then {{its term of degree of $n$ is always $(-1)^n \lambda^n$, making it monic only when $n$ is even}}. This polynomial is called {{the _characteristic polynomial_ of $A$}}. {{The monic polynomial}} will be used in the rest of this article. <!--SR:!2024-07-11,4,286!2024-07-10,3,266!2024-07-11,4,286!2024-07-11,4,286!2024-07-10,3,266!2024-07-10,3,266!2024-07-11,4,286-->

{{The [fundamental theorem of algebra](fundamental%20theorem%20of%20algebra.md)}} implies that the characteristic polynomial of an _n_-by-_n_ [matrix](matrix%20(mathematics).md) $A$, being {{a (monic) polynomial of [degree](degree%20of%20a%20polynomial.md) _n_}}, can {{be [factored](factorization.md) into the product of _n_ linear terms}}: {{$$\det(\lambda I - A) = (\lambda - \lambda_1) (\lambda - \lambda_2) \cdots (\lambda_n - \lambda) = \prod_{i = 1}^n (\lambda - \lambda_i)$$}}, where {{each $\lambda_i$ may be [real](real%20number.md) but in general is a [complex number](complex%20number.md)}}. The scalars $\lambda_1, \lambda_2, \ldots, \lambda_n$ are {{the roots of the polynomial and all also eigenvalues of $A$}}. The eigenvalues may or may not {{be zero, or be all distinct}}. {{Plugging the eigenvalues back into the eigenvalue equation and solving them}} gives {{the corresponding eigenvectors}}. <!--SR:!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286!2024-07-10,3,266!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286!2024-07-10,3,266!2024-07-11,4,304-->

If {{the entries of matrix $A$ are all [real numbers](real%20number.md)}}, then {{the coefficients of the characteristic polynomial are also all real numbers, but the eigenvalues may still have nonzero imaginary parts. Thus, the entires of the corresponding eigenvectors may also have nonzero imaginary parts}}. Similarly, {{eigenvalues and entries of their corresponding eigenvectors may be [irrational numbers](irrational%20number.md)}} even if {{all entries of $A$ are [rational numbers](rational%20number.md) or even [integers](integer.md)}}. However, if {{the entries of $A$ are all [algebraic numbers](algebraic%20number.md), which include the rationals}}, {{the eigenvalues and entries of their corresponding eigenvectors must also be algebraic numbers}}. <!--SR:!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286-->

The non-real roots of a real [polynomial](polynomial.md) with real coefficients can be {{grouped into pairs of [complex conjugates](complex%20conjugate.md), which are two [complex numbers](complex%20number.md) having the same real part and imaginary parts of opposing sign}}. The eigenvectors associated with these complex eigenvalues are {{also complex and also appear in complex conjugate pairs}}. If {{the [degree](degree%20of%20a%20polynomial.md) is odd}}, then {{by the [intermediate value theorem](intermediate%20value%20theorem.md) (an odd-degree polynomial goes to infinity as the variable goes to infinity, but with opposing sign comparing the variable going to negative infinity and positive infinity), at least one of the roots is real}}. Therefore, {{any real matrix with odd order has at least one real eigenvalue, whereas a real matrix with even order may not have any real eigenvalues}}. <!--SR:!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286!2024-07-10,3,266!2024-07-11,4,286-->

### algebraic multiplicity

Let $\lambda_i$ be an eigenvalue of an _n_-by-_n_ matrix $A$. The __algebraic multiplicity__ {{$\mu_A(λ_i)$ of the eigenvalue}} is {{its [multiplicity as a root](multiplicity%20(mathematics).md#multiplicity%20of%20a%20root%20of%20a%20polynomial) of the characteristic polynomial}}, that is, {{the largest integer $k$ such that $(\lambda − \lambda_i)^k$ [divides evenly](polynomial%20long%20division.md) that polynomial}}. <!--SR:!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286-->

Suppose in the $n$ eigenvalues of an _n_-by-_n_ matrix, there are {{$d$ distinct eigenvalues with $1 \le d \le n$}}. The characteristic polynomial factorized into $n$ linear terms above may have {{some terms potentially repeating}}. Instead, said polynomial can also be written as {{the product of $d$ distinct terms each corresponding to a distinct eigenvalue and raised to the power of algebraic multiplicity}}: {{$$\det(\lambda I - A) = (\lambda - \lambda_1)^{\mu_A(\lambda_1)} (\lambda - \lambda_2)^{\mu_A(\lambda_2)} \cdots (\lambda - \lambda_d)^{\mu_A(\lambda_d)}$$}}. <!--SR:!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286-->

If {{$d = n$}} then {{the right hand side is a product of $n$ distinct terms and is the same as the polynomial above}}. The size of each eigenvalue's algebraic multiplicity respect the following relations: {{$$\begin{aligned} 1 & \le \mu_A(\lambda_i) \le n \\ \mu_A & = \sum_{i = 1}^d \mu_A(\lambda_i) = n \end{aligned}$$}}. <!--SR:!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286-->

If {{$\mu_A(\lambda_i) = 1$}}, then $\lambda_i$ is {{said to be a _simple eigenvalue_}}. If {{$\mu_A(\lambda_i)$ equals the geometric multiplicity of $\lambda_i$, $\gamma_A(\lambda_i)$}}, defined in upcoming sections, then $\lambda_i$ is {{said to be a _semisimple eigenvalue_}}. <!--SR:!2024-07-11,4,286!2024-07-10,3,266!2024-07-11,4,286!2024-07-11,4,286-->

### eigenspaces

Given {{a particular eigenvalue $\lambda$ of the _n_-by-_n_ matrix $A$}}, define the set $E$ to be {{all vectors $\mathbf v$ that satisfy the eigenvalue equation}}: {{$$E = \set{ \mathbf v : (\lambda I - A) \mathbf v = \mathbf 0 }$$}}. On one hand, this is {{precisely the [kernel](kernel%20(linear%20algebra).md) or nullspace of the matrix $(\lambda I - A)$}}. On the other hand, {{by definition, any _nonzero_ vector that satisfies the eigenvalue equation is an eigenvector of $A$ associated with $\lambda$}}. So, the set $E$ is {{the [union](union%20(set%20theory).md) of the zero vector and the set of all eigenvectors of $A$ associated with $\lambda$}}, and also equals {{the nullspace of the matrix $(\lambda I - A)$}}. $E$ is called {{the __eigenspace__ or __characteristic space__ of $A$ associated with eigenvalue $\lambda$}}. In general, $\lambda$ is {{a [complex number](complex%20number.md) and the eigenvectors are complex $n$ by 1 matrices}}. An important property of the nullspace is that {{it is a [linear subspace](linear%20subspace.md), so $E$ is a linear subspace of $\mathbb C^n$}}. <!--SR:!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286!2024-07-10,3,266!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286!2024-07-10,3,266!2024-07-11,4,286-->

Because the eigenspace $E$ is {{a [linear subspace](linear%20subspace.md)}}, it is {{[closed](closure%20(mathematics).md) under [addition](addition.md) and [scalar multiplication](scalar%20multiplication.md)}}. That is, if {{two arbitrary vectors $\mathbf u$ and $\mathbf v$ belong to the set $E$ and $\alpha$ is an arbitrary [complex](complex%20number.md)}}, written {{$\mathbf u, \mathbf v \in E, \alpha \in \mathbb C$}}, then {{$(\alpha \mathbf u + \mathbf v) \in E$ or equivalently $A(\alpha \mathbf u + \mathbf v) = \lambda (\alpha \mathbf u + \mathbf v)$}}. This can be checked {{using the [distributive property](distributive%20property.md) of matrix multiplication (for addition) and noting that multiplication of complex matrices by complex numbers is [commutative](commutative%20property.md) (for scalar multiplication)}}. As long as {{the resulting vector $(\alpha \mathbf u + \mathbf v)$ is not the zero vector $\mathbf 0$ (the zero vector belongs to $E$, however)}}, then it is also {{an eigenvector of $A$ associated with $\lambda$}}. <!--SR:!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286-->

### geometric multiplicity

The __geometric multiplicity__ {{$\gamma_A(\lambda)$ of an eigenvalue $\lambda$}} is {{the [dimension](dimension%20(vector%20space).md) of the eigenspace $E$ associated with $\lambda$}}, or equivalently {{the maximum number of [linearly independent](linear%20independence.md) eigenvectors associated with $\lambda$}}. Because {{$E$ is also the nullspace of $(\lambda I - A)$}}, {{the geometric multiplicity is the dimension of nullspace of $(\lambda I - A)$}}, also called {{the _nullity_ of $(\lambda I - A)$}}, which {{relates to the dimension and rank of $(\lambda I - A)$ as}}: {{$$\gamma_A(\lambda) = n - \operatorname{rank}(\lambda I - A)$$}}. <!--SR:!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286!2024-07-10,3,266!2024-07-11,4,286!2024-07-11,4,286!2024-07-11,4,286-->

By {{the definition of eigenvalues and eigenvectors}}, each eigenvalue has {{at least one nonzero associated eigenvector}}, so an eigenvalue's geometric multiplicity must be {{at least one}}. Furthermore, an eigenvalue's geometric multiplicity {{cannot exceed its [algebraic multiplicity](#algebraic%20multiplicity)}}. So the following inequality holds: {{$$1 \le \gamma_A(\lambda) \le \mu_A(\lambda) \le n$$}}. <!--SR:!2024-07-11,4,304!2024-07-11,4,304!2024-07-11,4,304!2024-07-11,4,304!2024-07-11,4,304-->

To prove the inequality $\gamma_A(\lambda) \le \mu_A(\lambda)$, the idea is {{transforming $A$ into another matrix $B$ such that $B$ has the same determinant (and thus eigenvalues) as $A$ and $\det(\xi I - B)$ has the linear term $(\xi - \lambda)^{\gamma_A(\lambda)}$}}. We start with {{the definition of geometric multiplicity}}, which {{implies the existence of $\gamma_A(\lambda)$ [orthonormal](orthonormality.md) eigenvectors $\mathbf v_1, \ldots, \mathbf v_{\gamma_A(\lambda)}$ such that $A \mathbf v_i = \lambda \mathbf v_i$}}. We can find {{a matrix $V$ those first $\gamma_A(\lambda)$ columns are these eigenvectors, and the remaining columns can be any set of orthonormal set of $n - \gamma_A(\lambda)$ vectors $\mathbf u_{\gamma_A(\lambda) + 1}, \ldots, \mathbf u_n$ orthogonal to these eigenvectors: $$V = \begin{bmatrix} \mathbf v_1 & \cdots & \mathbf v_{\gamma_A(\lambda)} & \mathbf u_{\gamma_A(\lambda) + 1} & \cdots & \mathbf u_n \end{bmatrix}$$}}. The matrix has {{full [rank](rank%20(linear%20algebra).md) due to linearly independent column vectors, thus is invertible}}. The matrix is also {{unitary, that is, its [conjugate transpose](conjugate%20transpose.md) is its [matrix inverse](invertible%20matrix.md): $V^* V = V V^* = I$}}. This is obvious because {{the column vectors are unitary, and orthogonal to all column vectors except for itself}}. Using $V$, we can {{(partially) [diagonalize](eigendecomposition%20of%20a%20matrix.md) $A$ to produce $B$: $B := V^{-1} A V = V^* A V$, which preserves the determinant as they are [similar](matrix%20similarity.md): $\det(B) = \det\left(V^{-1} A V\right) = \det(A)$}}. We can also {{calculate some entries of $B$}} given the above facts. Consider {{applying $A$ on $V$, i.e. $A V$}}. This transforms $V$ {{by scaling the first $\gamma_A(\lambda)$ columns by the eigenvalue $\lambda$. The rest of the columns are unknowns}}; mathematically, that is, {{$$A V = \begin{bmatrix} \lambda \mathbf v_1 & \cdots & \lambda \mathbf v_{\gamma_A(\lambda)} & \mathbf u'_{\gamma_A(\lambda) + 1} & \cdots & \mathbf u'_n \end{bmatrix}$$}}. Now consider {{applying $V^{-1} = V^*$ on $A V$, i..e $V^* A V$}}. This transforms {{the most top-left $\lambda_{\gamma_A(\lambda)}$-sized block into an identity matrix multiplied by the eigenvalue. The remaining rows of the first $\lambda_{\gamma_A(\lambda)}$ columns are all zero}}. This is because {{the first $\lambda_{\gamma_A(\lambda)}$ column vectors are orthogonal to all row vectors in $V^*$ except for itself}}. The rest of {{the columns are unknown as the corresponding columns of $A V$ are also unknown}}. Mathematically, this is {{$$B = V^* A V = \begin{bmatrix} \lambda I_{\gamma_A(\lambda)} & U''_{\text{top} } \\ \mathbf 0 & U''_{\text{bottom} } \end{bmatrix}$$ (see [block matrix](block%20matrix.md))}}. Now, we show that {{($\xi I - B$) has the properties we desired}}. To {{prove $\det(\xi I - B) = \det(\xi I - A)$}}, note that {{$$\det(\xi I - A) = \det\left(V^* (\xi I - A) V \right) = \det\left(V^* (\xi I - A) V \right) = \det\left(V^* (\xi I) V - V^* A V \right) = \det(\xi I - B)$$. The first equality is by [similarity]((matrix%20similarity.md))}}. Second, to {{prove $\det(\xi I - B)$ has the linear term $(\xi - \lambda)^{\gamma_A(\lambda)}$}}, we note that {{the [block matrix](block%20matrix.md) above is in the form of $\begin{bmatrix} A & B \\ \mathbf 0 & D \end{bmatrix}$, which has the determinant $\det(A) \det(D)$. Therefore, $$\det(\xi I - B) = \det(\xi I_{\gamma_A(\lambda)} - \lambda I_{\gamma_A(\lambda)}) \det(\xi I - U''_{\text{bottom} }) = (\xi - \lambda)^{\gamma_A(\lambda)} \det(\xi I - U''_{\text{bottom} })$$}}. Thus, we have shown that $(\xi I - B)$ has the desired properties. This implies {{$\det(\xi I - A)$ must have the linear term $(\xi - \lambda)^{\gamma_A(\lambda)}$, with more $(\xi - \lambda)$ terms _possibly_ coming from $\det(\xi I - U''_{\text{bottom} })$}}. Therefore, {{the algebraic multiplicity of $\lambda$ is at least the geometric multiplicity of $\lambda$, i.e. $\mu_A(\lambda) \ge \gamma_A(\lambda)$}}. <!--SR:!2024-07-10,3,284!2024-07-11,4,304!2024-07-11,4,304!2024-07-11,4,304!2024-07-11,4,304!2024-07-11,4,304!2024-07-11,4,304!2024-07-11,4,304!2024-07-12,4,284!2024-07-11,4,304!2024-07-11,4,304!2024-07-11,4,304!2024-07-11,4,304!2024-07-10,3,284!2024-07-10,3,284!2024-07-11,4,304!2024-07-11,4,304!2024-07-11,4,304!2024-07-11,4,304!2024-07-10,3,284!2024-07-11,4,304!2024-07-11,4,304!2024-07-11,4,304!2024-07-10,3,284-->

Suppose $A$ has $d \le n$ distinct eigenvalues $\lambda_1, \ldots, \lambda_d$, where the geometric multiplicity of $\lambda_i$ is $\gamma_A(\lambda_i)$. The total geometric multiplicity of $A$ is {{the dimension of the [sum](linear%20subspace.md#sum) of all the eigenspaces of $A$'s eigenvalues}}, or equivalently {{the maximum number of [linearly independent](linear%20independence.md) eigenvectors of $A$}}. It respects the following relations: {{$$\begin{aligned} \gamma_A & = \sum_{i = 1}^d \gamma_A(\lambda_i) \\ d & \le \gamma_A \le n \end{aligned}$$}}. <!--SR:!2024-07-11,4,304!2024-07-11,4,304!2024-07-11,4,304-->

### eigenbasis for matrices

If {{$\gamma_A = n$}}, then <!--SR:!2024-07-11,4,304-->

- (if $\gamma_A = n$) [direct sum](direct%20sum.md) of the eigenspaces of all of $A$'s eigenvalues ::: The [direct sum](direct%20sum.md) of the eigenspaces of all of $A$'s eigenvalues is the entire vector space $\mathbb C^n$. <!--SR:!2024-07-11,4,304!2024-07-11,4,304-->
- (if $\gamma_A = n$) $n$ [linearly independent](linear%20independence.md) eigenvectors of $A$ ::: A [basis](basis%20(linear%20algebra).md) of $\mathbb C^n$ can be formed from $n$ [linearly independent](linear%20independence.md) eigenvectors of $A$; such a basis is called an __eigenbasis__. <!--SR:!2024-07-11,4,304!2024-07-11,4,304-->
- (if $\gamma_A = n$) [linear combination](linear%20combination.md) of eigenvectors of $A$ ::: Any vectors in $\mathbb C^n$ can be written as a [linear combination](linear%20combination.md) of eigenvectors of $A$. <!--SR:!2024-07-11,4,304!2024-07-11,4,304-->

### additional properties of eigenvalues

Let $A$ be an arbitrary $n \times n$ [square matrix](square%20matrix.md) of [complex numbers](complex%20number.md) with {{eigenvalues $\lambda_1, \ldots, \lambda_n$}}. $\mu_A(\lambda_i)$ and $\gamma_A(\lambda_i)$ are {{resp. algebraic multiplicity and geometric multiplicity of the eigenvalue $\lambda_i$}}. Each distinct eigenvalue appears {{$\mu_A(\lambda_i)$ times in this list}}. The following are properties of this matrix and its eigenvalues: <!--SR:!2024-07-11,4,304!2024-07-11,4,304!2024-07-11,4,304-->

- [trace](trace%20(linear%20algebra).md) of $A$, $\operatorname{tr}(A)$ ::: The trace of $A$, $\operatorname{tr}(A)$, defined as the sum of its diagonal elements, is also the sum of all eigenvalues: $$\operatorname{tr}(A) = \sum_{i = 1}^n a_{ii} = \sum_{i = 1}^n \lambda_i = \lambda_1 + \lambda_2 + \cdots + \lambda_n$$. <!--SR:!2024-07-11,4,304!2024-07-11,4,304-->
- [determinant](determinant.md) of $A$, $\det(A)$ ::: The determinant of $A$, $\det(A)$, is the product of all eigenvalues: $$\det(A) = \prod_{i = 1}^n \lambda_i = \lambda_1 \lambda_2 \cdots \lambda_n$$. <!--SR:!2024-07-11,4,304!2024-07-11,4,304-->
- eigenvalues of the $k$-th power of $A$, $A^k$ ::: The eigenvalues of the $k$-th power of $A$; i.e. the eigenvalues of $A^k$, for any positive integer $k$ (or including $k = 0$ if $0^0$ is defined as $1$), are $\lambda_1^k, \ldots, \lambda_n^k$. <!--SR:!2024-07-11,4,304!2024-07-11,4,304-->
- [invertibility](invertible%20matrix.md) of $A$ ::: The matrix $A$ is [invertible](invertible%20matrix.md) [iff](if%20and%20only%20if.md) every eigenvalue is nonzero. This follows from that the determinant is the product of all eigenvalues. <!--SR:!2024-07-11,4,304!2024-07-10,3,284-->
- [inverse](invertible%20matrix.md) of $A$, $A^{-1}$ ::: If $A$ is [invertible](invertible%20matrix.md), then the eigenvalues of $A^{-1}$ are $\frac 1 {\lambda_1}, \ldots, \frac 1 {\lambda_n}$. Each eigenvalue's geometric multiplicity coincides, i.e. $\gamma_A(\lambda_i) = \gamma_{A^{-1} }(1 / \lambda_i)$. Moreover, from above (trivially) and from that the the characteristic polynomial of the inverse is the [reciprocal polynomial](reciprocal%20polynomial.md) of the original, the eigenvalues share the same algebraic multiplicity, i.e. $\mu_A(\lambda_i) = \mu_{A^{-1} }(1 / \lambda_i)$. <!--SR:!2024-07-11,4,304!2024-07-11,4,304-->
- [Hermitian matrix](Hermitian%20matrix.md) ::: If $A$ equals to its conjugate transpose $A^*$, or equivalently if $A$ is [Hermitian](Hermitian%20matrix.md), then every eigenvalue is real. The same is true of any [symmetric](symmetric%20matrix.md) real matrix, as a subset of Hermitian matrices. <!--SR:!2024-07-10,3,284!2024-07-11,4,304-->
- [Hermitian matrix](Hermitian%20matrix.md) properties ::: If $A$ is not only [Hermitian](Hermitian%20matrix.md) but also positive-definite, positive-semidefinite, negative-definite, or negative-semidefinite, then every eigenvalue is positive, non-negative, negative, or non-positive, respectively. <!--SR:!2024-07-11,4,304!2024-07-11,4,304-->
- [unitary matrix](unitary%20matrix.md) ::: If $A$'s [inverse](invertible%20matrix.md) $A^{-1}$ equals to its conjugate transpose $A^*$, or equivalently if $A$ is [unitary](unitary%20matrix.md), then every eigenvalue has absolute value $\lvert \lambda_i \rvert = 1$. <!--SR:!2024-07-11,4,304!2024-07-11,4,304-->
- [matrix polynomial](matrix%20polynomial.md) ::: In general, for a [matrix polynomial](matrix%20polynomial.md) $P$, which are [polynomials](polynomial.md) with [square matrices](square%20matrix.md) as variables, then eigenvalues of matrix $P(A)$ are $p(\lambda_1), \ldots, p(\lambda_n)$, where $p$ is the same polynomial as $P$ but accepts and outputs a [complex number](complex%20number.md) instead of a square matrix. For example, for matrix $P(A) = I + A$, its eigenvalues are $1 + \lambda_1, \ldots, 1 + \lambda_n$. Another example is matrix $P(A) = \alpha I + A$ for $\alpha \in \mathbb C$, for which its eigenvalues are $\alpha + \lambda_1, \ldots, \alpha + \lambda_n$. An even more general example is matrix $P(A) = \alpha I + \beta A + A^2$ for $\alpha, \beta \in \mathbb C$, for which its eigenvalues are $\alpha + \beta \lambda_1 + \lambda_1^2, \ldots, \alpha + \beta \lambda_n + \lambda_n^2$. <!--SR:!2024-07-11,4,304!2024-07-10,3,284-->

### left and right eigenvectors

- see: [left and right (linear algebra)](left%20and%20right%20(linear%20algebra).md)

Many disciplines traditionally represent vectors as {{[matrices](matrix%20(mathematics).md) with a single [column](row%20and%20column%20vectors.md) (vertical) rather than as matrices with a single [row](row%20and%20column%20vectors.md) (horizontal)}}. For that reason, the word "eigenvector" in the context of matrices {{almost always refers to a __right eigenvector__}}, namely {{a _column_ vector that right multiplies the matrix $A$ in the eigenvalue equation mentioned first}}: {{$$A \mathbf v = \lambda \mathbf v$$}}.

The eigenvalue and eigenvector problem can also be {{defined for [row vectors](row%20and%20column%20vectors.md) that left multiply matrix $A$}}. In this formulation, the defining eigenvalue equation is: {{$$\mathbf u A = \kappa \mathbf u$$}}, where {{$\kappa$ is a [scalar](scalar%20(mathematics).md) and $\mathbf u$ is a $1 \times n$ matrix}}. Any row vector $\mathbf u$ satisfying this equation is called {{a __left eigenvector__ of $A$ and $\kappa$ is its associated eigenvalue}}.

Taking {{the [transpose](transpose.md)}} of the left eigenvalue equation: {{$$\begin{aligned} \mathbf u A & = \kappa \mathbf u \\ (\mathbf u A)^\intercal & = (\kappa \mathbf u)^\intercal \\ A^\intercal \mathbf u^\intercal & = \kappa \mathbf u^\intercal \end{aligned}$$}}. Comparing this equation to {{the right eigenvalue equation}}, it follows immediately that {{the left eigenvectors of $A$ are the same as the [transpose](transpose.md) of the right eigenvectors of $A^\intercal$, with the same eigenvalue}}. Furthermore, since {{the characteristic polynomial of $A^\intercal$ is the same as that of $A$}}, {{the left and right eigenvectors of $A$ are associated with the same eigenvalues}}.

Going a step further, taking {{the [conjugate transpose](conjugate%20transpos.md)}} of the left eigenvalue equation: {{$$\begin{aligned} \mathbf u A & = \kappa \mathbf u \\ (\mathbf u A)^* & = (\kappa \mathbf u)^* \\ A^* \mathbf u^* & = \overline \kappa \mathbf u^* \end{aligned}$$}}. Comparing this equation to {{the right eigenvalue equation}}, it follows immediately that {{the left eigenvectors of $A$ are the same as the [conjugate transpose](conjugate%20transpose.md) of the right eigenvectors of $A^*$, with the [complex conjugate](complex%20conjugate.md) as the eigenvalue}}. But unlike the regular [transpose](transpose.md), {{the characteristic polynomial of $A^*$ is the [complex conjugate](complex%20conjugate.md) of that of $A$}}, so little can be said.

### diagonalization and the eigendecomposition

- see: [eigendecomposition of a matrix](eigendecomposition%20of%20a%20matrix.md)

Suppose that $A$ has {{an eigenbasis, or equivalently $n$ [linear independent](linear%20independence.md) eigenvectors $\mathbf v_1, \ldots, \mathbf v_n$ with associated eigenvalues $\lambda_1, \ldots, \lambda_n$}}. The eigenvalues need not be distinct. Define {{a [square matrix](square%20matrix.md) $Q$ whose columns are the $n$ linearly independent eigenvectors of $A$}}, {{$$Q := \begin{bmatrix} \mathbf v_1 & \cdots & \mathbf v_n \end{bmatrix}$$}}.

Since each column of $Q$ is an eigenvector of $A$, {{left multiplying $Q$ by $A$ scales each column of $Q$ by its associated eigenvalue}}, {{$$AQ = \begin{bmatrix} \lambda_1 \mathbf v_1 & \cdots & \lambda_n \mathbf v_n \end{bmatrix}$$}}.

With the scaled $Q$ ($AQ$) in mind, we can {{express the same scaled $Q$ ($AQ$) in an alternative way}} by {{defining a [diagonal matrix](diagonal%20matrix.md) $\Lambda$ where each diagonal element $\Lambda_{ii}$ is the eigenvalue associated with the $i$-th column of $Q$}}. Then, {{$$AQ = Q\Lambda$$}}.

Consider $AQ = Q\Lambda$. Since {{the columns of $Q$ are [linearly independent](linear%20independence.md)}}, {{$Q$ is [invertible](invertible%20matrix.md)}}. {{Right multiplying both sides of the equation by $Q^{-1}$}} gives a decomposition of $A$: {{$$A = Q\Lambda Q^{-1}$$}}. Alternatively, {{left multiplying both sides of the equation by $Q^{-1}$}} gives a way to [diagonalize](diagonalizable%20matrix.md) the matrix: {{$$Q^{-1}AQ = \Lambda$$}}.

The above relations shows that $A$ can be {{decomposed into a matrix composed of its eigenvectors, a diagonal matrix with its eigenvalues along the diagonal, and the [inverse](invertible%20matrix.md) of the matrix of eigenvectors}}. This is called {{the [eigendecomposition](eigendecomposition%20of%20a%20matrix.md) and is a [similarity transform](matrix%20similarity.md)}}. Such a matrix $A$ is said to be {{_similar_ to the diagonal matrix $\Lambda$ or [diagonalizable](diagonalizable%20matrix.md)}}. The matrix $Q$ is {{the [change of basis](change%20of%20basis.md) matrix of the similarity transformation}}. Essentially, the matrices $A$ and $\Lambda$ represent {{the same [linear transformation](linear%20map.md) expressed in two different [bases](basis%20(linear%20algebra).md)}}. {{The eigenvectors are used as the basis}} when representing the linear transformation as $\Lambda$.

Conversely, suppose a matrix $A$ is {{[diagonalizable](diagonalizable%20matrix.md)}}. Let $P$ be {{an [invertible matrix](invertible%20matrix.md) such that $P^{-1} A P$ equals some [diagonal matrix](diagonal%20matrix.md) $D$}}. {{Left multiplying both sides by $P$}} to get {{$A P = P D$}}. If we consider {{each $i$-th column $P_i$ individually}}, we get {{$A P_i = d_{ii} P_i$, where $d_{ii}$ is the $i$-th diagonal entry of $D$}}. This is {{exactly the eigenvalue equation}}, so {{all columns of $P$ are eigenvectors}}. Since {{the columns of $P$ must be [linearly independent](linear%20independence.md) for $P$ to be invertible}}, there exists {{$n$ linearly independent eigenvectors of $A$}}. It then follows that {{$A$ has an eigenbasis [iff](if%20and%20only%20if.md) $A$ is diagonalizable}}.

A matrix that is not [diagonalizable](diagonalizable%20matrix.md) is said to be {{[defective](defective%20matrix.md)}}. For said matrices, the notion of eigenvectors generalizes to {{[generalized eigenvectors](generalized%20eigenvector.md)}}} and the diagonal matrix of eigenvalues generalizes to {{the [Jordan normal form](Jordan%20normal%20form.md)}}. Over {{an [algebraically closed field](algebraically%20closed%20field.md)}}, any matrix $A$ has {{a Jordan normal form and therefore admits a [basis](basis%20(linear%20algebra).md) of generalized eigenvectors and a decomposition into generalized eigenspaces}}.

### variational characterization

- see: [min-max theorem](min-max%20theorem.md)

For {{a [Hermitian matrix](Hermitian%20matrix.md) $H$}}, eigenvalues can be {{given a variational characterization}}. {{The largest eigenvalue of $H$}} is {{the maximum value of the [quadratic form](quadratic%20form.md) $\mathbf x^\intercal A \mathbf x / \mathbf x^\intercal \mathbf x$}}. A value of $\mathbf x$ that realizes that maximum is {{an eigenvector associated with the largest eigenvalue}}.

## eigenvalues and eigenfunctions of differential operators

- see: [eigenfunction](eigenfunction.md)

The definitions of eigenvalue and eigenvectors of a [linear transformation](linear%20map.md) $T$ remains valid even if {{the underlying vector space is an infinite-dimensional [Hilbert](Hilbert%20space.md) or [Banach space](Banach%20space.md)}}. A widely used class of linear transformations acting on infinite-dimensional spaces are {{the [differential operators](differential%20operator.md) on [function spaces](function%20space.md)}}.

Let $D$ be {{a linear differential operator on the space $C^\infty(\mathbb R, \mathbb R)$ of infinitely differentiable real functions of a real argument $t$}}. The eigenvalue equation for $D$ is {{the [differential equation](differential%20equation.md): $$D f(t) = \lambda f(t)$$}}. The functions that satisfy this equation are {{eigenvectors of $D$ and are commonly called __eigenfunctions__}}.

### derivative operator example

Consider {{the [derivative operator](derivative%20operator.md) $\frac {\mathrm d} {\mathrm dt}$}} with eigenvalue equation {{$$\frac {\mathrm d} {\mathrm dt} f(t) = \lambda f(t)$$}}. This [differential equation](differential%20equation.md) can be solved by {{multiplying both sides by $\mathrm dt / f(t)$ and [integrating](integration%20(calculus).md)}}: {{$$\begin{aligned} \frac {\mathrm d} {\mathrm dt} f(t) & = \lambda f(t) \\ \frac {\mathrm df(t)} {f(t)} & = \lambda \,\mathrm dt \\ \int \! \frac {\mathrm df(t)} {f(t)} & = \int \! \lambda \,\mathrm dt \\ \ln \lvert f(t) \rvert & = \lambda t + C \\ e^{\lambda t + C} & = f(t) && (\text{assuming }f(t) > 0\text{ for simplicity}) \\ C e^{\lambda t} & = f(t) \\ f(t) & = f(0) e^{\lambda t} && \left( f(0) = C e^{\lambda 0} = C \right) \end{aligned}$$}}.

The solution of the above differential equation, $f(t) = f(0) e^{\lambda t}$, is {{the eigenfunction of the [derivative operator](derivative%20operator.md)}}. In this case, the eigenfunction is {{itself a function of its associated eigenvalue}}. In particular, for $\lambda = 0$ {{the eigenfunction $f(t)$ is a constant}}. The $f(0)$ can be an arbitrary nonzero constant in the same way as {{scaling an eigenvector by an arbitrary nonzero constant still gives an eigenvector associated with the same eigenvalue}}.

## calculation

### classical method

#### eigenvalues

The eigenvalues of a matrix $A$ can be determined by {{finding the roots of the [characteristic polynomial](characteristic%20polynomial.md) ($\det(tI - A) = 0$)}}. This is {{easy for $2 \times 2$ matrices, but the difficulty increases rapidly with the size of the matrix}}. <!--SR:!2024-08-10,39,290!2024-09-11,68,310-->

#### eigenvectors

Once {{the (exact) values of an eigenvalue is known}}, the corresponding eigenvector can be {{calculated directly by finding nonzero solutions of the eigenvalue equation}}, which becomes {{a [system of linear equations](system%20of%20linear%20equations.md) with known coefficients}}. <!--SR:!2024-08-28,56,310!2024-07-16,20,250!2024-08-11,40,290-->

## references

This text incorporates [content](https://en.wikipedia.org/wiki/eigenvalues_and_eigenvectors) from [Wikipedia](Wikipedia.md) available under the [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) license.
