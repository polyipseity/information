---
aliases:
  - eigenvalue
  - eigenvalue and eigenvector
  - eigenvalues
  - eigenvalues and eigenvectors
  - eigenvector
  - eigenvector and eigenvalue
  - eigenvectors
  - eigenvectors and eigenvalues
tags:
  - flashcard/active/general/eigenvalues_and_eigenvectors
  - language/in/English
---

# eigenvalues and eigenvectors

In [linear algebra](linear%20algebra.md), it is often important to {{know which [vectors](vector%20space.md) have their directions unchanged by a given [linear transformation](linear%20map.md)}}. An {{__eigenvector__ (/ˈaɪɡən-/ EYE-gən-) or __characteristic vector__}} is such a vector. More precisely, an eigenvector $\mathbf v$ of a linear transformation $T$ is {{a nonzero vector that is [scaled by a constant factor](scalar%20multiplication.md) $\lambda$ when the linear transformation is applied to it: $T \mathbf v = \lambda \mathbf v \quad \mathbf v \ne \mathbf 0$}}. The multiplying factor $\lambda$ is {{the corresponding __eigenvalue__, __characteristic value__, or __characteristic root__}}. <!--SR:!2025-04-24,208,326!2025-09-02,327,346!2024-12-23,122,306!2025-03-13,185,326-->

## definition

Consider {{a [linear transformation](linear%20map.md) $T$ and a nonzero [vector](vector%20space.md) $\mathbf v$}}. If {{applying $T$ simply scales $\mathbf v$ by a factor of $\lambda$, where $\lambda$ is a [scalar](scalar%20(mathematics).md)}}, then {{$\mathbf v$ is an eigenvector of $T$, and $\lambda$ is the corresponding eigenvalue}}. This relationship can be expressed as: {{$$T \mathbf v = \lambda \mathbf v$$}}. <!--SR:!2025-10-09,357,346!2025-04-19,215,326!2025-06-03,254,346!2025-07-28,300,346-->

For {{finite-[dimensional](dimension%20(vector%20space).md) [vector spaces](vector%20space.md)}}, there is {{a direct correspondence between _n_-by-_n_ [square matrices](square%20matrix.md) and linear transformations from an [_n_-dimensional](dimension.md) vector space into itself}}, given {{any [basis](basis%20(linear%20algebra).md) of the vector space}}. Hence, it is {{equivalent to define eigenvalues and eigenvectors using either the language of [matrices](matrix%20(mathematics).md), or the language of linear transformations}}. In this case, the above equation can be rewritten as: {{$$A \mathbf u = \lambda \mathbf u$$, where $A$ is the matrix representation of $T$ and $\mathbf u$ is the [coordinate vector](coordinate%20vector.md) of $\mathbf v$}}. <!--SR:!2025-07-01,277,346!2025-07-07,283,346!2025-06-20,268,346!2025-07-29,300,346!2025-08-27,323,346-->

> [!tip] tips
>
> - geometric interpretation ::: Consider the action $\mathbf x \mapsto A \mathbf x$. (Right) eigenvectors $\mathbf x'$ of the [matrix](matrix%20(mathematics).md) $A$ are identified with lines through the origin that are parallel to the vectors and are sent to themselves or the set of zero $\set 0$ after the action: $$A \mathbf x' = \lambda \mathbf x'$$. Note that $\lambda$ can be zero. <!--SR:!2025-02-25,173,335!2025-12-08,410,375-->

## overview

Eigenvalues and eigenvectors give rise to {{many closely related mathematical concepts}}, and {{the prefix _eigen-_ is applied liberally when naming them}}: <!--SR:!2025-08-08,309,346!2025-04-03,192,326-->

- __eigensystem__ of a linear transformation ::: the set of all eigenvectors of the linear transformation, each paired with its corresponding eigenvalue <!--SR:!2025-09-07,332,346!2025-05-28,232,326-->
- __eigenspace__ or __characteristic space__ of a eigenvalue of a linear transformation ::: the set of all eigenvectors of the linear transformation corresponding to the same eigenvalue, together with the zero vector <!--SR:!2025-06-29,257,326!2025-03-25,183,326-->
- __eigenbasis__ :::  the set of eigenvectors of a linear transformation that also forms a [basis](basis%20(linear%20algebra.md)) of the domain of the same linear transformation <!--SR:!2025-07-27,280,306!2025-06-04,255,346-->

## eigenvalues and eigenvectors of matrices

Eigenvalues and eigenvectors are often introduced to students in {{the context of [linear algebra](linear%20algebra.md) courses focused on [matrices](matrix%20(mathematics).md)}}. Furthermore, linear transformations over {{a finite-[dimensional](dimnsion%20(vector%20space).md) [vector space](vector%20space.md)}} can be represented {{using matrices, which is especially common in numerical and computational applications}}. <!--SR:!2025-02-25,176,326!2025-05-30,234,326!2024-12-06,110,306-->

Consider _n_-dimensional vectors formed of {{a vertical list of _n_ scalars}}, such as the 2-dimensional vectors $$\mathbf x = \begin{bmatrix} 10 \\ -20 \end{bmatrix} \quad \text{ and } \quad \mathbf y = \begin{bmatrix} -1 \\ 2 \end{bmatrix}$$. These vectors are {{[scalar multiples](scalar%20multiplication.md) of each other, also called [parallel](parallel%20(geometry).md) or [collinear](collinearity.md)}}, because {{there is a [scalar](scalar%20(mathematics).md) $\lambda$ such that $$\mathbf x = \lambda \mathbf y$$}}. In this case, {{$\lambda = -10$}}. <!--SR:!2025-09-16,336,346!2025-01-07,127,306!2025-07-14,287,346!2025-09-01,328,346-->

Now consider the linear transformation of _n_-dimensional vectors defined by {{an _n_-by-_n_ matrix $A$}}, such as {{the scaling-by-negative-10 matrix}}: {{$$A = \begin{bmatrix} -10 & 0 \\ 0 & -10 \end{bmatrix}$$}}. Applying the linear transformation on a nonzero vector $\mathbf v$ to make a new vector $\mathbf w$ is represented by {{$$A \mathbf v = \mathbf w$$ or $$\begin{bmatrix} A_{11} & A_{12} & \cdots & A_{1n} \\ A_{21} & A_{22} & \cdots & A_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ A_{n1} & A_{n2} & \cdots & A_{nn} \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix} = \begin{bmatrix} w_1 \\ w_2 \\ \vdots \\ w_n \end{bmatrix}$$}}, where for each row of $\mathbf w$: {{$$w_i = A_{i1} v_1 + A_{i2} v_2 + \cdots + A_{in} v_n = \sum_{k = 1}^n A_{ik} v_k$$}}. This operation is known as {{[matrix multiplication](matrix%20(mathematics).md#matrix%20multiplication)}}. For example, applying $A$ on $\mathbf y = \begin{bmatrix} -1 \\ 2 \end{bmatrix}$ is: {{$$\begin{aligned} A \mathbf y & = \mathbf w \\ \begin{bmatrix} -10 & 0 \\ 0 & -10 \end{bmatrix} \begin{bmatrix} -1 \\ 2 \end{bmatrix} & = \begin{bmatrix} 10 \\ -20 \end{bmatrix} \end{aligned}$$}}, so the result is {{$\mathbf w = \begin{bmatrix} 10 \\ - 20 \end{bmatrix} = \mathbf x$}}. <!--SR:!2025-04-19,203,326!2025-06-19,267,346!2025-02-26,177,326!2025-06-21,269,346!2025-06-13,263,346!2025-08-07,308,346!2025-06-02,253,346!2025-05-14,223,326-->

If it happens that the original {{nonzero}} $\mathbf v$ and the resulting $\mathbf w$ are {{scalar multiples}}, that is: {{$$A \mathbf{v} = \mathbf{w} = \lambda \mathbf{v} \quad \mathbf{v} \ne \mathbf{0}$$}}, then {{$\mathbf v$ is the __eigenvector__ of the linear transformation $A$ and the scale factor $\lambda$ is the __eigenvalue__ corresponding to that eigenvector}}. The above equation is {{the __eigenvalue equation__ for the linear transformation $A$}}. For the example above after applying $A$ on $\mathbf y$ to get $\mathbf x = -10 \mathbf y$, {{$\mathbf y$ is an eigenvector of the matrix $A$ and $\lambda = -10$ is the corresponding eigenvalue}}. <!--SR:!2025-02-22,175,326!2025-07-01,277,346!2025-07-18,291,346!2025-04-01,190,326!2024-11-28,97,306!2025-08-01,303,346-->

The eigenvalue equation can also be equivalently stated as {{$$(A - \lambda I) \mathbf v = \mathbf 0 \quad \text{ or } \quad (\lambda I - A) \mathbf v = \mathbf 0$$}}, where {{$I$ is the _n_-by-_n_ [identity matrix](identity%20matrix.md) and $\mathbf 0$ is the zero vector}}. <!--SR:!2025-09-19,338,346!2025-08-26,322,346-->

From the eigenvector equation, given a eigenvector with its corresponding eigenvalue, a new eigenvector {{associated with the same eigenvalue can be obtained by scaling the original eigenvector by an arbitrary nonzero scalar}}. The resulting eigenvector still respects the eigenvector equation as {{this is equivalent to multiplying both sides by the nonzero scalar}}. <!--SR:!2025-08-02,304,346!2025-04-12,198,326-->

### eigenvalues and the characteristic polynomial

- see: [characteristic polynomial](charateristic%20polynomial.md)

The rewritten eigenvalue equation with $\lambda I$ in front: {{$$(\lambda I - A) \mathbf v = \mathbf 0$$}} has {{a nonzero vector solution $\mathbf v$ [iff](if%20and%20only%20if.md) the [determinant](determinant.md) of the matrix $(\lambda I - A)$ is zero}}. Therefore, the eigenvalues $\lambda$ are {{that satisfy the equation $\det(\lambda I - A) = 0$}}. This equation is called {{the __characteristic equation__ or the __secular equation__ of $A$}}. <!--SR:!2025-08-16,312,346!2025-03-22,193,326!2025-08-22,318,346!2025-01-14,143,326-->

Using {{the [Leibniz formula for determinants](Leibniz%20formula%20for%20determinants.md)}}, the left-hand side of equation is {{a [polynomial](polynomial.md) function of the variable $\lambda$ and the [degree](degree%20of%20a%20polynomial.md) of this polynomial is $n$, the order of the matrix $A$}}. Its coefficients depend on {{the entries of $A$, except that its term of degree of $n$ is always $\lambda^n$, making the polynomial a [monic polynomial](monic%20polynomial.md)}}. If {{the eigenvalue equation with $A$ in front is used instead}}, then {{its term of degree of $n$ is always $(-1)^n \lambda^n$, making it monic only when $n$ is even}}. This polynomial is called {{the _characteristic polynomial_ of $A$}}. {{The monic polynomial}} will be used in the rest of this article. <!--SR:!2025-09-16,336,346!2025-08-11,293,306!2025-02-07,146,306!2025-09-22,338,306!2025-03-06,184,326!2024-12-28,126,306!2025-06-28,275,346-->

{{The [fundamental theorem of algebra](fundamental%20theorem%20of%20algebra.md)}} implies that the characteristic polynomial of an _n_-by-_n_ [matrix](matrix%20(mathematics).md) $A$, being {{a (monic) polynomial of [degree](degree%20of%20a%20polynomial.md) _n_}}, can {{be [factored](factorization.md) into the product of _n_ linear terms}}: {{$$\det(\lambda I - A) = (\lambda - \lambda_1) (\lambda - \lambda_2) \cdots (\lambda_n - \lambda) = \prod_{i = 1}^n (\lambda - \lambda_i)$$}}, where {{each $\lambda_i$ may be [real](real%20number.md) but in general is a [complex number](complex%20number.md)}}. The scalars $\lambda_1, \lambda_2, \ldots, \lambda_n$ are {{the roots of the polynomial and also eigenvalues of $A$}}. The eigenvalues may or may not {{be zero, or be all distinct}}. {{Plugging the eigenvalues back into the eigenvalue equation and solving them}} gives {{the corresponding eigenvectors}}. <!--SR:!2025-04-09,207,326!2025-09-16,336,346!2025-04-15,199,326!2025-03-08,185,326!2025-09-16,336,346!2025-05-21,241,346!2025-07-22,295,346!2025-02-11,166,326!2025-10-27,377,364-->

If {{the entries of matrix $A$ are all [real numbers](real%20number.md)}}, then {{the coefficients of the characteristic polynomial are also all real numbers, but the eigenvalues may still have nonzero imaginary parts. Thus, the entires of the corresponding eigenvectors may also have nonzero imaginary parts}}. Similarly, {{eigenvalues and entries of their corresponding eigenvectors may be [irrational numbers](irrational%20number.md)}} even if {{all entries of $A$ are [rational numbers](rational%20number.md) or even [integers](integer.md)}}. However, if {{the entries of $A$ are all [algebraic numbers](algebraic%20number.md), which include the rationals}}, {{the eigenvalues and entries of their corresponding eigenvectors must also be algebraic numbers}}. <!--SR:!2025-07-27,299,346!2025-04-04,193,326!2024-12-10,101,286!2025-08-10,306,346!2025-01-27,152,326!2025-04-24,219,326-->

The non-real roots of a real [polynomial](polynomial.md) with real coefficients can be {{grouped into pairs of [complex conjugates](complex%20conjugate.md), which are two [complex numbers](complex%20number.md) having the same real part and imaginary parts of opposing sign}}. The eigenvectors associated with these complex eigenvalues are {{also complex and also appear in complex conjugate pairs}}. If {{the [degree](degree%20of%20a%20polynomial.md) is odd}}, then {{by the [intermediate value theorem](intermediate%20value%20theorem.md) (an odd-degree polynomial goes to infinity as the variable goes to infinity, but with opposing sign comparing the variable going to negative infinity and positive infinity), at least one of the roots is real}}. Therefore, {{any real matrix with odd order has at least one real eigenvalue, whereas a real matrix with even order may not have any real eigenvalues}}. <!--SR:!2025-09-16,336,346!2024-11-22,98,306!2025-09-30,350,346!2025-10-15,350,326!2025-04-16,200,326-->

### algebraic multiplicity

Let $\lambda_i$ be an eigenvalue of an _n_-by-_n_ matrix $A$. The __algebraic multiplicity__ {{$\mu_A(λ_i)$ of the eigenvalue}} is {{its [multiplicity as a root](multiplicity%20(mathematics).md#multiplicity%20of%20a%20root%20of%20a%20polynomial) of the characteristic polynomial}}, that is, {{the largest integer $k$ such that $(\lambda − \lambda_i)^k$ [divides evenly](polynomial%20long%20division.md) that polynomial}}. <!--SR:!2025-04-20,204,326!2025-09-19,338,346!2025-08-25,321,346-->

Suppose in the $n$ eigenvalues of an _n_-by-_n_ matrix, there are {{$d$ distinct eigenvalues with $1 \le d \le n$}}. The characteristic polynomial factorized into $n$ linear terms above may have {{some terms potentially repeating}}. Instead, said polynomial can also be written as {{the product of $d$ distinct terms each corresponding to a distinct eigenvalue and raised to the power of algebraic multiplicity}}: {{$$\det(\lambda I - A) = (\lambda - \lambda_1)^{\mu_A(\lambda_1)} (\lambda - \lambda_2)^{\mu_A(\lambda_2)} \cdots (\lambda - \lambda_d)^{\mu_A(\lambda_d)}$$}}. <!--SR:!2025-07-02,278,346!2025-09-16,336,346!2025-04-25,207,326!2025-04-30,224,326-->

If {{$d = n$}} then {{the right hand side is a product of $n$ distinct terms and is the same as the polynomial above}}. The size of each eigenvalue's algebraic multiplicity respect the following relations: {{$$\begin{aligned} 1 & \le \mu_A(\lambda_i) \le n \\ \mu_A & = \sum_{i = 1}^d \mu_A(\lambda_i) = n \end{aligned}$$}}. <!--SR:!2025-09-16,336,346!2025-07-24,296,346!2025-08-22,318,346-->

If {{$\mu_A(\lambda_i) = 1$}}, then $\lambda_i$ is {{said to be a _simple eigenvalue_}}. If {{$\mu_A(\lambda_i)$ equals the geometric multiplicity of $\lambda_i$, $\gamma_A(\lambda_i)$}}, defined in upcoming sections, then $\lambda_i$ is {{said to be a _semisimple eigenvalue_}}. <!--SR:!2025-07-29,301,346!2024-12-18,118,306!2025-06-15,264,346!2025-01-30,156,326-->

### eigenspaces

Given {{a particular eigenvalue $\lambda$ of the _n_-by-_n_ matrix $A$}}, define the set $E$ to be {{all vectors $\mathbf v$ that satisfy the eigenvalue equation}}: {{$$E = \set{ \mathbf v : (\lambda I - A) \mathbf v = \mathbf 0 }$$}}. On one hand, this is {{precisely the [kernel](kernel%20(linear%20algebra).md) or nullspace of the matrix $(\lambda I - A)$}}. On the other hand, {{by definition, any _nonzero_ vector that satisfies the eigenvalue equation is an eigenvector of $A$ associated with $\lambda$}}. So, the set $E$ is {{the [union](union%20(set%20theory).md) of the zero vector and the set of all eigenvectors of $A$ associated with $\lambda$}}, and also equals {{the nullspace of the matrix $(\lambda I - A)$}}. $E$ is called {{the __eigenspace__ or __characteristic space__ of $A$ associated with eigenvalue $\lambda$}}. In general, $\lambda$ is {{a [complex number](complex%20number.md) and the eigenvectors are complex $n$ by 1 matrices}}. An important property of the nullspace is that {{it is a [linear subspace](linear%20subspace.md), so $E$ is a linear subspace of $\mathbb C^n$}}. <!--SR:!2025-05-12,232,326!2025-05-25,229,326!2025-07-11,286,346!2025-02-21,169,326!2025-02-20,157,306!2025-07-02,278,346!2025-05-09,219,326!2025-04-23,218,326!2025-03-27,199,326!2025-08-04,305,346-->

Because the eigenspace $E$ is {{a [linear subspace](linear%20subspace.md)}}, it is {{[closed](closure%20(mathematics).md) under [addition](addition.md) and [scalar multiplication](scalar%20multiplication.md)}}. That is, if {{two arbitrary vectors $\mathbf u$ and $\mathbf v$ belong to the set $E$ and $\alpha$ is an arbitrary [complex](complex%20number.md)}}, written {{$\mathbf u, \mathbf v \in E, \alpha \in \mathbb C$}}, then {{$(\mathbf u + \mathbf v) \in E$ and $\alpha \mathbf u \in E$, or equivalently $A(\mathbf u + \mathbf v) = \lambda (\mathbf u + \mathbf v)$ and $A(\alpha \mathbf u) = \lambda (\alpha \mathbf u)$}}. For addition, it can be checked using {{the [distributive property](distributive%20property.md) of matrix multiplication}}. For scalar multiplication, it can be checked by {{noting that multiplication of complex matrices by complex numbers is [commutative](commutative%20property.md)}}. As long as {{the resulting vector $(\mathbf u + \mathbf v)$ or $\alpha \mathbf u$ is not the zero vector $\mathbf 0$ (the zero vector belongs to $E$, however)}}, then it is also {{an eigenvector of $A$ associated with $\lambda$}}. <!--SR:!2025-06-10,260,346!2025-04-19,214,326!2025-09-30,350,346!2025-08-17,313,346!2025-06-09,260,346!2025-07-17,290,346!2025-10-09,357,346!2025-03-23,182,326!2025-09-18,345,375-->

### geometric multiplicity

The __geometric multiplicity__ {{$\gamma_A(\lambda)$ of an eigenvalue $\lambda$}} is {{the [dimension](dimension%20(vector%20space).md) of the eigenspace $E$ associated with $\lambda$}}, or equivalently {{the maximum number of [linearly independent](linear%20independence.md) eigenvectors associated with $\lambda$}}. Because {{$E$ is also the nullspace of $(\lambda I - A)$}}, {{the geometric multiplicity is the dimension of nullspace of $(\lambda I - A)$}}, also called {{the _nullity_ of $(\lambda I - A)$}}, which {{relates to the dimension and rank of $(\lambda I - A)$ as}}: {{$$\gamma_A(\lambda) = n - \operatorname{rank}(\lambda I - A)$$}}. <!--SR:!2025-07-06,282,346!2025-09-16,336,346!2024-12-11,114,306!2024-12-31,123,306!2025-02-19,170,326!2025-05-29,249,346!2025-05-05,216,326!2025-04-15,200,326-->

By {{the definition of eigenvalues and eigenvectors}}, each eigenvalue has {{at least one nonzero associated eigenvector}}, so an eigenvalue's geometric multiplicity must be {{at least one}}. Furthermore, an eigenvalue's geometric multiplicity {{cannot exceed its [algebraic multiplicity](#algebraic%20multiplicity)}}. So the following inequality holds: {{$$1 \le \gamma_A(\lambda) \le \mu_A(\lambda) \le n$$}}. <!--SR:!2025-12-24,423,364!2025-04-25,205,324!2025-08-13,296,344!2025-11-03,383,364!2025-10-27,377,364-->

To prove the inequality $\gamma_A(\lambda) \le \mu_A(\lambda)$, the idea is {{transforming $A$ into another matrix $B$ such that $B$ has the same determinant (and thus eigenvalues) as $A$ and $\det(\xi I - B)$ has the linear term $(\xi - \lambda)^{\gamma_A(\lambda)}$}}. We start with {{the definition of geometric multiplicity}}, which {{implies the existence of $\gamma_A(\lambda)$ [orthonormal](orthonormality.md) eigenvectors $\mathbf v_1, \ldots, \mathbf v_{\gamma_A(\lambda)}$ such that $A \mathbf v_i = \lambda \mathbf v_i$}}. We can find {{a matrix $V$ those first $\gamma_A(\lambda)$ columns are these eigenvectors, and the remaining columns can be any set of orthonormal set of $n - \gamma_A(\lambda)$ vectors $\mathbf u_{\gamma_A(\lambda) + 1}, \ldots, \mathbf u_n$ orthogonal to these eigenvectors: $$V = \begin{bmatrix} \mathbf v_1 & \cdots & \mathbf v_{\gamma_A(\lambda)} & \mathbf u_{\gamma_A(\lambda) + 1} & \cdots & \mathbf u_n \end{bmatrix}$$}}. The matrix has {{full [rank](rank%20(linear%20algebra).md) due to linearly independent column vectors, thus is invertible}}. The matrix is also {{unitary, that is, its [conjugate transpose](conjugate%20transpose.md) is its [matrix inverse](invertible%20matrix.md): $V^* V = V V^* = I$}}. This is obvious because {{the column vectors are unitary, and orthogonal to all column vectors except for itself}}. Using $V$, we can {{(partially) [diagonalize](eigendecomposition%20of%20a%20matrix.md) $A$ to produce $B$: $B := V^{-1} A V = V^* A V$, which preserves the determinant as they are [similar](matrix%20similarity.md): $\det(B) = \det\left(V^{-1} A V\right) = \det(A)$}}. We can also {{calculate some entries of $B$}} given the above facts. Consider {{applying $A$ on $V$, i.e. $A V$}}. This transforms $V$ {{by scaling the first $\gamma_A(\lambda)$ columns by the eigenvalue $\lambda$. The rest of the columns are unknowns}}; mathematically, that is, {{$$A V = \begin{bmatrix} \lambda \mathbf v_1 & \cdots & \lambda \mathbf v_{\gamma_A(\lambda)} & \mathbf u'_{\gamma_A(\lambda) + 1} & \cdots & \mathbf u'_n \end{bmatrix}$$}}. Now consider {{applying $V^{-1} = V^*$ on $A V$, i..e $V^* A V$}}. This transforms {{the most top-left $\lambda_{\gamma_A(\lambda)}$-sized block into an identity matrix multiplied by the eigenvalue. The remaining rows of the first $\lambda_{\gamma_A(\lambda)}$ columns are all zero}}. This is because {{the first $\lambda_{\gamma_A(\lambda)}$ column vectors are orthogonal to all row vectors in $V^*$ except for itself}}. The rest of {{the columns are unknown as the corresponding columns of $A V$ are also unknown}}. Mathematically, this is {{$$B = V^* A V = \begin{bmatrix} \lambda I_{\gamma_A(\lambda)} & U''_{\text{top} } \\ \mathbf 0 & U''_{\text{bottom} } \end{bmatrix}$$ (see [block matrix](block%20matrix.md))}}. Now, we show that {{($\xi I - B$) has the properties we desired}}. To {{prove $\det(\xi I - B) = \det(\xi I - A)$}}, note that {{$$\det(\xi I - A) = \det\left(V^* (\xi I - A) V \right) = \det\left(V^* (\xi I) V - V^* A V \right) = \det(\xi I - B)$$. The first equality is by [similarity]((matrix%20similarity.md))}}. Second, to {{prove $\det(\xi I - B)$ has the linear term $(\xi - \lambda)^{\gamma_A(\lambda)}$}}, we note that {{the [block matrix](block%20matrix.md) above is in the form of $\begin{bmatrix} A & B \\ \mathbf 0 & D \end{bmatrix}$, which has the determinant $\det(A) \det(D)$. Therefore, $$\det(\xi I - B) = \det(\xi I_{\gamma_A(\lambda)} - \lambda I_{\gamma_A(\lambda)}) \det(\xi I - U''_{\text{bottom} }) = (\xi - \lambda)^{\gamma_A(\lambda)} \det(\xi I - U''_{\text{bottom} })$$}}. Thus, we have shown that $(\xi I - B)$ has the desired properties. This implies {{$\det(\xi I - A)$ must have the linear term $(\xi - \lambda)^{\gamma_A(\lambda)}$, with more $(\xi - \lambda)$ terms _possibly_ coming from $\det(\xi I - U''_{\text{bottom} })$}}. Therefore, {{the algebraic multiplicity of $\lambda$ is at least the geometric multiplicity of $\lambda$, i.e. $\mu_A(\lambda) \ge \gamma_A(\lambda)$}}. <!--SR:!2024-12-04,99,304!2025-03-28,204,344!2025-08-06,291,344!2025-09-30,350,364!2025-08-13,296,344!2025-03-23,180,324!2025-08-06,291,344!2024-12-07,102,304!2025-06-12,261,344!2025-07-08,280,344!2025-03-27,203,344!2025-09-30,350,364!2025-05-08,212,324!2025-03-29,205,344!2024-12-13,107,304!2025-02-05,151,324!2024-12-25,116,304!2025-10-27,377,364!2024-12-13,108,304!2025-02-14,163,324!2025-12-04,406,364!2025-01-28,145,324!2025-07-31,288,344!2025-01-19,145,324-->

Suppose $A$ has $d \le n$ distinct eigenvalues $\lambda_1, \ldots, \lambda_d$, where the geometric multiplicity of $\lambda_i$ is $\gamma_A(\lambda_i)$. The total geometric multiplicity of $A$ is {{the dimension of the [sum](linear%20subspace.md#sum) of all the eigenspaces of $A$'s eigenvalues}}, or equivalently {{the maximum number of [linearly independent](linear%20independence.md) eigenvectors of $A$}}. It respects the following relations: {{$$\begin{aligned} \gamma_A & = \sum_{i = 1}^d \gamma_A(\lambda_i) \\ d & \le \gamma_A \le n \end{aligned}$$}}. <!--SR:!2024-11-20,36,324!2025-04-09,212,344!2025-08-30,326,364-->

### eigenbasis

If {{$\gamma_A = n$}}, then <!--SR:!2025-10-27,377,364-->

- (if $\gamma_A = n$) [direct sum](direct%20sum.md) of the eigenspaces of all of $A$'s eigenvalues ::: The [direct sum](direct%20sum.md) of the eigenspaces of all of $A$'s eigenvalues is the entire vector space $\mathbb C^n$. <!--SR:!2025-06-21,267,344!2025-11-03,383,364-->
- (if $\gamma_A = n$) $n$ [linearly independent](linear%20independence.md) eigenvectors of $A$ ::: A [basis](basis%20(linear%20algebra).md) of $\mathbb C^n$ can be formed from $n$ [linearly independent](linear%20independence.md) eigenvectors of $A$; such a basis is called an __eigenbasis__. <!--SR:!2025-10-27,377,364!2025-09-30,350,364-->
- (if $\gamma_A = n$) [linear combination](linear%20combination.md) of eigenvectors of $A$ ::: Any vectors in $\mathbb C^n$ can be written as a [linear combination](linear%20combination.md) of eigenvectors of $A$. <!--SR:!2025-08-13,296,344!2025-11-20,397,364-->

### additional properties of eigenvalues

Let $A$ be an arbitrary $n \times n$ [square matrix](square%20matrix.md) of [complex numbers](complex%20number.md) with {{eigenvalues $\lambda_1, \ldots, \lambda_n$}}. $\mu_A(\lambda_i)$ and $\gamma_A(\lambda_i)$ are {{resp. algebraic multiplicity and geometric multiplicity of the eigenvalue $\lambda_i$}}. Each distinct eigenvalue appears {{$\mu_A(\lambda_i)$ times in this list}}. The following are properties of this matrix and its eigenvalues: <!--SR:!2025-12-24,423,364!2025-07-28,293,344!2025-09-26,351,364-->

- [trace](trace%20(linear%20algebra).md) of $A$, $\operatorname{tr}(A)$ ::: The trace of $A$, $\operatorname{tr}(A)$, defined as the sum of its diagonal elements, is also the sum of all eigenvalues: $$\operatorname{tr}(A) = \sum_{i = 1}^n a_{ii} = \sum_{i = 1}^n \lambda_i = \lambda_1 + \lambda_2 + \cdots + \lambda_n$$. <!--SR:!2025-12-10,414,364!2025-12-15,416,364-->
- [determinant](determinant.md) of $A$, $\det(A)$ ::: The determinant of $A$, $\det(A)$, is the product of all eigenvalues: $$\det(A) = \prod_{i = 1}^n \lambda_i = \lambda_1 \lambda_2 \cdots \lambda_n$$. <!--SR:!2025-12-10,414,364!2025-06-21,254,344-->
  - [invertibility](invertible%20matrix.md) of $A$ ::: The matrix $A$ is [invertible](invertible%20matrix.md) [iff](if%20and%20only%20if.md) every eigenvalue is nonzero. This follows from that the determinant is the product of all eigenvalues. <!--SR:!2025-03-18,196,344!2025-05-19,242,344-->
- [inverse](invertible%20matrix.md) of $A$, $A^{-1}$ ::: If $A$ is [invertible](invertible%20matrix.md), then the eigenvalues of $A^{-1}$ are $\frac 1 {\lambda_1}, \ldots, \frac 1 {\lambda_n}$. Each eigenvalue's geometric multiplicity coincides, i.e. $\gamma_A(\lambda_i) = \gamma_{A^{-1} }(1 / \lambda_i)$. Moreover, from above (trivially) and from that the the characteristic polynomial of the inverse is the [reciprocal polynomial](reciprocal%20polynomial.md) of the original, the eigenvalues share the same algebraic multiplicity, i.e. $\mu_A(\lambda_i) = \mu_{A^{-1} }(1 / \lambda_i)$. <!--SR:!2025-03-05,127,324!2025-01-20,146,324-->
- [Hermitian matrix](Hermitian%20matrix.md) ::: If $A$ equals to its conjugate transpose $A^*$, or equivalently if $A$ is [Hermitian](Hermitian%20matrix.md), then every eigenvalue is real. The same is true of any [symmetric](symmetric%20matrix.md) real matrix, as a subset of Hermitian matrices. <!--SR:!2024-12-06,101,304!2025-03-09,171,324-->
- [Hermitian matrix](Hermitian%20matrix.md) properties ::: If $A$ is not only [Hermitian](Hermitian%20matrix.md) but also positive-definite, positive-semidefinite, negative-definite, or negative-semidefinite, then every eigenvalue is positive, non-negative, negative, or non-positive, respectively. <!--SR:!2025-04-24,226,344!2025-08-13,296,344-->
- [unitary matrix](unitary%20matrix.md) ::: If $A$'s [inverse](invertible%20matrix.md) $A^{-1}$ equals to its conjugate transpose $A^*$, or equivalently if $A$ is [unitary](unitary%20matrix.md), then every eigenvalue has absolute value $\lvert \lambda_i \rvert = 1$. <!--SR:!2024-11-22,27,284!2025-01-13,121,284-->
- [matrix polynomial](matrix%20polynomial.md) ::: In general, for a [matrix polynomial](matrix%20polynomial.md) $P$, which are [polynomials](polynomial.md) with [square matrices](square%20matrix.md) as variables, then eigenvalues of matrix $P(A)$ are $p(\lambda_1), \ldots, p(\lambda_n)$, where $p$ is the same polynomial as $P$ but accepts and outputs a [complex number](complex%20number.md) instead of a square matrix. For example, for matrix $P(A) = I + A$, its eigenvalues are $1 + \lambda_1, \ldots, 1 + \lambda_n$. Another example is matrix $P(A) = \alpha I + A$ for $\alpha \in \mathbb C$, for which its eigenvalues are $\alpha + \lambda_1, \ldots, \alpha + \lambda_n$. An even more general example is matrix $P(A) = \alpha I + \beta A + A^2$ for $\alpha, \beta \in \mathbb C$, for which its eigenvalues are $\alpha + \beta \lambda_1 + \lambda_1^2, \ldots, \alpha + \beta \lambda_n + \lambda_n^2$. <!--SR:!2025-09-30,350,364!2025-05-20,243,344-->
  - eigenvalues of the $k$-th power of $A$, $A^k$ ::: The eigenvalues of the $k$-th power of $A$; i.e. the eigenvalues of $A^k$, for any positive integer $k$ (or including $k = 0$ if $0^0$ is defined as $1$), are $\lambda_1^k, \ldots, \lambda_n^k$. <!--SR:!2025-10-27,377,364!2025-11-26,402,364-->

### left and right eigenvectors

- see: [left and right (linear algebra)](left%20and%20right%20(linear%20algebra).md)

Many disciplines traditionally represent vectors as {{[matrices](matrix%20(mathematics).md) with a single [column](row%20and%20column%20vectors.md) (vertical) rather than as matrices with a single [row](row%20and%20column%20vectors.md) (horizontal)}}. For that reason, the word "eigenvector" in the context of matrices {{almost always refers to a __right eigenvector__}}, namely {{a _column_ vector that right multiplies the matrix $A$ in the eigenvalue equation mentioned first}}: {{$$A \mathbf v = \lambda \mathbf v$$}}. <!--SR:!2025-07-16,274,350!2025-12-24,423,370!2025-09-03,330,370!2025-11-03,383,370-->

The eigenvalue and eigenvector problem can also be {{defined for [row vectors](row%20and%20column%20vectors.md) that left multiply matrix $A$}}. In this formulation, the defining eigenvalue equation is: {{$$\mathbf u A = \kappa \mathbf u$$}}, where {{$\kappa$ is a [scalar](scalar%20(mathematics).md) and $\mathbf u$ is a $1 \times n$ matrix}}. Any row vector $\mathbf u$ satisfying this equation is called {{a __left eigenvector__ of $A$ and $\kappa$ is its associated eigenvalue}}. <!--SR:!2025-11-20,397,370!2025-06-18,263,350!2025-10-27,377,370!2025-08-11,314,370-->

Taking {{the [transpose](transpose.md)}} of the left eigenvalue equation: {{$$\begin{aligned} \mathbf u A & = \kappa \mathbf u \\ (\mathbf u A)^\intercal & = (\kappa \mathbf u)^\intercal \\ A^\intercal \mathbf u^\intercal & = \kappa \mathbf u^\intercal \end{aligned}$$}}. Comparing this equation to {{the right eigenvalue equation}}, it follows immediately that {{the left eigenvectors of $A$ are the same as the [transpose](transpose.md) of the right eigenvectors of $A^\intercal$, with the same eigenvalue}}. Furthermore, since {{the characteristic polynomial of $A^\intercal$ is the same as that of $A$}}, {{the left and right eigenvectors of $A$ are associated with the same eigenvalues}}. <!--SR:!2025-12-28,426,370!2025-05-06,231,350!2025-09-09,336,370!2025-05-22,245,350!2025-06-15,248,350!2025-11-03,383,370-->

Going a step further, taking {{the [conjugate transpose](conjugate%20transpos.md)}} of the left eigenvalue equation: {{$$\begin{aligned} \mathbf u A & = \kappa \mathbf u \\ (\mathbf u A)^* & = (\kappa \mathbf u)^* \\ A^* \mathbf u^* & = \overline \kappa \mathbf u^* \end{aligned}$$}}. Comparing this equation to {{the right eigenvalue equation}}, it follows immediately that {{the left eigenvectors of $A$ are the same as the [conjugate transpose](conjugate%20transpose.md) of the right eigenvectors of $A^*$, with the [complex conjugate](complex%20conjugate.md) as the eigenvalue}}. But unlike the regular [transpose](transpose.md), {{the characteristic polynomial of $A^*$ is the [complex conjugate](complex%20conjugate.md) of that of $A$}}, so {{the left and right eigenvectors of $A$ are respectively associated with two eigenvalues that are a conjugate pair}}. <!--SR:!2025-11-02,380,370!2025-01-21,141,330!2025-03-12,174,330!2025-06-16,263,350!2025-05-13,225,350!2025-07-06,281,375-->

> [!tip] tips
>
> - geometric interpretation of left eigenvectors ::: The geometric interpretation of left eigenvectors $\mathbf y$ is slightly different as row vectors are conventionally thought of as [hyperplanes](hyperplane.md) ($n - 1$-dimensional [subspace](linear%20subspace.md)) instead, with the direction of a row vector being the [normal](normal%20(geometry).md) of the corresponding hyperplane: $\set{\mathbf x \in V : \mathbf y \mathbf x = 0}$. (Also see [dual basis](dual%20basis.md).) Under this interpretation, consider the action $\mathbf y \mapsto \mathbf y A$. Left eigenvectors $\mathbf y'$ are identified with hyperplanes through the origin that are perpendicular to the row vectors that are sent to themselves or the entire vector space $V$ after the action: $$\set{\mathbf x \in V : (\mathbf y' A) \mathbf x = 0} = \set{\mathbf x \in V : \lambda \mathbf y' \mathbf x = 0}$$. Note that $\lambda$ can be zero, so $\lambda \mathbf y' \mathbf x = 0 \;\not\nobreak\!\!\!\!\implies \mathbf y' \mathbf x = 0$. If $A$ is [invertible](invertible%20matrix.md), equivalently, each point $\mathbf x$ of the hyperplane is transformed by $A^{-1}$: $$0 = \mathbf y' \mathbf x = (\mathbf y' A) \left(A^{-1} \mathbf x\right)$$. <!--SR:!2024-11-05,73,315!2025-05-16,220,335-->

### diagonalization and the eigendecomposition

- see: [eigendecomposition of a matrix](eigendecomposition%20of%20a%20matrix.md)

Suppose that $A$ has {{an eigenbasis, or equivalently $n$ [linear independent](linear%20independence.md) eigenvectors $\mathbf v_1, \ldots, \mathbf v_n$ with associated eigenvalues $\lambda_1, \ldots, \lambda_n$}}. The eigenvalues need not be distinct. Define {{a [square matrix](square%20matrix.md) $Q$ whose columns are the $n$ linearly independent eigenvectors of $A$}}, {{$$Q := \begin{bmatrix} \mathbf v_1 & \cdots & \mathbf v_n \end{bmatrix}$$}}. <!--SR:!2025-08-16,298,350!2025-06-02,237,350!2025-12-01,404,370-->

Since each column of $Q$ is an eigenvector of $A$, {{left multiplying $Q$ by $A$ scales each column of $Q$ by its associated eigenvalue}}, {{$$AQ = \begin{bmatrix} \lambda_1 \mathbf v_1 & \cdots & \lambda_n \mathbf v_n \end{bmatrix}$$}}. <!--SR:!2024-12-12,104,310!2025-09-16,343,370-->

With the scaled $Q$ ($AQ$) in mind, we can {{express the same scaled $Q$ ($AQ$) in an alternative way}} by {{defining a [diagonal matrix](diagonal%20matrix.md) $\Lambda$ where each diagonal element $\Lambda_{ii}$ is the eigenvalue associated with the $i$-th column of $Q$}}. Then, {{$$AQ = Q\Lambda$$}}. <!--SR:!2025-08-20,321,370!2025-02-22,159,330!2025-09-30,350,370-->

Consider $AQ = Q\Lambda$. Since {{the columns of $Q$ are [linearly independent](linear%20independence.md)}}, {{$Q$ is [invertible](invertible%20matrix.md)}}. {{Right multiplying both sides of the equation by $Q^{-1}$}} gives a decomposition of $A$: {{$$A = Q\Lambda Q^{-1}$$}}. Alternatively, {{left multiplying both sides of the equation by $Q^{-1}$}} gives a way to [diagonalize](diagonalizable%20matrix.md) the matrix: {{$$Q^{-1}AQ = \Lambda$$}}. <!--SR:!2025-08-22,302,350!2025-04-22,219,350!2024-12-17,111,310!2025-04-09,194,330!2025-02-08,154,330!2025-01-19,143,330-->

The above relations shows that $A$ can be {{decomposed into a matrix composed of its eigenvectors, a diagonal matrix with its eigenvalues along the diagonal, and the [inverse](invertible%20matrix.md) of the matrix of eigenvectors}}. This is called {{the [eigendecomposition](eigendecomposition%20of%20a%20matrix.md) and is a [similarity transform](matrix%20similarity.md)}}. Such a matrix $A$ is said to be {{_similar_ to the diagonal matrix $\Lambda$ or [diagonalizable](diagonalizable%20matrix.md)}}. The matrix $Q$ is {{the [change of basis](change%20of%20basis.md) matrix of the similarity transformation}}. Essentially, the matrices $A$ and $\Lambda$ represent {{the same [linear transformation](linear%20map.md) expressed in two different [bases](basis%20(linear%20algebra).md)}}. {{The eigenvectors are used as the basis}} when representing the linear transformation as $\Lambda$. <!--SR:!2025-07-26,280,350!2025-02-23,160,330!2025-10-27,377,370!2025-06-17,250,350!2025-07-27,280,350!2025-07-10,268,350-->

Conversely, suppose a matrix $A$ is {{[diagonalizable](diagonalizable%20matrix.md)}}. Let $P$ be {{an [invertible matrix](invertible%20matrix.md) such that $P^{-1} A P$ equals some [diagonal matrix](diagonal%20matrix.md) $D$}}. {{Left multiplying both sides by $P$}} to get {{$A P = P D$}}. If we consider {{each $i$-th column $P_i$ individually}}, we get {{$A P_i = d_{ii} P_i$, where $d_{ii}$ is the $i$-th diagonal entry of $D$}}. This is {{exactly the eigenvalue equation}}, so {{all columns of $P$ are eigenvectors}}. Since {{the columns of $P$ must be [linearly independent](linear%20independence.md) for $P$ to be invertible}}, there exists {{$n$ linearly independent eigenvectors of $A$}}. It then follows that {{$A$ has an eigenbasis [iff](if%20and%20only%20if.md) $A$ is diagonalizable}}. <!--SR:!2025-06-11,257,350!2025-03-18,177,330!2025-11-26,402,370!2025-02-17,166,330!2024-11-25,98,310!2025-09-07,334,370!2025-09-08,335,370!2025-03-20,179,330!2025-12-01,404,370!2024-11-26,95,310!2025-05-20,230,350-->

A matrix that is not [diagonalizable](diagonalizable%20matrix.md) is said to be {{[defective](defective%20matrix.md)}}. For said matrices, the notion of eigenvectors generalizes to {{[generalized eigenvectors](generalized%20eigenvector.md)}} and the diagonal matrix of eigenvalues generalizes to {{the [Jordan normal form](Jordan%20normal%20form.md)}}. Over {{an [algebraically closed field](algebraically%20closed%20field.md)}}, any matrix $A$ has {{a Jordan normal form and therefore admits a [basis](basis%20(linear%20algebra).md) of generalized eigenvectors and a decomposition into generalized eigenspaces}}. <!--SR:!2025-06-16,251,350!2025-01-23,149,330!2025-06-19,252,350!2025-05-16,227,350!2025-04-25,227,350-->

### variational characterization

- see: [min-max theorem](min-max%20theorem.md)

For {{a [Hermitian matrix](Hermitian%20matrix.md) $H$}}, eigenvalues can be {{given a variational characterization}}. {{The largest eigenvalue of $H$}} is {{the maximum value of the [quadratic form](quadratic%20form.md) $\mathbf x^\intercal H \mathbf x / \mathbf x^\intercal \mathbf x$}}. A value of $\mathbf x$ that realizes that maximum is {{an eigenvector associated with the largest eigenvalue}}. <!--SR:!2025-10-27,377,370!2025-09-17,342,370!2025-05-13,236,350!2025-12-15,416,370!2025-06-06,241,350-->

## eigenvalues and eigenfunctions of differential operators

- see: [eigenfunction](eigenfunction.md)

The definitions of eigenvalue and eigenvectors of a [linear transformation](linear%20map.md) $T$ remains valid even if {{the underlying vector space is an infinite-dimensional [Hilbert](Hilbert%20space.md) or [Banach space](Banach%20space.md)}}. A widely used class of linear transformations acting on infinite-dimensional spaces are {{the [differential operators](differential%20operator.md) on [function spaces](function%20space.md)}}. <!--SR:!2025-12-18,418,370!2025-11-02,380,370-->

Let $D$ be {{a linear differential operator on the space $C^\infty(\mathbb R, \mathbb R)$ of infinitely differentiable real functions of a real argument $t$}}. The eigenvalue equation for $D$ is {{the [differential equation](differential%20equation.md): $$D f(t) = \lambda f(t)$$}}. The functions that satisfy this equation are {{eigenvectors of $D$ and are commonly called __eigenfunctions__}}. <!--SR:!2025-02-24,161,330!2025-11-06,385,370!2025-10-27,377,370-->

### derivative operator example

Consider {{the [derivative operator](derivative%20operator.md) $\frac {\mathrm d} {\mathrm dt}$}} with eigenvalue equation {{$$\frac {\mathrm d} {\mathrm dt} f(t) = \lambda f(t)$$}}. This [differential equation](differential%20equation.md) can be solved by {{multiplying both sides by $\mathrm dt / f(t)$ and [integrating](integration%20(calculus).md)}}: {{$$\begin{aligned} \frac {\mathrm d} {\mathrm dt} f(t) & = \lambda f(t) \\ \frac {\mathrm df(t)} {f(t)} & = \lambda \,\mathrm dt && (\text{assuming }f(t) \ne 0\text{ for simplicity}) \\ \int \! \frac {\mathrm df(t)} {f(t)} & = \int \! \lambda \,\mathrm dt \\ \ln \lvert f(t) \rvert & = \lambda t + C \\ e^{\lambda t + C} & = \lvert f(t) \rvert \\ C e^{\lambda t} & = f(t) && (\text{absorb }e^*\text{ and }\lvert * \rvert\text{ into }C) \\ f(t) & = f(0) e^{\lambda t} && \left( f(0) = C e^{\lambda 0} = C \right) \end{aligned}$$}}. <!--SR:!2025-09-28,353,370!2025-06-29,276,350!2025-09-30,350,370!2025-02-07,153,330-->

The solution of the above differential equation, $f(t) = f(0) e^{\lambda t}$, is {{the eigenfunction of the [derivative operator](derivative%20operator.md)}}. In this case, the eigenfunction is {{itself a function of its associated eigenvalue}}. In particular, for $\lambda = 0$ {{the eigenfunction $f(t)$ is a constant}}. The $f(0)$ can be an arbitrary nonzero constant in the same way as {{scaling an eigenvector by an arbitrary nonzero constant still gives an eigenvector associated with the same eigenvalue}}. <!--SR:!2025-08-03,307,370!2025-11-23,397,370!2025-08-01,288,350!2025-12-15,416,370-->

## general definition

The concept of eigenvalues and eigenvectors extends naturally to {{arbitrary [linear transformations](linear%20map.md) on arbitrary [vector spaces](vector%20space.md)}}. Let $V$ be {{any vector space over some [field](field%20(mathematics).md) $K$ of [scalars](scalar%20(mathematics).md), and let $T$ be a linear transformation mapping $V$ into itself, $$T : V \to V$$}}. <!--SR:!2025-05-19,229,350!2025-09-20,345,370-->

We say that {{a nonzero [vector](vector%20space.md) $\mathbf v \in V_{\ne \mathbf 0}$}} is an __eigenvector__ of $T$ {{[iff](if%20and%20only%20if.md) there exists a [scalar](scalar%20(mathematics).md) $\lambda \in K$ such that $$T(\mathbf v) = \lambda \mathbf v$$}}. This equation is called {{the __eigenvalue equation__ for $T$, and the scalar $\lambda$ is the __eigenvalue__ of $T$ corresponding to the eigenvector $\mathbf v$}}. $T(\mathbf v)$ is {{the result of applying the transformation $T$ to the vector $\mathbf v$, while $\lambda \mathbf v$ is the product of the scalar $\lambda$ with $\mathbf v$}}. <!--SR:!2025-10-27,377,370!2026-01-10,437,370!2025-06-09,258,350!2025-12-15,416,370-->

### general eigenspaces

Given an eigenvalue $\lambda$, consider the set {{$$E = \set{ \mathbf v : T(\mathbf v) = \lambda \mathbf v }$$}}, which is {{the [union](union%20(set%20theory).md) of the zero vector and the set of all eigenvectors associated with the eigenvalue $\lambda$}}. <!--SR:!2024-11-01,92,355!2026-01-10,437,375-->

By the definition of [linear transformation](linear%20map.md), the following properties hold: {{$$\begin{aligned} T(\mathbf x + \mathbf y) & = T(\mathbf x) + T(\mathbf y) \\ T(\alpha \mathbf x) & = \alpha T(\mathbf x) \end{aligned}$$}} for {{arbitrary $\mathbf x, \mathbf y \in V$ and arbitrary $\alpha \in K$}}. Therefore, if $\mathbf u, \mathbf v$ are eigenvectors of $T$ associated with eigenvalue $\lambda$, namely $\mathbf u, \mathbf v \in E$, then {{$$\begin{aligned} T(\mathbf u + \mathbf v) & = T(\mathbf u) + T(\mathbf v) = \lambda \mathbf u + \lambda \mathbf v = \lambda (\mathbf u + \mathbf v) \\ T(\alpha \mathbf u) & = \alpha T(\mathbf u) = \alpha (\lambda \mathbf u) = \lambda (\alpha \mathbf u) \end{aligned}$$}}. <!--SR:!2025-11-03,383,375!2025-04-28,225,355!2025-12-10,414,375-->

So, both $\mathbf u + \mathbf v$ and $\alpha \mathbf u$ are {{either zero or eigenvectors of $T$ associated with $\lambda$, namely $\mathbf u + \mathbf v, \alpha \mathbf u \in E$}}, and thus {{$E$ is closed under [addition](addition.md) and [scalar multiplication](scalar%20multiplication.md)}}. The eigenspace $E$ associated with $\lambda$ is therefore {{a [linear subspace](linear%20subspace.md) of $V$}}. If {{that subspace has [dimension](dimension%20(vector%20space).md) 1}}, it is sometimes called {{an __eigenline__}}. <!--SR:!2025-11-19,394,375!2025-10-27,377,375!2025-05-27,247,355!2025-12-24,423,375!2025-09-30,355,375-->

### general geometric multiplicity

The __geometric multiplicity__ {{$\gamma_T(\lambda)$}} of an eigenvalue $\lambda$ is {{the [dimension](dimension%20(vector%20space).md) of the eigenspace associated with $\lambda$, i.e., the maximum number of [linearly independent](linear%20independence.md) eigenvectors associated with that eigenvalue}}. By {{the definition of eigenvalues and eigenvectors}}, {{$\gamma_T(\lambda) \ge 1$ because every eigenvalue has at least one eigenvector}}. <!--SR:!2025-07-10,283,355!2024-11-03,92,358!2025-08-29,309,355!2024-11-01,92,355-->

### general eigenbasis

The eigenspaces of $T$ always {{form a [direct sum](direct%20sum.md)}}. As a consequence, eigenvectors of _different_ eigenvalues are {{always [linearly independent](linearly%20independent.md)}}. Therefore, the sum of the [dimensions](dimension%20(vector%20space).md) of the eigenspaces cannot {{exceed the dimension $n$ of the vector space on which $T$ operates}}, and there cannot be {{more than $n$ distinct}} eigenvalues. <!--SR:!2025-12-10,414,375!2025-11-06,385,375!2025-12-24,423,375!2025-05-24,244,355-->

Any [subspace](linear%20subspace.md) [spanned](linear%20span.md) by eigenvectors of $T$ is {{an [invariant subspace](invariant%20subspace.md) of $T$ (i.e. applying $T$ on any vectors in the subspace produces a vector in the subspace)}}, and the restriction of $T$ {{to such a subspace is [diagonalizable](diagonalizable%20matrix.md)}}. Moreover, if {{the entire vector space $V$ can be [spanned](linear%20span.md) by the eigenvectors of $T$, or equivalently if the [direct sum](direct%20sum.md) of the eigenspaces associated with all the eigenvalues of T is the entire vector space $V$}}, then {{a [basis](basis%20(linear%20algebra).md) of $V$ called an __eigenbasis__ can be formed from [linearly independent](linear%20independence.md) eigenvectors of $T$}}. When $T$ admits {{an __eigenbasis__, $T$ is diagonalizable}}. <!--SR:!2025-02-13,164,335!2025-05-09,234,355!2025-02-08,159,335!2025-03-05,170,335!2025-12-18,418,375-->

### spectral theory

- see: [spectral theory](spectral%20theory.md)

If {{$\lambda$ is an eigenvalue of $T$}}, then {{the operator $(T − \lambda I)$ is not one-to-one, and therefore its [inverse](invertible%20matrix.md) $(T − \lambda I)^{-1}$ does not exist}}. The converse is {{true for finite-[dimensional](dimension%20(vector%20space).md) [vector spaces](vector%20space.md), but not for infinite-dimensional vector spaces}}. In general, the operator $(T − \lambda I)$ {{may not have an inverse even if $\lambda$ is not an eigenvalue}}. <!--SR:!2025-06-04,239,355!2025-08-08,304,355!2025-12-15,416,375!2025-09-06,315,355-->

For this reason, in [functional analysis](functional%20analysis.md) eigenvalues can be {{generalized to the [spectrum](spectrum%20(functional%20analysis).md) of a [linear operator](linear%20map.md) $T$ as the set of all [scalars](scalar%20(mathematics).md) $\lambda$ for which the operator $(T − \lambda I)$ has no bounded inverse}}. The spectrum of an operator always {{contain all its eigenvalues but is not limited to them}}. <!--SR:!2024-12-26,115,315!2025-12-24,423,375-->

## dynamic equations

- see: [ordinary differential equation](ordinary%20differential%20equation.md), [recurrence relation](recurrence%20relation.md)

The simplest [recurrence relation](recurrence%20relation.md) have the form {{$$x_t = a_1 x_{t - 1} + a_2 x_{t - 2} + \cdots + a_k x_{t - k}$$, where $a_n$ are arbitrary complex constants}}. The solution of this equation for $x$ in terms of $t$ is found by {{using its [characteristic equation](characteristic%20equation%20(calculus).md) $$\lambda^k - a_1 \lambda^{k - 1} - a_2 \lambda^{k - 2} - \cdots - a_k \lambda^0 = 0$$}}, which can be found by {{stacking into matrix form a set of equations consisting of the above recurrence relation and the $k - 1$ equations $x_{t - 1} = x_t, x_{t - 2} = x_{t - 1}, \ldots, x_{t - k + 1} = x_{t - k}$}}: {{$$\begin{aligned} \begin{bmatrix} x_t \\ x_{t - 1} \\ x_{t - 2} \\ \vdots \\ x_{t - k + 1} \end{bmatrix} = \begin{bmatrix} a_1 & a_2 & \cdots & a_{k - 1} & a_k \\ 1 & 0 & \cdots & 0 & 0 \\ 0 & 1 & \cdots & 0 & 0 \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ 0 & 0 & \cdots & 1 & 0 \end{bmatrix} \begin{bmatrix} x_{t - 1} \\ x_{t - 2} \\ x_{t - 3} \\ \vdots \\ x_{t - k} \end{bmatrix} \end{aligned}$$}}, which is {{a $k$-dimensional system of the first order relating the state vector $\begin{bmatrix} x_t & x_{t - 1} & \cdots & x_{t - k + 1} \end{bmatrix}^\intercal$ in terms of its once-lagged state vector $\begin{bmatrix}x_{t - 1} & x_{t - 2} & \cdots & x_{t - k} \end{bmatrix}^\intercal$}}. The [determinant](determinant.md) of {{$(\lambda I - A)$, where $A$ is the transition matrix above, gives the above characteristic polynomial}}. The above characteristic equation {{gives $k$ solutions $\lambda_1, \ldots, \lambda_k$}}, for use {{in forming the solution to the recurrence relation}}: {{$$x_t = c_1 \lambda_1^t + c_2 \lambda_2^t + \cdots + c_k \lambda_k^t = \sum_{i = 1}^k c_i \lambda_i^t$$, where $c_i$ are arbitrary complex constants (which are usually determined by the initial states)}}. <!--SR:!2025-08-06,291,355!2025-03-04,169,335!2024-11-22,92,315!2025-01-04,122,315!2025-06-07,257,355!2026-01-15,441,378!2025-09-18,325,355!2025-08-13,316,375!2025-02-17,156,335-->

In detail, it is clear that the above system's evolution is described by {{left multiplying the state vector by the transition matrix $A$ repeatedly, i.e. [power iteration](power%20iteration.md)}}. This means {{the state vector tends to an eigenvector of $A$}}. Furthermore, as the effect of left multiplying an eigenvector by $A$ is {{the same as multiplying the eigenvector by its associated eigenvalue $\lambda$}}, so the above equation {{tends to simplify into the following as the state vector tends to an eigenvector}}: {{$$\begin{bmatrix} x_t \\ \vdots \\ x_{t - k + 1} \end{bmatrix} = \lambda \begin{bmatrix} x_{t - 1} \\ \vdots \\ x_{t - k} \end{bmatrix}$$}}, from which we can {{extract a simple recurrence relation: $$x_t = \lambda x_{t - 1}$$}}, or in a closed expression of the solution: {{$$x_t = x_0 \lambda^t$$}}. Of course, $A$ has {{$k$ eigenvalues (with multiplicity) and $k$ eigenvectors (with multiplicity), and each pair of them produces a valid closed expression of the solution}}. As the solutions are {{linear, which is evident from that the recurrence relation can be expressed as the matrix $A$}}, the solution in {{its most general form is simply adding all $k$ solutions together}}, producing the solution in the above paragraph. <!--SR:!2025-09-29,354,375!2025-11-06,385,375!2025-06-25,271,355!2026-01-04,432,375!2025-05-10,234,355!2025-10-27,377,375!2025-09-30,350,375!2025-09-30,350,375!2024-11-01,92,355!2025-06-03,253,355-->

A somewhat similar idea can also be applied to {{solving simple [ordinary differential equations](ordinary%20differential%20equation.md) (see [matrix differential equation](matrix%20differential%20equation.md))}}, which has the following form: {{$$D^k x + a_{k - 1} D^{k - 1} x + \cdots + a_1 D^1 x + a_0 x = 0$$, where $D = \frac {\mathrm d} {\mathrm dx}$ is the [differential operator](differential%20operator.md) (n.b. $D^n = \frac {\mathrm d^n} {\mathrm dx^n}$), and $a_n$ are arbitrary complex constants}}. <!--SR:!2025-12-01,404,375!2025-01-08,134,335-->

## calculation

### classical method

#### eigenvalues

The eigenvalues of a matrix $A$ can be determined by {{finding the roots of the [characteristic polynomial](characteristic%20polynomial.md) ($\det(tI - A) = 0$)}}. This is {{easy for $2 \times 2$ matrices, but the difficulty increases rapidly with the size of the matrix}}. <!--SR:!2024-12-04,114,290!2025-06-26,288,330-->

#### eigenvectors

Once {{the (exact) value of an eigenvalue is known}}, the corresponding eigenvector can be {{calculated directly by finding nonzero solutions of the eigenvalue equation}}, which becomes {{a [system of linear equations](system%20of%20linear%20equations.md) with known coefficients}}. <!--SR:!2025-02-19,175,310!2025-06-13,262,290!2025-01-18,159,310-->

## references

This text incorporates [content](https://en.wikipedia.org/wiki/eigenvalues_and_eigenvectors) from [Wikipedia](Wikipedia.md) available under the [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) license.
