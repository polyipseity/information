---
aliases:
  - COMP 3211
  - COMP 3211 index
  - COMP3211
  - COMP3211 index
  - Fundamentals of Artificial Intelligence
  - Fundamentals of Artificial Intelligence index
  - HKUST COMP 3211
  - HKUST COMP 3211 index
  - HKUST COMP3211
  - HKUST COMP3211 index
tags:
  - flashcard/active/special/academia/HKUST/COMP_3211
  - function/index
  - language/in/English
---

# index

- HKUST COMP 3211
- name: Fundamentals of Artificial Intelligence

The content is in teaching order.

- grading
  - scheme
    - assignment ×3: 15%
    - midterm exam: 35%
      - datetime: 2025-03-26T19:30:00+08:00/2025-03-26T21:30:00+08:00, PT2H
    - final exam: 50%
- logistics
  - objectives ::@:: appreciate AI problem solving; learn fundamental AI algorithm; understand AI challenges and goals <!--SR:!2025-10-22,183,310!2026-01-13,255,330-->
  - course intended learning outcomes (CILOs) ::@:: appreciate cutting edge AI research; identify AI fundamental concepts and techniques; understand and apply state space search techniques <!--SR:!2026-02-07,276,330!2026-08-25,417,310-->
  - syllabus
    - simple intelligent agents: machine evolution, machine learning, rule-based systems
    - search: adversarial, heuristic, uniformed
    - learning
    - knowledge planning, knowledge reasoning, knowledge representation
    - auction, game theory, multi-agent systems
    - uncertainty

## children

- [assignments](assignments/index.md)
<!-- - [questions](questions.md) -->

## week 1 tutorial

- datetime: 2025-02-04T12:30:00+08:00/2025-02-04T13:20:00+08:00
- status: unscheduled, no tutorial

## week 1 lecture

- datetime: 2025-02-05T13:30:00+08:00/2025-02-04T14:50:00+08:00
- topic: introduction, reactive agents
- [artificial intelligence](../../../../general/artificial%20intelligence.md) (AI) ::@:: John McCarthy, one of the founders of AI — "It is the science and engineering of making intelligent machines, especially intelligent computer programs. It is related to the similar task of using computers to understand human intelligence, but AI does not have to confine itself to methods that are biologically observable." <!--SR:!2026-10-18,457,310!2026-08-12,406,310-->
  - artificial intelligence / intuition ::@:: What is it? A calculator? An operating system? A text editor? ChatGPT or DeepSeek? <!--SR:!2025-12-31,242,330!2025-12-30,241,330-->
  - artificial intelligence / motivations ::@:: forefront of computer applications; make machines do things so far that can be done by humans; in the long-term, make machines that can act, sense, and think intelligently <!--SR:!2025-12-29,240,330!2026-01-30,268,330-->
  - artificial intelligence / algorithms, techniques ::@:: heuristic search algorithms, knowledge representation languages, machine learning algorithms and reasoners, etc. <!--SR:!2026-10-19,443,310!2026-10-20,444,310-->
  - artificial intelligence / impacts ::@:: engineering, science; everyday life; humanity in general <!--SR:!2026-10-18,442,310!2026-03-09,285,330-->
  - artificial intelligence / in computer science (CS) ::@:: science and engineering of making intelligent computer programs, the frontier of computer applications; new ways to solve problems with a computer <!--SR:!2026-09-09,423,310!2026-09-18,432,310-->
  - artificial intelligence / origins ::@:: Ada Lovelace (1815–1852), Alan Turing (1912–1954) <!--SR:!2025-10-12,162,270!2026-08-26,415,310-->
    - artificial intelligence / origins / Ada Lovelace (1815–1852) ::@:: Considered as the first computer programmer. Her Note G describes an algorithm for Babbage's analytical engine to compute Bernoulli numbers. She described the analytical engine as able to do what we tell it to perform, but cannot anticipate things that we do not tell it to perform. <!--SR:!2026-09-25,439,310!2026-01-05,247,330-->
    - artificial intelligence / origins / Alan Turing (1912–1954) ::@:: Computing machinery and intelligence. _Mind_, 59:433-460, 1950. — "I propose to consider the question, 'Can machines think?' This should begin with definitions of the meaning of the terms 'machine' and 'think.'" <!--SR:!2025-12-29,215,270!2025-12-04,190,270-->
      - Turing test ::@:: A test to empirically determine whether a computer has achieved intelligence. A human questioner questions two respondents, one human, the other a computer function, whose identities are unknown to the human questioner. Using a specified format and context, the questioner interrogates the two respondents within a specific subject area. After a preset length of time or number of questions, the questioner is then asked to decide which respondent is human and which is a computer. <!--SR:!2025-09-25,166,310!2026-01-13,255,330-->
  - artificial intelligence / history
    - artificial intelligence / history / 1956 ::@:: John McCarthy coined the term "__artificial intelligence__" as the topic of the Dartmouth Conference, the first conference devoted to the subject. The conference was proposed by John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon: "2 month, 10 man study of artificial intelligence". <!--SR:!2025-12-21,207,270!2025-12-07,193,270-->
    - artificial intelligence / history / Marvel (1992) ::@:: a real-time expert system that monitors the massive stream of data from the Voyager spacecraft, handling routine tasks, and alerting the analysts to more serious problems at Jet Propulsion Laboratory (JPL) <!--SR:!2026-02-22,270,290!2026-08-15,408,310-->
    - artificial intelligence / history / Pegasus (1994) ::@:: a speech understanding program that handles airline ticket reservations <!--SR:!2026-08-05,399,310!2026-10-26,450,310-->
    - artificial intelligence / history / Gulf War ::@:: An AI planning and scheduling system employed by US military. A DARPA report said the saving from this AI system had paid back the US Government all its investment on AI during the past 20 years. <!--SR:!2025-09-20,162,310!2026-01-26,264,330-->
    - artificial intelligence / history / older AIs & misc ::@:: Deep Blue (1997), IBM Watson (2011), AlphaFold (2020) <!--SR:!2025-10-16,166,270!2026-08-11,405,310-->
    - artificial intelligence / history / playing Go ::@:: AlphaGo (2016), AlphaGo Zero (2017) <!--SR:!2026-09-22,436,310!2025-10-02,172,310-->
    - artificial intelligence / history / GPT ::@:: GPT-3 (2020), ChatGPT (2022), GPT4 (2023), GPT4o (2024) <!--SR:!2026-11-12,477,310!2025-08-23,122,270-->
    - artificial intelligence / history / chatbots ::@:: ELIZA (1963, Weizenbaum; a computer psychotherapist) <br/> BlenderBot 3 (2022, Meta; letdown) <br/> AI Test Kitchen (2022, Google; announced only) <br/> ChatGPT (2022, OpenAI; breakthrough) <!--SR:!2026-04-11,289,290!2026-10-27,451,310-->
  - artificial intelligence / brittleness ::@:: Current AI systems are still brittle. Examples include hallucinations, object recognition, sensitivity to disturbance, etc. <!--SR:!2026-09-05,419,310!2026-08-16,409,310-->
- [hallucination](../../../../general/hallucination%20(artificial%20intelligence).md) ::@:: A response generated by AI that contains false or misleading information presented as fact, e.g. false negatives, false positives, incorrect predictions. It is associated with unjustified responses or beliefs rather than perceptual experiences. <!--SR:!2026-09-17,431,310!2026-08-24,415,310-->
  - hallucination / causes ::@:: These errors can be caused by a variety of factors, including data biases, incorrect model assumptions, insufficient training data, etc. <!--SR:!2025-09-22,164,310!2026-01-13,255,330-->
  - hallucination / examples ::@:: DALL-E 2 or Midjourney unable to create the image you want because the AI does not understand your desired context. <!--SR:!2026-08-14,408,310!2025-09-12,158,310-->
- [model collapse](../../../../general/model%20collapse.md) ::@:: It is a phenomenon where machine learning models gradually degrade due to errors coming from uncurated training on the outputs of another model, including prior versions of itself. Such outputs are known as synthetic data. <p> A study in 2024 shows this. For example, after successive generations, nonsense text are generated. Biases in the dataset get amplified, e.g. forgetting obscure dog breeds exist due to under-representation in the initial dataset. <!--SR:!2026-08-13,407,310!2026-01-01,243,330-->
- [intelligent agent](../../../../general/intelligent%20agent.md) ::@:: It is an entity that perceives its environment, takes actions autonomously to achieve goals, and may improve its performance through machine learning or by acquiring knowledge. <!--SR:!2026-02-17,265,349!2026-02-21,269,349-->
  - intelligent agent / features ::@:: can perceive, can perform actions, has an objective/goal <!--SR:!2026-06-23,383,369!2026-02-19,267,349-->
  - intelligent agent / learning objectives \(this course\) ::@:: stimulus-response agent (stateless), then add states <!--SR:!2026-08-08,408,369!2026-02-20,268,349-->
  - intelligent agent / control ::@:: e.g. designed vs. evolved/learned, genetic algorithms, neural networks, rules <!--SR:!2026-06-24,384,369!2026-06-04,364,369-->
  - intelligent agent / stimulus-response agent ::@:: Stateless machines that reacts to _immediate_ stimulus from the environment. <!--SR:!2026-08-08,412,369!2026-06-23,383,369-->
    - intelligent agent / stimulus-response agent / example ::@:: actions: up/down/left/right (cannot go into walls) <br/> environment: an enclosed 2D grid with walls <br/> objective: follow the boundary of the first obstacle met <br/> perceptions: 8 sensors for 8 adjacent cells, each testing if the corresponding cell is occupied <br/> policy: an algorithm to control the robot <!--SR:!2026-08-05,406,369!2026-06-24,384,369-->

## week 1 lecture 2

- datetime: 2025-02-07T13:30:00+08:00/2025-02-07T14:50:00+08:00
- status: canceled

> Dear All,
>
> The lectures tomorrow (both L1 and L2) will be canceled due to my sickness.
>
> Because of the canceled lectures tomorrow, the tutorial starting date changes accordingly: T2 will start on Feb 18. But T1 still starts on Feb 14 (next Friday). You may check the tutorial arrangements at the tutorial page.
>
> Sorry for any possible inconvenience that may cause. See you next week!
>
> Best regards,
> \[redacted\]

## week 2 tutorial

- datetime: 2025-02-11T12:30:00+08:00/2025-02-11T13:20:00+08:00
- status: canceled

## week 2 lecture

- datetime: 2025-02-12T13:30:00+08:00/2025-02-12T14:50:00+08:00
- topic: reactive agents
- [perceptron](../../../../general/perceptron.md)
  - perceptron / alias ::@:: threshold logical unit (TLU) <!--SR:!2026-06-15,375,363!2026-06-27,387,363-->
  - perceptron / supervised learning ::@:: It requires a training dataset. Each data in the dataset consists of _n_ _inputs_ as a _n_-dimensional vector and a _label_ (e.g. desired output). <p> Then, the task is computing a function that computes a "good" label from arbitrary _n_ inputs. This usually means agreeing with the training dataset's labels as much as possible (accuracy metric). <p> Here, we consider using a threshold linear unit (TLU; a.k.a perceptron) to learn the function. <!--SR:!2025-09-02,146,323!2026-04-11,318,353-->
  - perceptron / computation ::@:: Given a data vector $\mathbf x$; and weights $\mathbf w$, the bias $\theta$, and the activation function $f$ of a perceptron, the result is computed by: $$y = f(\mathbf x \cdot \mathbf w + \theta) \,.$$ <p> Given $n$ row data vectors vertically stacked as a matrix $\mathbf X$, the $n$ results as a column vector is computed by: $$\mathbf y = f\left(\mathbf X \mathbf w + \mathbf 1 \theta \right) \,.$$ <!--SR:!2026-06-07,367,363!2025-12-18,229,343-->
    - perceptron / computation / activation function ::@:: $$f(u) = \begin{cases} 1 & \text{if }u \ge \theta \\ 0 & \text{if }u < \theta \end{cases}$$ <p> Usually, the threshold _θ_ is chosen to be 0. This is because we can always add to a perceptron, an new input that is always 1, and its weight set to the negation of the original threshold. Such a weight is known as the _bias_. <!--SR:!2026-06-26,386,363!2026-06-13,373,363-->
  - perceptron / Boolean functions ::@:: A function whose inputs are Booleans (0: false, 1: true) and the output is also a Boolean (0: false, 1: true). <p> _Linearly separable_ functions can be _learnt_ by a perceptron. But not all such functions are linearly separable, e.g. $x_1 \overline {x_2} + \overline {x_1} x_2$ (($x_1$ AND NOT $x_2$) OR (NOT $x_1$ AND $x_2$)) \(or better known as $x_1 \oplus x_2$ \($x_1$ XOR $x_2$\)\). <!--SR:!2026-04-05,312,353!2026-04-21,328,353-->
  - [perceptron § steps](../../../../general/perceptron.md#steps)
    - [perceptron § steps](../../../../general/perceptron.md#steps) / initialization ::@:: Initialize the weights arbitrarily. Weights may be initialized to 0 or small random values. <!--SR:!2026-04-30,337,353!2026-04-01,308,353-->
    - [perceptron § steps](../../../../general/perceptron.md#steps) / training ::@:: For each sample $j$ in the training dataset, perform the following steps over the input $\mathbf{x}_j$ and the desired output $d_j$: <!--SR:!2026-04-16,323,353!2026-06-05,365,363-->
      - [perceptron § steps](../../../../general/perceptron.md#steps) / training / forward ::@:: Calculate the actual output: $$y_j(t) = f(\mathbf{w}(t) \cdot \mathbf{x}_j) = f(w_0(t) x_{j, 0} + w_1(t) x_{j, 1} + \cdots + w_n(t) x_{j, n})$$. <p> The dot product can be interpreted as a weighted sum. Using linear algebra, this is: $$y_j(t) = f\left(\mathbf w(t)^\intercal \mathbf x_j \right)$$ <!--SR:!2025-10-21,182,333!2025-10-31,192,333-->
      - [perceptron § steps](../../../../general/perceptron.md#steps) / training / backward ::@:: Update the weights: $$w_i(t + 1) = w_i(t) + r (d_j - y_j(t)) x_{j, i}$$ for all features $0 \le i \le n$. $r$ is the [learning rate](learning%20rate.md). <p> Since $x_{j, 0} = 1$ always, $w_0$ is effectively the bias $b$. Thus the above algorithm already includes updating the bias: $$b(t + 1) = b(t) + r(d_j - y_j(t)) \,.$$ <p> Using linear algebra, this is: $$\mathbf w(t + 1) = \mathbf w(t) + r (d_j - y_j(t)) \mathbf x_j$$ <!--SR:!2026-09-02,407,313!2025-09-03,147,323-->
    - [perceptron § steps](../../../../general/perceptron.md#steps) / termination ::@:: For [offline training](offline%20training.md), the second step may be repeated until the batch or iteration error $\frac 1 s \sum_{j = 1}^s \lvert d_j - y_j(t) \rvert$ is less than an user-defined threshold $\gamma$, or a predetermined number of batches or iterations have been completed. $s$ is the batch or iteration (not to be confused with epoch) size. <p> If the training set is linearly separable, the above (i.e. termination) will eventually happen. The number of steps depends on the dataset ordering, initial weights, learning rate. <!--SR:!2025-10-28,189,333!2025-11-09,200,333-->
  - perceptron / biological neuron ::@:: inputs: dendrites; node/computation unit: cell nucleus; outputs: axons; weights: synapses <p> TLUs are is a very simple model of biological neuron. <!--SR:!2026-04-02,309,353!2026-05-17,354,363-->
  - perceptron / output ::@:: \(__this course__: A perceptron can only output 0 or 1 \(after activation function\).\) <!--SR:!2026-07-22,395,383!2026-09-21,452,383-->
- [linear separability](../../../../general/linear%20separability.md)
  - linear separability / determination ::@:: It is co-NP-complete to decide whether a Boolean function given in disjunctive or conjunctive normal form is linearly separable. <p> For manual determination, it is usually impossible. The most common situation in which you can manually determine it is by finding a XOR operation in the Boolean function. \(This is tested in assignment 1 and likely in examinations.\) <!--SR:!2025-08-04,116,391!2027-03-31,606,411-->
- [activation function](../../../../general/activation%20function.md) ::@:: In the context of an artificial neural network, it is a function that calculates the output of the node based on its individual inputs and their weights. Nontrivial problems can be solved using only a few nodes if the above is nonlinear. <!--SR:!2026-05-16,353,363!2026-05-15,352,363-->
  - activation function / examples ::@:: ReLU (rectified linear unit), logistic (sigmoid), hyperbolic tangent (tanh) <!--SR:!2026-04-17,324,353!2025-12-17,224,333-->
- [ReLU](../../../../general/rectifier%20(neural%20networks).md) ::@:: $$f(x) = \max(0, x)$$ <!--SR:!2026-06-15,375,363!2026-04-10,317,353-->
  - ReLU / properties ::@:: continuous, linear, piecewise, differentiable almost everywhere <!--SR:!2026-05-24,353,353!2026-06-16,376,363-->
- [sigmoid](../../../../general/logistic%20function.md) ::@:: $$f(x) = \frac 1 {1 + e^{-x} }$$ <!--SR:!2026-06-20,380,363!2026-06-21,381,363-->
- [hyperbolic tangent](../../../../general/hyperbolic%20functions.md) ::@:: $$f(x) = \frac {e^{x} - e^{-x} } {e^x + e^{-x} } = \frac {e^{2x} - 1} {e^{2x} + 1}$$ <!--SR:!2026-06-26,386,363!2025-08-26,139,323-->
- [identity (activation)](../../../../general/identity%20function.md) (linear) ::@:: $$f(x) = x$$ <!--SR:!2026-05-15,352,363!2026-04-18,325,353-->
  - identity (activation) / use ::@:: For output layers, target values used to train a model with such an activation function in the output layer are typically scaled before modeling using normalization or standardization transforms. Useful for _regression problems_. <!--SR:!2025-09-01,144,323!2025-08-25,138,323-->
- [neural network](../../../../general/neural%20network%20(machine%20learning).md) ::@:: __(Artificial) neural network__ (__(A)NN__) is one of the most powerful artificial intelligence and machine learning algorithms. It can approximate any [function](../../../../general/function%20(mathematics).md) from a certain [function space](../../../../general/function%20space.md), i.e. an _universal approximator_, by the [universal approximation theorem](../../../../general/universal%20approximation%20theorem.md). <p> As the name suggests, it draws inspiration from neurons in our brain and the way they are connected. <!--SR:!2026-04-12,319,353!2025-10-25,186,333-->
  - neural network / basic structure ::@:: A directed graph whose nodes are neurons. Those with no incoming edges are inputs/sources. Those wih no outgoing edges are output/targets. The remaining are internal nodes and represent _hidden_ features. <!--SR:!2026-05-19,356,363!2026-06-09,369,363-->
  - neural network / training overview ::@:: We initialize the weights of directed edges in the graph. Then while a condition to keep updating the weights is satisfied (e.g. accuracy target, number of updates), "improve" the weights. When the condition is no longer met, training has completed. <p> Two popular ways to "improve" the weights are gradient descent and _stochastic_ gradient descent (they are different things!). <!--SR:!2026-06-29,389,363!2026-06-30,390,363-->
- intelligent agent
  - intelligent agent / basic stateless architecture ::@:: sensory input → perceptual processing → feature vector → action function (specified by the designer) → action <!--SR:!2026-06-27,387,363!2026-07-31,383,313-->
  - intelligent agent / perception ::@:: Represented using a set of $x_i$. Note that there can be illegal combinations of values for the set $x_i$. <!--SR:!2026-06-28,388,363!2026-06-10,370,363-->
  - intelligent agent / action function ::@:: It consists of several statement in the form: If the set of $x_i$ has this combination of values, then perform a certain action. <!--SR:!2025-10-26,187,333!2026-03-31,307,353-->
- [Boolean algebra](../../../../general/Boolean%20algebra.md)
  - Boolean algebra / overview ::@:: $a$ AND $b$: $ab$ or $a \cdot b$; $a$ OR $b$: $a + b$; NOT $a$: $\overline a$ <p> AND and OR are associative and commutative. <!--SR:!2026-05-19,348,353!2025-10-31,192,333-->
- [production system](../../../../general/production%20system%20(computer%20science).md) ::@:: Essentially a giant if-else if-...-else program. <p> It is a _sequence_ (so it is ordered) in the form of: $c \to a$, where $c$ is a Boolean function and $a$ is an action. Find the first statement where $c$ evaluates to 1. Its action $a$ is the action the agent will take. <p> Commonly, we add a fallback case: $1 \to a$, where $1$ is the constant Boolean function always returning true. <!--SR:!2025-10-26,187,333!2026-05-23,352,353-->

## week 2 lecture 2

- datetime: 2025-02-14T13:30:00+08:00/2025-02-14T14:50:00+08:00
- topic: reactive agents
- [genetic programming](../../../../general/genetic%20programming.md) (GP) ::@:: An evolutionary algorithm, an artificial intelligence technique mimicking natural evolution, which operates on a population of programs. <!--SR:!2025-10-29,190,333!2026-06-29,389,363-->
  - genetic programming / motivation ::@:: Human evolves to become what we are now (unless you are not human). Maybe we can adapt evolution for machine learning? Evolution has two key components: reproduction and selection (survival of the fittest). <!--SR:!2025-10-25,186,333!2026-05-19,348,353-->
  - genetic programming / overview ::@:: Decide on how to represent _legal_ programs. Define a fitness function. Select some programs as generation 0. Produce the next generation until you have a "good enough" program. <p> 3 common methods to produce next generations: copy, crossover, and mutate. <!--SR:!2026-06-11,371,363!2026-05-17,354,363-->
  - genetic programming / program representation ::@:: Typically represented as tree structures. <p> We can consider statements in a program as functions. For example, AND(a, b) stands for a AND b and IF(a, b, c) stands for if a is true, then b, otherwise c. Then a program is a composition of many functions. <p> With this function composition in mind, we convert it into a tree recursively: A constant value or a input value is represented by a leaf. A function application/call is represented by having the function itself as a node and its arguments as its children nodes. <!--SR:!2025-10-22,183,333!2025-08-30,143,313-->
  - genetic programming / initialization ::@:: _full_, _grow_, _ramped half-and-half_, etc. <!--SR:!2026-05-16,353,363!2026-05-22,351,353-->
    - genetic programming / initialization / _grow_ ::@:: It creates the individuals sequentially. Every GP tree is created starting from the root, creating functional nodes with children as well as terminal nodes up to a certain depth. <!--SR:!2025-10-25,186,333!2025-08-17,130,313-->
    - genetic programming / initialization / _full_ ::@:: It is similar to _Grow_. The difference is that all branches in a tree are of same predetermined depth. <!--SR:!2026-04-27,334,353!2026-06-08,368,363-->
    - genetic programming / initialization / _ramped half-and-half_ ::@:: It creates a populations consisting of $md-1$ parts and a maximum depth of $md$ for its trees. The first part has a maximum depth of 2, second of 3 and so on up to the $md-1$-th part with maximum depth $md$. Half of every part is created by _Grow_, while the other part is created by _Full_. <!--SR:!2026-07-15,381,313!2025-09-10,141,313-->
  - genetic programming / selection ::@:: You need a fitness function to do so. <!--SR:!2026-05-15,352,363!2025-10-22,183,333-->
    - genetic programming / selection / example ::@:: We run the program for a certain number of steps and count the number of steps with an adjacent wall. We clamp its value between 0 and 32. We do 10 runs at random starting positions and sum the values (clamped between 0 and 320). <!--SR:!2026-05-20,349,353!2026-06-12,372,363-->
  - genetic programming / reproduction ::@:: Common methods for producing a new generation: copy, crossover, mutate. <!--SR:!2026-06-14,374,363!2025-08-31,143,323-->
    - genetic programming / reproduction / copy ::@:: Copy some programs from the previous generation. They are called the _parents_. A _tournament selection_ is used: randomly select some programs and choose the best one to copy. Other methods are possible. <p> A typical percentage of the new generation copied is 10%. <!--SR:!2025-08-09,123,313!2026-05-16,353,363-->
    - genetic programming / reproduction / crossover ::@:: From the copied _parents_, select 2 programs. A randomly chosen subtree of one parent is used to replace (including the subtree root node) a randomly selected subtree of another parent. <p> A typical percentage of the new generation crossover-ed is 90%. <!--SR:!2026-01-24,241,333!2026-01-23,240,333-->
    - genetic programming / reproduction / mutate ::@:: From the copied _parents_, select 1 program. Replace a randomly selected subtree by a new randomly generated subtree. <p> A typical percentage of the new generation mutated is 1% \(i.e. rarely occurs\). <!--SR:!2025-12-12,223,343!2026-05-21,350,353-->
  - genetic programming / performance ::@:: It depends on the initial generation size, copy/crossover/mutate rates, and tournament selection parameters. <p> The wall-following example in the slides generates a perfect tree after 10 generations. <!--SR:!2026-06-30,390,363!2026-06-20,380,363-->

## week 3 tutorial

- datetime: 2025-02-18T12:30:00+08:00/2025-02-18T13:20:00+08:00
- topic: TLU basics
- [object detection](../../../../general/object%20detection.md)
  - object detection / transplanted objects ::@:: elephant in the room: Transplanting such an object onto an image, it itself is often not detected or assumes wrong identities. It also has non-local effects, causing other previously correctly detected objects to go missing. <!--SR:!2026-02-20,268,349!2026-06-22,382,369-->
- [computer vision](../../../../general/computer%20vision.md)
  - computer vision / image recognition
    - computer vision / image recognition / fooling them ::@:: Current systems are easily fooled by: adding human-undetectable noise, adding small obstructions to an image, random patterns, geometric transformation of an object in the image, etc. <!--SR:!2026-08-02,402,369!2026-06-18,378,369-->
- [natural language processing](../../../../general/natural%20language%20processing.md) (NLP)
  - natural language processing / hallucinations ::@:: They are prone to creating content that do not match real-world facts (factuality hallucination) or user inputs (faithfulness hallucination, e.g. the year of an event being replaced by the year of another event). <p> Vision language models (VLMs) are also prone to this. <!--SR:!2026-02-18,266,349!2026-02-21,269,349-->
- [artificial intelligence](../../../../general/artificial%20intelligence.md) (AI)
  - artificial intelligence / recent trend ::@:: Its performance has been improving! For example, see the performance of works from different years on an object detection benchmark (COCO). <!--SR:!2026-06-22,382,369!2026-06-30,389,369-->
- [perceptron](../../../../general/perceptron.md)
  - perceptron / alias
  - perceptron / supervised learning
  - perceptron / computation
    - perceptron / computation / activation function
  - [perceptron § steps](../../../../general/perceptron.md#steps)
    - [perceptron § steps](../../../../general/perceptron.md#steps) / initialization
    - [perceptron § steps](../../../../general/perceptron.md#steps) / training
      - [perceptron § steps](../../../../general/perceptron.md#steps) / training / forward
      - [perceptron § steps](../../../../general/perceptron.md#steps) / training / backward
    - [perceptron § steps](../../../../general/perceptron.md#steps) / termination
- [feature engineering](../../../../general/feature%20engineering.md) ::@:: It is a preprocessing step in supervised machine learning and statistical modeling which transforms raw data into a more effective set of inputs. <p> Input variables may be created, modified, or selected (filtered/filtered out). <!--SR:!2026-07-26,396,369!2026-06-17,377,369-->
  - feature engineering / example: perceptron ::@:: A perceptron can only classify linearly separable data. <p> Given a non-linearly separable data, sometimes we may be able to derive a new input from the original inputs (e.g. _z_ = _x_<sup>2</sup> + _y_<sup>2</sup>) such that the data based on the new input is linearly separable. <!--SR:!2026-06-24,384,369!2026-06-18,378,369-->
- [Anaconda](../../../../general/Anaconda%20(Python%20distribution).md) ::@:: It is an open source data science and artificial intelligence distribution platform for Python and R programming languages. <!--SR:!2026-06-17,377,369!2026-06-25,385,369-->
  - Anaconda / download ::@:: <https://anaconda.com/download> <!--SR:!2026-06-19,379,369!2026-06-25,385,369-->
  - Anaconda / installation
  - Anaconda / usage

## week 3 lecture

- datetime: 2025-02-19T13:30:00+08:00/2025-02-19T14:50:00+08:00
- topic: reactive agents, search
- [finite state machine](../../../../general/finite%20state%20machine.md)
  - finite state machine / motivation for agents ::@:: The above agents we have introduced are stateless: they only respond to the current stimuli. They may not be powerful enough. <p> State machines can additionally remembered their previous actions, features, and its internal states (kinda like mental states). Then its action function additionally accepts the previous actions, features, and internal states. <!--SR:!2026-06-25,385,363!2025-10-28,189,333-->
  - finite state machine / agent architecture ::@:: same as statelessness: sensory input → perceptual processing → feature vector → action function (specified by the designer) → action <br/> added for statefulness: feature vector, action → memory (stores previous features and actions) → perceptual processing <!--SR:!2026-09-06,417,323!2026-10-15,447,323-->
  - finite state machine / production system ::@:: They can be represented by production system like stateless machines. The only difference is that the Boolean function also accepts machine states as input, not just input features. <!--SR:!2026-05-23,352,353!2025-10-25,186,333-->
- [search problem](../../../../general/search%20problem.md) ::@::  It is a type of computational problem represented by a binary relation. Intuitively, the problem consists in finding structure "y" in object "x". <!--SR:!2025-09-11,147,343!2026-10-06,467,383-->
  - search problem / examples ::@:: missionaries and cannibals problem, 8/15 puzzle <!--SR:!2026-09-08,439,383!2026-08-10,410,383-->
- [missionaries and cannibals problem](../../../../general/missionaries%20and%20cannibals%20problem.md) ::@:: Three missionaries and three cannibals must cross a river using a boat which can carry at most two people, under the constraint that, for both banks, if there are missionaries present on the bank, they cannot be outnumbered by cannibals (if they were, the cannibals would eat the missionaries). The boat cannot cross the river by itself with no people on board. <!--SR:!2026-07-27,396,383!2025-11-11,192,343-->
  - missionaries and cannibals problem / variations ::@:: These variations do not affect the solution: <br/> One of the cannibals has only one arm and cannot row. <br/> The missionaries and cannibals become three married couples, with the constraint that no woman can be in the presence of another man unless her husband is also present. <!--SR:!2026-10-06,468,383!2026-09-16,447,383-->
  - missionaries and cannibals problem / solving ::@:: The current state is represented by a simple vector ⟨m, c, b⟩. The vector's elements represent the number of missionaries, cannibals, and whether the boat is on the wrong (original) side, respectively. <p> Then we can think of the possible actions, also as vectors ⟨m, c, 1⟩ (b = 1 will be apparent in the next sentence). The boating rowing to the correct side subtracts an action vector while rowing to the wrong side adds an action vector. <p> Now we can _search_: Construct the state tree, perhaps using DFS. Start with an initial node with the initial state vector. Each valid action vector generates a child node, and we prune away nodes that violate the constraints. Repeat until the desired node is found (⟨0, 0, 0⟩), and the path from the initial node to the desired node is the solution. <!--SR:!2025-09-17,137,323!2026-09-20,452,383-->
- [15 puzzle](../../../../general/15%20puzzle.md) (8 puzzle) ::@:: It has 15 square tiles numbered 1 to 15 in a frame that is 4 tile positions high and 4 tile positions wide, with one unoccupied position. Tiles in the same row or column of the open position can be moved by sliding them horizontally or vertically, respectively. The goal of the puzzle is to place the tiles in numerical order (from left to right, top to bottom). <!--SR:!2025-11-07,188,343!2026-04-28,327,363-->
  - 15 puzzle / 8 puzzle (in the slides) ::@:: states: any arrangement of numbers 1 to 8 in the 3-by-3 board <br/> initial state: any given state <br/> goal: any other state; in the slides, a blank in the middle, with numbers ordered clockwise starting from the top-left corner <br/> actions: move the blank left, up, right, or down <br/> (path) cost: number of actions (length of the path) <!--SR:!2026-07-24,393,383!2026-08-05,409,383-->
- search problem
  - search problem / elements ::@:: set of states, start state, goal state (goal test), successor function (deterministic actins); if cost needs to be considered \(__this course__: considered\), a path cost function <!--SR:!2026-08-07,407,383!2026-07-21,401,383-->

## week 3 lecture 2

- datetime: 2025-02-21T13:30:00+08:00/2025-02-21T14:50:00+08:00
- topic: search
- search problem
  - search problem / problem-solving agent ::@:: These agents solve search problems, and are often _goal-directed_. <p> They find the best or "good enough" solution by _systematically_ considering different paths (sequences of actions) that may lead to the goal state. The paths are in a _representation space_, and this is where the agents search in. <p> After _finding a solution_, the agents _executes_ it. <!--SR:!2025-09-18,138,323!2026-03-29,305,363-->
  - search problem / [search algorithm](../../../../general/search%20algorithm.md) (search method) ::@:: Searching a solution can be thought of lazily expanding a search tree. The root is the initial state. A leaf node is expanded by applying all possible actions to the corresponding state. If a state that is a goal state is found, return it. Otherwise, if there are no more leaf nodes to expand, we could not a find a solution and return failure. <p> Choosing a leaf node to expand defines the _search strategy_, which is what we are studying here. <!--SR:!2026-08-11,412,383!2026-07-28,397,383-->
- [search algorithm](../../../../general/search%20algorithm.md) (search method) ::@:: an algorithm designed to solve a search problem <!--SR:!2026-08-01,405,383!2026-08-29,433,383-->
  - search algorithm / types ::@:: game tree, heuristic/informed, local, uninformed/blind, etc. <!--SR:!2026-03-27,303,363!2026-03-13,289,363-->
  - search algorithm / uninformed, blind ::@:: breadth-first search (BFS), depth-first search (DFS, backtracking search), iterative deepening search (IDS, IDDFS), etc. <!--SR:!2026-07-21,390,383!2026-08-29,429,383-->
  - search algorithm / explicit graph ::@:: A graph that is not too large (or infinite) to store explicitly. Often, repetitions can be identified and avoided. <p> We can measure its number of vertices $\lvert V \rvert$ and number of edges $\lvert E \rvert$. Note that $O(\lvert E \rvert)$ varies in between $O(1)$ and $O\left(\lvert V \rvert^2 \right)$, depending on how sparse the input graph is. <!--SR:!2026-07-31,404,383!2026-08-04,415,383-->
  - search algorithm / implicit graph ::@:: A graph that is too large (or infinite) to store explicitly. <p> We can measure its branching factor $b$ (average out-degree). <!--SR:!2026-08-11,411,383!2026-09-18,449,383-->
- [breadth-first search](../../../../general/breadth-first%20search.md) (BFS) ::@:: It is an algorithm for searching a tree data structure for a node that satisfies a given property. It starts at the tree root and explores all nodes at the present depth prior to moving on to the nodes at the next depth level. Extra memory, usually a queue, is needed to keep track of the child nodes that were encountered but not yet explored. <!--SR:!2026-09-28,459,383!2026-10-07,469,383-->
  - breadth-first search / completeness ::@:: If there is a solution, BFS will find it eventually, given unlimited time and space. \(__this course__: yes\) <!--SR:!2026-06-29,379,383!2026-09-02,433,383-->
  - breadth-first search / optimality ::@:: Yes. If there are multiple solutions, BFS will find a solution with a shortest path from the root. \(__this course__: yes\) <!--SR:!2026-09-23,454,383!2026-08-16,416,383-->
  - breadth-first search / worst-case time complexity ::@:: Explicit graph (without repetition): $O(\lvert V \rvert + \lvert E \rvert)$. Implicit graph (possibly with repetition): $O\left(b^d \right)$, where $b$ is the branching factor (average out-degree) and $d$ is the number of edges to reach the solution. \(__this course__: use the implicit one\) <p> It is impractical for most real world problems, and there are algorithms with better time complexity. <!--SR:!2025-09-21,141,323!2026-08-14,415,383-->
    - breadth-first search / worst-case time complexity / intuition ::@:: For explicit graph, each vertex and edge needs to be explored in the worst case, hence $O(\lvert V \rvert + \lvert E \rvert)$. <p> For implicit graph, if $d$ is the number of edges to reach the solution, the nodes that take not greater than $d$ edges are visited in the worst case, hence $O\left(b^d\right)$ to visit them. <!--SR:!2026-07-20,390,404!2026-07-08,356,384-->
  - breadth-first search / worst-case space complexity ::@:: Explicit graph (without repetition): $O(\lvert V \rvert)$. Implicit graph (possibly with repetition): $O\left(b^d \right)$, where $b$ is the branching factor (average out-degree) and $d$ is the number of edges to reach the solution. \(__this course__: use the implicit one\) <p> It is impractical for most real world problems, and there are algorithms with better space complexity. <!--SR:!2025-09-20,140,323!2026-05-28,337,363-->
    - breadth-first search / worst-case space complexity / intuition ::@:: For explicit graph, each vertex needs to be stored in the visited set in the worst case, hence $O(\lvert V \rvert)$. <p> For implicit graph, if $d$ is the number of edges to reach the solution, the nodes that take not greater than $d$ edges are visited in the worst case, hence $O\left(b^d\right)$ to store them. <!--SR:!2025-09-01,130,404!2025-09-05,134,404-->
- search algorithm
  - search algorithm / importance ::@:: It determines the order of states expanded in the search state. Different algorithms lead to different search trees, and have different time and space complexities. <!--SR:!2026-09-08,443,383!2026-08-09,409,383-->
  - search algorithm / properties ::@:: completeness, optimality, space complexity, time complexity <!--SR:!2026-03-23,299,363!2026-10-01,462,383-->
  - search algorithm / completeness ::@:: Is the algorithm guaranteed to find a solution if it exists? <!--SR:!2026-09-03,435,383!2026-08-05,405,383-->
  - search algorithm / time complexity ::@:: How long does the algorithm take to find a solution? Note that it is frequently denoted using big O notation. <!--SR:!2026-07-27,407,383!2026-08-11,415,383-->
  - search algorithm / space complexity ::@:: How much memory does the algorithm use? Note that it is frequently denoted using big O notation. <!--SR:!2026-09-08,440,383!2026-09-17,448,383-->
  - search algorithm / optimality ::@:: Does the algorithm find an "_optimal_" solution if there are multiple solutions? <!--SR:!2026-09-09,440,383!2026-08-06,406,383-->
- [depth-first search](../../../../general/depth-first%20search.md) (DFS, backtracking search) ::@:: It is an algorithm for traversing or searching tree or graph data structures. The algorithm starts at the root node (selecting some arbitrary node as the root node in the case of a graph) and explores as far as possible along each branch before backtracking. Extra memory, usually a stack, is needed to keep track of the nodes discovered so far along a specified branch which helps in backtracking of the graph. <!--SR:!2025-11-09,190,343!2025-11-18,199,343-->
  - depth-first search / note ::@:: \(__this course__: __Important__. In breadth-first search \(BFS\), all children are generated when expanding a node. In depth-first search \(DFS\), exactly one not-yet-generated child is generated when expanding a node, including expansion due to backtracking. Implementations outside this course differ, but use this for examinations.\) <!--SR:!2025-11-26,153,428!2025-11-27,155,428-->
  - depth-first search / depth bound ::@:: A variation uses the _depth bound_, denoted $m$. It means the maximum depth (in terms of edges, inclusive) that DFS will search. Any deeper states are left un-generated (thus unexplored). <!--SR:!2026-07-24,394,383!2026-03-28,304,363-->
  - depth-first search / completeness ::@:: If there is a solution, DFS will find it eventually, given unlimited time and space. If _depth bound_ is used, we also require $m \ge d$, where $d$ is the number of edges to reach the solution. \(__this course__: yes, if $m \ge d$\) <!--SR:!2026-08-28,428,383!2026-08-22,423,383-->
  - depth-first search / optimality ::@:: If there are multiple solutions, DFS will find a solution that comes first in lexicographic DFS ordering. \(__this course__: no\) <!--SR:!2025-11-10,191,343!2026-09-04,436,383-->
  - depth-first search / worst-case time complexity ::@:: Explicit graph (without repetition): $O(\lvert V \rvert + \lvert E \rvert)$. Implicit graph (possibly with repetition): $O\left(b^m \right)$, where $b$ is the branching factor (average out-degree) and $m$ is the depth bound (number of edges). \(__this course__: use the implicit one\) <p> It is impractical for most real world problems, and there are algorithms with better time complexity. <!--SR:!2025-09-19,139,323!2025-09-19,139,323-->
    - depth-first search / worst-case time complexity / intuition ::@:: For explicit graph, each vertex and edge needs to be explored in the worst case, hence $O(\lvert V \rvert + \lvert E \rvert)$. <p> For implicit graph, if $m$ is the depth bound \(number of edges\), the each vertex not greater than $m$ edges away needs to be explored in the worst case, hence $O\left(b^m\right)$ to visit them. <!--SR:!2025-08-27,126,404!2026-11-21,484,404-->
  - depth-first search / worst-case space complexity ::@:: Explicit graph (without repetition): $O(\lvert V \rvert)$. Implicit graph (possibly with repetition): $O(m)$ or $O(bm)$, where $b$ is the branching factor \(average out-degree\) and $m$ is the depth bound (number of edges). \(__this course__: use $O(bm)$; but the implementation in the slides should be $O(m)$?\) <p> It is impractical for most real world problems, and there are algorithms with better space complexity. <!--SR:!2026-08-03,404,383!2026-02-14,254,323-->
    - depth-first search / worst-case space complexity / intuition ::@:: For explicit graph, each vertex needs to be stored in the visited set in the worst case, hence $O(\lvert V \rvert)$. <p> For implicit graph, if $m$ is the depth bound, the longest path has $m$ edges, hence $O(m)$ to store them. However, if your implementation \(which we assume above\) stores all generated but unvisited children along the path in the queue instead of generating them dynamically, then we need to multiply by $b$, hence $O(bm)$. \(__this course__: use $O(bm)$; but the implementation in the slides should be $O(m)$?\) <!--SR:!2025-09-10,138,404!2025-08-26,125,404-->
- [iterative deepening search](../../../../general/iterative%20deepening%20depth-first%20search.md) (IDS, IDDFS) ::@:: A state space/graph search strategy in which a depth-limited version of depth-first search is run repeatedly with increasing depth limits until the goal is found. <!--SR:!2026-03-30,306,363!2026-10-05,466,383-->
  - iterative deepening search / motivation ::@:: What if we want the completeness, optimality of BFS while having the space complexity of DFS? (For time complexity, BFS and DFS are similar.) <!--SR:!2026-09-21,453,383!2026-09-07,438,383-->
  - iterative deepening search / description ::@:: Perform DFS with a depth bound repeatedly. The depth bound starts from 0, and increase by 1 each time DFS finishes, until a goal state is found. <!--SR:!2026-08-27,428,383!2026-09-30,461,383-->
  - iterative deepening search / completeness ::@:: If there is a solution, IDDFS will find it eventually, given unlimited time and space. \(__this course__: yes\) <!--SR:!2026-09-24,456,383!2026-08-10,411,383-->
  - iterative deepening search / optimality ::@:: Yes. If there are multiple solutions, IDDFS will find a solution with a shortest path from the root. This is because the cumulative order in which nodes are first visited is effectively the same as in BFS. \(__this course__: yes\) <!--SR:!2026-07-31,400,383!2026-09-02,433,383-->
  - iterative deepening search / worst-case time complexity ::@:: Implicit graph (possibly with repetition): $O\left(b^d \right)$, where $b$ is the branching factor (average out-degree) and $d$ is the number of edges to reach the solution. <!--SR:!2025-09-18,138,323!2026-09-22,453,383-->
    - iterative deepening search / worst-case time complexity / intuition ::@:: For implicit graph, if $d$ is the number of edges to reach the solution, the each vertex not greater than $d$ edges away needs to be explored in the worst case, hence $O\left(b^d\right)$ to visit them. <!--SR:!2026-07-20,389,404!2026-12-10,495,406-->
  - iterative deepening search / worst-case space complexity ::@:: Implicit graph (possibly with repetition): $O(d)$ or $O(bd)$, where $b$ is the branching factor \(average out-degree\) and $d$ is the number of edges to reach the solution.  \(__this course__: use $O(bd)$; but the implementation in the slides should be $O(d)$?\) <!--SR:!2026-10-03,465,383!2025-11-12,193,343-->
    - iterative deepening search / worst-case space complexity / intuition ::@:: For implicit graph, if $d$ is the number of edges to reach the solution, the longest path has $d$ edges, hence $O(d)$ to store them. However, if your implementation \(which we assume above\) stores all generated but unvisited children along the path in the queue instead of generating them dynamically, then we need to multiply by $b$, hence $O(bd)$. \(__this course__: use $O(bd)$; but the implementation in the slides should be $O(d)$?\) <!--SR:!2026-07-08,373,386!2025-08-30,129,406-->
- search algorithm
  - search algorithm / implicit graph
    - search algorithm / implicit graph / repetition ::@:: As mentioned above, implicit graphs may repeat states. There are several ways to deal with this, in increasing effectiveness and overhead: do not return to the state we have just come from, which requires tracking the last state; do not create cycles, which requires tracking the current path; do not generate any state that has been generated, which requires tracking the set of generated states.  <p> \(Note that for A\*, unless the heuristic is also consistent, you can at most deduplicate _visited_ instead of _generated_ states, otherwise the path may not be optimal.\) <!--SR:!2026-11-14,499,396!2026-09-29,460,396-->
  - search algorithm / heuristic (informed) ::@:: The search uses a _heuristic function_, which given the current path and search tree, maps states to real numbers. The leaf with the smallest heuristic function value is expanded first. <!--SR:!2026-12-16,521,396!2026-12-12,517,396-->
    - search algorithm / heuristic / heuristic function ::@:: Given the current path and search tree, it usually measures how far the inputted state and path is from a goal state. <!--SR:!2026-12-09,514,396!2026-10-14,474,396-->
      - search algorithm / heuristic / heuristic function / examples ::@:: 8 puzzle: number of tiles out of place, current path length + number of tiles out of place, etc. <!--SR:!2026-10-26,483,396!2026-10-08,469,396-->

## week 4 tutorial

- datetime: 2025-02-25T12:30:00+08:00/2025-02-25T13:20:00+08:00
- topic: simple agents
- intelligent agent
  - intelligent agent / aspects ::@:: memory, action, optimization, etc. <!--SR:!2026-11-09,495,396!2026-10-09,470,396-->
    - intelligent agent / aspects / memory ::@:: without: stimulus-response agent, with: state machine <!--SR:!2026-10-23,481,396!2026-06-13,352,376-->
    - intelligent agent / aspects / action ::@:: neural network, rule-based <!--SR:!2026-11-26,507,396!2026-10-09,470,396-->
    - intelligent agent / aspects / optimization ::@:: genetic programming, gradient descent <!--SR:!2026-12-10,515,396!2026-06-12,362,376-->
- production system
  - production system / capabilities ::@:: goal representation, memory (e.g. How many past readings?), sensors, stopping guarantee; multi-agents? <!--SR:!2026-11-24,505,396!2026-12-13,518,396-->
- genetic programming
  - genetic programming / motivation
  - genetic programming / overview
  - genetic programming / initialization
  - genetic programming / selection
  - genetic programming / reproduction
  - genetic programming / optimization ::@:: It can be considered as a zeroth-order optimization method (evolution). That is, the fitness/loss function is used directly, and its derivatives are not used. <!--SR:!2026-11-03,489,396!2026-12-18,523,396-->
    - genetic programming / optimization / gradient descent, Newton's method ::@:: They are respectively one of the first-order and second-order optimization methods. <p> The first uses first derivatives (gradient). The second uses both first and second derivatives (Hessian matrix). <p> Usually, the higher the order of the derivatives used, the more efficient and also computationally expensive the method is. <p> The first is commonly used in deep learning and machine learning. The second is good for convex problems. <!--SR:!2026-12-17,522,396!2026-10-08,469,396-->

## week 4 lecture

- datetime: 2025-02-26T13:30:00+08:00/2025-02-26T14:50:00+08:00
- topic: search
- [A\* search algorithm](../../../../general/A*%20search%20algorithm.md) ::@:: It is a graph traversal and pathfinding algorithm that is used in many fields of computer science due to its completeness, optimality, and optimal efficiency. Given a weighted graph, a source node and a goal node, the algorithm finds the shortest path (with respect to the given weights) from source to goal. <!--SR:!2026-06-03,342,376!2026-09-17,419,336-->
  - A\* search algorithm / idea ::@:: We want to find the shortest path from $a$ to $b$. We have $g(n)$ which is just the path cost function. We also have $h(n)$ which is an _estimate_ of the path cost from $n$ to $b$. We always expand the node with the lowest $g(n) + h(n)$ until we find the goal. <p> Under some condition on $h(n)$, which is that it never overestimates the path cost from $n$ to $b$ (_admissible_), A\* must return a least-cost path. Example: On a map, $h(n)$ can be the straight line distance (such a function is further _consistent_). <!--SR:!2026-06-23,362,376!2025-09-03,137,336-->
  - A\* search algorithm / algorithm ::@:: Start with a list \(min-heap or priority queue is better\) of nodes to be (re-)expanded, which will be called the _open set_. Add the starting node to it. <p> Repeat this paragraph until the _open set_ is empty. Select and remove the first node _n_ from the _open set_. If _n_ is the goal, output _n_. Otherwise, expand _n_ and add its children \(i.e. exclude the _direct_ parent of _n_ as the parent may also happen to be a "child" of _n_\) to the _open set_. Sort the _open set_ in increasing $g(n) + h(n)$ values, tie-breaking with the deepest node first \(i.e. LIFO\). Repeat starting from this paragraph. <p> If we have reached here \(no _n_ has been outputted but the _open set_ is empty\), output _failure_. <!--SR:!2025-09-04,138,336!2026-06-15,354,376-->
    - A\* search algorithm / algorithm / deduplication ::@:: Note that when expanding _n_, we exclude the _direct_ parent of the currently-being-expanded node \(but not generated nodes, i.e. a node can be generated multiple times before being expanded\). This is a kind of deduplication. It assumes that there are no paths of negative costs, so going back to the _direct_ parent must not decrease the path cost. <p> More advanced deduplication is possible: Also track the _currently known_ cost of the cheapest path from start to $n$ (this is actually just $g(n)$ but lazily computed and updated), called $g'(n)$. For each children $n'$ of $n$, check if $g(n) + w(n \to n') < g'(n)$, and only add $n'$ to the _open set_ if yes \(if $n'$ is already in the set, replace it with this new lower cost\). <p> If $h(n)$ is not only _admissible_, but also _consistent_, then using the above more advanced deduplication, each node is visited at most once. <!--SR:!2025-11-01,157,336!2026-06-22,361,376-->
  - A\* search algorithm / vs. Dijkstra's algorithm ::@:: When $h(n) = 0$ is a constant zero function, this is just Dijkstra's algorithm. A\* achieves better performance by using heuristics to guide its search. <!--SR:!2026-11-02,488,396!2026-11-23,504,396-->
- [admissible heuristic](../../../../general/admissible%20heuristic.md) ::@:: A heuristic function is said to be __admissible__ if it never overestimates the cost of reaching the goal, i.e. the cost it estimates to reach the goal is not higher than the lowest possible cost from the current point in the path. In other words, it should act as a lower bound. <p> Intuitively, as one walks along _any_ path from the start to the goal, $g(n) + h(n)$ is always not greater than the actual cost to the goal. <!--SR:!2026-12-21,526,396!2026-12-22,527,396-->
- [consistent heuristic](../../../../general/consistent%20heuristic.md) ::@:: A heuristic function is said to be __consistent__, or __monotone__, if its estimate is always less than or equal to the estimated distance from any neighbouring vertex to the goal, plus the cost of reaching that neighbour. <p> Intuitively, as one walks along _any_ path from the start to the goal, $g(n) + h(n)$ is non-decreasing. <!--SR:!2026-11-06,492,396!2026-11-28,509,396-->
  - consistent heuristic / example ::@:: A simple example is $h(n) = 0$. A more practical example is on a map, $h(n)$ is the straight line distance. <!--SR:!2026-10-17,475,396!2026-11-23,506,396-->
  - consistent heuristic / from admissible heuristic ::@:: If the number of operators is finite (thus finite branching factor), and the cost of each operator is greater than a positive real $\varepsilon$, then an admissible heuristic can be made into a consistent heuristic. <!--SR:!2026-11-10,496,396!2026-11-03,489,396-->
- A\* search algorithm
  - A\* search algorithm / behavior ::@:: Assuming $h(n)$ is _admissible_. Let $f*$ be the path cost to the optimal solution. Then the algorithm will explore all nodes with $f(n) < f*$, some of the non-solution nodes with $f(n) = f*$, and none of the nodes with $f(n) > f*$. <p> Assuming $h(n)$ is further _consistent_ (implies being _admissible_). Then the nodes are visited in increasing $f(n) = g(n) + h(n)$ up until $f*$ (inclusive). Some of the non-solution nodes with $f(n) = f$ may be visited. Further, it is _optimally efficient_ in that no other algorithms expand a subset of the nodes expanded by this algorithm (Dechter and Pearl, 1985). <!--SR:!2026-01-26,235,356!2026-11-13,498,396-->
  - A\* search algorithm / completeness ::@:: If there is a solution, yes, for finite graphs with nonnegative edges, and infinite graphs with finite branching factors and edge costs larger than a positive real $\varepsilon$. If there are no solutions, the former is guaranteed to terminate while the latter is not. <p> Proof is by realizing that there are a finite number of nodes to expand (nodes with $f(n) < f*$) before expanding nodes with $f*$. <!--SR:!2026-10-09,470,396!2026-11-27,508,396-->
  - A\* search algorithm / optimality ::@:: Given the heuristic function is _admissible_, yes. <p> Proof is by assuming a non-optimal path is outputted and deriving a contradiction: <p> __S__<sub>__true__</sub> \< __T__<sub>__true__</sub>: __S__ is an optimal path and __T__ is a non-optimal path. <br/> __T__<sub>__eval__</sub> ≤ __S__<sub>__eval__</sub>: __T__ was picked instead of __S__ before terminating. <br/> __T__<sub>__eval__</sub> ≤ __T__<sub>__true__</sub>, __S__<sub>__eval__</sub> ≤ __S__<sub>__true__</sub>: The heuristic is admissible. <!--SR:!2026-10-28,485,396!2026-06-11,349,376-->
  - A\* search algorithm / worst-case space complexity ::@:: $O(|V|)=O(b^{d})$, where _d_ is the depth of the solution \(the length of the shortest path\) and _b_ is the [branching factor](../../../../general/branching%20factor.md) \(the maximum number of successors for a state\), as it stores all generated nodes in memory. <!--SR:!2026-11-04,490,396!2025-12-30,216,356-->
    - A\* search algorithm / worst-case space complexity / intuition ::@:: Its space complexity is roughly the same as that of all other graph search algorithms, as it keeps all generated nodes in memory. <!--SR:!2025-09-02,131,404!2025-09-11,139,406-->
  - A\* search algorithm / worst-case time complexity ::@:: $O(|E|\log |V|)=O(b^{d})$ <!--SR:!2025-12-01,137,276!2026-04-30,271,336-->
    - A\* search algorithm / worst-case time complexity / intuition ::@:: For explicit graph, it is hard to explain why it is $O(\lvert E \rvert \log \lvert V \rvert)$. <p> For implicit graph, if $d$ is the number of edges to reach the solution, the nodes that take not greater than $d$ edges are visited in the worst case, hence $O\left(b^d\right)$ to store them. <!--SR:!2026-07-20,389,404!2026-04-25,300,386-->
  - A\* search algorithm / heuristic function ::@:: _Admissible_ heuristic functions that give higher values (i.e. more accurate) tend to (but not _always_) make the search more efficient. <p> (The slides say if an admissible heuristic function $h$ dominates over another $h'$ ($h(n) \ge h'(n)$ for all $n$), then it is _always_ better to use it. This should not be correct... the _always_ should be replaced by _probably_.) <!--SR:!2026-11-30,511,396!2026-10-14,474,396-->
  - A\* search algorithm / variants ::@:: iterative deepening A\* (IDA\*), memory-bounded A\*, simplified memory bounded A\* (SMA\*) <!--SR:!2026-12-13,518,396!2026-10-27,484,396-->
- [iterative deepening A\*](../../../../general/iterative%20deepending%20A*.md) ::@:: It works as follows: at each iteration, perform a [depth-first search](../../../../general/depth-first%20search.md), cutting off a branch when its total cost $f(n)=g(n)+h(n)$ exceeds a given _threshold_. This threshold starts at the estimate of the cost at the initial state, and increases for each iteration of the algorithm. At each iteration, the threshold used for the next iteration is the minimum cost of all values that exceeded the current threshold. <!--SR:!2025-09-02,136,336!2026-03-02,250,356-->
- [SMA\*](../../../../general/SMA*.md) ::@:: Literally just A\* but nodes with the highest f-cost ($f(n) = g(n) + h(n)$) are pruned from the queue when there isn't any space left. Because those nodes are deleted, simple memory bounded A\* has to remember the f-cost of the best forgotten child of the parent node. When it seems that all explored paths are worse than such a forgotten path, the path is regenerated. <!--SR:!2026-11-10,496,396!2026-10-15,475,396-->

## week 4 lecture 2

- datetime: 2025-02-28T13:30:00+08:00/2025-02-28T14:50:00+08:00
- topic: search, constraint satisfaction problem \(CSP\)
- admissible heuristic
  - admissible heuristic / construction ::@:: It can be derived from a relaxed version of the problem (dropping conditions, but remember to check if the resulting heuristic is _admissible_), or by information from pattern databases that store exact solutions to subproblems of the problem, or by using inductive learning methods. <!--SR:!2026-12-19,524,396!2026-12-08,513,396-->
    - admissible heuristic / construction / examples ::@:: ABSOLVER (Prieditis, 1993) is a computer program that automatically generates heuristics based on relaxing problems and other techniques. <!--SR:!2026-11-25,506,396!2026-12-06,511,396-->
- [constraint satisfaction problem](../../../../general/constraint%20satisfaction%20problem.md) (CSP) ::@:: They are mathematical questions defined as a set of objects whose state must satisfy a number of constraints or limitations. CSPs represent the entities in a problem as a homogeneous collection of finite constraints over variables, which is solved by constraint satisfaction methods. <!--SR:!2026-03-15,291,376!2026-10-22,480,396-->
  - constraint satisfaction problem / definition ::@:: It is defined as a triple $\langle X,D,C\rangle$, where <p> - $X=\{X_{1},\ldots ,X_{n}\}$ is a set of variables, <br/> - $D=\{D_{1},\ldots ,D_{n}\}$ is a set of their respective domains of values, and <br/> - $C=\{C_{1},\ldots ,C_{m}\}$ is a set of constraints. <p> Each variable $X_{i}$ can take on the values in the nonempty domain $D_{i}$. Every constraint $C_{j}\in C$ is in turn a pair $\langle t_{j},R_{j}\rangle$, where $t_{j}\subseteq \{1,2,\ldots ,n\}$ is a set of $k$ indices and $R_{j}$ is a $k$-ary [relation](../../../../general/relation%20(mathematics).md) on the corresponding product of domains $\times _{i\in t_{j} }D_{i}$ where the product is taken with indices in ascending order. (Simply put, each $C_j$ relates a set of $k$ variables together.) <!--SR:!2026-10-26,483,396!2026-10-15,475,396-->
    - constraint satisfaction problem / definition / evaluation ::@:: It is a function from a subset of variables to a particular set of values in the corresponding subset of domains. (Simply put, the function assigns values to some of the variables.) An evaluation $v$ satisfies a constraint $\langle t_{j},R_{j}\rangle$ if the values assigned to the variables $t_{j}$ satisfy the relation $R_{j}$. <!--SR:!2026-11-05,491,396!2026-10-07,468,396-->
    - constraint satisfaction problem / definition / solution ::@:: An evaluation is _consistent_ if it does not violate any of the constraints. An evaluation is _complete_ if it includes all variables. An evaluation is a _solution_ if it is consistent and complete; such an evaluation is said to _solve_ the constraint satisfaction problem. <!--SR:!2026-11-19,502,396!2026-11-15,500,396-->
  - constraint satisfaction problem / examples ::@:: eight queens puzzle, logic puzzles, map coloring problem, maximum cut problem, type inference, etc. <!--SR:!2026-12-11,516,396!2026-11-16,501,396-->
- [eight queens puzzle](../../../../general/eight%20queens%20puzzle.md) ::@:: It is the problem of placing eight chess queens on an 8×8 chessboard so that no two queens threaten each other; thus, a solution requires that no two queens share the same row, column, or diagonal. There are 92 solutions. <!--SR:!2026-11-29,510,396!2026-06-15,355,376-->
  - eight queens puzzle / generalization ::@:: _n_ queens on a _n_-by-_n_ chessboard <!--SR:!2026-11-16,501,396!2026-10-29,486,396-->
  - eight queens puzzle / CSP ::@:: The set of variables can be the row position of the queen in each column. The set of domains is from 1 to 8. Note that this already encodes the constraint that no two queens share the same column. <p> There are two more constraint sets to encode: No two queens share the same row. No two queens share the same diagonal. <!--SR:!2026-10-16,474,396!2026-12-05,510,396-->
- constraint satisfaction problem
  - constraint satisfaction problem / constructive methods ::@:: Use search algorithms, e.g. depth-first search, to find a solution. To encode a CSP as a search problem: <p> Any partial assignment is a state. The initial state is the empty assignment. The actions are assign a value to a new variable. The goal is a solution assignment. A path cost function is not used (the path cost is always 0). <p> The problem with this approach is that, without _pruning_, the search space (the number of states, including constraint-violating ones) explodes exponentially. <!--SR:!2026-11-15,500,396!2026-06-04,343,376-->
- [local consistency](../../../../general/local%20consistency.md) ::@:: They are properties of constraint satisfaction problems related to the consistency of subsets of variables or constraints. They can be used to reduce the search space and make the problem easier to solve. <!--SR:!2026-11-12,497,396!2026-12-20,525,396-->
  - local consistency / constraint propagation ::@:: Every local consistency condition can be enforced by a transformation that changes the problem without changing its solutions (but solutions can be hidden away). <p> It works by reducing domains of variables, strengthening constraints, or creating new constraints. This leads to a reduction of the search space, making the problem easier to solve by some algorithms. <!--SR:!2026-12-07,512,396!2026-12-12,517,396-->
    - local consistency / constraint propagation / methods ::@:: Boolean satisfiability problem (SAT), constraint graph, etc. <!--SR:!2026-11-02,488,396!2026-12-14,519,396-->
- [constraint graph](../../../../general/constraint%20graph.md) ::@:: They are used to represent relations among constraints in a constraint satisfaction problem. <!--SR:!2026-11-04,490,396!2026-11-07,493,396-->
  - constraint graph / hypergraph ::@:: It is a hypergraph in which the vertices correspond to the variables, and the hyperedges correspond to the constraints. A set of vertices forms a hyperedge if the corresponding variables are those occurring in some constraint. <!--SR:!2026-10-13,473,396!2026-12-05,510,396-->
  - constraint graph / graph ::@:: The hypergraph can be converted into a _bipartite_ graph if each hyperedge is replaced by a new vertex with edges to all vertices previously connected by the hyperedge. <!--SR:!2026-12-01,512,396!2026-10-29,486,396-->
  - constraint graph / consistency propagation ::@:: Assign a value to a variable first. Then remove constraint-violating elements in vertices (representing other variables) adjacent to the variables. If you have removed any element in a vertex, run the above thing again for that vertex's adjacent vertices. This is done recursively (in BFS or DFS order) until no new elements have been removed. <p> Repeat the above until a solution has been found yet or could not be found. Note that failing to find a solution _does not_ imply a solution does not exist. <!--SR:!2025-09-08,128,336!2026-11-05,491,396-->
    - constraint graph / consistency propagation / properties ::@:: This procedure does not create new solutions but may remove some solutions. It is possible for this procedure to remove all solutions even if solutions exist (and _backtracking_ is needed to solve this). <!--SR:!2026-11-08,494,396!2026-12-07,512,396-->
  - constraint graph / note ::@:: \(__this course__: Our constraint graph only contains constraints involving exactly two variables as edges. Constraints involving more variables are not shown on the constraint graph, but still needs to be reasoned.\) <!--SR:!2026-11-19,489,406!2025-09-07,135,404-->
- [Boolean satisfiability problem](../../../../general/Boolean%20satisfiability%20problem.md) \(SAT\) ::@:: It asks whether there exists an interpretation that satisfies a given Boolean formula. In other words, it asks whether the formula's variables can be consistently replaced by the values TRUE or FALSE to make the formula evaluate to TRUE. If this is the case, the formula is called satisfiable, else unsatisfiable. <!--SR:!2025-08-12,124,388!2025-08-20,130,391-->
  - Boolean satisfiability problem / conjunctive normal form \(CNF\) ::@:: A _literal_ is either a variable \(in which case it is called a _positive literal_\) or the negation of a variable \(called a _negative literal_\). A _clause_ is a disjunction \(OR\) of literals \(or a single literal\). A clause is called a _[Horn clause](../../../../general/Horn%20clause.md)_ if it contains at most one positive literal. A formula is in _[conjunctive normal form](../../../../general/conjunctive%20normal%20form.md)_ \(CNF\) if it is a conjunction \(AND\) of clauses \(or a single clause\). <!--SR:!2026-08-09,410,388!2026-05-01,309,371-->
  - Boolean satisfiability problem / status ::@:: It is the first _NP-complete_ problem discovered by Cook. It is also one of the most intensely studied NP-complete problems as it has a huge literature and many applications. <!--SR:!2027-03-24,599,408!2025-08-03,115,391-->
  - Boolean satisfiability problem / 3-satisfiability \(3SAT\) ::@:: Each _clause_ has no more than 3 _literals_. In terms of computational complexity, it is equivalent to SAT \(NP-complete\). <!--SR:!2025-08-03,115,391!2025-08-10,121,391-->
    - Boolean satisfiability problem / 3-satisfiability / reduction ::@:: To reduce the unrestricted SAT problem to 3-SAT, transform each clause _l_<sub>1</sub> ∨ ⋯ ∨ _l_<sub>_n_</sub> to a conjunction of _n_ - 2 clauses <p> \(_l_<sub>1</sub> ∨ _l_<sub>2</sub> ∨ _x_<sub>2</sub>\) ∧ <br/> \(¬<!-- markdown separator -->_x_<sub>2</sub> ∨ _l_<sub>3</sub> ∨ _x_<sub>3</sub>\) ∧ <br/> \(¬<!-- markdown separator -->_x_<sub>3</sub> ∨ _l_<sub>4</sub> ∨ _x_<sub>4</sub>\) ∧ ⋯ ∧ <br/> \(¬<!-- markdown separator -->_x_<sub>_n_<!-- markdown separator -->−3</sub> ∨ _l_<sub>_n_<!-- markdown separator -->−2</sub> ∨ _x_<sub>_n_<!-- markdown separator -->−2</sub>\) ∧ <br/> \(¬<!-- markdown separator -->_x_<sub>_n_<!-- markdown separator -->−2</sub> ∨ _l_<sub>_n_<!-- markdown separator -->−1</sub> ∨ _l_<sub>_n_</sub>\) <p> where _x_<sub>2</sub>, ⋯ , <!-- markdown separator -->_x_<sub>_n_<!-- markdown separator -->−2</sub> are [fresh variables](../../../../general/fresh%20variable.md) not occurring elsewhere. Although the two formulas are not [logically equivalent](../../../../general/logically%20equivalent.md), they are [equisatisfiable](../../../../general/equisatisfiable.md). <!--SR:!2026-04-30,329,388!2025-08-20,130,391-->
- [SAT solver](../../../../general/SAT%20solver.md) ::@:: It is a computer program which aims to solve the Boolean satisfiability problem (SAT). <!--SR:!2025-08-12,123,388!2025-08-18,129,388-->
  - SAT solver / soundness ::@:: The program returns yes _only if_ the input is satisfiable. We want _\(this\)_ property. <!--SR:!2025-08-08,120,388!2025-08-05,117,391-->
    - [soundness](../../../../general/soundness.md)
  - SAT solver / completeness ::@:: The program returns yes _if and only if_ the input is satisfiable. We want _\(this\)_ property, but we may give up this for performance. <!--SR:!2025-08-04,117,391!2027-04-05,611,408-->
    - [completeness](../../../../general/completeness%20(logic).md)
- [DPLL algorithm](../../../../general/DPLL%20algorithm.md) ::@:: It is a _complete_, backtracking-based search algorithm for deciding the satisfiability of propositional logic formulae in conjunctive normal form, i.e. for solving the CNF-SAT problem. <p> It is a _constructive_ method. <!--SR:!2026-08-13,414,391!2025-08-14,125,388-->
  - DPLL algorithm / full name ::@:: Davis–Putnam–Logemann–Loveland algorithm <!--SR:!2025-08-08,120,388!2025-08-08,120,388-->
  - DPLL algorithm / algorithm ::@:: <pre>__function__ _DPLL_\(Φ\)<br/>    // unit propagation:<br/>    __while__ there is a unit clause {_l_} in Φ __do__<br/>        Φ ← _unit-propagate_\(_l_, Φ\);<br/>    // pure literal elimination:<br/>    __while__ there is a literal _l_ that occurs pure in Φ __do__<br/>        Φ ← _pure-literal-assign_\(_l_, Φ\);<br/>    // stopping conditions:<br/>    __if__ Φ is empty __then__<br/>        __return__ true;<br/>    __if__ Φ contains an empty clause __then__<br/>        __return__ false;<br/>    // DPLL procedure:<br/>    _l_ ← _choose-literal_\(Φ\);<br/>    __return__ _DPLL_\(Φ __∧__ {l}\) __or__ _DPLL_\(Φ __∧__ {¬l}\);</pre> <!--SR:!2025-08-19,119,331!2026-10-01,433,351-->
    - DPLL algorithm / algorithm / pruning ::@:: In this pseudocode, `unit-propagate(l, Φ)` and `pure-literal-assign(l, Φ)` are functions that return the result of applying unit propagation and the pure literal rule, respectively, to the literal `l` and the formula `Φ`. In other words, they replace every occurrence of `l` with "true" and every occurrence of `not l` with "false" in the formula `Φ`, and simplify the resulting formula. <p> For `pure-literal-assign(l, Φ)`, `l` appears only _purely positively_. That is, if `m` appears only _purely negatively_, then `l` should be `not m`. <!--SR:!2025-08-09,120,391!2025-08-04,117,391-->
      - DPLL algorithm / algorithm / pruning / detail ::@:: Provided a literal `l` to assign "true", clauses with `l` are removed from the CNF, because they have been satisfied. Clauses with `not l` have `not l` removed from itself \(but the clause itself is not removed, even if it will become empty afterwards\). Then the resulting CNF formula is processed further as outlined above. <!--SR:!2025-10-16,141,416!2025-09-18,133,415-->
    - DPLL algorithm / algorithm / note ::@:: The __`or`__ in the __`return`__ statement is a [short-circuiting operator](../../../../general/short-circuiting%20operator.md). `Φ ∧ {l}` denotes the simplified result of substituting "true" for `l` in `Φ`. <!--SR:!2025-08-07,119,388!2025-08-19,130,391-->
    - DPLL algorithm / algorithm / lecture slides ::@:: \(__this course__: In the lecture slides, the checking steps are in a different order: check empty, check has empty clause, then check for a pure literal, then check for a unit clause, then finally select a variable \(assigning "true" before assigning "false"\). If a check passes, either yes, no, or the algorithm is recursively called after replacing a literal with "true", so no `while` appears in the lecture slides. <p> The intermediate steps and CNFs are different. While the algorithm returns the same result \(satisfiability\) as the original, the instructors may prefer us to show the steps in the above order instead.\) <!--SR:!2026-08-02,382,396!2025-09-08,103,397-->
  - DPLL algorithm / completeness ::@:: It is complete \(_constructive_ method\). <p> Notice that the algorithm without the pure literal rule is _almost_ simply trying all 2<sup>_n_</sup> possible inputs for the _n_ variables \(the unit propagation rule eliminates unit clauses in the _initial_ CNF formula, hence "almost"\). This is _backtracking_. Then, the pure literal rule is simply a _sound_ rule to _prune_ more of the search space. <!--SR:!2027-04-08,614,411!2025-08-03,116,388-->

## week 5 tutorial

- datetime: 2025-03-04T12:30:00+08:00/2025-03-04T13:20:00+08:00
- topic: search
- intelligent agents
  - intelligent agents / search problem ::@:: Simple agents \(production system, state machine\) cannot do search problem by itself alone. Solving a search problem requires planning ahead \(e.g. evaluating states that have not been reached yet\). <p> After an agent computes a solution to a search problem, we can simply input the solution into another simple agent to "solve" it by executing the solution \(the simple agent does not even need computation and sensing\). <!--SR:!2026-08-06,407,391!2025-08-15,126,388-->
- search problem
  - search problem / notations ::@:: common notations: set of states $\mathcal S$, initial state $I \in \mathcal S$, set of goal states $\mathcal G \subseteq \mathcal S$ \(goal state $G \in \mathcal G \subseteq \mathcal S$\), set of actions $\mathcal A$, successor function $T: \mathcal S \times \mathcal A \to \mathcal S$, cost function $c: \mathcal S \times \mathcal A \to \mathbb R$, solution: a sequence of actions from $I$ to some $G \in \mathcal G$ <!--SR:!2025-08-03,115,391!2026-08-18,418,391-->
- search algorithm
  - search algorithm / terminology ::@:: expansion, frontier/fringe, search tree, strategy, state space graph <!--SR:!2025-08-08,120,388!2025-08-20,130,391-->
  - search algorithm / breadth-first search \(BFS\) ::@:: The strategy is expanding shallowest nodes first. It can be implemented using a FIFO queue. <p> It can be reduced from _uniform cost search_ by setting the cumulative cost function to the number of edges to reach the node. <!--SR:!2026-03-31,299,371!2025-08-19,130,391-->
  - search algorithm / uniform cost search \(UCS\) ::@:: The strategy is expanding nodes with the lowest cumulative cost first. It can be implemented using a priority queue that puts nodes with lower cumulative cost earlier in the queue. <p> It generalizes _breadth-first search_ by allowing the edges to have weights, which specifies the cost of taking the edge in a solution. <p> An example is Dijkstra's algorithm. <!--SR:!2025-08-18,129,388!2025-08-19,130,391-->
  - search algorithm / greedy search ::@:: The strategy is expanding nodes with the lowest _estimated_ cost to a goal state first. It can be implemented using a priority queue that puts nodes with lower estimated cost earlier in the queue. <p> It differs from BFS and UCS in that it uses the _estimated_ cost to a goal state rather than the cumulative cost from the initial state. <!--SR:!2026-11-12,484,388!2025-08-15,126,388-->
  - search algorithm / A\* search algorithm ::@:: The strategy is expanding nodes with the lowest _sum_ of cumulative cost and _estimated_ cost to a goal state first. It can be implemented using a priority queue \(tie-breaking with the deepest node, i.e. LIFO\). <p> It can be seen as a hybrid of _uniform cost search_ \(cumulative cost\) and _greedy search_ \(estimated cost to a goal state\). <!--SR:!2025-08-04,117,391!2025-08-18,129,388-->

## week 5 lecture

- datetime: 2025-03-05T13:30:00+08:00/2025-03-05T14:50:00+08:00
- topic: constraint satisfaction problem \(CSP\)
- DPLL algorithm
  - DPLL algorithm / state of the art ::@:: The main improvement has been Conflict-Driven Clause Learning (CDCL), which is similar to DPLL but after reaching a conflict "learns" the root causes (assignments to variables) of the conflict, and uses this information to perform non-chronological backtracking (aka backjumping) in order to avoid reaching the same conflict again. <!--SR:!2025-08-06,118,388!2027-03-17,592,408-->
- [conflict-driven clause learning](../../../../general/conflict-driven%20clause%20learning.md) \(CDCL\) ::@:: It is an algorithm for solving the Boolean satisfiability problem (SAT). <!--SR:!2025-08-13,125,391!2025-08-03,116,391-->
  - conflict-driven clause learning / difference from DPLL ::@:: The main difference between CDCL and DPLL is that CDCL's backjumping is non-chronological. Another difference is that when a conflict occurs, a new clause is learnt to avoid it again. <!--SR:!2025-08-11,122,391!2025-08-20,130,391-->
  - conflict-driven clause learning / algorithm ::@:: 1. Select a variable and assign True or False. This is called decision state. Remember the assignment. <br/> 2. Apply Boolean constraint propagation (unit propagation). <br/> 3. Build the [implication graph](../../../../general/implication%20graph.md). <br/> 4. If there is any conflict <br/> &emsp; 1. Find the cut in the implication graph that led to the conflict <br/> &emsp; 2. Derive a new clause which is the negation of the assignments that led to the conflict <br/> &emsp; 3. Non-chronologically backtrack ("back jump") to the appropriate decision level, where the first-assigned variable involved in the conflict was assigned <br/> 5. Otherwise continue from step 1 until all variable values are assigned. <!--SR:!2026-02-26,245,351!2025-08-26,115,328-->
- GSAT, [WalkSAT](../../../../general/WalkSAT.md) ::@:: They are local search algorithms to solve Boolean satisfiability problems. <p> They are _heuristic repair_ methods. <!--SR:!2025-08-20,130,391!2025-08-09,121,391-->
  - GSAT, WalkSAT / overview ::@:: They start by assigning a random value to each variable in the formula. If the assignment satisfies all clauses, the algorithm terminates, returning the assignment. Otherwise, a variable is flipped and the above is then repeated until all the clauses are satisfied. WalkSAT and GSAT differ in the methods used to select which variable to flip. <p> Both algorithms may restart with a new random assignment if no solution has been found for too long, as a way of getting out of local minima of numbers of unsatisfied clauses. <!--SR:!2025-08-10,122,391!2025-08-11,123,391-->
    - GSAT, WalkSAT / overview / GSAT ::@:: It makes the change which minimizes the number of unsatisfied clauses in the new assignment, or with some probability picks a variable at random. <p> It can be seen as a _min-conflicts algorithm_. <p> \(__this course__: minimize the number of unsatisfied clauses\) <!--SR:!2026-08-09,409,388!2026-08-10,410,388-->
  - GSAT, WalkSAT / algorithm
    - GSAT, WalkSAT / algorithm / GSAT ::@:: Given a CNF formula Φ, max restarts _mr_, and max climbs _mc_, it returns either a SAT solution or fails. <p> 1. Repeat this block for at most _mr_ times. <br/> &emsp; 1. Randomly generate an assignment. <br/> &emsp; 2. Repeat this block for at most _mc_ times. <br/> &emsp;&emsp; 1. If the assignment satisfies Φ, return the assignment. <br/> &emsp;&emsp; 2. Generate successors of the assignment by flipping exactly one variable. _n_ variables should generate _n_ successors. <br/> &emsp;&emsp; 3. Select the assignment that minimizes the number of unsatisfied clauses. <br/> 2. Return failure. <!--SR:!2025-08-06,118,388!2026-08-04,404,388-->
  - GSAT, WalkSAT / completeness ::@:: It is sound but incomplete \(_heuristic repair_ method\). <!--SR:!2025-08-16,127,388!2025-08-08,119,391-->
- [heuristic](../../../../general/heuristic%20(computer%20science).md) ::@:: It is a technique designed for problem solving more quickly when classic methods are too slow for finding an exact or approximate solution, or when classic methods fail to find any exact solution in a search space. <!--SR:!2025-08-19,130,391!2025-08-03,116,388-->
  - heuristic / repair ::@:: It starts with a proposed solution that probably does not satisfy all constraints. Then it repairs the solution until it does. <!--SR:!2027-04-09,615,408!2025-08-11,123,391-->
- [min-conflicts algorithm](../../../../general/min-conflicts%20algorithm.md) ::@:: It is a search algorithm or heuristic method to solve constraint satisfaction problems. \(__this course__: It is by Gu.\) <p> It can be seen as a _hill climbing_ algorithm, where the "hill" to be climbed is the number of satisfied constraints. <!--SR:!2027-03-30,605,408!2025-08-04,117,391-->
- [unsupervised learning](../../../../general/unsupervised%20learning.md) ::@:: It is a framework in machine learning where, in contrast to supervised learning, algorithms learn patterns exclusively from unlabeled data. <!--SR:!2027-04-06,612,408!2025-08-20,130,391-->
  - unsupervised learning / contrast ::@:: reinforcement learning \(even though it also does not require labeled data\), supervised learning <!--SR:!2025-08-20,130,391!2025-08-11,123,391-->
  - unsupervised learning / examples ::@:: Brown/IBM clustering, image clustering <!--SR:!2025-08-07,119,388!2025-08-11,123,391-->
- [reinforcement learning](../../../../general/reinforcement%20learning.md) \(RL\) ::@:: It is an interdisciplinary area of machine learning and optimal control concerned with how an intelligent agent should take actions in a dynamic environment in order to maximize a reward signal.  It is one of the three basic machine learning paradigms, alongside supervised learning and unsupervised learning. <!--SR:!2025-08-09,121,388!2025-08-07,119,388-->
- [Brown clustering](../../../../general/Brown%20clustering.md)
  - Brown clustering / natural language processing ::@:: In natural language processing \(NLP\), it is a form of hierarchical clustering of words based on the contexts in which they occur, proposed in 1992. <!--SR:!2025-08-16,127,388!2026-08-07,408,388-->

## week 5 lecture 2

- datetime: 2025-03-07T13:30:00+08:00/2025-03-07T14:50:00+08:00
- topic: constraint satisfaction problem \(CSP\), game tree search
- [_k_-means clustering](../../../../general/k-means%20clustering.md) ::@:: It is a method of vector quantization, originally from signal processing, that aims to partition _n_ observations into _k_ clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. <!--SR:!2025-08-04,116,391!2027-04-02,608,411-->
  - _k_-means clustering / characteristics ::@:: does not require data to be labeled, hyperparameter _k_, parametric (centroid locations), unsupervised <!--SR:!2025-08-13,124,388!2025-08-06,118,388-->
  - _k_-means clustering / algorithm ::@:: Choose _k_ initial centroids. They are usually data points in the training dataset. <p> Find distances (using a distance function) of training data to the centroids. Assign each data point to the closest centroid (a tie-breaking method may be required). Re-compute the centroids using the centroid memberships (_k_-means use the mean, _k_-medians use the median, _k_-modes use the mode, _k_-medoids use an actual training point). If a _stopping criterion_ is not met, repeat the above steps again. <!--SR:!2025-08-20,130,391!2025-08-04,116,391-->
  - _k_-means clustering / CSP ::@:: The variables are cluster assignments of each data point \(_n_ data points\) and cluster centroid positions \(_k_ clusters\), for a total of _n_ + _k_ variables. Cluster assignments have a domain of {1, ..., _n_}. Cluster centroid positions have a domain of the feature space. The constraint is minimizing the sum of squared distances to assigned cluster centroids. <p> The resulting algorithm is essentially the same as the usual one described above, but using CSP terminology, as described below. <p> Initialize the cluster centroid positions. Repeat the following two statements until termination condition has been met: Compute the best cluster assignments. Then compute the best centroid positions. <!--SR:!2026-04-24,313,371!2026-08-17,417,391-->
- [hill climbing](../../../../general/hill%20climbing.md) ::@:: It is a mathematical optimization technique which belongs to the family of local search. <p> It is an iterative algorithm that starts with an arbitrary solution to a problem, then attempts to find a better solution by making an incremental change to the solution. If the change produces a better solution, another incremental change is made to the new solution, and so on until no further improvements can be found. <!--SR:!2025-08-03,115,391!2025-08-10,122,391-->
  - hill climbing / search problem ::@:: A search problem can be converted into a function maximization problem by designing such a function that is maximum for a solution state. <p> For example, designing very large scale integration \(VLSL\) circuits requires finding a layout satisfying \(many\) constraints. <!--SR:!2025-08-17,128,388!2025-08-03,116,391-->
  - hill climbing / differences ::@:: Major differences include that it does not maintain a search tree. It also does not backtrack. <!--SR:!2025-08-06,118,388!2027-04-04,610,408-->
  - hill climbing / algorithm ::@:: Set the current state to the initial state. Then, literally just keep choosing the _highest_ valued successor for the current state until all successors have lower values. Then return the current state. <!--SR:!2025-08-19,130,391!2025-08-10,122,388-->
  - hill climbing / problems ::@:: A major problem is that it does not necessarily find the _global_ maximum. It may find a _local_ maximum instead. _Simulated annealing_ is one way of mitigating this. <p> \(There are other major problems unmentioned here.\) <!--SR:!2025-08-04,117,388!2025-08-14,125,388-->
- [simulated annealing](../../../../general/simulated%20annealing.md) \(SA\) ::@:: It is a probabilistic technique for _approximating_ the global optimum of a given function. Specifically, it is a metaheuristic to approximate global optimization in a large search space for an optimization problem. For large numbers of local optima, SA can find the global optimum. <!--SR:!2025-08-04,117,391!2025-08-04,117,391-->
  - simulated annealing / hill climbing ::@:: Instead of choosing the best successor, a random successor is chosen. If has a higher \(or equal\) value, the move is executed. Otherwise, the move is executed with a probability that decreases as the algorithm progresses. If not executed, randomly pick another successor and repeat the above. The probability is computed from a function called the _annealing schedule_. <!--SR:!2025-08-17,128,388!2025-08-03,116,388-->
  - simulated annealing / naming ::@:: It comes from annealing in metallurgy, a technique involving heating and controlled cooling of a material to alter its physical properties. <!--SR:!2025-08-09,120,391!2025-08-07,119,388-->
- [game](../../../../game.md) ::@:: They are the _oldest, most well-studied domain_ in artificial intelligence. <!--SR:!2025-08-12,124,388!2025-08-13,124,388-->
  - game / motivation ::@:: They are fun. <br/> Easy to represent, rules are clear. <br/> Possible combination of move can be big, e.g., there are about 10<sup>154</sup> possible moves in chess. <br/> Like the "real world" in that decisions have to be made and time is important. <br/> Easy to determine when a program is doing well. <!--SR:!2027-04-03,609,411!2027-03-28,603,408-->
  - game / uncertainty ::@:: One is we do not know what the opponents do in advance. The other is we have practical time limits. To stay within the limit limits, the decision we make may be suboptimal. <!--SR:!2025-08-10,121,391!2025-08-19,130,391-->
  - game / characteristics ::@:: (non-)deterministic, perfect information/imperfect information, (non-)zero-sum <!--SR:!2025-08-05,117,391!2025-08-08,119,391-->
    - [perfect information](../../../../general/perfect%20information.md) ::@:: In [game theory](../../../../general/game%20theory.md), a [sequential game](../../../../general/sequential%20game.md) has _\(this\)_ property if each player, when making any decision, is perfectly informed of all the events that have previously occurred, including the "initialization event" of the [game](../../../../general/game.md) \(e.g. the starting hands of each player in a card game\). <!--SR:!2025-08-10,121,391!2027-04-01,607,411-->
    - [zero-sum game](../../../../general/zero-sum%20game.md) ::@:: It is a mathematical representation in game theory and economic theory of a situation that involves two competing entities, where the result is an advantage for one side and an equivalent loss for the other. In other words, _player one's gain is equivalent to player two's loss_, with the result that the net improvement in benefit of the game is zero. <!--SR:!2025-08-04,117,388!2025-08-19,130,391-->
    - deterministic game ::@:: It is a game that does not involve random choices. <!--SR:!2025-08-19,130,391!2027-03-29,604,411-->
    - game / characteristics / tic-tac-toe ::@:: deterministic, perfect information, zero-sum <!--SR:!2025-08-20,130,391!2027-03-19,594,408-->
- [tic-tac-toe](../../../../general/tic-tac-toe.md) ::@:: It is a [paper-and-pencil game](../../../../general/paper-and-pencil%20game.md) for two players who take turns marking the spaces in a three-by-three grid with _X_ or _O_. The player who succeeds in placing three of their marks in a horizontal, vertical, or diagonal row is the winner. It is a [solved game](../../../../general/solved%20game.md), with a forced draw assuming [best play](../../../../general/best%20response.md) from both players. <!--SR:!2025-08-10,122,391!2025-08-11,122,391-->
- game
  - game / search problem ::@:: Many games can be formulated as a search problem. States are the game states. Initial state is the initial game state \(duh\). Actions are legal moves in the game. Goal states are terminal game states. Path cost function is the utility/payoff function \(which evaluates the goal states\), and we actually want to maximize it instead of minimizing it. <!--SR:!2025-08-09,121,388!2025-08-19,130,391-->
- [minimax](../../../../general/minimax.md) ::@:: It is a _recursive algorithm_ which is used to _choose an optimal move_ for a player assuming that _the other player is also playing optimally_. <p> The two players are called _maximizer_ (denoted as MAX) and _minimizer_ (denoted as MIN), and define a scoring method from the standpoint of the MAX player. The maximizer (the _AI_) tries to maximize its score while the minimizer (the _human_) tries to minimize the score of AI. <!--SR:!2025-08-20,130,391!2025-08-16,127,388-->
  - minimax / names ::@:: It is called "minimax" because it helps in _minimizing_ the _loss_ the other players can force us to receive. It may also be called "maxmini" if we want to _maximize_ the _gain_ that others players try to minimize. <!--SR:!2027-04-07,613,408!2025-08-19,130,391-->
  - minimax / steps ::@:: Construct the game tree. The initial node containing the current state is a MAX node if we are the controlling player, and MIN otherwise. The nodes alternate between MAX and MIN in increasing tree depth. MAX nodes are assigned ∞ while MIN nodes are assigned +∞. Evaluate and assign to each terminal state using a utility function (e.g. win (for me, not the controlling player given the terminal state) = <!-- +1/ -->+∞, lose = <!-- −1/ -->−∞, draw = 0). The setup is complete. <p> Using depth-first traversal, explore as far as possible, and when backtracking, update the utility of non-terminal nodes according to the MAX or MIN between the value of the current node (the node we have just backtracked to) and the just-explored child (the node we have just backtracked from). <p> Finally, make the decision either using the MAX (this is the usual case) or the MIN (evaluating the optimal decision for other players) rule. <!--SR:!2026-04-10,308,371!2025-08-09,120,391-->
  - minimax / deterministic ::@:: It requires determinism, otherwise we cannot make choices deterministically. <p> (Actually, strictly speaking, if we are not forced to make random choices but others are, we can still use minimax. Even if we are, we can use expected values, i.e. expectiminimax. But ignore this for exams...) <!--SR:!2025-08-08,119,391!2025-08-04,116,391-->
    - [expectiminimax](../../../../general/expectiminimax.md)
  - minimax / perfect information ::@:: It requires perfect information, otherwise we cannot construct the game tree. <p> If a game has probabilistic elements but the results and probabilities are still known, they are still regarded as games of perfect information (but not deterministic). <p> (Of course there are extensions of minimax to imperfect games, but ignore this for exams...) <!--SR:!2025-09-06,135,406!2026-10-26,467,406-->
  - minimax / zero-sum game ::@:: The original version requires zero-sum. In two-player zero-sum games, others minimizing my payoff is the same as maximizing others' own payoff. In a non-zero-sum game, this is not necessarily the case. <p> More information: "Maximin" is a term commonly used for non-zero-sum games to describe the strategy which maximizes one's own minimum payoff. In non-zero-sum games, this is _not generally the same_ as minimizing the opponent's maximum gain, nor the same as the Nash equilibrium strategy. <p> (There are extensions to more complex games, but ignore this for exams...) <!--SR:!2025-09-06,135,406!2025-10-07,132,344-->
  - minimax / number of states ::@:: If the game is deterministic, has perfect information, and is zero-sum, then we can apply minimax to it _theoretically_. But in _practice_, games with too many states are also not suitable (even with alpha–beta pruning). <p> A way to handle this to evaluate on the _partial_ game tree, as described below. <!--SR:!2025-08-31,130,406!2025-08-30,129,406-->
  - minimax / visual execution ::@:: Setup the tree as above. <p> Explore the state tree in in depth-first traversal. In DFS, backtracking is needed. Whenever a backtracking occurs, update the utility of non-terminal nodes according to the MAX or MIN between the value of the current node (the node we have just backtracked to) and the just-explored child (the node we have just backtracked from). Repeat until the entire tree is explored. <p> Finally, make the decision either using the MAX (this is the usual case) or the MIN (evaluating the optimal decision for other players) rule. <!--SR:!2025-11-26,182,364!2026-08-11,408,404-->
  - minimax / non-optimal play ::@:: The definition of optimal play for MAX assumes MIN plays optimally, i.e., maximizes the worst-case outcome for MAX. If _MIN does not play optimally_, MAX, if _playing optimally_, will do the same or even better, i.e. has a equal or higher payoff than that given by the minimax algorithm. <!--SR:!2025-09-03,132,406!2025-08-26,125,404-->
  - minimax / partial ::@:: As mentioned above, games with too many states are impractical to evaluate using the original minimax algorithm. A variant that sacrifice perfect decision \(i.e. _imperfect decision_\) for performance is presented below. <p> The game tree is now _partial_. One way to make a _partial_ tree is to limit by depth bound \(number of edges\). Also, the utility/payoff function is extended to a _heuristic evaluation function_. The function agrees with the original utility/payoff function for terminal game states, and additionally _approximates_ the utility/payoff for non-terminal game states. Its computation should also be _efficient_. Otherwise, everything remains the same. <!--SR:!2026-03-03,271,384!2026-03-28,276,384-->
    - minimax / partial / heuristic ::@:: A common way to construct such a function is using a _weighted linear function_, with features in a game state as inputs. Another way is to _learn_ such a function automatically from past experience using other techniques. <p> For example, the heuristic for tic-tac-toe for non-terminal game states can be the number of available winning positions \(columns, diagonals, rows\) for MAX subtracted by that for MIN. <!--SR:!2025-09-01,130,404!2025-09-07,135,406-->

## week 6 tutorial

- datetime: 2025-03-11T12:30:00+08:00/2025-03-11T13:20:00+08:00
- topic: constraint satisfaction problem \(CSP\)
- constraint satisfaction problem \(CSP\)
  - constraint satisfaction problem / definition
    - constraint satisfaction problem / definition / evaluation
    - constraint satisfaction problem / definition / solution
- [Sudoku](../../../../general/Sudoku.md) ::@:: It is a logic-based, combinatorial number-placement puzzle. In classic Sudoku, the objective is to fill a 9 × 9 grid with digits so that each column, each row, and each of the nine 3 × 3 subgrids that compose the grid (also called "boxes", "blocks", or "regions") contains all of the digits from 1 to 9. The puzzle setter provides a partially completed grid, which for a well-posed puzzle has a single solution. <!--SR:!2025-09-09,137,406!2025-09-05,134,406-->
- constraint satisfaction problem
  - constraint satisfaction problem / constructive methods
- local consistency
  - local consistency / constraint propagation
    - local consistency / constraint propagation / methods
  - local consistency / forms ::@:: arc consistency, hyper-arc consistency, path consistency <!--SR:!2025-08-25,124,404!2025-09-08,136,404-->
  - local consistency / arc consistency ::@:: A variable of a constraint satisfaction problem is arc consistent with another one if each of its admissible values are consistent with some admissible value of the second variable. Formally, a variable $x_{i}$ is arc consistent with another variable $x_{j}$ if, for every value $a$ in the domain of $x_{i}$ there exists a value $b$ in the domain of $x_{j}$ such that $(a,b)$ satisfies the binary constraint between $x_{i}$ and $x_{j}$. A problem is arc consistent if every variable is arc consistent with every other one. <!--SR:!2026-06-23,363,404!2025-08-30,129,406-->
    - local consistency / arc consistency / example ::@:: For example, consider the constraint $x<y$ where the variables range over the domain 1 to 3. Because $x$ can never be 3, there is no arc from 3 to a value in $y$ so it is safe to remove. Likewise, $y$ can never be 1, so there is no arc, therefore it can be removed. <!--SR:!2026-11-13,484,404!2026-10-30,471,406-->
  - local consistency / path consistency ::@:: Path consistency is a property similar to arc consistency, but considers pairs of variables instead of only one. A pair of variables is path-consistent with a third variable if each consistent evaluation of the pair can be extended to the other variable in such a way that all _binary_ constraints are satisfied. Formally, $x_{i}$ and $x_{j}$ are path consistent with $x_{k}$ if, for every pair of values $(a,b)$ that satisfies the binary constraint between $x_{i}$ and $x_{j}$, there exists a value $c$ in the domain of $x_{k}$ such that $(a,c)$ and $(b,c)$ satisfy the constraint between $x_{i}$ and $x_{k}$ and between $x_{j}$ and $x_{k}$, respectively. <!--SR:!2026-12-19,504,406!2026-06-18,357,404-->
- Boolean satisfiability problem
  - Boolean satisfiability problem / conjunctive normal form \(CNF\)
  - Boolean satisfiability problem / search problem ::@:: A CNF formula is given. The states are partial \(including empty and full\) assignments of variables. The initial state is the empty assignment. The goal state is an assignment that satisfies the CNF formula. The actions are assigning a Boolean to an unassigned variable. The path cost is 1 per action. <p> Setting the path cost to 1 per action means we try to find a solution that has the least number of assigned variables. The remaining unassigned variables can be assigned arbitrarily. <!--SR:!2026-11-05,471,404!2026-08-03,402,404-->

## week 6 lecture

- datetime: 2025-03-12T13:30:00+08:00/2025-03-12T14:50:00+08:00
- topic: game tree search
- [game tree](../../../../general/game%20tree.md) ::@:: It is a graph representing all possible game states within a sequential game that has perfect information. <!--SR:!2025-09-12,140,404!2025-09-02,131,406-->
  - game tree / simplification ::@:: Manually drawing a game tree is a laborious process, and is only plausible for \(very\) small games. <p> A trick is to make use of the game state symmetry to reduce the number of states drawn. Remember to note that you have omitted symmetric game states. <!--SR:!2025-09-05,134,404!2025-08-31,130,406-->
  - game tree / partial ::@:: As mentioned above, games with too many states are impractical to have its whole game tree generated. <p> There are several ways to deal with this. Two simple ways are DFS with depth bound or iterative deepening search \(IDDFS\). A more advanced way is _quiescence search_. <p> Typically, the utility/payoff function is extended to a _heuristic evaluation function_. The function agrees with the original utility/payoff function for terminal game states, and additionally _approximates_ the utility/payoff for non-terminal game states. Its computation should also be _efficient_. Otherwise, everything remains the same. <!--SR:!2026-03-18,265,386!2026-08-16,413,404-->
- [quiescence search](../../../../general/quiescence%20search.md) ::@:: It is an algorithm typically used to extend search at unstable nodes in minimax game trees in game-playing computer programs. It is an extension of the evaluation function to defer evaluation until the position is stable enough to be evaluated statically, that is, without considering the history of the position or future moves from the position. It mitigates the effect of the horizon problem. <!--SR:!2025-08-29,128,406!2026-03-04,272,386-->
  - quiescence search / detail ::@:: Essentially, it is DFS, but the expansion termination is decided by some criterion to distinguish _quiet_ \(_quiescence_\) positions from _volatile_ positions instead of by depth bound. <p> _Quiet_  \(_quiescence_\) positions are positions that are _unlikely_ to have large variations in the future \(unexpanded states\). <!--SR:!2025-09-10,138,406!2025-09-12,140,406-->
- [alpha–beta pruning](../../../../general/alpha–beta%20pruning.md) ::@:: It is a _search algorithm_ that seeks to decrease the number of nodes that are evaluated by the minimax algorithm in its search tree. It stops evaluating a move when at least one possibility has been found that proves the move to be _worse than a previously examined move_. Such moves need not be evaluated further. When applied to a standard minimax tree, it returns _the same move as minimax_ would, but prunes away branches that _cannot possibly influence_ the final decision. <!--SR:!2025-09-12,140,404!2026-12-17,512,406-->
  - alpha–beta pruning / idea ::@:: The algorithm maintains two values, alpha and beta, which respectively represent the minimum score that the maximizing player is assured of and the maximum score that the minimizing player is assured of. Initially, alpha is negative infinity and beta is positive infinity, i.e. both players start with their worst possible score. Whenever the maximum score that the minimizing player (i.e. the "beta" player) is assured of becomes less than the minimum score that the maximizing player (i.e., the "alpha" player) is assured of (i.e. beta < alpha), the maximizing player need not consider further descendants of this node, as they will never be reached in the actual play. <p> To summarize, it prevents ourselves from exploring branches of the game tree that are _not worth exploring_, because they will have _no effect on the final outcome_. <!--SR:!2025-09-09,137,406!2026-11-05,477,406-->
    - alpha–beta pruning / idea / example ::@:: To illustrate this with a real-life example, suppose somebody is playing chess, and it is their turn. Move "A" will improve the player's position (_alpha_ = score of Move "A"). The player continues to look for moves to make sure a better one hasn't been missed (exploring a new branch for the _player_). Move "B" is also a good move, but the player then realizes that it will allow the opponent to force checkmate in two moves. Thus, other outcomes from playing move B no longer need to be considered since the opponent can force a win. The maximum score that the opponent could force after move "B" is negative infinity: a loss for the player (_beta_ = score of the move for the _opponent_ checkmating). This is less than the minimum position that was previously found (_alpha_ > _beta_); move "A" does not result in a forced loss in two moves. <!--SR:!2025-09-12,140,406!2026-08-12,409,404-->
  - alpha–beta pruning / alpha ::@:: It represents the _minimum_ score that the _maximizing_ player is assured of. Initially, it is negative infinity (worst possible score). <p> For visual execution on a tree, given a search path from the root to the currently being visited vertex in DFS, it is the _maximum_ score among the _maximizing_ player vertices in the path, assuming the scores are _updated dynamically_ while doing DFS \(that is, nodes that have at least one children fully explored have scores, which may or may not be the final score\). <!--SR:!2026-08-13,410,404!2025-08-24,123,404-->
  - alpha–beta pruning / beta ::@:: It represents the _maximum_ score that the _minimizing_ player is assured of. Initially, it is positive infinity (worst possible score). <p> For visual execution on a tree, given a search path from the root to the currently being visited vertex in DFS, it is the _minimum_ score among the _minimizing_ player vertices in the path, assuming the scores are _updated dynamically_ while doing DFS \(that is, nodes that have at least one children fully explored have scores, which may or may not be the final score\). <!--SR:!2025-08-29,128,406!2026-11-18,483,406-->
  - alpha–beta pruning / visual execution ::@:: If you use the above visual interpretations of _alpha_ and _beta_, it will become easier. Then the algorithm works the same way as in minimax, except that every time before you decide to explore a new unexplored branch, evaluate if _alpha_ ≥ _beta_. If no, continue. Otherwise, do not explore said branch and backtrack from the current vertex now, as the remaining branches need no more exploration. <!--SR:!2025-08-29,128,406!2026-10-24,465,406-->
  - alpha–beta pruning / recursive execution ::@:: Assume there is a minimax function that recurses on children. Add parameters _alpha_ and _beta_ to it. <p> For the root, _alpha_ and _beta_ are their worst possible scores. Otherwise, _alpha_ and _beta_ are copied from the parent. <p> First, return the score if the current vertex is to be evaluated. If not returned, for each child, run the minimax function recursively. When it returns, update _alpha_ to be the larger value between _alpha_ and the return value if the current vertex is _maximizing_, otherwise update _beta_ to be the smaller value between _beta_ and the return value (as the current vertex is _minimizing_). Note that these updates do not update the _alpha_ or _beta_ of the parent vertex. Then check if _alpha_ ≥ _beta_. If so, return from the function now \(return value is _beta_ if _maximizing_, _alpha_ if _minimizing_\) without recursing on the remaining children (this prunes the remaining branches). <!--SR:!2026-06-30,366,384!2026-06-14,336,386-->
  - alpha–beta pruning / search order ::@:: Note that the branches prune away can be different depending on the search order. It is also possible that no branches are pruned away at all. <p> Assuming all nodes have the same branching factor, all leaf nodes are evaluated at the same fixed depth, and the leaf nodes have randomly distributed scores, an optimal search order reduces the branching factor from $b$ to $\sqrt b$ \(Knuth and Moore, 1975\). <!--SR:!2026-06-15,355,404!2025-09-11,139,406-->
  - alpha–beta pruning / advantages ::@:: It _reduces the number of explored nodes_. When one chance or _option is found at the minimum_, it _stops assessing a move_. So it helps to _improve the search procedure_ in an effective way. <!--SR:!2025-09-02,131,404!2025-08-28,127,406-->
  - alpha–beta pruning / disadvantages ::@:: It prunes _a large part_ of search space, but still needs to search all the way to _terminal states_. And this may not prune enough states such that the moves can be _made in a reasonable amount of time_. <!--SR:!2025-09-10,138,406!2025-09-10,138,406-->
  - alpha–beta pruning / extensions ::@:: We could set a _depth limit_, and then use a _heuristic evaluation function_, which estimates desirability or utility of position that _correlates_ with the actual chance of winning. But then the algorithm is _no longer guaranteed_ to find a winning strategy _if one exists_. <!--SR:!2025-08-25,124,406!2026-10-22,463,404-->
- [Monte Carlo method](../../../../general/Monte%20Carlo%20method.md) ::@:: a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results <!--SR:!2025-09-06,135,406!2025-08-28,127,406-->
- [Monte Carlo tree search](../../../../general/Monte%20Carlo%20tree%20search.md) \(MCTS\) ::@:: It is a heuristic search algorithm for some kinds of decision processes, most notably those employed in software that plays board games. In that context MCTS is used to solve the game tree. <!--SR:!2025-09-12,140,404!2025-08-31,130,406-->
  - Monte Carlo tree search / steps ::@:: selection → expansion → simulation → backpropagation <!--SR:!2025-08-24,123,404!2025-08-30,129,406-->
    - Monte Carlo tree search / steps / selection ::@:: Start from root _R_ and select successive child nodes until a leaf node _L_ is reached. The root is the current game state and a leaf is any node that has a potential child from which no simulation \(playout\) has yet been initiated. <!-- The section below says more about a way of biasing choice of child nodes that lets the game tree expand towards the most promising moves, which is the essence of Monte Carlo tree search. --> <!--SR:!2026-06-14,357,404!2026-06-16,356,404-->
    - Monte Carlo tree search / steps / expansion ::@:: Unless _L_ ends the game decisively \(e.g. win/loss/draw\) for either player, create one \(or more\) child nodes and choose node _C_ from one of them. Child nodes are any valid moves from the game position defined by _L_. <!--SR:!2026-06-13,356,404!2025-09-04,133,404-->
    - Monte Carlo tree search / steps / simulation ::@:: Complete one random playout from node _C_. This step is sometimes also called playout or rollout. A playout may be as simple as choosing [uniform random](../../../../general/discrete%20uniform%20distribution.md) moves until the game is decided \(for example in chess, the game is won, lost, or drawn\). <!--SR:!2025-08-28,127,406!2025-09-04,133,406-->
    - Monte Carlo tree search / steps / backpropagation ::@:: Use the result of the playout to update information in the nodes on the path from _C_ to _R_. <!--SR:!2025-08-31,130,406!2025-08-27,126,406-->
    - Monte Carlo tree search / steps / lecture slides ::@:: The steps given in the lecture slides are different. The above steps assumes you are evaluating a game tree starting from the current node, in order to decide the next move. The steps in the lecture slides assuming you are in a leaf node of the game tree, and then you evaluate its children to decide the next move. So in particular, the _selection_ step is missing. <p> \(The _selection_ step is skipped.\) Expand the current node \(corresponds to _expansion_\). For each child, play a large number of random games to the finish and compute the average payoffs \(values\) \(corresponds to simulation\). Play the child move that has the largest average value \(this does not directly correspond to anything above). <!--SR:!2025-10-07,132,344!2026-06-14,353,386-->
  - Monte Carlo tree search / average payoff ::@:: It is the expected value of a game, calculated from the simulations. <p> For example, in a zero-sum game, it is the number of wins divided by the number of simulations. <!--SR:!2025-10-15,140,415!2025-10-13,138,415-->
  - Monte Carlo tree search / idea ::@:: It analyzes the most promising moves in the leaf nodes of a game tree. A promising move could be a leaf node with the highest value evaluated by a _heuristic function_. <p> The simplest simulation \(playout\) is by _randomly_ playing the game until its end for a fixed number of times, starting from the leaf node child, and record the scores \(e.g. number of wins\). Choose the child with the highest score, and break ties randomly. This is applicable for games with a _finite_ number of moves and length. <!--SR:!2025-09-06,135,406!2026-11-25,491,406-->
  - Monte Carlo tree search / improvements ::@:: For the method itself, we could use upper confidence bound \(UCB\) instead of average payoff to calculate the score a child. <p> For combining with other methods, we could use minimax with alpha–beta pruning up to a depth, and then Monte Carlo tree search is used to _selectively_ search promising nodes, or as a _heuristic function_. We could also make use of domain knowledge \(knowledge about the game that does not apply in general\) to combine it with other learning techniques. <!--SR:!2025-08-31,130,406!2025-08-25,124,406-->

## week 6 lecture 2

- datetime: 2025-03-14T13:30:00+08:00/2025-03-14T14:50:00+08:00
- topic: game tree search, Markov decision process \(MDP\)
- Monte Carlo tree search
  - Monte Carlo tree search / upper confidence bound \(UCB\) ::@:: The formula is $${\frac {w_{i} }{n_{i} } }+c{\sqrt {\frac {\ln N_{i} }{n_{i} } } } \,.$$ In this formula: <p> - _w_<sub>_i_</sub> stands for the number of wins for the node considered after the _i_-th move <br/> - _n_<sub>_i_</sub> stands for the number of simulations for the node considered after the _i_-th move <br/> - _N_<sub>_i_</sub> stands for the total number of simulations after the _i_-th move run by the parent node of the one considered <br/> - _c_ is the exploration parameter—theoretically equal to √2; in practice usually chosen empirically  <p> \(__this course__: In the lecture slides, $\frac {w_i} {n_i}$ is replaced by $\mu_i$, which is the _average payoff_. $N_i$ is the number of simulations run under the parent node.\) <!--SR:!2025-11-07,163,346!2025-10-20,145,344-->
    - Monte Carlo tree search / upper confidence bound / interpretation ::@:: The first component of the formula above corresponds to exploitation; it is high for moves with high average win ratio. The second component corresponds to exploration; it is high for moves with few simulations. <!--SR:!2025-09-12,140,404!2025-09-12,140,406-->
  - Monte Carlo tree search / examples ::@:: checker: Chinook \(Schaeffer _et al._\) became the world champion in 1994. <br/> chess: Deep Blue \(Benjamin, Tan, _et al._\) defeated the world champion Gary Kasparov on 1997-05-11. <br/> Go: AlphaGo \(Silver _et al._\) defeated a top-level player in 2016. <!--SR:!2025-10-06,131,344!2026-07-16,385,404-->
- [Markov decision process](../../../../general/Markov%20decision%20process.md) \(MDP\) ::@:: It is a model for sequential decision making when outcomes are uncertain. <!--SR:!2025-08-31,130,406!2025-09-02,131,404-->
  - Markov decision process / motivation ::@:: The above algorithms consider deterministic actions. What if the actions are nondeterministic, i.e. involving chances? This nondeterminism comes from, e.g. one-shot decisions \(cannot undo\) that have random outcomes, resource constraints \(e.g. money, time, etc.\), uncertainty about the environment \(you could not always _predict_ the exact state after you have performed an action\), etc. <!--SR:!2025-08-28,127,404!2025-08-27,126,404-->
- search problem
  - search problem / elements
- Markov decision process
  - Markov decision process / vs. search problem ::@:: Recall the elements of a search problem. A MDP differs from it in that the actions \(successor function\) are nondeterministic, and becomes a _probability distribution function_ of reaching a certain state given a starting state. The path cost function becomes a _reward function_ \(becomes higher is better instead of lower\). Additionally, a discount factor $\gamma$ between 0 and 1 \(inclusive\) is added. <!--SR:!2025-08-24,123,404!2026-12-08,493,404-->
  - Markov decision process / transition probability ::@:: $T(s, a, s')$ gives the probability of reaching the ending state $s'$ given the action $a$ is performed when in the starting state $s$. <p> A possible successor $s'$ of a state $s$ has $T(s, a, s') > 0$. For a fixed starting state $s$ and action $a$, it is a _probability distribution function_ over the ending states, so we have $\sum_{s'} T(s, a, s') = 1$ for all $s, a$. <!--SR:!2025-08-28,127,406!2025-09-01,130,406-->
  - Markov decision process / reward function ::@:: $\operatorname{reward}(s, a, s')$ gives the reward for reaching the ending state $s'$ by taking the action $a$ when in the starting state $s$. <!--SR:!2025-09-12,140,406!2025-09-03,132,406-->
  - Markov decision process / policy function ::@:: It is a \(potentially probabilistic\) mapping from state space \($S$\) to action space \($A$\). \(__this course__: Ignore the potentially probabilistic nature, and thus it is $\pi: S \to A$.\) <p> It maps the current state to an action, which is the action that we will then take. <!--SR:!2025-08-25,124,404!2025-09-01,130,404-->
  - Markov decision process / run ::@:: A way to express a run of a MDP is: $$\text{state, action, reward, state, action, reward, state, ..., state} \,.$$ <p> For a typical MDP, there are many possible runs, usually infinite. <!--SR:!2025-08-26,125,406!2025-09-03,132,404-->
  - Markov decision process / optimization objective ::@:: The reward of a run is easily calculated. The formula is simply the expression in the expected value operator below. Note that the first reward obtained is not discounted, i.e. multiplied by $\gamma^0 = 1$. <p> But there are many possible runs for a typical MDP. The objective is to choose a policy $\pi$ that will maximize some cumulative function of the random rewards, typically the expected discounted sum over a potentially infinite horizon <p> &emsp; $E\left[\sum _{t=0}^{\infty }{\gamma ^{t}R_{a_{t} }(s_{t},s_{t+1})}\right]$ \(where we choose $a_{t}=\pi (s_{t})$, i.e. actions given by the policy\). And the expectation is taken over $s_{t+1}\sim P_{a_{t} }(s_{t},s_{t+1})$ <p> where $\ \gamma \ {}$ is the discount factor satisfying $0\leq \ \gamma \ \leq \ 1$. <!--SR:!2026-12-01,497,404!2026-10-25,466,406-->
  - Markov decision process / discount factor ::@:: $\ \gamma \ {}$ a discount factor satisfying $0\leq \ \gamma \ \leq \ 1$, which is usually close to $1$ \(for example, $\gamma =1/(1+r)$ for some discount rate $r$\). A lower discount factor motivates the decision maker to favor taking actions early, rather than postpone them indefinitely. <!--SR:!2025-08-27,126,404!2025-09-06,135,406-->

## week 7 tutorial

- datetime: 2025-03-18T12:30:00+08:00/2025-03-18T13:20:00+08:00
- topic: minimax, alpha–beta pruning
- minimax
- game
  - game / uncertainty
  - game / characteristics
    - perfect information
    - zero-sum game
    - deterministic game
- minimax
  - minimax / visual execution
  - minimax / recursive execution
- alpha–beta pruning
  - alpha–beta pruning / idea
  - alpha–beta pruning / alpha
  - alpha–beta pruning / beta
  - alpha–beta pruning / visual execution
  - alpha–beta pruning / recursive execution
  - alpha–beta pruning / search order

## week 7 lecture

- datetime: 2025-03-19T13:30:00+08:00/2025-03-19T14:50:00+08:00
- topic: Markov decision process \(MDP\)
- Markov decision process
  - Markov decision process / value function ::@:: Given a policy, the value function gives the expected utility starting from a certain state. It can be defined using recursion \(notice $V_\pi$ appears in its own definition\): $$V_\pi(s) = \begin{cases} 0 & \text{if }s\text{ is an ending state} \\ \sum_{s'} T(s, \pi(s), s') (R(s, \pi(s), s') + \gamma V_\pi(s') ) & \text{otherwise} \end{cases} \,,$$ where $\sum_{s'}$ sums over all states. <p> Intuitively, after a state transition due to an action, the future expected utility only depends on the current state, but not past state. So recursion can be used. <!--SR:!2025-09-04,133,406!2026-11-20,490,406-->
    - Markov decision process / value function / Q-value ::@:: \(__this course__: For some reason, it mentions Q-value not in the context of Q-learning, but in the context of evaluating the value function...\) <p> Q-value is the expected utility of doing $a$ in $s$, and then following the policy $\pi$: $$Q_\pi(s, a) = \sum_{s'} T(s, a, s') (R(s, a, s') + \gamma V_\pi(s') ) \,.$$ So the value function can be rewritten as: $$V_\pi(s) = \begin{cases} 0 & \text{if }s\text{ is an ending state} \\ Q_\pi(s, \pi(a)) & \text{otherwise} \,. \end{cases}$$ <p> Using this, for very simple MDPs, we can get a closed solution for the value function. <!--SR:!2026-08-14,411,406!2026-07-17,386,406-->
    - Markov decision process / value function / iteration ::@:: What if a closed solution for the value function does not exist or is difficult to find? <p> The idea is we initialize the value function to all 0s. Then we use the recursive definition to get a new value function in terms of the old value function. Repeat this until max iteration or some termination condition, e.g. the new value function does not differ too much from the old value function. <p> Formally, each iteration involves: $$V_\pi^{t + 1}(s) = \sum_{s'} T(s, \pi(s), s') (R(s, \pi(s), s') + \gamma V_\pi^t(s')) \,,$$ where the superscript denote the value function at iteration $t$. The starting values are $$V_\pi^0(s) = 0 \,.$$ <!--SR:!2026-12-04,500,404!2026-11-11,477,406-->
    - Markov decision process / value function / optimal ::@:: Given a starting state, an _optimal_ policy is one that _maximizes_ the value function evaluated at the starting state. The value function is said to be an _optimal_ value function. <p> Two popular methods to find the optimal value function are policy iteration and value iteration. <!--SR:!2026-11-21,487,404!2025-09-09,137,406-->
      - Markov decision process / value function / optimal / note ::@:: \(__this course__: __Important__. The lecture slides uses that an optimal policy is one that maximizes the value function _for all_ starting states. But this is not exactly correct: A MDP can have multiple distinct optimal policies, and is a function of the starting state. Indeed, an optimal policy as defined in the lecture slides may not even exist. <p> For optimal value function, the definition is the same: Given a starting state, an optimal value function is one has the maximum value evaluated at the starting state.\) <!--SR:!2026-07-04,373,406!2026-04-12,290,384-->
  - Markov decision process / policy iteration ::@:: The idea is we start with a random policy. Then we improve the policy \(_policy improvement_\) until max iteration or some termination condition, e.g. the policy stops changing. <p> We initialize the policy function randomly. We initialize the value function to all 0s. Then while the policy function has changed in the last iteration \(for the first iteration, this is considered true\), iterate: update the value function _iteratively_ \(i.e. repeatedly update the value function until it _converges_\), then update the policy function using Q-values _calculated_ from the value function. <!--SR:!2026-02-25,265,384!2026-04-05,284,384-->
    - Markov decision process / policy iteration / policy improvement ::@:: Given we have a \(not necessarily optimal\) policy $\pi$ and a \(not necessarily optimal\) value function $V_\pi(s)$, we can improve the policy. <p> Calculate the Q-value for each combination of state and action. Then for each state, choose the action that has the highest Q-value. This is the new policy function. Mathematically, this is $$\pi_{\text{new} }(s) = \operatorname{argmax}_{a \in \operatorname{actions}(s)} Q(s, a) \,.$$ <!--SR:!2026-08-04,403,404!2025-08-29,128,406-->
    - Markov decision process / policy iteration / monotonicity ::@:: The sequence of value functions obtained is monotonically increasing. The sequence of action functions is monotonically improving \(i.e. it cannot regress\). <!--SR:!2025-09-05,134,406!2025-08-27,126,404-->

## week 7 lecture 2

- datetime: 2025-03-21T13:30:00+08:00/2025-03-21T14:50:00+08:00
- topic: Markov decision process \(MDP\), reinforcement learning
- Markov decision process
  - Markov decision process / value iteration ::@:: \(Bellman 1957\) In policy iteration, we update the value function _iteratively_ and policy function alternatively. What if we combine these together into one step? This is value iteration. <p> We initialize the value function to all 0s. Repeat this until max iteration or some termination condition, e.g. the new value function does not differ too much from the old value function: update the value function as follows: $$V_{\text{opt} }^{t + 1}(s) = \max_{a \in \operatorname{action}(s)} \sum_{s'} T(s, a, s') (R(s, a, s') + \gamma V_{\text{opt} }^t(s') ) \,.$$ That is, it is similar to finding the value function for a fixed policy using iteration, but this time the action used is not from a fixed policy, but instead maximizes the value function. <!--SR:!2026-07-21,390,404!2026-01-04,221,384-->
    - Markov decision process / value iteration / policy extraction ::@:: Finally, the policy function can be extracted \(_policy extraction_) from the almost optimal value function, which is simply finding the action that maximizes the expected utility for each state. <p> Note that an almost optimal value function can yield the same policy function as an optimal value function \(given the actions are discrete\). <!--SR:!2025-09-04,133,404!2025-09-07,135,404-->
  - Markov decision process / convergence ::@:: If either the MDP is acyclic \(directed acyclic graph\) or the discount factor is less than 1, then both policy iteration and value iteration converges to an optimal solution. <p> The intuition is that if the MDP is acyclic, each run is finite. For if the discount factor is less than 1, it is harder to explain, but consider that a geometric sum converges if the factor is less than 1. <p> \(__this course__: Proof is not required here.\) <!--SR:!2025-08-26,125,404!2025-08-30,129,406-->
  - Markov decision process / policy iteration
    - Markov decision process / policy iteration / vs. value iteration ::@:: Policy iteration has the advantage that there is a definite stopping condition: when the array $\pi$ does not change in the course of applying step 1 to all states, the algorithm is completed. <p> Policy iteration is usually slower than value iteration for a large number of possible states.  \(__this course__: __Important__. Policy iteration is faster?!?\) <!--SR:!2025-09-08,136,404!2025-08-31,130,406-->
      - Markov decision process / policy iteration / vs. value iteration / note ::@:: \(__this course__: <p> initialization: random policy vs. 0 <br/> update: policy \(explicit\) vs. value \(implicit\) <br/> phases: 2 \(evaluate, improve\) vs. 1 <br/> convergence: both converges to an optimal solution <br/> \# iterations: less vs. more <br/> speed: faster vs. slower\) <!--SR:!2025-08-14,103,396!2026-03-22,269,375-->
- reinforcement learning
  - reinforcement learning / motivation ::@:: In MDPs, the environment or the model is fully specified, only that action outcomes are random. That is, we know the actions, reward functions, states, and transition probabilities. <p> But it is possible that not all of the above information is available, or too difficult to represent. <p> Reinforcement learning is about learning using reward signals. <!--SR:!2026-06-14,354,404!2025-08-26,125,404-->
  - reinforcement learning / considerations ::@:: Assume we know the states and actions available in a state. We want to learn a policy that maximizes the rewards. <!--SR:!2025-08-29,128,406!2025-09-11,139,404-->
  - reinforcement learning / framework ::@:: The agent is in a state. The agent acts. The agent observes the updated state and the rewards. The agent updates itself \(its parameters\) to better maximize the rewards. <!--SR:!2025-08-25,124,404!2026-10-23,464,404-->
  - reinforcement learning / tradeoff ::@:: Reinforcement learning is about balancing _exploration_ and _exploitation_. Unless we have a perfect model of the world, both needs to be done. <p> Exploration tries to improve the agent itself to get better long-term benefits, at the cost of getting less rewards in the near future. Exploitation tries to get the most rewards, preferably in the near future, but this may lead to the agent missing out on actions that gives more rewards in the long-term. <!--SR:!2025-08-24,123,404!2025-08-24,123,404-->
  - reinforcement learning / ε-greedy ::@:: $0<\varepsilon <1$ is a parameter controlling the amount of exploration vs. exploitation. With probability $1-\varepsilon$, exploitation is chosen, and the agent chooses the action that it believes has the best long-term effect \(ties between actions are broken uniformly at random\). Alternatively, with probability $\varepsilon$, exploration is chosen, and the action is chosen uniformly at random. $\varepsilon$ is usually a fixed parameter but can be adjusted either according to a schedule \(making the agent explore progressively less\), or adaptively based on heuristics. <p> mnemonics: $\varepsilon$ is the probability of exploration because $\varepsilon$ usually stands for a small number and the probability of exploration should be small. <!--SR:!2026-08-01,401,404!2025-09-12,140,404-->
    - reinforcement learning / ε-greedy / Q-value ::@:: In terms of Q-value, this can be expressed as: $$\pi(s) = \begin{cases} \operatorname{argmax}_{a \in \operatorname{action}(s)} \hat Q(s, a) & \text{with probability }1 - \varepsilon \\ \text{random }a \in \operatorname{action}(s) & \text{with probability }\varepsilon \,, \end{cases}$$ where $\hat Q(s, a)$ is the current _estimated_ Q-value. <!--SR:!2025-08-31,130,406!2025-09-08,136,406-->

## week 8 tutorial

- datetime: 2025-03-25T12:30:00+08:00/2025-03-25T13:20:00+08:00, PT50M
- topic: midterm Q&A
- [§ midterm examination](#midterm%20examination)

## week 8 lecture

- datetime: 2025-03-26T13:30:00+08:00/2025-03-26T14:50:00+08:00, PT1H20M
- topic: reinforcement learning
- reinforcement learning
  - reinforcement learning / key algorithms ::@:: Monte Carlo, Q-learning, Q-values, deep Q-learning, policy gradient, temporal difference, etc. <!--SR:!2025-10-17,142,416!2025-10-18,143,416-->
    - reinforcement learning / key algorithms / representation ::@:: approximation: efficiently approximate the observed values by functions, e.g. deep Q-learning, policy gradient <br/> tabular representation: store the observed values in tables, e.g. Monte Carlo, Q-values, temporal difference, Q-learning <!--SR:!2025-08-03,92,376!2026-08-14,393,396-->
  - reinforcement learning / Monte Carlo ::@:: Given a sample run $D = (s_1, a_1, r_1, s_2, a_2, r_2, \ldots)$, estimate the sample transition probabilities and reward functions. In particular, we have $$\begin{aligned} \hat T(s, a, s') & = \frac {\#(s, a, s') \in D} {\sum_{s''} \#(s, a, s'') \in D} \\ R(s, a, s') & = \operatorname E\set{r : (s, a, r, s') \in D} \\ R(s, a) & = \operatorname E\set{r: (s, a, r) \in D} \,, \end{aligned}$$ which you should be able to infer yourself. <!--SR:!2025-09-18,133,416!2025-08-10,99,396-->
  - reinforcement learning / Q-values ::@:: Given a sample run $D = (s_1, a_1, r_1, s_2, a_2, r_2, \ldots)$, estimate the Q-values $Q_\pi(s, a)$ for each combination of state $s$ and action $a$. <p> We can derive the utility at state $s_t$, where $t$ is time: $$u_t = r_t + \gamma r_{t + 1} + \gamma^2 r_{t + 2} + \cdots \,,$$ where $\gamma$ is the discount factor. Then, the Q-value $Q_\pi(s, a)$ is estimated as $$Q_\pi(s, a) = \operatorname E\set{u_t : (s_t, a_t) = (s, a)} \,,$$ that is, the average utility at time $t$ where the state is $s$ and you take action $a$. <!--SR:!2025-10-09,102,336!2026-08-11,390,396-->
  - reinforcement learning / temporal difference ::@:: It refers to a class of model-free reinforcement learning methods which learn by bootstrapping from the current estimate of the value function. These methods sample from the environment, like Monte Carlo methods, and perform updates based on current estimates, like dynamic programming methods. <p> This method is in contrast to Monte Carlo methods. <p> examples: Q-learning <!--SR:!2025-10-19,144,416!2025-08-22,111,396-->
  - reinforcement learning / Q-learning ::@:: Initialize Q-values $\hat Q(s, a)$ for each combination of state $s$ and $a$ to zero. <p> An agent at state $s$ performs an action $a$, recevies reward $r$, and reaches state $s'$, i.e. $(s, a, r, s')$. The original new value of Q-value is thus: $$\hat Q'(s, a) = r + \gamma \hat V(s') \,,$$ where $\hat V(s') = \max_a \hat Q(s', a)$ is the highest possible Q-value for state $s'$. <p> The method of _temporal difference_ additionally requires a _learning rate_ $\mu \in [0, 1]$ to control how much the new value replaces the old value: $$\hat Q_{\text{new} }(s, a) = (1 - \mu) \hat Q(s, a) + \mu \hat Q'(s, a) = (1 - \mu) \hat Q(s, a) + \mu(r + \gamma \hat V(s')) \,.$$ <!--SR:!2025-12-02,142,356!2026-01-09,196,356-->
  - reinforcement learning / deep Q-learning ::@:: When there are many states, Q-learning may not be efficient enough at estimating the real Q-values. A deep neural network is used to represent the Q-values as a Q-function. <p> The DeepMind system used a deep convolutional neural network, with layers of tiled convolutional filters to mimic the effects of receptive fields. \(<https://arxiv.org/pdf/1312.5602.pdf>\) <!--SR:!2025-09-17,132,416!2025-10-19,144,416-->
  - reinfrocement learning / policy gradient ::@:: [Gradient](../../../../general/gradient.md)-based methods \(_policy gradient methods_\) start with a mapping from a finite-dimensional \(parameter\) space to the space of policies: given the parameter vector $\theta$, let $\pi _{\theta }$ denote the policy associated to $\theta$. Defining the performance function by $\rho (\theta )=\rho ^{\pi _{\theta } }$ under mild conditions this function will be differentiable as a function of the parameter vector $\theta$. If the gradient of $\rho$ was known, one could use [gradient ascent](../../../../general/gradient%20descent.md). <!--SR:!2025-09-17,132,416!2025-08-21,110,396-->
    - reinforcement learning / policy gradient / approximation ::@:: Since an analytic expression for the gradient is not available, only a noisy estimate is available. Such an estimate can be constructed in many ways, giving rise to algorithms such as Williams's REINFORCE method \(which is known as the likelihood ratio method in the [simulation-based optimization](../../../../general/simulation-based%20optimization.md) literature\). <!--SR:!2025-10-15,140,415!2025-10-17,142,416-->

## midterm examination

- datetime: 2025-03-26T19:30:00+08:00/2025-03-26T21:30:00+08:00, PT2H
- venue: Lecture Theater B
- format
  - calculator: yes
  - cheatsheet: no
  - referencing: closed book, closed notes
  - provided: select lecture slides
  - questions: long questions ×8
- grades: 98/100 → 98/100
  - statistics
    - timestamps: 2025-04-07T15:40+08:00 → 2025-04-10T22+08:00
    - mean: 69.58 \(provided: 69.56\) → 69.79
    - standard deviation: ? \(provided: 22.158\) → ?
    - low: 0 → 0
    - lower quartile: 60.5 → 60.88
    - median: 76 → 76
    - upper quartile: 84 → 84.25
    - high: 99 → 99
    - distribution: ? → ?
    - data: ? → ?
- report
  - breadth-first search, depth-first search, A\* search \(0\) ::@:: Take note of how states are deduplicated. In the exam, BFS and DFS deduplicates _generated_ states, while A\* search deduplicates _visited_/_expanded_ states \(wording: "ancestors"\). <p> It turns out it is very easy to get this wrong if you try to run the algorithm by hand... For BFS and DFS, it is easy to get the depth wrong, since the depth is based on _generation_ instead of _expansion_. For A\* search, it is troublesome in general, since the same state may be generated multiple times with different _g_ and thus _f_ values, so these generated states are considered distinct. <!--SR:!2026-08-26,400,396!2026-08-03,383,395-->
  - depth-first search \(–1\) ::@:: Apparently, when expanding a node, the lecture slides generate a single unvisited child instead of generating all children at a time. <!--SR:!2025-11-28,156,428!2025-11-26,154,428-->
  - DPLL algorithm \(–1\) ::@:: Yeah, you need to follow the exact checking order on the lecture slides... <!--SR:!2025-11-26,153,428!2025-11-26,153,428-->
- check
  - datetime: 2025-04-10T19:00:00+08:00/2025-04-10T20:30:00+08:00, PT1H30M
  - venue: Room 4621, Academic Building
  - report: \(none\)

> __<big>Midterm Information and Past Midterm Exam Papers</big>__
>
> <big>Midterm Information</big>
>
> COMP3211 midterm exam will be held __on March 26 \(Wed\), from 7:30pm to 9:30pm, in LTB__. The special arrangement exam venue for those with time conflicts will be informed via email in a few days.
>
> - The midterm coverage is from the beginning to Page 30 \(including\) of _Lecture 6: MDP and Reinforcement Learning_, everything in the lecture notes and the first two assignments.
> - The exam is closed-book. No cheat sheet is allowed.
> - Calculators are allowed.
> - Written programming/coding won't be tested in the exam.
>
> <big>Midterm Q&A Sessions</big>
>
> The tutorials on March 21 \(Fri\) and March 25 \(Tue\) will be midterm Q&A sessions. No new materials will be conducted. You can feel free to go to the Q&A sessions if you have any question regarding the course contents.
>
> <big>Past Midterm Exam Papers</big>
>
> 23-midterm.pdf, 20-midterm.pdf

## week 8 lecture 2

- datetime: 2025-03-28T13:30:00+08:00/2025-03-28T14:50:00+08:00, PT1H20M
- topic: game theory
- [multi-agent system](../../../../general/multi-agent%20system.md) \(MAS\) ::@:: It is a computerized system composed of multiple interacting intelligent agents. <!--SR:!2025-10-19,144,417!2025-09-17,132,416-->
  - multi-agent system / issues ::@:: communication, cooperation, reaction \(facing adversity\), etc. <!--SR:!2025-09-21,136,416!2025-10-21,146,416-->
  - multi-agent system / examples ::@:: chess, contract bridge, market mechanism, most sport games, resource allocation, tic-tac-toe <!--SR:!2025-10-21,146,416!2025-09-10,105,396-->
- [game theory](../../../../general/game%20theory.md) ::@:: It is the study of [mathematical models](../../../../general/mathematical%20model.md) of strategic interactions. <p> It studies players (decision makers), strategies (players' complete plan of actions), and payoffs (outcomes, which can be rewards or punishments). Usually there are two players, but any number of players can be analyzed. <!--SR:!2025-10-20,145,415!2025-09-23,138,416-->
  - game theory / normal form ::@:: They are the simplest games studied in game theory and also most fundamental ones. \(To be defined later...\) <!--SR:!2025-10-16,141,416!2025-09-19,134,416-->
- [preference](../../../../general/preference%20(economics).md) ::@:: It refers to an order by which an agent, while in search of an "optimal choice", ranks alternatives based on their respective utility. <!--SR:!2025-09-22,137,417!2025-09-20,135,416-->
  - preference / properties ::@:: completeness, continuity, decomposability, monotonicity, substitutability, transitivity <!--SR:!2025-09-18,133,416!2025-10-16,141,416-->
    - preference / properties / completeness ::@:: Every pair of two bundles is comparable by ≽. Either _A_ ≽ _B_, _B_ ≽ _A_, or both. <!--SR:!2025-10-19,144,416!2025-10-20,145,416-->
    - preference / properties / transitivity ::@:: _A_ ≽ _B_ and _B_ ≽ _C_ implies _A_ ≽ _C_. <!--SR:!2025-10-20,145,416!2025-09-19,134,416-->
    - preference / properties / substitutability ::@:: In a bundle _A_, there may be multiple \(repeated\) elements. The desirability of an element is not affected by the presence or absence of other elements. <!--SR:!2025-08-08,97,396!2025-09-20,135,416-->
    - preference / properties / decomposability ::@:: The set of bundles _O_ can be decomposed into two sets _O_<sub>1</sub> and _O_<sub>2</sub>. The most preferred option of _O_ is either that in _O_<sub>1</sub> or _O_<sub>2</sub>. <!--SR:!2025-10-18,143,416!2025-10-18,143,415-->
    - preference / properties / monotonicity ::@:: Comparing two bundles, if an element occurs in a bundle _A_ more times than the other bundle _B_, and no less for other elements, then _A_ ≽ _B_. <!--SR:!2025-10-17,142,416!2025-09-09,104,396-->
    - preference / properties / continuity ::@:: If _A_ ≽ _B_, and _C_ is sufficiently close to _A_, then _C_ ≽ _B_. <!--SR:!2025-10-18,143,416!2025-10-14,139,416-->
  - preference / utility function ::@:: \(von Neumann and Morgenstern, 1944\) If a preference relation ≽ satisfies 6 properties:  completeness, continuity, decomposability, monotonicity, substituability, transitivity, then: <p> We can map the bundles to a real number in \[0, 1\]. This function can be considered a _utility function_. It respects that if _A_ ≽ _B_, then _u_(_A_) ≥ _u_(_B_). Further, the utility of a lottery of bundles is simply the weighted sum of the bundle utilities, weighted by their probabilities. <!--SR:!2025-08-14,103,396!2025-09-17,132,416-->
- [normal-form game](../../../../general/normal-form%20game.md) ::@:: It is a description of a _game_. Unlike [extensive form](../../../../general/extensive-form%20game.md), normal-form representations are not [graphical](../../../../general/graph%20(discrete%20mathematics).md) _per se_, but rather represent the game by way of a [matrix](../../../../general/matrix%20(mathematics).md). <!--SR:!2025-10-19,144,416!2025-10-22,147,416-->
  - normal-form game / formal definition ::@:: The game has _n_ players. Each player has their own set of actions _A_<sub>_i_</sub>. Each player also has their own utility function _u_<sub>_i_</sub>, which maps from the Cartesian product of all sets of actions to a real number. <p> This is because the utility for a player also depends on the actions of other players. <!--SR:!2025-09-17,132,416!2025-10-18,143,417-->
  - normal-form game / matrix ::@:: For _n_ players, we can represent the _n_ utilities for _n_ players in a _n_-dimensional matrix. Each dimension corresponds to the set of actions for a player. <!--SR:!2026-08-28,402,396!2025-10-20,145,415-->
- [prisoners' dilemma](../../../../general/prisoners'%20dilemma.md) ::@:: It is a [game theory](../../../../general/game%20theory.md) thought experiment involving two [rational agents](../../../../general/rational%20agent.md), each of whom can either cooperate for mutual benefit or betray their partner \("defect"\) for individual gain. The dilemma arises from the fact that while defecting is rational for each agent, cooperation yields a higher payoff for each. <!--SR:!2025-10-14,139,416!2025-10-15,140,416-->
- [coordination game](../../../../general/coordination%20game.md) ::@:: It is a type of simultaneous game found in game theory. It describes the situation where a player will earn a higher payoff when they select the same course of action as another player. The game is not one of pure conflict, which results in multiple pure strategy Nash equilibria in which players choose matching strategies. <!--SR:!2025-10-18,143,416!2025-10-23,148,416-->
- [matching pennies](../../../../general/matching%20pennies.md) ::@:: __Matching pennies__ is a [non-cooperative game](../../../../general/non-cooperative%20game%20theory.md) studied in [game theory](../../../../general/game%20theory.md). It is played between two players, Even and Odd. Each player has a [penny](../../../../general/penny.md) and must secretly turn the penny to heads or tails. The players then reveal their choices simultaneously. If the pennies match \(both heads or both tails\), then Even wins and keeps both pennies. If the pennies do not match \(one heads and one tails\), then Odd wins and keeps both pennies. <!--SR:!2025-10-16,141,416!2025-10-12,137,415-->
- [Nash equilibrium](../../../../general/Nash%20equilibrium.md) ::@:: In [game theory](../../../../general/game%20theory.md), it is the most commonly-used [solution concept](../../../../general/solution%20concept.md) for [non-cooperative games](../../../../general/non-cooperative%20game%20theory.md). It is a situation where no player could gain by changing their own strategy \(holding all other players' strategies fixed\). <p> If each player has chosen a [strategy](../../../../general/strategy%20(game%20theory).md) – an action plan based on what has happened so far in the game – and no one can increase one's own expected payoff by changing one's strategy while the other players keep theirs unchanged, then the current set of strategy choices constitutes _\(this\)_. <p> Note that the strategy may be pure (one could also considered pure as a degenerate form of mixed) or mixed. <!--SR:!2025-09-08,103,396!2025-10-20,145,416-->
  - Nash equilibrium / number ::@:: There may be one, multiple, or no _pure_ Nash equilibria. <p> Under some conditions, a _possibly mixed_ Nash equilibrium must exist. <!--SR:!2025-09-18,133,417!2025-09-18,133,416-->
- prisoners' dilemma
  - prisoners' dilemma / Nash equilibrium ::@:: Both prisoners confessing \(which is worse than both prisoners denying for both prisoners\) is the unique Nash equilibrium. <!--SR:!2025-10-17,142,417!2025-10-16,141,416-->
- coordination game
  - coordination game / Nash equilibrium ::@:: Both players doing the same thing, for which there are two profiles, are the two Nash equilibra. <!--SR:!2025-09-24,139,417!2025-10-19,144,416-->

## week 9 tutorial

- datetime: 2025-04-01T12:30:00+08:00/2025-04-01T13:20:00+08:00, PT50M
- status: unscheduled, midterm break

## week 9 lecture

- datetime: 2025-04-02T13:30:00+08:00/2025-04-02T14:50:00+08:00, PT1H20M
- status: unscheduled, midterm break

## week 9 lecture 2

- datetime: 2025-04-04T13:30:00+08:00/2025-04-04T14:50:00+08:00, PT1H20M
- status: unscheduled, midterm break

## week 10 tutorial

- datetime: 2025-04-08T12:30:00+08:00/2025-04-08T13:20:00+08:00, PT50M
- topic: Markov decision process
- Markov decision process
  - Markov decision process / vs. search problem
  - Markov decision process / solution concept ::@:: Without the Markov property, you need to know all history until now for optimizing a sequential stochastic decision problem. <p> With the Markov property, the current state already holds all information summarizing the history \(like a "sufficient statistic"\), so you only need to optimize based on the current state. This is why the solution is simply a map from possible states to actions. <!--SR:!2025-11-02,109,412!2025-12-27,157,432-->
- [Bellman equation](../../../../general/Bellman%20equation.md) ::@:: A \(_this_\), named after Richard E. Bellman, is a necessary condition for optimality associated with the mathematical optimization method known as dynamic programming. It writes the "value" of a decision problem at a certain point in time in terms of the payoff from some initial choices and the "value" of the remaining decision problem that results from those initial choices. <!--SR:!2025-12-29,159,436!2025-12-13,148,436-->
  - Bellman equation / principle of optimality ::@:: Principle of Optimality: An optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision. \(See Bellman, 1957, Chap. III.3.\) <!--SR:!2025-12-29,157,436!2025-12-14,154,436-->
  - Bellman equation / Markov decision process ::@:: The equation for the optimal policy is referred to as the _Bellman optimality equation_: $$V^{\pi *}(s)=\max _{a}\left\{ \sum_{s'} P(s' | s, a) \left(R(s,a, s')+\gamma V^{\pi *}(s')\right) \right\}.\ {}$$ where ${\pi *}$ is the optimal policy and $V^{\pi *}$ refers to the value function of the optimal policy. The equation above describes the reward for taking the action giving the highest expected return. <p> Alternative form using Q-values: $$Q^{\pi *}(s, a) = \sum_{s'} T(s, a, s') \left(R(s, a, s') + \gamma \max_{a'} Q^{\pi *}(s', a') \right) \,.$$ <!--SR:!2025-12-06,146,436!2025-11-04,111,414-->
  - Bellman equation / solution ::@:: The equation is nonlinear, thus has no closed form solution in general. <p> Instead, iterative methods are used, e.g. value iteration and policy iteration. <!--SR:!2025-12-21,152,436!2026-01-01,160,436-->
- Markov decision process
  - Markov decision process / value iteration
    - Markov decision process / value iteration / policy extraction
  - Markov decision process / policy iteration
    - Markov decision process / policy iteration / vs. value iteration

## week 10 lecture

- datetime: 2025-04-09T13:30:00+08:00/2025-04-09T14:50:00+08:00, PT1H20M
- topic: game theory, auctions
- [zero-sum game](../../../../general/zero-sum%20game.md) ::@:: \(_this_\) is a mathematical representation in game theory and economic theory of a situation that involves two competing entities, where the result is an advantage for one side and an equivalent loss for the other. In other words, player one's gain is equivalent to player two's loss, with the result that the net improvement in benefit of the game is zero. <!--SR:!2026-01-02,161,436!2025-12-30,158,436-->
  - zero-sum game / two players ::@:: No matter how the two players play, the sum of the utilities for the two players is zero. <!--SR:!2026-01-03,162,436!2026-01-03,162,436-->
  - zero-sum game / solution ::@:: For two-player finite zero-sum games, if the players are allowed to play a mixed strategy, the game always has at least one equilibrium solution. The different game theoretic solution concepts of Nash equilibrium, minimax, and maximin all give the same solution. Notice that this is not true for pure strategy. <!--SR:!2025-12-27,155,432!2026-01-03,162,436-->
    - zero-sum game / solution / universal solution ::@:: If avoiding a zero-sum game is an action choice with some probability for players, avoiding is always an equilibrium strategy for at least one player at a zero-sum game. For any two players zero-sum game where a zero-zero draw is impossible or non-credible after the play is started, such as poker, there is no \(pure\) Nash equilibrium strategy other than avoiding the play. Even if there is a credible zero-zero draw after a zero-sum game is started, it is not better than the avoiding strategy. <!--SR:!2025-09-25,85,392!2025-12-29,157,432-->
    - zero-sum game / solution / minimax ::@:: If $(a, b)$ is a Nash equilibrium, then $$\min_{y \in B} u_1(a, y) = u_1(a, b) = \max_{x \in A} \min_{y \in B} u_1(x, y) \,.$$ A similar relation applies for player 2. <p> The first equality is saying for our Nash equilibrium strategy $a$, the opponent Nash equilibrium strategy _minimizes_ our utility. The second equality is saying our Nash equilibrium strategy _maximizes_ our utility given the opponent _minimizes_ our utility. <p> The implication is that for a zero-sum game, both players maximize their minimum gains \(minimize their maximum loss\), which is the _minimax algorithm_! If one or multiple pure Nash equilibria exist in such a game, their payoffs are the same \(not necessarily zero\). <!--SR:!2025-12-28,156,434!2025-12-27,157,434-->
      - zero-sum game / solution / minimax / proof ::@:: Let $(a, b)$ be a Nash equilibrium. <p> Proof of first equality is by realizing a Nash equilibrium requires the opponent strategy to maximize their utility, which is equivalent to minimizing our utility by the zero-sum property. <p> Proof of second equality is by supposing a Nash equilibrium $(a', b')$ of _strictly_ higher own utility exist. The opponent chooses $b'$ that _minimizes_ our utility. Then $(a', b)$ is a strategy with _strictly_ higher own utility, and we could switch from $(a, b)$ to $(a', b)$. But then $(a, b)$ is not a Nash equilibrium. <!--SR:!2026-01-03,162,436!2025-11-05,112,416-->
- [strategy](../../../../general/strategy%20(game%20theory).md)
  - strategy / pure strategy ::@:: A \(_this_\) means a strategy where the player must play a particular strategy from their strategy set. <!--SR:!2025-12-11,146,436!2026-01-02,161,436-->
  - strategy / mixed strategy ::@:: A \(_this_\) means a strategy where the player plays a strategy in their strategy set with a probability. Pure strategy can be treated as a degenerate form of mixed strategy, where one strategy is assigned the probability of 1 and the rest assigned 0. <p> In this context, the payoff may be called _expected payoff_ instead. <!--SR:!2026-01-01,160,436!2025-12-13,144,434-->
    - strategy / mixed strategy / utility function ::@:: The utility function can be extended to mixed strategies. Simply find all possible strategy profiles \(combinations of strategies\) with nonzero probabilities, and then find the sum of utilities weighted by their probabilities. <p> Note the strategy probabilities of different players are mutually independent, i.e. the probabilities in a mixed strategy of a player is not affected by that of other players. This is used to find the probability of a strategy profile. <!--SR:!2025-11-04,111,416!2025-12-27,155,434-->
- Nash equilibrium
  - Nash equilibrium / Nash's existence theorem ::@:: Nash proved that if [mixed strategies](../../../../general/strategy%20(game%20theory).md#pure%20and%20mixed%20strategies) \(where a player chooses probabilities of using various pure strategies\) are allowed, then every game with a finite number of players in which each player can choose from finitely many pure strategies has at least one Nash equilibrium, which might be a pure strategy for each player or might be a probability distribution over strategies for each player. <p> Nash equilibria need not exist if the set of choices is infinite and non-compact. However, a Nash equilibrium exists if the set of choices is [compact](../../../../general/compact%20space.md) with each player's payoff continuous in the strategies of all the players. <!--SR:!2025-12-19,150,436!2026-01-01,160,436-->
- [auction](../../../../general/auction.md) ::@:: An \(_this_\) is usually a process of buying and selling goods or services by offering them up for bids, taking bids, and then selling the item to the highest bidder or buying the item from the lowest bidder. Some exceptions to this definition exist and are described in the section about different types. <!--SR:!2025-12-29,157,434!2025-12-05,145,434-->
  - auction / uses ::@:: Widely used in computer science, consumer, and corporate settings. They also provide a general theoretical framework to analyze resource allocation among self-interest agents. <!--SR:!2025-12-13,153,436!2026-01-02,161,436-->
  - auction / types ::@:: Some common ones: <p> - English auction: A starting price is set, and agents bid successively. <br/> - Dutch auction: A high starting price is set and decreases successively. First agent to buy wins. <br/> - Japanese auction: A low starting price is set and increases successively. Last agent to give up wins. <br/> sealed-bid auctions \(multiple subtypes\): Agents send in bids secretly. A winner is decided using a protocol known beforehand to agents. <br/> - \(Modern Art\): A game that combines multiple auction mechanisms. <!--SR:!2025-12-19,150,434!2025-12-18,149,434-->
- [auction theory](../../../../general/auction%20theory.md) ::@:: \(_this_\) is a branch of applied economics that deals with how bidders act in auctions and researches how the features of auctions incentivise predictable outcomes. <!--SR:!2026-01-03,162,436!2025-12-28,156,434-->
  - auction theory / uses ::@:: Auction theory is a tool used to inform the design of real-world auctions. Sellers use auction theory to raise higher revenues while allowing buyers to procure at a lower cost. The confluence of the price between the buyer and seller is an economic equilibrium. Auction theorists design rules for auctions to address issues that can lead to market failure. The design of these rulesets encourages optimal bidding strategies in a variety of informational settings. <!--SR:!2025-12-29,157,432!2025-12-08,148,436-->
  - auction theory / rules ::@:: - bidding rules: how offers are made <br/> - clearing rules: how trades occur, according to the offers <br/> - information rules: how much information each agent knows <!--SR:!2025-12-29,159,436!2025-12-23,154,436-->
- [first-price sealed-bid auction](../../../../general/first-price%20sealed-bid%20auction.md) ::@:: A \(_this_\) \(\(_this_\)\) is a common type of auction. It is also known as blind auction. In this type of auction, all bidders simultaneously submit sealed bids so that no bidder knows the bid of any other participant. The highest bidder pays the price that was submitted. <p> \(__this course__: Ties are broken randomly with uniform distribution.\) <!--SR:!2025-12-31,161,436!2025-12-17,157,432-->
  - first-price sealed-bid auction / utility function ::@:: Assume common knowledge of all agents' values of the item. <p> If an agent wins the auction, the utility is the agent's value minus the price, which is the highest bid among the agents. Otherwise, the utility is zero. <p> For ties, the utility is weighing the above two scenarios by their probabilities given by the tie-breaking rule. <!--SR:!2025-12-13,153,436!2025-12-27,155,434-->
- [second-price sealed-bid auction](../../../../general/Vickrey%20auction.md) ::@:: A \(_this_\) or \(_this_\) \(\(_this_\)\) is a type of sealed-bid auction. Bidders submit written bids without knowing the bid of the other people in the auction. The highest bidder wins but the price paid is the second-highest bid. <p> \(__this course__: Ties are broken randomly with uniform distribution.\) <!--SR:!2025-12-22,153,436!2025-12-20,151,436-->
  - second-price sealed-bid auction / utility function ::@:: Assume common knowledge of all agents' values of the item. <p> If an agent wins the auction, the utility is the agent's value minus the price, which is the _second_ highest bid among the agents. Otherwise, the utility is zero. <p> For ties, the utility is weighing the above two scenarios by their probabilities given by the tie-breaking rule. <!--SR:!2025-12-30,158,434!2025-12-26,154,432-->
- auction theory
  - auction theory / this course ::@:: \(__this course__: Assumptions to make auctions easier to analyze: Ties are broken randomly with uniform distribution. Possible bidding prices are discrete, i.e. can only pick from a set of prices. Agents' value of the item are the same \(make it 1 for simplicity\). Then agents will not bid more than their values of the item \(which is 1 for simplicity\).\) <!--SR:!2025-12-14,154,436!2025-12-29,157,434-->
- first-price sealed-bid auction
  - first-price sealed-bid auction / Nash equilibria ::@:: \(__this course__: Assumptions to make first-price sealed-bid auctions easier to analyze: Ties are broken randomly with uniform distribution. Possible bidding prices are discrete, i.e. can only pick from a set of prices. Agents' value of the item are the same \(make it 1 for simplicity\). Then agents will not bid more than their values of the item \(which is 1 for simplicity\).\) <!--SR:!2025-12-29,157,434!2025-12-12,143,432-->
    - first-price sealed-bid auction / Nash equilibria / 2 prices, 2 players ::@:: Assume the above stated assumptions. There are 2 possible bidding prices $0 \le a < b \le 1$. There are 2 players. <p> Both bidding the high price is always a Nash equilibrium. Both bidding the low price is a Nash equilibrium iff $\frac {1 - a} 2 \ge 1 - b \,,$ and if this is the case, the game becomes prisoner's dilemma. No other cases are Nash equilibria. <p> This is easy to prove using a payoff table. <!--SR:!2025-12-26,157,432!2025-12-04,144,436-->

## week 10 lecture 2

- datetime: 2025-04-11T13:30:00+08:00/2025-04-11T14:50:00+08:00, PT1H20M
- topic: auctions
- first-price sealed-bid auction
  - first-price sealed-bid auction / Nash equilibria
    - first-price sealed-bid auction / Nash equilibria / multiple prices, 2 players ::@:: Assume the above stated assumptions. There are many possible bidding prices $0 \le a_i \le 1$ sorted by \(strictly\) increasing value. There are 2 players. <p> Both bidding the highest price is always a Nash equilibrium. Otherwise, both bidding the price $a_i$ is a Nash equilibrium if $\frac {1 - a_i} 2 \ge 1 - {a_{i + 1} }$. No other cases are Nash equilibria. <p> This is easy to prove using a payoff table. Additionally, observe that if $\frac {1 - a_i} 2 \ge 1 - {a_{i + 1} }$, then $\frac {1 - a_i} 2 \ge 1 - {a_{i + k} }$ for $k \ge 1$. <!--SR:!2025-12-13,153,434!2025-11-05,111,416-->
    - first-price sealed-bid auction / Nash equilibria / multiple prices, multiple players ::@:: Assume the above stated assumptions. There are many possible bidding prices $0 \le a_i \le 1$ sorted by \(strictly\) increasing value. There are $n \ge 2$ players. <p> Every player bidding $\max a_i$ is a Nash equilibrium. If $\frac {1 - a_i} n \ge 1 - {a_{i + 1} }$, everyone bidding $a_i$ is also a Nash equilibrium. Additionally, if $\max a_i$ equals their \(common\) item value \(which is 1 for simplicity\), bid profiles with at least 2 players bidding $\max a_i = 1$ are also Nash equilibria. <p> The first two statement are proved as in the 2-player case. To prove the last statement, consider the highest bid in a bid profile. Assume $\max a_i$ is lower than the \(common\) item value. Then any losing player can improve their payoff by setting their bid to $\max a_i$ to either win or enter tie-breaking. In both cases, the utility must be positive, better than losing. Everyone can do this so eventually everyone bidding $\max a_i$ is a Nash equilibrium. Now assume $\max a_i = 1$. Additional Nash equilibria appears because entering tie-breaking may yield zero utility, same as losing. Consider the highest bid again. If the highest bid is shared by 1 player only, then either the winning player can lower their bid and still win or a losing player can raise their bid to win. Otherwise, if the highest bid is not $\max a_i = 1$, a losing player can raise their bid to win. So what remains are bid profiles at least 2 players bidding $\max a_i = 1$. <!--SR:!2025-11-06,113,416!2025-09-28,77,396-->
    - first-price sealed-bid auction / Nash equilibria / continuous prices, multiple players ::@:: Assume the above stated assumptions. The possible bidding prices are continuous in $[a, b]$ or $[a, b)$, where $0 \le a < b \le 1$. There are $n \ge 2$ players. <p> Assume the upper bound is closed, i.e. $[a, b]$. Then the results for multiple prices, multiple players still carries over \(including the case for when $b = 1$\). So the proofs are essentially the same. Note that the condition analogous to $\frac {1 - a_i} n \ge 1 - {a_{i + 1} }$ is always false, so there cannot be Nash equilibria with a highest bid that is not $b$. <p> Assume the upper bound is open, i.e. $[a, b)$. Then there are no \(pure\) Nash equilibria, since the open upper bound means a player cannot guarantee winning or at least tieing by setting their bid to $b$, since $b \notin [a, b)$. <!--SR:!2025-12-28,158,434!2025-10-01,80,396-->
- [Bayesian game](../../../../general/Bayesian%20game.md) ::@:: In game theory, a \(_this_\) is a strategic decision-making model which assumes players have incomplete information. Players may hold private information relevant to the game, meaning that the payoffs are not common knowledge. <!--SR:!2025-12-27,155,434!2025-12-06,146,436-->
  - Bayesian game / motivation ::@:: In real world, not all information is available. So use probabilities to model them. <p> For example, we do not know others' value of an item in an auction. <!--SR:!2025-12-29,157,432!2025-12-30,160,432-->
  - Bayesian game / sealed-bid auction ::@:: To model a sealed-bid auction as a game, we define: <p> - a set of $n$ agents $N$ <br/> - a set of possible _private_ values for each agent $v_i \in V_i := [0, 1]$ <br/> - a set of possible bids for each agent $b_i \in B_i := [0, 1]$ <br/> - a prior probability distribution function for private agent values $p: V^n \to [0, 1]$, which is common to all agents <br/> - a payment function for each agent $\sigma_i : V^n \to \Sigma := [0, 1]$ <br/> - a winner selection function $\tau : V^n \to N$ <!--SR:!2025-12-16,147,434!2025-10-31,110,416-->

## week 11 tutorial

- datetime: 2025-04-15T12:30:00+08:00/2025-04-15T13:20:00+08:00, PT50M
- topic: game theory
- game theory
- normal-form game
  - normal-form game / formal definition
- strategy
  - strategy / pure strategy
  - strategy / mixed strategy
    - strategy / mixed strategy / utility function
- Nash equilibrium
  - Nash equilibrium / Nash's existence theorem
  - Nash equilibrium / computation
    - Nash equilibrium / computation / mixed ::@:: In games with mixed-strategy Nash equilibria, the probability of a player choosing any particular \(so pure\) strategy can be computed by assigning a variable to each strategy that represents a fixed probability for choosing that strategy. In order for a player to be willing to randomize, their expected payoff for each \(pure\) strategy should be the same. In addition, the sum of the probabilities for each strategy of a particular player should be 1. This creates a system of equations from which the probabilities of choosing each strategy can be derived. <!--SR:!2025-11-03,110,416!2025-12-15,146,436-->
- matching pennies
  - matching pennies / mixed Nash equilibrium ::@:: Both players playing both strategies half of the time. <!--SR:!2025-12-30,158,436!2025-12-14,145,436-->
- prisoner's dilemma
  - prisoner's dilemma / mixed Nash equilibrium ::@:: There is no mixed Nash equilibrium. There is only a pure Nash equilibrium. We could also count the pure Nash equilibrium as a special case of mixed Nash equilibrium. <!--SR:!2025-12-12,147,434!2025-12-26,154,432-->
- coordination game
  - coordination game / mixed Nash equilibrium ::@:: Both players choosing both actions half of the time. Note that its expected utility is less than that of the pure Nash equilibria. <!--SR:!2025-12-20,151,436!2026-01-01,162,436-->

## week 11 lecture

- datetime: 2025-04-16T13:30:00+08:00/2025-04-16T14:50:00+08:00, PT1H20M
- topic: auctions, propositional logic
- Bayesian game
  - Bayesian game / sealed-bid auction
    - Bayesian game / sealed-bid auction / utility function ::@:: Now a strategy for player $i$ is a function from its private values to bids $f_i : V_i \to B_i$. The utility received by player $i$ is $u_i(b, v) = v_i - \sigma_i(b)$ if the player wins according to $\tau$, otherwise $u_i(b, v) = -\sigma_i(b)$, where $v$ is the private value profile and $b$ is the bid profile. <p> The utility function of a strategy profile for player $i$ is the above utility weighted by the prior probability distribution of private agent values: $$u_i(f_1, \ldots, f_n) = \int_{v \in V^n} \! p(v) u_i((f_1(v_1), \ldots, f_n(v_n)), v) \,\mathrm dv \,.$$ <!--SR:!2025-11-04,111,416!2025-11-09,110,416-->
- first-price sealed-bid auction
  - first-price sealed-bid auction / properties ::@:: There is no dominant strategy. The Bayesian Nash equilibrium is player $i$ bidding $\frac {n - 1} n v_i$; that is, underbidding than the private value slightly. <p> If the private values of the agents are drawn _independently_ and _uniformly_ at random from $[0, 1]$, and winner pays $\frac {n - 1} n$ of their bid \(this is to make FPSBA Bayesian-Nash incentive compatible \(BNIC\)\), the expected revenue is $\frac {n - 1} {n + 1}$. <!--SR:!2026-01-02,161,436!2025-11-06,113,416-->
- second-price sealed-bid auction
  - second-price sealed-bid auction / properties ::@:: Bidding truthfully \(player $i$ bidding $v_i$\) is a dominant strategy. The Bayesian Nash equilibrium is player $i$ bidding $v_i$; that is, bidding the private value. <p> If the private values of the agents are drawn _independently_ and _uniformly_ at random from $[0, 1]$, the expected revenue is $\frac {n - 1} {n + 1}$. <!--SR:!2025-12-31,159,436!2025-11-05,111,416-->
  - second-price sealed-bid auction / proof of dominance of truthful bidding ::@:: Consider a player underbidding. If the player would win or lose regardless if the player underbids or not, the utility are the same. If the player would lose with underbidding but win with bidding truthfully, the utility from the latter is higher. So underbidding is \(weakly\) dominated by bidding truthfully. <p> Consider a player overbidding. The proof proceeds similarly. So overbidding is \(weakly\) dominated by bidding truthfully. <!--SR:!2025-12-27,155,434!2025-12-27,157,432-->
- [revenue equivalence](../../../../general/revenue%20equivalence.md) ::@:: \(_this_\) is a concept in auction theory that states that given certain conditions, any mechanism that results in the same outcomes \(i.e. allocates items to the same bidders\) also has the same expected revenue. <!--SR:!2025-12-16,156,436!2026-01-01,160,436-->
  - revenue equivalence / conditions ::@:: \(__this course__: The conditions are rather complex. We state simpler result. In an auction, if the bidder with the highest private value always wins, the bidder with the lowest possible private value expects zero payoff, the bidder are risk-neutral, and the private values are draw from a strictly increasing and atomless \(not containing discrete distributions\) distribution _independently_, the seller receives the same expected revenue.\) <!--SR:!2025-11-15,113,416!2025-10-20,99,412-->
- prisoner's dilemma
  - prisoner's dilemma / notations ::@:: If both players cooperate, they both receive the reward $R$ for cooperating. If both players defect, they both receive the punishment payoff $P$. If Blue defects while Red cooperates, then Blue receives the temptation payoff $T$, while Red receives the "sucker's" payoff, $S$. Similarly, if Blue cooperates while Red defects, then Blue receives the sucker's payoff $S$, while Red receives the temptation payoff $T$. <!--SR:!2026-01-02,161,436!2025-12-30,158,436-->
  - prisoner's dilemma / iterated ::@:: If two players play the prisoner's dilemma more than once in succession, remember their opponent's previous actions, and are allowed to change their strategy accordingly, the game is called the iterated prisoner's dilemma. <p> In addition to the general form above, the iterative version also requires that ⁠$2R>T+S$⁠, to prevent alternating cooperation and defection giving a greater reward than mutual cooperation. <!--SR:!2025-12-31,162,436!2025-12-18,158,434-->
    - prisoner's dilemma / iterated / known bound ::@:: If the iterated prisoner's dilemma is played a finite number of times and both players know this, then the dominant strategy and Nash equilibrium is to defect in all rounds. The proof is inductive: one might as well defect on the last turn, since the opponent will not have a chance to later retaliate. Therefore, both will defect on the last turn. Thus, the player might as well defect on the second-to-last turn, since the opponent will defect on the last no matter what is done, and so on. The same applies if the game length is unknown but has a known upper limit. <!--SR:!2025-12-14,145,436!2025-12-13,144,436-->
    - prisoner's dilemma / iterated / motivation ::@:: The iterated prisoner's dilemma is fundamental to some theories of human cooperation and trust. Assuming that the game effectively models transactions between two people that require trust, cooperative behavior in populations can be modeled by a multi-player iterated version of the game. <!--SR:!2025-12-30,158,434!2025-11-06,113,414-->
    - prisoner's dilemma / iterated / Axelrod 1984 ::@:: Robert Axelrod ortanized a tournament of the _N_-step prisoner's dilemma \(with _N_ fixed\) in which participants have to choose their strategy repeatedly and remember their previous encounters. <p> The winning deterministic strategy was tit for tat \(TFT\), developed and entered into the tournament by Anatol Rapoport. It was the simplest of any program entered, containing only four lines of BASIC, and won the contest. The strategy is simply to cooperate on the first iteration of the game; after that, the player does what his or her opponent did on the previous move. <!--SR:!2025-12-19,159,436!2025-12-22,153,436-->
    - prisoner's dilemma / iterated / discount factor ::@:: It is common to introduce a discount factor $0 \le w \le 1$, which can be interpreted as the probability of playing another game. <p> If this factor is sufficiently large, then there is no strategy best against all possible strategies used by the other player. <!--SR:!2025-12-24,155,436!2025-12-29,159,436-->
    - prisoner's dilemma / iterated / nice ::@:: The strategy will not be the first to defect \(this is sometimes referred to as an "optimistic" algorithm\), i.e., it will not "cheat" on its opponent for purely self-interested reasons first. Almost all the top-scoring strategies were nice. <!--SR:!2025-12-03,143,432!2025-12-17,157,434-->
    - prisoner's dilemma / iterated / collectively stable ::@:: A strategy is \(_this_\) if given a sufficiently large group of individuals using that strategy, an _individual_ \(not a _group_\) using any other strategy cannot get a better score. <!--SR:!2025-12-18,158,434!2025-12-30,158,434-->
      - prisoner's dilemma / iterated / collectively stable / examples ::@:: Tit for tat \(TFT\) is collectively stable iff the discount factor $w$ is large enough, which is a function of the payoff parameters $R, P, T, S$. <p> All defect is always collectively stable. But it can be invaded by a small _group_ of individuals \(not just an _individual_\) using other strategies. <p> In contrast, if a _nice_ strategy cannot be invaded by an _individual_ using another strategy \(i.e. collectively stable against that other strategy\), the nice strategy also cannot be invaded by a _group_ of individuals using that other strategy. <!--SR:!2025-11-06,112,416!2025-12-26,157,432-->
- artificial intelligence
  - artificial intelligence / non-intelligence ::@:: Examples include copying agents \(copycats\), simple pattern-matching-substitution agents, etc. <!--SR:!2025-12-16,156,436!2025-12-17,148,436-->
  - artificial intelligence / intelligence ::@:: Intelligent agents may reason logically, dynamically \(based on observations and states\), or based on other agents, etc. <!--SR:!2025-12-30,158,434!2025-12-26,156,434-->
- [knowledge base](../../../../general/knowledge%20base.md) \(KB\) ::@:: In computer science, a \(_this_\) \(\(_this_\)\) is a set of sentences, each sentence given in a knowledge representation language, with interfaces to tell new sentences and to ask questions about what is known, where either of these interfaces might use inference. It is a technology used to store complex structured data used by a computer system. <!--SR:!2026-01-03,162,436!2026-01-01,162,436-->
  - knowledge base / inference ::@:: Using _sentences_ expressed in a _language_ in the knowledge base, reason. <p> Possible algorithms include search algorithms, etc. <!--SR:!2025-12-27,155,434!2025-12-09,144,432-->

## week 11 lecture 2

- datetime: 2025-04-18T13:30:00+08:00/2025-04-18T14:50:00+08:00, PT1H20M
- status: unscheduled, public holiday: Good Friday

## week 12 tutorial

- datetime: 2025-04-22T12:30:00+08:00/2025-04-22T13:20:00+08:00, PT50M
- topic: auctions
- auctions
- auction theory
- first-price sealed-bid auction
- Bayesian game
  - Bayesian game / Bayesian Nash equilibrium ::@:: To derive the Bayesian Nash equilibrium, we calculate the expected utility in terms of the current player's strategy and making use of prior probabilities of other players' strategies. <!--SR:!2025-12-28,156,436!2026-01-01,160,434-->
- second-price sealed-bid auction
  - second-price sealed-bid auction / proof of dominance of truthful bidding
- revenue equivalence
  - revenue equivalence / derivation ::@:: You can construct the CDF for the revenue first \(FPSBA: $\max\left\{\frac {n - 1} n a, \frac {n - 1} n b, \ldots \right\}$, SPSBA: $\max\set{a, b, \ldots}$\), and then differentiate it to find its PDF, and finally find its expected value \(integrated over the possible revenue range with nonzero probability\). <!--SR:!2025-12-26,154,432!2025-11-16,114,414-->

## week 12 lecture

- datetime: 2025-04-23T13:30:00+08:00/2025-04-23T14:50:00+08:00, PT1H20M
- topic: propositional logic
- [knowledge representation and reasoning](../../../../general/knowledge%20representation%20and%20reasoning.md) \(KRR\) ::@:: \(_this_\) aims to model information in a structured manner to formally represent it as knowledge in knowledge-based systems. Whereas \(_this_\) also aims to understand, reason and interpret knowledge. <!--SR:!2025-12-27,157,432!2025-12-14,154,436-->
  - knowledge representation and reasoning / language ::@:: It has two components: _syntax_ and _semantics_. The former determines legal sentences, while the latter determines _meaning_ of the legal sentences. <p> A good language should be _concise_, _expressive_, _precise_, and _unambiguous_. <!--SR:!2025-12-08,148,436!2025-12-14,149,436-->
  - knowledge representation and reasoning / inference ::@:: It is the process of deriving new sentences from existing sentences. <p> Two common kinds of logic used for this process includes _propositional logic_ and and _first-order logic_. <!--SR:!2025-12-25,156,432!2025-12-10,150,434-->
- [propositional calculus](../../../../general/propositional%20calculus.md) ::@:: It deals with propositions \(which can be true or false\) and relations between propositions, including the construction of arguments based on them. Compound propositions are formed by connecting propositions by logical connectives representing the truth functions of conjunction, disjunction, implication, biconditional, and negation. Some sources include other connectives, as in the table below. <!--SR:!2026-01-01,160,436!2025-12-19,159,436-->
  - propositional calculus / syntax ::@:: Given a set of atomic propositional variables $p_{1}$, $p_{2}$, $p_{3}$, ..., and a set of propositional connectives $c_{1}^{1}$, $c_{2}^{1}$, $c_{3}^{1}$, ..., $c_{1}^{2}$, $c_{2}^{2}$, $c_{3}^{2}$, ..., $c_{1}^{3}$, $c_{2}^{3}$, $c_{3}^{3}$, ..., a formula of propositional logic is [defined recursively](../../../../general/recursive%20definition.md) by these definitions: <p> &emsp; __Definition 1__: Atomic propositional variables are formulas. <br/> &emsp; __Definition 2__: If $c_{n}^{m}$ is a propositional connective, and $\langle$A, B, C, …$\rangle$ is a sequence of m, possibly but not necessarily atomic, possibly but not necessarily distinct, formulas, then the result of applying $c_{n}^{m}$ to $\langle$A, B, C, …$\rangle$ is a formula. <br/> &emsp; __Definition 3:__ Nothing else is a formula. <!--SR:!2026-01-03,162,436!2025-12-13,144,434-->
    - propositional calculus / syntax / intuition ::@:: The atomic propositional variables $p_1, p_2, p_3, \ldots$ are also called _non-logical symbols_, whose meaning depends on the situation at hand. <p> A propositional connective $c^m_n$ connects $m$ formulas together to form a formula. <!--SR:!2025-12-29,159,434!2025-12-28,156,432-->
    - propositional calculus / syntax / connectives ::@:: \(__this course__: We use the symbols listed below.\) <p> - conjunction $\land$: and <br/> - disjunction $\lor$: or <br/> - equivalence/biconditional $\equiv$: Both sides have the same truth value. <br/> - implication/conditional $\supset$: If the condition is true, the conclusion is true. If the condition is false, the conclusion could be false or true. <br/> - negation $\lnot$: not <br/> - parentheses $()$: for precedence <!--SR:!2025-12-13,144,436!2025-12-17,148,436-->
    - propositional calculus / syntax / precedence ::@:: Adding parentheses for every connective to specify the order is too annoying! Thus in practice, an implicit precedence is assumed, overridable by parentheses. \(__this course__: $$\text{first} \quad (\lnot) \succ (\land, \lor) \succ (\supset) \succ (\equiv) \quad \text{last}$$\) <!--SR:!2025-12-31,159,436!2025-12-13,144,434-->
  - propositional calculus / semantics ::@:: To serve as a model of the logic of a given [natural language](../../../../general/natural%20language.md), a formal language must be semantically interpreted. In [classical logic](../../../../general/classical%20logic.md), all propositions evaluate to exactly one of two [truth-values](../../../../general/truth%20value.md): _True_ or _False_. <!--SR:!2025-12-11,146,436!2025-12-17,148,436-->
    - propositional calculus / semantics / translation ::@:: We can _try_ to translate natural language sentences to a logical interpretation. <p> Some ambiguity and loss exists: <p> - "either... or ...": Is it disjunction or exclusive disjunction \(xor\)? \(__this course__: __important__: In assignments and examinations, we use disjunction by default, unless the question specifies "not both".\) <br/> - "but": It seems to be conjunction, since both statements are true, but the "but" seems to carry additional meaning that cannot be expressed in propositional calculus... <!--SR:!2025-12-15,155,432!2025-12-27,155,432-->
    - propositional calculus / semantics / interpretation ::@:: For a given language ${\mathcal {L} }$, an __interpretation__, __valuation__, __Boolean valuation__, or __case__, is an [assignment](../../../../general/assignment%20(mathematical%20logic).md) of _semantic values_ to each formula of ${\mathcal {L} }$. For a formal language of classical logic, a case is defined as an _assignment_, to each formula of ${\mathcal {L} }$, of one or the other, but not both, of the [truth values](../../../../general/truth%20value.md), namely [truth](../../../../general/truth.md) \(__T__, or 1\) and [falsity](../../../../general/false%20(logic).md) \(__F__, or 0\). An interpretation that follows the rules of classical logic is sometimes called a __Boolean valuation__. An interpretation of a formal language for classical logic is often expressed in terms of [truth tables](../../../../general/truth%20tables.md). Since each formula is only assigned a single truth-value, an interpretation may be viewed as a [function](../../../../general/function%20(mathematics).md), whose [domain](../../../../general/domain%20of%20a%20function.md) is ${\mathcal {L} }$, and whose [range](../../../../general/range%20of%20a%20function.md) is its set of semantic values ${\mathcal {V} }=\{ {\mathsf {T} },{\mathsf {F} }\}$, or ${\mathcal {V} }=\{1,0\}$. <!--SR:!2026-01-01,160,436!2025-12-28,158,436-->
    - propositional calculus / semantics / truth table ::@:: Since logical connectives are defined semantically only in terms of the truth values that they take when the propositional variables that they're applied to take either of the two possible truth values, the semantic definition of the connectives is usually represented as a truth table for each of the connectives. <!--SR:!2025-12-20,151,432!2025-12-16,147,436-->
- [logical consequence](../../../../general/logical%20consequence.md) ::@:: \(_this_\) \(also \(_this_\) or \(_this_\)\) is a fundamental concept in logic which describes the relationship between statements that hold true when one statement logically _follows_ from one or more statements. A valid logical argument is one in which the conclusion is entailed by the premises, because the conclusion is the consequence of the premises. <!--SR:!2025-12-20,151,436!2025-12-03,143,432-->
  - logical consequence / definition ::@:: An assignment $v$ satisfies a sentence $\alpha$ if the sentence under the assignment is truthy: $v(\alpha) = \mathrm T$. <p> A set of sentences $\Sigma$ _entail_ a sentence $\alpha$, denoted $\Sigma \models \alpha$ iff every assignment satisfying every sentence of $\Sigma$ also satisfies $\alpha$. <p> A sentence $\alpha$ is a _tautology_ or _valid_ if the empty set entail it, i.e. $\emptyset \models \alpha$. Since every assignment satisfies the empty set, this means every assignment satisfies $\alpha$. <!--SR:!2025-12-26,154,432!2025-12-03,143,432-->
- [deduction theorem](../../../../general/deduction%20theorem.md) ::@:: In mathematical logic, a \(_this_\) is a metatheorem that justifies doing conditional proofs from a hypothesis in systems that do not explicitly axiomatize that hypothesis, i.e. to prove an implication _A_ → _B_, it is sufficient to assume _A_ as a hypothesis and then proceed to derive _B_. Deduction theorems exist for both propositional logic and first-order logic. <!--SR:!2025-12-16,147,436!2025-12-27,155,432-->
  - deduction theorem / propositional logic ::@:: $\set{\alpha_1, \ldots, \alpha_n} \models \alpha$ iff $\emptyset \models \alpha_1 \land \cdots \land \alpha_n \supset \alpha$. <p> This is saying entailment is equivalent to tautology. <p> Proof is by proving the above statement in both directions. Split cases for which the assignment does not satisfy and satisfy the set of sentences. <!--SR:!2025-12-28,158,434!2025-12-28,158,432-->
- [tautology](../../../../general/tautology%20(logic).md) ::@:: In mathematical logic, a \(_this_\) \(from Ancient Greek: ταυτολογία\) is a formula that is true regardless of the interpretation of its component terms, with only the logical constants having a fixed meaning. <!--SR:!2025-12-21,152,434!2025-12-20,151,436-->
  - tautology / examples ::@:: De Morgan's laws, contradiction, contraposition, distributive laws, exportation, excluded middle, etc. <!--SR:!2025-12-27,155,432!2025-12-28,156,434-->
- knowledge base
  - knowledge base / applications ::@:: It can answer queries given several statements. <p> Given several statements, convert it into a knowledge base $\text{KB}$ expressed in propositional logic formulas. Then we can answer a query $\alpha$. Specifically, it is asking if the knowledge base models the query, i.e. $\text{KB} \models \alpha$. <!--SR:!2025-11-09,116,416!2025-12-07,147,434-->

## week 12 lecture 2

- datetime: 2025-04-25T13:30:00+08:00/2025-04-25T14:50:00+08:00, PT1H20M
- topic: propositional logic
- knowledge base
  - knowledge base / conversion ::@:: Identify propositions in the text. They should carry semantics that cannot be \(easily\) expressed in propositional logic. Then convert any remaining semantics into propositional logic formulas using the above propositions, and the resulting set of formulas form the knowledge base. <p> Then, any truth assignment satisfying the knowledge base is a solution. <p> There may be multiple ways to specify the propositions, with some simpler than others in terms of the resulting knowledge base. You may need to identify hidden assumptions, e.g. something has exactly one rank, no two something have the same rank, etc. <!--SR:!2025-12-13,144,436!2026-01-01,162,436-->
- [abductive reasoning](../../../../general/abductive%20reasoning.md) ::@::It is a form of logical inference that seeks the simplest and most likely conclusion from a set of observations. <!--SR:!2025-12-30,158,434!2025-12-27,157,432-->
  - abductive reasoning / knowledge base ::@:: Given a knowledge base $\text{KB}$ and a proposition $\alpha$ observed to be true, find possible causes. <p> Formally, an _abduction_ of $\alpha$ under $\text{KB}$ is a formula $\beta$ such that $\mathrm{KB} \cup \set{\beta} \models \alpha$ \(but $\mathrm{KB} \models \alpha$ may be true or false\). <!--SR:!2025-12-29,157,432!2025-12-26,157,432-->
- [conjunctive normal form](../../../../general/conjunctive%20normal%20form.md) \(CNF\) ::@:: In [Boolean algebra](../../../../general/Boolean%20algebra.md), a [formula](../../../../general/formula%20(mathematical%20logic).md) is in __conjunctive normal form__ \(__CNF__\) or __clausal normal form__ if it is a [conjunction](../../../../general/logical%20conjunction.md) of one or more [clauses](../../../../general/clause%20(logic).md), where a clause is a [disjunction](../../../../general/logical%20disjunction.md) of [literals](../../../../general/literal%20(mathematical%20logic).md); otherwise put, it is a __product of sums__ or __an AND of ORs__. <!--SR:!2025-12-23,154,436!2025-12-31,159,436-->
  - conjunctive normal form / conversion to CNF ::@:: - Rewrite biconditionals in terms of conditionals: $a \equiv b \implies (a \supset b) \land (b \supset a)$. <br/> - Rewrite conditionals into clauses: $a \supset b \implies \lnot a \lor b$. <br/> - Push negation to atomic variables using De Morgan's laws: $\lnot(a \land b) \implies \lnot a \lor \lnot b$ and $\lnot(a \lor b) \implies \lnot a \land \lnot b$. <br/> - Distribute $\lor$ over $\land$ so that $\lor$ moves inward to form clauses and $\land$ moves outward: $(a \land b) \lor c \implies (a \lor c) \land (a \lor b)$. <br/> - Eliminate tautological clauses: $a \lor \lnot a \implies$ <br/> - Eliminate duplicate clauses. <!--SR:!2025-12-12,152,432!2025-11-06,112,416-->
- [resolution](../../../../general/resolution%20(logic).md) ::@:: In mathematical logic and automated theorem proving, \(_this_\) is a rule of inference leading to a refutation-complete theorem-proving technique for sentences in propositional logic and first-order logic. <!--SR:!2025-12-27,155,434!2025-12-29,159,432-->
  - resolution / propositional logic ::@:: For propositional logic, systematically applying the resolution rule acts as a decision procedure for formula unsatisfiability, solving the \(complement of the\) Boolean satisfiability problem. <!--SR:!2025-12-30,160,436!2025-12-12,143,432-->
  - resolution / first-order logic ::@:: For first-order logic, resolution can be used as the basis for a semi-algorithm for the unsatisfiability problem of first-order logic, providing a more practical method than one following from Gödel's completeness theorem. <!--SR:!2025-12-13,144,436!2026-01-01,162,436-->
  - resolution / resolvent ::@:: The clause produced by a resolution rule is sometimes called a \(_this_\). <!--SR:!2025-12-28,156,434!2025-12-14,145,436-->
  - resolution / resolution rule ::@:: Formally: $${\frac {a_{1}\lor a_{2}\lor \cdots \lor c,\quad b_{1}\lor b_{2}\lor \cdots \lor \neg c}{a_{1}\lor a_{2}\lor \cdots \lor b_{1}\lor b_{2}\lor \cdots } }$$ where <p> &emsp; all $a_{i}$, $b_{i}$, and $c$ are literals, <br/> &emsp; the dividing line stands for "[entails](../../../../general/logical%20consequence.md)". <p> The above may also be written schematically as: $${\frac {\Gamma _{1}\cup \left\{\ell \right\}\,\,\,\,\Gamma _{2}\cup \left\{ {\overline {\ell } }\right\} }{\Gamma _{1}\cup \Gamma _{2} } }|\ell |$$ <!--SR:!2025-12-26,154,432!2025-12-24,155,436-->
    - resolution / resolution rule / terminology ::@:: We have the following terminology: <p> - The clauses $\Gamma _{1}\cup \left\{\ell \right\}$ and $\Gamma _{2}\cup \left\{ {\overline {\ell } }\right\}$ are the inference's premises <br/> - $\Gamma _{1}\cup \Gamma _{2}$ \(the resolvent of the premises\) is its conclusion. <br/> - The literal $\ell$ is the left resolved literal, <br/> - The literal ${\overline {\ell } }$ is the right resolved literal, <br/> - $|\ell |$ is the resolved atom or pivot. <!--SR:!2025-12-31,161,434!2025-11-06,112,416-->
    - resolution / resolution rule / modus ponens ::@:: [Modus ponens](../../../../general/modus%20ponens.md) can be seen as a special case of resolution \(of a one-literal clause and a two-literal clause\). $${\frac {p\rightarrow q,\quad p}{q} }$$ is equivalent to $${\frac {\lnot p\lor q,\quad p}{q} }$$ <!--SR:!2025-12-08,143,432!2025-12-31,159,436-->
    - resolution / resolution rule / contradiction ::@:: Contradiction can also be seen as a special case of resolution: $${\frac {p, \quad \lnot p}{} } \,,$$ where the empty conclusion is false \(an empty CNF clause is false\). <!--SR:!2025-12-20,160,436!2025-12-16,156,436-->
  - resolution / proof by contradiction ::@:: When coupled with a complete [search algorithm](../../../../general/search%20algorithm.md), the resolution rule yields a [sound](../../../../general/soundness.md) and [complete](../../../../general/completeness%20(logic).md) algorithm for deciding the _satisfiability_ of a propositional formula, and, by extension, the [validity](../../../../general/validity%20(logic).md) of a sentence under a set of axioms. <p> This resolution technique uses [proof by contradiction](../../../../general/proof%20by%20contradiction.md) and is based on the fact that any sentence in propositional logic can be transformed into an equivalent sentence in [conjunctive normal form](../../../../general/conjunctive%20normal%20form.md). <!--SR:!2025-12-27,155,434!2025-12-13,144,436-->
    - resolution / proof by contradiction / intuition ::@:: Formally, define $\Sigma \to a$ as $a$ can be _derived_ from a set of clauses \(CNF\) $\Sigma$. A _derivation_ is a sequence of clauses \(can be considered as "steps"\) such that each clause is either in $\Sigma$ or is derived from previous clauses using the resolution rule. <p> Since the resolution rule gives: $$\set{p \lor a, \lnot p \lor b} \models a \lor b \,,$$ so $\Sigma \to a$ implies $\Sigma \models a$. <p> As a special case, if $\Sigma \models []$, then $\Sigma \models \mathrm F$, because an empty CNF clause is false. This also means no assignment satisfies $\Sigma$. <p> Proof by contradiction makes use of this. To prove a proposition $p$ is implied by a knowledge base, insert the negation of $p$ into the knowledge base. If the knowledge base does imply $p$, a contradiction occurs, eventually producing an empty CNF clause. Then this proves $p$ must be true in the knowledge base. <!--SR:!2025-12-27,155,432!2026-01-01,160,434-->
    - resolution / proof by contradiction / properties ::@:: It is _sound_: if an empty clause is derived, the set of formulas is _unsatisfiable_. It is _\(refutation-\)complete_: if the set of formulas is _unsatisfiable_, then an empty clause will be derived. <p> These properties are similar to that of the DPLL algorithm, but the DPLL algorithm checks if a CNF is _satisfiable_, as opposed to proof by contradiction checking if a CNF is _unsatisfiable_. <!--SR:!2025-12-31,159,436!2025-12-12,147,434-->
    - resolution / proof by contradiction / steps ::@:: - All sentences in the knowledge base and the _negation_ of the sentence to be proved \(the _conjecture_\) are conjunctively connected. <br/> - The resulting sentence is transformed into a conjunctive normal form with the conjuncts viewed as elements in a set, _S_, of clauses. <br/> &emsp; - For example, $(A_{1}\lor A_{2})\land (B_{1}\lor B_{2}\lor B_{3})\land (C_{1})$ gives rise to the set $S=\{A_{1}\lor A_{2},B_{1}\lor B_{2}\lor B_{3},C_{1}\}$. <br/> - The resolution rule is applied to all possible pairs of clauses that contain complementary literals. After each application of the resolution rule, the resulting sentence is simplified by removing repeated literals. If the clause contains complementary literals, it is discarded \(as a tautology\). If not, and if it is not yet present in the clause set _S_, it is added to _S_, and is considered for further resolution inferences. <br/> - If after applying a resolution rule the _empty clause_ is derived, the original formula is unsatisfiable \(or _contradictory_\), and hence it can be concluded that the initial conjecture [follows from](../../../../general/logical%20consequence.md) the axioms. <br/> - If, on the other hand, the empty clause cannot be derived, and the resolution rule cannot be applied to derive any more new clauses, the conjecture is not a theorem of the original knowledge base. <!--SR:!2025-12-08,148,436!2025-12-22,153,436-->
      - resolution / proof by contradiction / steps / note ::@:: \(__this course__: I don't think they restrict which clause you choose to derive from first, unlike that in the DPLL algorithm. So choose a way that requires the least amount of work.\) <!--SR:!2025-12-28,156,436!2025-12-28,159,436-->
- propositional calculus
  - propositional calculus / limitations ::@:: If you have many propositions, then some rules relating many of those propositions will encode to many clauses. <p> First-order logic \(FOL\) can help reduce complexity. More strictly, FOL is more _concise_ and _expressive_. <!--SR:!2025-12-21,152,434!2025-12-26,154,432-->

## week 13 tutorial

- datetime: 2025-04-29T12:30:00+08:00/2025-04-29T13:20:00+08:00, PT50M
- topic: propositional logic
- propositional calculus
  - propositional calculus / examples
- [Z3 Theorem Prover](../../../../general/Z3%20Theorem%20Prover.md) ::@:: \(_this_\), also known as the \(_this_\), is a satisfiability modulo theories \(SMT\) solver developed by Microsoft. <!--SR:!2025-12-23,154,436!2026-01-01,162,436-->
  - Z3 Theorem Prover / installation ::@:: `pip install z3 z3-solver` <!--SR:!2025-12-26,154,432!2025-12-31,159,436-->
  - Z3 Theorem Prover / variables ::@:: It supports many types of variables, including boolean \(`Bool`\), integer \(`Int`\), real \(`Real`\), etc. <!--SR:!2025-12-18,158,434!2025-12-29,157,436-->
  - Z3 Theorem Prover / constraints ::@:: It supports many types of constraints, including arithmetic \(`+`, `-`, `*`, `/`\), conjunction \(`And`\), disjunction \(`Or`\), distinct \(`Distinct`\), equality \(`==`, `!=`\), etc. <!--SR:!2025-12-31,159,436!2025-12-29,157,436-->
  - Z3 Theorem Prover / solving ::@:: Create a solver using `Solver()`. Add constraints using `solver.add(constraints...)`. Finally, check if the model is satisfiable using `solver.check()`, and if yes, get the model using `solver.model()`. <!--SR:!2025-12-26,154,432!2025-12-31,159,436-->
  - Z3 Theorem Prover / beyond propositional calculus ::@:: It also supports first-order logic, quantifier-free logic, etc. They are much more expressive and requires defining less variables. \(__this course__: Do not use these in examinations for propositional calculus!\) <!--SR:!2025-12-22,153,436!2026-01-02,161,434-->

## week 13 lecture

- datetime: 2025-04-30T13:30:00+08:00/2025-04-30T14:50:00+08:00, PT1H20M
- topic: first-order logic
- [first-order logic](../../../../general/first-order%20logic.md) ::@:: It is a collection of formal systems used in mathematics, philosophy, linguistics, and computer science. First-order logic uses quantified variables over non-logical objects, and allows the use of sentences that contain variables. <!--SR:!2026-01-02,161,436!2025-12-30,158,434-->
  - first-order logic / vs. propositional calculus ::@:: Rather than propositions such as "all men are mortal", in first-order logic one can have expressions in the form "for all _x_, if _x_ is a man, then _x_ is mortal"; where "for all _x"_ is a quantifier, _x_ is a variable, and "... _is a man_" and "... _is mortal_" are predicates. This distinguishes it from [propositional logic](../../../../general/propositional%20logic.md), which does not use quantifiers or [relations](../../../../general/finitary%20relation.md); in this sense, propositional logic is the foundation of first-order logic. <!--SR:!2026-01-03,162,436!2025-12-26,154,432-->
  - first-order logic / expressiveness ::@:: It is much more expressive than propositional calculus. It describes the world as consisting of _objects_, and then facts about the world are _properties_ of or _relations_ between these objects. <!--SR:!2025-12-18,149,432!2025-12-30,158,436-->
  - first-order logic / syntax ::@:: Unlike natural languages, such as English, the language of first-order logic is completely formal, so that it can be mechanically determined whether a given expression is [well formed](../../../../general/well-formedness.md). There are two key types of well-formed expressions: _terms_, which intuitively represent objects, and _formulas_, which intuitively express statements that can be true or false. The terms and formulas of first-order logic are strings of _[symbols](../../../../general/symbol%20(formal).md)_, where all the symbols together form the _[alphabet](../../../../general/alphabet%20(formal%20languages).md)_ of the language. <!--SR:!2026-01-02,161,436!2025-12-29,157,436-->
    - first-order logic / syntax / alphabets ::@:: As with all [formal languages](../../../../general/formal%20language.md), the nature of the symbols themselves is outside the scope of formal logic; they are often regarded simply as letters and punctuation symbols. <p> It is common to divide the symbols of the alphabet into _logical symbols_, which always have the same meaning, and _non-logical symbols_, whose meaning varies by interpretation. For example, the logical symbol $\land$ always represents "and"; it is never interpreted as "or", which is represented by the logical symbol $\lor$. However, a non-logical predicate symbol such as Phil\(_x_\) could be interpreted to mean "_x_ is a philosopher", "_x_ is a man named Philip", or any other unary predicate depending on the interpretation at hand. <!--SR:!2025-12-23,154,436!2025-12-13,144,436-->
    - first-order logic / syntax / logical symbols ::@:: \(__this course__: We use the symbols listed below.\) <p> It includes logical symbols in propositional logic. New logical symbols include: <p> - for every $\forall$ <br/> - there exists $\exists$ <br/> - equality $=$ <br/> - variables $x, y, z, \ldots$ <!--SR:!2025-12-24,155,436!2025-12-27,158,434-->
    - first-order logic / syntax / predicate symbols ::@:: \(__this course__: We use the symbols listed below.\) <p> - A _predicate symbol_ \(or _relation symbol_\) with some _valence_ \(or _arity_, number of arguments\) greater than or equal to 0. These are often denoted by uppercase letters such as _P_, _Q_ and _R_. Examples: <br/> &emsp; - In _P_\(_x_\), _P_ is a predicate symbol of valence 1. One possible interpretation is "_x_ is a man". <br/> &emsp; - In _Q_\(_x_,_y_\), _Q_ is a predicate symbol of valence 2. Possible interpretations include "_x_ is greater than _y_" and "_x_ is the father of _y_". <br/> &emsp; - Relations of valence 0 can be identified with [propositional variables](../../../../general/propositional%20variable.md), which can stand for any statement. One possible interpretation of _R_ is "Socrates is a man". <!--SR:!2026-01-01,160,434!2025-12-14,145,436-->
    - first-order logic / syntax / function symbols ::@:: - A _function symbol_, with some valence greater than or equal to 0. These are often denoted by lowercase [roman letters](../../../../general/Latin%20script.md) such as _f_, _g_ and _h_. Examples: <br/> &emsp; - _f_\(_x_\) may be interpreted as "the father of _x_". In [arithmetic](../../../../general/arithmetic.md), it may stand for "-x". In set theory, it may stand for "the [power set](../../../../general/power%20set.md) of x". <br/> &emsp; - In arithmetic, _g_\(_x_,_y_\) may stand for "_x_+_y_". In set theory, it may stand for "the union of _x_ and _y_". <br/> &emsp; - Function symbols of valence 0 are called _constant symbols_, and are often denoted by lowercase letters at the beginning of the alphabet such as _a_, _b_ and _c_. The symbol _a_ may stand for Socrates. In arithmetic, it may stand for 0. In set theory, it may stand for the [empty set](../../../../general/empty%20set.md). <!--SR:!2026-01-03,162,436!2025-12-12,143,432-->
    - first-order logic / syntax / terms ::@:: The set of _[terms](../../../../general/term%20(logic).md)_ is [inductively defined](../../../../general/inductive%20definition.md) by the following rules: <p> 1. _Variables_. Any variable symbol is a term. <br/> 2. _Functions_. If _f_ is an _n_-ary function symbol, and _t_<sub>1</sub>, ..., _t_<sub>_n_</sub> are terms, then _f_\(_t_<sub>1</sub>,...,_t_<sub>_n_</sub>\) is a term. In particular, symbols denoting individual constants are nullary function symbols, and thus are terms. <o> Only expressions which can be obtained by finitely many applications of rules 1 and 2 are terms. For example, no expression involving a predicate symbol is a term. <!--SR:!2025-12-15,155,436!2025-12-13,153,436-->
    - first-order logic / syntax / formulas ::@:: The set of _[formulas](../../../../general/formula%20(mathematical%20logic).md)_ \(also called _[well-formed formulas](../../../../general/well-formed%20formula.md)_ or _WFFs_\) is inductively defined by the following rules: <p> 1. _Predicate symbols_. If _P_ is an _n_-ary predicate symbol and _t_<sub>1</sub>, ..., _t_<sub>_n_</sub> are terms then _P_\(_t_<sub>1</sub>,...,_t_<sub>_n_</sub>\) is a formula. <br/> 2. _[Equality](../../../../general/logical%20equality.md)_. If the equality symbol is considered part of logic, and _t_<sub>1</sub> and _t_<sub>2</sub> are terms, then _t_<sub>1</sub> = _t_<sub>2</sub> is a formula. <br/> 3. _Negation_. If $\varphi$ is a formula, then $\lnot \varphi$ is a formula. <br/> 4. _Binary connectives_. If ⁠$\varphi$⁠ and ⁠$\psi$⁠ are formulas, then \($\varphi \rightarrow \psi$\) is a formula. Similar rules apply to other binary logical connectives. <br/> 5. _Quantifiers_. If $\varphi$ is a formula and _x_ is a variable, then $\forall x\varphi$ \(for all x, $\varphi$ holds\) and $\exists x\varphi$ \(there exists x such that $\varphi$\) are formulas. <p> Only expressions which can be obtained by finitely many applications of rules 1–5 are formulas. The formulas obtained from the first two rules are said to be _[atomic formulas](../../../../general/atomic%20formula.md)_. <!--SR:!2025-12-28,156,432!2025-12-15,146,434-->
    - first-order logic / syntax / examples ::@:: For example: $$\forall x\forall y(P(f(x))\rightarrow \neg (P(x)\rightarrow Q(f(y),x,z)))$$ is a formula, if _f_ is a unary function symbol, _P_ a unary predicate symbol, and Q a ternary predicate symbol. However, $\forall x\,x\rightarrow$ is not a formula, although it is a string of symbols from the alphabet. <!--SR:!2025-12-05,145,434!2025-12-11,151,432-->
  - first-order logic / syntax / precedence ::@:: For convenience, conventions have been developed about the precedence of the logical operators, to avoid the need to write parentheses in some cases. These rules are similar to the order of operations in arithmetic. \(__this course__: $$\text{first} \quad (\lnot) \succ (\land, \lor) \succ (\supset) \succ (\equiv) \succ (\forall, \exists) \quad \text{last}$$ <p> But tutorial shows: $$\text{first} \quad (\forall, \exists) \succ (\lnot) \succ (\land, \lor) \succ (\supset) \succ (\equiv) \quad \text{last} \,,$$ so I guess best is to always add parentheses.\) <!--SR:!2025-12-13,144,436!2025-12-31,159,436-->
- [universal quantification](../../../../general/universal%20quantification.md) ::@:: $\forall x \, p(x)$ means for all $x$ in the universe $p(x)$ is true. If the universe is empty, $\forall x \, p(x)$ is true for any predicate $p(x)$ (including predicates that look "opposite", i.e. something is true and something is not true). It relates to existential quantification by $$\lnot \forall x \, p(x) \equiv \exists x \, \lnot p(x)$$. <!--SR:!2025-12-28,156,432!2025-12-19,150,436-->
  - universal quantification / distributive law ::@:: It distributes over logical conjunctions: $$\forall x \, (p(x) \land q(x)) \equiv \forall x \, p(x) \land \forall x \, q(x)$$. But it does NOT distribute over logical disjunctions: $$\forall x \, (p(x) \lor q(x)) \leftarrow \forall x \, p(x) \lor \forall x \, q(x)$$ and only the converse (shown above) is true. <!--SR:!2025-12-24,155,436!2025-12-28,156,436-->
- [existential quantification](../../../../general/existential%20quantification.md) ::@:: $\exists x \, p(x)$ means there is at least one $x$ in the universe that $p(x)$ is true. If the universe is empty, $\exists x \, p(x)$ is false for any predicate $p(x)$ (including predicates that look "opposite", i.e. something is true and something is not true). It relates to universal quantification by $$\lnot \exists x \, p(x) \equiv \forall x \, \lnot p(x)$$. <!--SR:!2025-12-15,146,434!2025-12-28,156,432-->
  - existential quantification / distributive law ::@:: It distributes over logical disjunctions: $$\exists x \, (p(x) \lor q(x)) \equiv \exists x \, p(x) \lor \exists x \, q(x)$$. But it does NOT distribute over logical conjunctions: $$\exists x \, (p(x) \land q(x)) \rightarrow \exists x \, p(x) \land \exists x \, q(x)$$ but the converse is not necessarily true. <!--SR:!2025-12-29,157,436!2025-12-27,157,434-->
- first-order logic
  - first-order logic / semantics
    - first-order logic / semantics / first-order structures ::@:: The most common way of specifying an interpretation \(especially in mathematics\) is to specify a _structure_ \(also called a _model_; see below\). The structure consists of a domain of discourse _D_ and an interpretation function _I_ mapping non-logical symbols to predicates, functions, and constants. <p> The domain of discourse _D_ is a nonempty set of "objects" of some kind. Intuitively, given an interpretation, a first-order formula becomes a statement about these objects; for example, $\exists xP(x)$ states the existence of some object in _D_ for which the predicate _P_ is true \(or, more precisely, for which the predicate assigned to the predicate symbol _P_ by the interpretation is true\). For example, one can take _D_ to be the set of [integers](../../../../general/integer.md). <p> Non-logical symbols are interpreted as follows: <p> - The interpretation of an _n_-ary function symbol is a function from _D_<sup>_n_</sup> to _D_. For example, if the domain of discourse is the set of integers, a function symbol _f_ of arity 2 can be interpreted as the function that gives the sum of its arguments. In other words, the symbol _f_ is associated with the function ⁠$I(f)$⁠ which, in this interpretation, is addition. <br/> - The interpretation of a constant symbol \(a function symbol of arity 0\) is a function from _D_<sup>0</sup> \(a set whose only member is the empty [tuple](tuple.md)\) to _D_, which can be simply identified with an object in _D_. For example, an interpretation may assign the value $I(c)=10$ to the constant symbol $c$. <br/> - The interpretation of an _n_-ary predicate symbol is a set of _n_-tuples of elements of _D_, giving the arguments for which the predicate is true. For example, an interpretation $I(P)$ of a binary predicate symbol _P_ may be the set of pairs of integers such that the first one is less than the second. According to this interpretation, the predicate _P_ would be true if its first argument is less than its second argument. Equivalently, predicate symbols may be assigned [Boolean-valued functions](../../../../general/Boolean-valued%20function.md) from _D_<sup>_n_</sup> to $\{\mathrm {true,false} \}$. <!--SR:!2026-01-03,162,436!2025-12-30,158,436-->
- logical consequence
- resolution
  - resolution / resolution rule
  - resolution / first-order logic ::@:: Resolution rule can be generalized to [first-order logic](../../../../general/first-order%20logic.md) to: $${\frac {\Gamma _{1}\cup \left\{L_{1}\right\}\,\,\,\,\Gamma _{2}\cup \left\{L_{2}\right\} }{(\Gamma _{1}\cup \Gamma _{2})\phi } }\phi$$ where $\phi$ is a [most general unifier](../../../../general/most%20general%20unifier.md#syntactic%20unification%20of%20first-order%20terms) of $L_{1}$ and ${\overline {L_{2} } }$, and $\Gamma _{1}$ and $\Gamma _{2}$ have no common variables. <!--SR:!2025-12-18,149,434!2026-01-01,160,436-->
    - resolution / first-order logic / most general unifier ::@:: Replace terms in $L_1$ and $\overline L_2$ such that they refer to the same atomic formulas. <p> For example, if one is an \(implicit\) universal quantified variable and the other is a constant term $c$, then $[x / c]$ is the most general unifier. <!--SR:!2025-12-15,155,436!2025-12-21,152,436-->
    - resolution / first-order logic / example ::@:: The clauses $P(x),Q(x)$ and $\neg P(b)$ can apply this rule with $[b/x]$ as unifier. <p> Here x is a variable and b is a constant.  $${\frac {P(x),Q(x)\,\,\,\,\neg P(b)}{Q(b)} }[b/x]$$ Here we see that <p> - The clauses $P(x),Q(x)$ and $\neg P(b)$ are the inference's premises <br/> - $Q(b)$ \(the resolvent of the premises\) is its conclusion. <br/> - The literal $P(x)$ is the left resolved literal, <br/> - The literal $\neg P(b)$ is the right resolved literal, <br/> - $P$ is the resolved atom or pivot. <br/> - $[b/x]$ is the most general unifier of the resolved literals. <!--SR:!2026-01-02,161,436!2026-01-02,161,436-->

## week 13 lecture 2

- datetime: 2025-05-02T13:30:00+08:00/2025-05-02T14:50:00+08:00, PT1H20M
- topic: first-order logic
- resolution
  - resolution / proof by contradiction
- conjunctive normal form
  - conjunctive normal form / first-order logic ::@:: In first order logic, conjunctive normal form can be taken further to yield the clausal normal form of a logical formula, which can be then used to perform [first-order resolution](../../../../general/resolution%20(logic).md#resolution%20in%20first-order%20logic). <p> \(__this course__: The steps are quite involved. Instead we only consider simple cases where you only need to apply the steps for propositional calculus only. After doing so, the quantifiers should be all universal and are the outermost layers.\) <!--SR:!2025-12-08,143,432!2025-12-28,156,436-->
- resolution
  - resolution / proof by contradiction
    - resolution / proof by contradiction / first-order logic ::@:: Normally the queries are like $P(a)$, where $a$ is a known constant. <p> Sometimes, the queries are like $\exists x P(x)$, which asks if there is an object satisfying $P(x)$. We may also want to know what that object is. <p> We change the query to $\exists x (P(x) \land \lnot A(x))$ where $A$ is a new predicate not appearing anywhere else called the _answer predicate_. Then its negation becomes $\forall x (\lnot P(x) \lor A(x))$. The goal becomes deriving $A(x)$ instead of an empty clause $[]$. The substitution steps in resolution will replace the $x$ in $A(x)$ with the object satisfying $P(x)$. <p> If the resulting clause has connectives connecting $A$, then the object satisfying $P(x)$ is obtained by discarding the $A$ but keeping the connectives. For example, $A(a) \lor A(b)$ becomes $a \lor b$, which means $a$ or $b$ or both satisfies $P(x)$. <!--SR:!2025-09-24,73,376!2025-10-06,81,396-->
- [rule-based system](../../../../general/rule-based%20system.md) ::@:: In computer science, a \(_this_\) is a computer system in which domain-specific knowledge is represented in the form of rules and general-purpose reasoning is used to solve problems in the domain. <!--SR:!2025-12-23,154,436!2025-12-31,159,436-->
- [expert system](../../../../general/expert%20system.md) ::@:: In [artificial intelligence](../../../../general/artificial%20intelligence.md) \(AI\), an \(_this_\) is a computer system emulating the decision-making ability of a human [expert](../../../../general/expert.md). Expert systems are designed to solve complex problems by [reasoning](../../../../general/automated%20reasoning%20system.md) through bodies of knowledge, represented mainly as [if–then rules](../../../../general/rule-based%20system.md) rather than through conventional [procedural programming](../../../../general/procedural%20programming.md) code. Expert systems were among the first truly successful forms of AI software. <!--SR:!2025-12-27,155,434!2025-12-19,159,436-->
  - expert system / examples ::@:: - American Express loan processing expert system <br/> - DENDRAL \(1965–1983\): molecular structure analysis <br/> - MYCIN \(1972–1980\): medical diagnosis <br/> - PROSPECTOR: mining <br/> - XCOM \(R1\): computer configuration; used in DEC <!--SR:!2025-12-09,149,434!2025-12-28,156,432-->
  - expert system / basic architecture ::@:: A knowledge engineer constructs the knowledge base and inference engine. <p> An expert uses a knowledge acquisition system to manage the knowledge base. <p> An user interacts with the knowledge base and the inference engine via an explanation subsystem with an user interface. <!--SR:!2025-12-31,161,434!2026-01-01,160,436-->
- [Horn clause](../../../../general/Horn%20clause.md) ::@:: In mathematical logic and logic programming, a \(_this_\) is a logical formula of a particular rule-like form that gives it useful properties for use in logic programming, formal specification, universal algebra and model theory. <!--SR:!2025-12-16,156,436!2025-12-13,153,436-->
  - Horn clause / definition ::@:: A Horn clause is a disjunctive clause \(a disjunction of literals\) with at most one positive, i.e. unnegated, literal. <!--SR:!2025-12-27,155,434!2025-12-31,159,436-->
  - Horn clause / implication form ::@:: A Horn clause can be converted into an implication with at most one literal in its conclusion. <!--SR:!2025-12-29,157,434!2025-12-26,157,434-->
- [rule induction](../../../../general/rule%20induction.md) ::@:: \(_this_\) is an area of machine learning in which formal rules are extracted from a set of observations. The rules extracted may represent a full scientific model of the data, or merely represent local patterns in the data. <!--SR:!2025-12-10,145,436!2025-12-25,155,432-->
  - rule induction / motivation ::@:: It takes a lot of effort to collect rules. So why not try acquiring the rules automagically? <p> An algorithm for this is the _generic separate-and-conquer algorithm_ \(GSCA\). <!--SR:!2025-12-30,161,434!2025-12-26,156,434-->
- generic separate-and-conquer algorithm \(GSCA\) ::@:: It is a simple divide-and-conquer algorithm for finding rules or Horn clauses from a dataset. <!--SR:!2025-12-12,152,432!2025-12-14,154,434-->
  - generic separate-and-conquer algorithm / inputs ::@:: A database. Each column is an atom \(property\). One of the column is the _label_. The remaining columns are the _features_. Each row is a data entry, containing a _boolean_ value for each column. <p> We will learn rules for the label from the features. <!--SR:!2025-12-18,149,432!2026-01-01,160,436-->
  - generic separate-and-conquer algorithm / steps ::@:: Initialize an empty set for storing Horn clauses. While the set of rules \(Horn clauses\) do not cover all data with positive _label_ $\ell$, find a new rule using the following steps: <p> Initialize an _temporary_ database that excludes data already covered by the existing Horn clauses. Initialize an empty rule: $\set{} \supset \ell$. While the new rule covers any negative _label_ $\lnot \ell$ in the _temporary_ database, select an unused column according to some _heuristic_ and add it to the new rule premise using conjunction $\land$. Finally, add the new rule to the set of rules. <p> \(__this course__: The above steps checks the condition _before_ running the two "while"-loops. The steps in the lecture slides checks the condition _after_ running the two "while"-loops. This may matter in exams... The lecture slides also update the temporary database differently.\) <!--SR:!2025-12-26,154,432!2026-01-02,161,436-->
  - generic separate-and-conquer algorithm / heuristic ::@:: To select an unused column to add to the new rule, we need a _heuristic_. <p> Consider the temporary database; that is, rows that is not covered by the existing set of rules \(excluding the new rule currently being added\). Then, consider part of the temporary database covered by the new rule right now. Usually, there should be both positive rows and negative rows. <p> For each unused column $a$, calculate the ratio: $$r_a = \frac {n_a^+} {n_a} = \frac {\#(a = 1, \gamma = 1)} {\#(a = 1)} \,.$$ Then choose the column with the highest ratio $r_a$, tiebreaking with a different criterion \(e.g. $\#(a = 1)$\). \(If $r_a = 1$, this also implies after adding this column, the new rule will then be added to the set of rules.\) <!--SR:!2025-11-02,109,412!2025-10-21,100,416-->
  - generic separate-and-conquer algorithm / failure ::@:: It can fail to find rules or Horn clauses from a dataset. This is because rules or Horn clauses cannot contain negative literals as rule premises. <p> The simplest example dataset consists of two rows each with two features \(first two values\) and one label \(the last value\): <p> - \(true, false, true\) <br/> - \(true, true, false\) <p> \(__this course__: Try to run the algorithm up until the point of failure. Then explain why it fails, _explicitly_ mentioning that GSCA cannot learn _negative literals_.\) <!--SR:!2026-01-07,158,448!2026-01-14,165,448-->

> Dear All,
>
> Due to a severe vomit this morning, it's hard for me to take bus/taxi to the campus. Therefore, today's COMP3211 classes will be online for both sessions. Please join the zoom link at your class time:
>
> L1 \(9am - 10:20am\): \[redacted\]
>
> L2 \(1:30pm - 2:50pm\): \[redacted\]
>
> Sorry for the possible inconvenience that may cause. See you then!
>
> Best regards, <br/>
> \[redacted\]

## week 14 tutorial

- datetime: 2025-05-06T12:30:00+08:00/2025-05-06T13:20:00+08:00, PT50M
- topic: first-order logic
- first-order logic
  - first-order logic / examples

## week 14 lecture

- datetime: 2025-05-07T13:30:00+08:00/2025-05-07T14:50:00+08:00, PT1H20M
- topic: reasoning under uncertainty
- [uncertainty](../../../../general/uncertainty.md) ::@:: \(_this_\) or \(_this_\) refers to situations involving imperfect or unknown information. It applies to predictions of future events, to physical measurements that are already made, or to the unknown, and is particularly relevant for decision-making. <!--SR:!2025-12-30,158,436!2025-12-30,160,436-->
- [reasoning system](../../../../general/reasoning%20system.md) ::@:: In information technology a \(_this_\) is a software system that generates conclusions from available knowledge using logical techniques such as deduction and induction. \(_this_\) play an important role in the implementation of artificial intelligence and knowledge-based systems. <!--SR:!2025-12-15,146,436!2025-12-28,156,432-->
  - reasoning system / reasoning under uncertainty ::@:: Many reasoning systems provide capabilities for reasoning under uncertainty. This is important when building situated reasoning agents which must deal with uncertain representations of the world. There are several common approaches to handling uncertainty. These include the use of certainty factors, probabilistic methods such as Bayesian inference or Dempster–Shafer theory, multi-valued \('fuzzy'\) logic and various connectionist approaches. <!--SR:!2025-12-31,161,434!2025-12-28,158,434-->
- [Bayesian inference](../../../../general/Bayesian%20inference.md) ::@:: It is a method of statistical inference in which Bayes' theorem is used to calculate a probability of a hypothesis, given prior evidence, and update it as more information becomes available. <!--SR:!2025-12-29,160,436!2025-12-18,149,432-->
  - Bayesian inference / idea ::@:: Fundamentally, Bayesian inference uses a prior distribution to estimate posterior probabilities. <!--SR:!2025-12-30,160,432!2025-12-30,158,434-->
  - Bayesian inference / formal explanation ::@:: Bayesian inference derives the [posterior probability](../../../../general/posterior%20probability.md) as a [consequence](../../../../general/consequence%20relation.md) of two [antecedents](../../../../general/antecedent%20(logic).md): a [prior probability](../../../../general/prior%20probability.md) and a "[likelihood function](../../../../general/likelihood%20function.md)" derived from a [statistical model](../../../../general/statistical%20model.md) for the observed data. Bayesian inference computes the posterior probability according to [Bayes' theorem](../../../../general/Bayes'%20theorem.md): $$P(H\mid E)={\frac {P(E\mid H)\cdot P(H)}{P(E)} },$$ <!-- where <p> --> <!--SR:!2025-12-26,154,432!2025-12-27,155,434-->
  - Bayesian inference / hypothesis ::@:: - _H_ stands for any _hypothesis_ whose probability may be affected by [data](../../../../general/experimental%20data.md) \(called _evidence_ below\). Often there are competing hypotheses, and the task is to determine which is the most probable. <!--SR:!2025-12-24,155,436!2025-12-29,159,436-->
  - Bayesian inference / prior probability ::@:: - $P(H)$, the _[prior probability](../../../../general/prior%20probability.md)_, is the estimate of the probability of the hypothesis _H_ _before_ the data _E_, the current evidence, is observed. <!--SR:!2025-12-20,151,436!2026-01-02,161,436-->
  - Bayesian inference / evidence ::@:: - _E_, the _evidence_, corresponds to new data that were not used in computing the prior probability. <!--SR:!2025-12-29,157,434!2025-12-20,160,436-->
  - Bayesian inference / posterior probability ::@:: - $P(H\mid E)$, the _[posterior probability](../../../../general/posterior%20probability.md)_, is the probability of _H_ _given_ _E_, i.e., _after_ _E_ is observed. This is what we want to know: the probability of a hypothesis _given_ the observed evidence. <!--SR:!2026-01-03,162,436!2025-12-13,144,436-->
  - Bayesian inference / likelihood ::@:: - $P(E\mid H)$ is the probability of observing _E_ _given_ _H_ and is called the _[likelihood](../../../../general/likelihood%20function.md)_. As a function of _E_ with _H_ fixed, it indicates the compatibility of the evidence with the given hypothesis. The likelihood function is a function of the evidence, _E_, while the posterior probability is a function of the hypothesis, _H_. <!--SR:!2025-12-28,156,436!2025-12-10,150,434-->
  - Bayesian inference / marginal likelihood ::@:: - $P(E)$ is sometimes termed the [marginal likelihood](../../../../general/marginal%20likelihood.md) or "model evidence". This factor is the same for all possible hypotheses being considered \(as is evident from the fact that the hypothesis _H_ does not appear anywhere in the symbol, unlike for all the other factors\) and hence does not factor into determining the relative probabilities of different hypotheses. <br/> - $P(E)>0$ \(Else one has $0/0$.\) <!--SR:!2025-12-19,150,436!2026-01-03,162,436-->
  - Bayesian inference / in practice ::@:: Modern probabilistic reasoning systems work _directly_ with conditional probabilities. <!--SR:!2025-12-21,161,436!2025-12-10,145,434-->
- [independence](../../../../general/independence%20(probability%20theory).md) ::@:: __Independence__ is a fundamental notion in [probability theory](../../../../general/probability%20theory.md), as in [statistics](../../../../general/statistics.md) and the theory of [stochastic processes](../../../../general/stochastic%20processes.md). Two [events](../../../../general/event%20(probability%20theory).md) are __independent__, __statistically independent__, or __stochastically independent__ if, informally speaking, the occurrence of one does not affect the probability of occurrence of the other or, equivalently, does not affect the [odds](../../../../general/odds.md). <!--SR:!2025-12-07,147,434!2025-12-25,155,434-->
  - independence / two events ::@:: Two events $A$ and $B$ are independent \(often written as $A\perp B$ or $A\perp \!\!\!\perp B$, where the latter symbol often is also used for [conditional independence](../../../../general/conditional%20independence.md)\) if and only if their [joint probability](../../../../general/joint%20probability.md) equals the product of their probabilities: $$\mathrm {P} (A\cap B)=\mathrm {P} (A)\mathrm {P} (B)$$ <!--SR:!2025-12-12,143,432!2025-12-18,158,436-->
  - independence / odds ::@:: Stated in terms of [odds](../../../../general/odds.md), two events are independent if and only if the [odds ratio](../../../../general/odds%20ratio.md) of ⁠$A$⁠ and ⁠$B$⁠ is unity \(1\). Analogously with probability, this is equivalent to the conditional odds being equal to the unconditional odds: $$O(A\mid B)=O(A){\text{ and } }O(B\mid A)=O(B),$$ or to the odds of one event, given the other event, being the same as the odds of the event, given the other event not occurring: $$O(A\mid B)=O(A\mid \neg B){\text{ and } }O(B\mid A)=O(B\mid \neg A).$$ <!--SR:!2025-12-30,158,436!2025-09-25,74,396-->
- [conditional independence](../../../../general/conditional%20independence.md) ::@:: In [probability theory](../../../../general/probability%20theory.md), \(_this_\) describes situations wherein an observation is irrelevant or redundant when evaluating the certainty of a hypothesis. Conditional independence is usually formulated in terms of [conditional probability](../../../../general/conditional%20probability.md), as a special case where the probability of the hypothesis given the uninformative observation is equal to the probability without. <!--SR:!2025-12-31,159,436!2025-12-09,149,436-->
  - conditional independence / definition ::@:: If $A$ is the hypothesis, and $B$ and $C$ are observations, conditional independence can be stated as an equality: $$P(A\mid B,C)=P(A\mid C)$$ where $P(A\mid B,C)$ is the probability of $A$ given both $B$ and $C$. Since the probability of $A$ given $C$ is the same as the probability of $A$ given both $B$ and $C$, this equality expresses that $B$ contributes nothing to the certainty of $A$. In this case, $A$ and $B$ are said to be __conditionally independent__ given $C$, written symbolically as: $(A\perp \!\!\!\perp B\mid C)$. <!--SR:!2025-12-22,153,436!2025-12-26,154,432-->
- [joint probability distribution](../../../../general/joint%20probability%20distribution.md) ::@:: Given [random variables](../../../../general/random%20variable.md) $X,Y,\ldots$, that are defined on the same [probability space](../../../../general/probability%20space.md), the __multivariate__ or __joint probability distribution__ for $X,Y,\ldots$ is a [probability distribution](../../../../general/probability%20distribution.md) that gives the probability that each of $X,Y,\ldots$ falls in any particular range or discrete set of values specified for that variable. <!--SR:!2025-11-06,113,412!2025-12-25,156,434-->
  - joint probability distribution / reasoning system ::@:: Reasoning systems are often interested in finding conditional probabilities. <p> Summing over irrelevant variables \(variables not appearing in the conditional probabilities\) to find the marginal probabilities, and then applying the conditional probability definition suffices. <!--SR:!2025-12-13,144,436!2026-01-03,162,436-->
    - joint probability distribution / reasoning system / advantages ::@:: The semantics are clear and intuitive. In theory, arbitrary inference between the variables can be performed. <!--SR:!2025-12-31,159,436!2026-01-03,162,436-->
    - joint probability distribution / reasoning system / disadvantages ::@:: Many numbers are needed. If the entire model has $n$ _discrete_ variables, then $2^n - 1$ probabilities are needed, with the remaining probability inferred using the unity principle \(the probabilities sum up to 1\). Storage and inference complexity increases. <p> Joint probabilities are also unnatural to specify by expert systems. <!--SR:!2025-12-19,159,436!2026-01-02,161,436-->
- [chain rule](../../../../general/chain%20rule%20(probability).md) ::@:: In [probability theory](../../../../general/probability%20theory.md), the __chain rule__ \(also called the __general product rule__\) describes how to calculate the probability of the intersection of, not necessarily [independent](../../../../general/independence%20(probability%20theory).md), events or the [joint distribution](../../../../general/joint%20distribution.md) of [random variables](../../../../general/random%20variables.md) respectively, using [conditional probabilities](../../../../general/conditional%20probabilities.md). This rule allows one to express a joint probability in terms of only conditional probabilities. <!--SR:!2025-12-31,161,436!2025-12-09,144,434-->
  - chain rule / use ::@:: The rule is notably used in the context of discrete [stochastic processes](../../../../general/stochastic%20process.md) and in applications, e.g. the study of [Bayesian networks](../../../../general/Bayesian%20network.md), which describe a [probability distribution](../../../../general/probability%20distribution.md) in terms of conditional probabilities. <!--SR:!2025-12-22,162,436!2025-12-25,156,436-->
  - chain rule / two events ::@:: For two [events](event%20(probability%20theory).md) $A$ and $B$, the chain rule states that $$\mathbb {P} (A\cap B)=\mathbb {P} (B\mid A)\mathbb {P} (A)\,$$ where $\mathbb {P} (B\mid A)$ denotes the [conditional probability](../../../../general/conditional%20probabilities.md) of $B$ given $A$. <!--SR:!2026-01-02,161,436!2025-12-12,143,432-->
  - chain rule / finitely many rules ::@:: For events $A_{1},\ldots ,A_{n}$ whose intersection has not probability zero, the chain rule states $${\begin{aligned}\mathbb {P} \left(A_{1}\cap A_{2}\cap \ldots \cap A_{n}\right)&=\mathbb {P} \left(A_{n}\mid A_{1}\cap \ldots \cap A_{n-1}\right)\mathbb {P} \left(A_{1}\cap \ldots \cap A_{n-1}\right)\\&=\mathbb {P} \left(A_{n}\mid A_{1}\cap \ldots \cap A_{n-1}\right)\mathbb {P} \left(A_{n-1}\mid A_{1}\cap \ldots \cap A_{n-2}\right)\mathbb {P} \left(A_{1}\cap \ldots \cap A_{n-2}\right)\\&=\mathbb {P} \left(A_{n}\mid A_{1}\cap \ldots \cap A_{n-1}\right)\mathbb {P} \left(A_{n-1}\mid A_{1}\cap \ldots \cap A_{n-2}\right)\cdot \ldots \cdot \mathbb {P} (A_{3}\mid A_{1}\cap A_{2})\mathbb {P} (A_{2}\mid A_{1})\mathbb {P} (A_{1})\\&=\mathbb {P} (A_{1})\mathbb {P} (A_{2}\mid A_{1})\mathbb {P} (A_{3}\mid A_{1}\cap A_{2})\cdot \ldots \cdot \mathbb {P} (A_{n}\mid A_{1}\cap \dots \cap A_{n-1})\\&=\prod _{k=1}^{n}\mathbb {P} (A_{k}\mid A_{1}\cap \dots \cap A_{k-1})\\&=\prod _{k=1}^{n}\mathbb {P} \left(A_{k}\,{\Bigg |}\,\bigcap _{j=1}^{k-1}A_{j}\right).\end{aligned} }$$ <!--SR:!2025-12-30,158,436!2025-12-28,156,434-->
  - chain rule / reasoning system ::@:: Using the chain rule, much fewer numbers are needed to infer a joint probability. These numbers are conditional probabilities, which are much more natural for an expert system to assess. <p> Using conditional independence, the involved conditional probabilities can be simplified. <!--SR:!2025-12-30,160,434!2025-12-15,155,436-->
- [Bayesian network](../../../../general/Bayesian%20network.md) ::@:: It is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph \(DAG\). <!--SR:!2025-12-25,156,436!2025-12-31,159,436-->
  - Bayesian network / graphical model ::@:: Formally, Bayesian networks are [directed acyclic graphs](../../../../general/directed%20acyclic%20graph.md) \(DAGs\). <!--SR:!2025-12-26,154,432!2025-12-15,155,436-->
    - Bayesian network / graphical model / nodes ::@:: ... whose nodes represent variables in the [Bayesian](../../../../general/Bayesian%20probability.md) sense: they may be observable quantities, [latent variables](../../../../general/latent%20variable.md), unknown parameters or hypotheses. <!--SR:!2025-12-22,153,436!2025-12-31,161,436-->
    - Bayesian network / graphical model / edges ::@:: Each edge represents a direct conditional dependency. Any pair of nodes that are not connected \(i.e. no \(_undirected_\) path connects one node to the other\) represent variables that are [conditionally independent](../../../../general/conditional%20independence.md) of each other. <!--SR:!2025-12-21,152,436!2026-01-01,162,436-->
    - Bayesian network / graphical model / probability function ::@:: Each node is associated with a [probability function](../../../../general/probability%20distribution.md) that takes, as input, a particular set of values for the node's [parent](../../../../general/glossary%20of%20graph%20theory.md#directed%20acyclic%20graphs) variables, and gives \(as output\) the probability \(or probability distribution, if applicable\) of the variable represented by the node. For example, if $m$ parent nodes represent $m$ [Boolean variables](../../../../general/Boolean%20data%20type.md), then the probability function could be represented by a table of  $2^{m}$ entries, one entry for each of the  $2^{m}$ possible parent combinations. Similar ideas may be applied to undirected, and possibly cyclic, graphs such as [Markov networks](../../../../general/Markov%20network.md). <!--SR:!2025-12-25,156,436!2025-12-27,155,432-->
  - Bayesian network / joint probabilities ::@:: It can represent joint probabilities _compactly_. You simply follow the network around to reach all nodes, keeping in mind which pair of nodes are _d_-separated. Then multiply the collected conditional probabilities together by the chain rule. <!--SR:!2025-12-30,160,436!2026-01-01,160,436-->
  - Bayesian network / _d_-separation ::@:: This definition can be made more general by defining the "d"-separation of two nodes, where d stands for directional. We first define the "d"-separation of a trail and then we will define the "d"-separation of two nodes in terms of that. <p> Let _P_ be a trail from node _u_ to _v_. A trail is a loop-free, undirected \(i.e. all edge directions are ignored\) path between two nodes. Then _P_ is said to be _d_-separated by a set of nodes _Z_ if any of the following conditions holds: <p> - _P_ contains \(but does not need to be entirely\) a directed chain, $u\cdots \leftarrow m\leftarrow \cdots v$ or $u\cdots \rightarrow m\rightarrow \cdots v$, such that the middle node _m_ is in _Z_, <br/> - _P_ contains a fork, $u\cdots \leftarrow m\rightarrow \cdots v$, such that the middle node _m_ is in _Z_, or <br/> - _P_ contains an inverted fork \(or collider\), $u\cdots \rightarrow m\leftarrow \cdots v$, such that the middle node _m_ is not in _Z_ and no descendant of _m_ is in _Z_. <p> The nodes _u_ and _v_ are _d_-separated by _Z_ if all trails between them are _d_-separated. If _u_ and _v_ are not d-separated, they are d-connected. <!--SR:!2025-10-05,81,396!2025-12-04,144,432-->
    - Bayesian network / _d_-separation / use ::@:: Given a set of nodes are known. Make that set of nodes _Z_. <p> Even if _Z_ is empty, two nodes can still be _d_-separated because all trails connecting these two nodes each have an inverted fork \(or collider\). <!--SR:!2025-12-26,154,432!2025-12-28,156,434-->
    - Bayesian network / _d_-separation / graph ::@:: There is an alternative way to see _d_-separation by manipulating the graph. Assume you want to find if _u_ and _v_ are d-separated given _Z_ is known. <p> 1. Keep only _u_, _v_, _Z_, and their ancestors. \(ancestral graph\) <br/> 2. Add undirected edges between all pairs of each node's parents. \(moral graph\) <br/> 3. Convert all directed edges to undirected edges. \(undirected graph\) <br/> 4. Remove _Z_. <p> If the resulting graph connects _u_ and _v_, they are _d_-connected. Otherwise they are _d_-separated. <!--SR:!2025-12-15,150,436!2025-12-26,154,432-->
  - Bayesian network / causal networks ::@:: A causal network is a Bayesian network with the requirement that the relationships be causal. The additional semantics of causal networks specify that if a node _X_ is actively caused to be in a given state _x_ \(an action written as do\(_X_ = _x_\)\), then the probability density function changes to that of the network obtained by cutting the links from the parents of _X_ to _X_, and setting _X_ to the caused value _x_. Using these semantics, the impact of external interventions from data obtained prior to intervention can be predicted. <p> Note that the act of cutting the links makes it different from simply observing that _X_ = _x_. This mirrors the difference between _seeing_ and _doing_. <!--SR:!2025-11-03,110,416!2025-11-01,111,416-->
    - Bayesian network / casual networks / non-necessity ::@:: Although Bayesian networks are often used to represent [causal](../../../../general/causality.md) relationships, this need not be the case: a directed edge from _u_ to _v_ does not require that _X<sub>v</sub>_ be causally dependent on _X<sub>u</sub>_. This is demonstrated by the fact that Bayesian networks on the graphs: $$a\rightarrow b\rightarrow c\qquad {\text{and} }\qquad a\leftarrow b\leftarrow c$$ are equivalent: that is they impose exactly the same conditional independence requirements. <!--SR:!2025-12-19,150,436!2025-12-17,157,432-->

## week 14 lecture 2

- datetime: 2025-05-09T13:30:00+08:00/2025-05-09T14:50:00+08:00, PT1H20M
- topic: final examination Q&A
- [§ final examination](#final%20examination)

> __<big><big>Final Q&A Office Hour on This Friday from 1:30pm-2:50pm</big></big>__
>
> The L2 class on this Friday \(May 9, 1:30pm-2:50pm\) will be in the form of Final Q&A Office Hour at my office \(Rm 2538, Lift 25/26\).
>
> All students \(including L1 and L2 sessions\) are welcome to come to ask questions related with this course.

## final examination

- datetime: 2025-05-28T12:30:00+08:00/2025-05-28T15:30:00+08:00, PT3H
- venue: Tsang Shiu Tim Art Hall
- format
  - calculator: yes
  - cheatsheet: no
  - referencing: closed book, closed notes
  - provided: \(none\)
  - questions: long questions ×8
- grades: 88.5/100 → 89.5/100
  - statistics
    - timestamps: 2025-06-01T10:46+08:00 → 2025-06-03T18:00+08:00
    - mean: 54.88 \(provided: 54.87\) → 55.45
    - standard deviation: ? \(provided: 17.845\) → ?
    - low: 0 → 0
    - lower quartile: 43.05 → 43.53
    - median: 53.9 \(provided: 53.65\) → 54.7
    - upper quartile: 67.18 → 67.48
    - high: 96 → 96
    - distribution: ? → ?
    - data: ? → ?
- report
  - question 5: auction
    - proving Nash equilibria of second-price sealed-bid auction \(−8\) ::@:: Maybe you could look up the Nash equilibria of SPSBA beforehand... <!--SR:!2026-01-08,159,448!2026-01-20,171,448-->
  - question 6: logic
    - declaring propositions \(−1\) ::@:: When axiomatizing natural text, you need to state the propositions. <!--SR:!2026-01-15,166,448!2026-01-09,160,448-->
    - declaring answer predicate \(−0.5\) ::@:: You need to declare what the answer predicate $A(x)$ is, not just simply use it. <!--SR:!2026-01-13,164,448!2026-01-09,160,448-->
  - question 7: perceptron and GSCA
    - explaining GSCA failure \(−1\) ::@:: You need to _explicitly_ state that GSCA cannot learn negative literals. <!--SR:!2026-01-20,171,448!2026-01-14,165,448-->
- check
  - datetime: 2025-06-02T13:30:00+08:00/2025-06-02T15:30:00+08:00, PT2H
  - venue: Room 6573, Academic Building
  - report
  - question 6: logic
    - using answer predicate \(+1\) ::@:: No idea why one of the correct answers Zoey was marked wrong...? <!--SR:!2026-01-21,172,448!2026-01-21,172,448-->

> __<big><big>Final Exam Information and Past Final Exam Papers</big></big>__
>
> __<big>----------------Update Information-------------------</big>__
>
> Please check the reply under this announcement about the typos in the released past exam papers.
>
> __<big>----------------Original Contents----------------------</big>__
>
> __<big>Final Exam Information</big>__
>
> COMP3211 final exam will be held __on May 28 \(Wed\), from 12:30pm to 3:30pm, in Tsang Shiu Tim Art Hall__.
>
> - The final coverage is from the beginning to the end, everything in the lecture notes and the assignments. There are in total 11 lecture notes.
> - The exam is closed-book. No cheat sheet is allowed.
> - Calculators are allowed.
> - Written programming/coding won't be tested in the exam.
>
> __<big>Final Exam Time Conflicts</big>__
>
> If you have time conflicts with the final exam due to other academic activities, e.g., conference, competition, etc, __please let me know by Apr 28 (Mon) with your proof documents__, so that I can arrange your make-up final exam.
>
> If you cannot attend the final exam due to medical reasons, you need to apply to the Academic Registry for a make-up exam. You can find the application form and related information at [https://registry.hkust.edu.hk/resource-library/examination-regulations-student](https://registry.hkust.edu.hk/resource-library/examination-regulations-student).
>
> __<big>Past Final Exam Papers</big>__
>
> 23-final.pdf, 22-final.pdf

## aftermath

### total

- grades: 94.05/100
  - statistics: \(none\)
