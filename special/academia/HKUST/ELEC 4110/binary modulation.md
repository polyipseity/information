---
aliases:
  - ELEC 4110 binary modulation
  - ELEC4110 binary modulation
  - binary modulation
tags:
  - flashcard/active/special/academia/HKUST/ELEC_4110/binary_modulation
  - language/in/English
---

# binary modulation

{@{The simplest analytical model}@}, often called the {@{__binary channel__ or __binary symmetric channel with additive white Gaussian noise__ (__AWGN__)}@}, reduces {@{the whole communication link to a black box}@} that accepts {@{binary input symbols at the transmitter and produces binary output symbols at the receiver}@}. {@{All intermediate physical-layer details}@} – {@{propagation, multipath, fading, etc.}@} – are {@{absorbed into this single stochastic channel model}@}. <!--SR:!2026-10-31,287,330!2026-08-30,238,330!2026-08-24,232,330!2026-06-13,174,310!2026-11-07,293,330!2026-10-09,268,330!2026-10-12,271,330-->

{@{This simple binary channel model}@} underpins {@{many higher-level analyses in digital communications}@}: it provides the baseline against which {@{coding gains, diversity techniques, or more complex modulation schemes}@} are measured.  In practice, {@{real systems}@} often {@{deviate from this idealization}@} due to {@{multipath fading, colored noise, timing errors and non-binary signalling}@}; nevertheless, it remains {@{a cornerstone of communication theory}@}. <!--SR:!2026-10-07,268,330!2026-11-03,290,330!2026-10-15,274,330!2026-10-03,264,330!2026-10-09,270,330!2026-10-16,275,330!2026-10-05,266,330-->

Here, as {@{binary modulation is considered}@}, {@{a _bit_ and a _symbol_ refer to the same thing}@}, and are {@{used interchangeably}@} here. In {@{[_M_-ary transmission](M-ary%20transmission.md)}@}, {@{a _symbol_ can represent more than one _bit_}@}, and thus should be {@{distinguished in that context}@}. <!--SR:!2026-04-13,99,379!2026-04-11,97,379!2026-03-25,80,369!2026-03-31,86,369!2026-04-10,96,379!2026-04-09,95,377-->

## binary channel

{@{The transmitter}@} sends {@{a sequence of binary symbols $b_k \in \{0,1\}$}@}.  Each symbol occupies {@{a fixed duration $T$}@} and is represented by {@{a pulse waveform}@} {@{$$s(t) = \begin{cases} + A\,p(t), & b_k = 1\\[4pt] - A\,p(t), & b_k = 0 \end{cases}\qquad 0 \le t < T,$$}@} where <!--SR:!2026-08-31,239,330!2026-09-23,254,330!2026-06-10,171,310!2026-10-21,279,330!2026-10-11,270,330-->

- $A>0$ ::@:: is the pulse amplitude, and <!--SR:!2026-10-19,277,330!2026-11-08,294,330-->
- $p(t)$ ::@:: is a _shaping pulse_ of unit energy (e.g., a rectangular or raised-cosine pulse). Here, we assume it is simply a unit rectangular pulse lasting for symbol time $T$. <!--SR:!2026-10-11,270,330!2026-09-25,256,330-->

{@{The transmitted signal}@} is thus {@{$$s_k(t) = \pm A\,p(t), \qquad 0 \le t < T \,.$$}@} {@{The channel}@} adds {@{white Gaussian noise $n(t)$ to the waveform}@}, yielding {@{the received continuous-time signal}@} {@{$$r(t)= s_k(t)+ n(t).   \tag{1}$$}@} Here {@{$n(t)$}@} is {@{a __Gaussian process__ \(_white_ noise\) with zero mean}@} and {@{autocorrelation function}@} {@{$$R_n(t_1, t_2)=E\{n(t_1) n^*(t_2)\}= \frac{N_0}{2}\,\delta(t_1 - t_2),$$}@} where {@{$N_0/2$}@} is {@{the two-sided power spectral density of the noise}@}. \(annotation: If {@{one-sided \(no negative frequencies\)}@}, then {@{the power spectral density is $N_0$}@}. This explains {@{the division by 2}@}.\) <!--SR:!2026-09-10,241,330!2026-08-30,238,330!2026-09-21,252,330!2026-10-14,273,330!2026-09-16,247,330!2026-10-08,269,330!2026-08-29,237,330!2026-10-22,280,330!2026-11-04,290,330!2026-09-20,251,330!2026-09-21,252,330!2026-10-11,272,330!2026-03-03,92,362!2026-03-03,92,362!2026-03-03,92,362-->

## receiver

{@{The receiver}@} performs {@{a _matched filter_}@} (or equivalently, {@{an integral over one symbol period}@}) to {@{maximise the signal-to-noise ratio}@}: {@{$$V = \int_{0}^{T} r(t)\,dt.$$}@} <!--SR:!2026-10-07,268,330!2026-09-01,240,330!2026-10-15,274,330!2026-10-02,263,330!2026-10-23,281,330-->

Because {@{$s_k(t)=\pm A\,p(t)$}@} and {@{$p(t)$}@} is {@{assumed to be a rectangular pulse of duration $T$}@}, {@{the deterministic part of $V$}@} equals {@{$$E[V|b_k=1]   = +AT,\qquad  E[V|b_k=0]   = -AT.$$}@} <!--SR:!2026-11-01,288,330!2026-10-30,286,330!2026-10-06,267,330!2026-09-24,255,330!2026-09-10,241,330-->

{@{The random part}@} comes from {@{integrating the noise \(of two-sided power spectral density $N_0 / 2$\)}@}: {@{$$N_T=\int_{0}^{T} n(t)\,dt,$$}@} which is {@{a Gaussian random variable}@} with mean {@{$$E[N_T]=\int_{0}^{T} E[n(t)]\,dt = \int_{0}^{T} 0\,dt = 0,$$}@} and variance {@{$$\sigma^2_{n_T}=E[(N_T)^2]-[E(N_T)]^2 = \int_{0}^{T}\!\!\int_{0}^{T} E[n(t)n(s)]\,dt\,ds = \int_{0}^{T}\!\!\int_{0}^{T}\frac{N_0}{2}\,\delta(t-s)\,dt\,ds = \frac{N_0}{2}\,T.$$}@} Thus we obtain {@{$$E[N_T]=0,\qquad \sigma^2_{n_T}= \frac{N_0}{2}\,T.$$}@} <!--SR:!2026-09-02,241,330!2026-10-02,263,330!2026-10-10,271,330!2026-10-27,284,330!2026-10-16,275,330!2026-10-09,268,330!2026-05-28,160,310-->

Thus {@{the decision statistic $V$}@} conditioned on {@{the transmitted bit}@} is {@{Gaussian}@}: {@{$$V|b_k=1 \sim \mathcal N(AT,\,\sigma^2_{n_T}),\qquad V|b_k=0 \sim \mathcal N(-AT,\,\sigma^2_{n_T}).$$}@} The receiver makes {@{a hard decision}@} by comparing $V$ with {@{a threshold $V_{\!th}$}@}: {@{$$\hat b_k = \begin{cases} 1,& V>0\\[4pt] 0,& V\le0 \end{cases}.$$}@} If {@{the threshold is zero}@}, because {@{the two symbols are mirror images about zero}@}, {@{a noise sample that flips the sign of $V$}@} causes {@{an error}@}. The threshold can also {@{be nonzero}@}. <!--SR:!2026-10-07,268,330!2026-10-09,268,330!2026-10-23,281,330!2026-11-06,292,330!2026-09-18,249,330!2026-08-21,229,330!2026-09-13,244,330!2026-09-22,253,330!2026-09-28,259,330!2026-10-10,269,330!2026-10-09,270,330!2026-10-12,271,330-->

## bit error rate

In {@{digital communications}@} {@{the _bit error rate_ (BER)}@} is {@{one of the most common metrics}@} used to quantify {@{how reliably a transmitter–receiver pair can convey data over a noisy medium}@}. <!--SR:!2026-08-24,232,330!2026-09-18,249,330!2026-10-27,284,330!2026-10-03,264,330-->

The following section develops {@{the BER expression for this simple model}@} from {@{first principles}@}, detailing the assumptions made about {@{the transmitted waveform, the receiver structure, the noise statistics}@} and finally the mathematical derivation of {@{the BER in terms of the Q-function and signal energy}@}. <!--SR:!2026-11-08,294,330!2026-11-02,289,330!2026-09-22,253,330!2026-08-25,233,330-->

Let {@{the prior probabilities}@} be {@{$$P(b_k=0)=p_0,\qquad P(b_k=1)=p_1,$$}@} with {@{$p_0+p_1=1$}@}. {@{The a-priori bit error probabilities}@} are {@{no longer equal}@}; they become {@{$$P_e^{(0)} = P(\hat b_k \neq 0 | b_k=0),\qquad P_e^{(1)} = P(\hat b_k \neq 1 | b_k=1).$$}@} {@{The overall BER}@} is {@{the weighted sum}@} {@{$$P_e = p_0\,P_e^{(0)} + p_1\,P_e^{(1)} \,.$$}@} <!--SR:!2026-10-08,269,330!2026-09-19,250,330!2026-09-29,260,330!2026-10-10,271,330!2026-09-11,242,330!2026-10-16,275,330!2026-09-17,248,330!2026-10-07,268,330!2026-10-20,278,330-->

Below, we split {@{the analysis into two conditioned cases}@}. Then we avoid dealing with {@{mixed-distribution integrals that are analytically intractable}@}. <!--SR:!2026-08-27,235,330!2026-09-03,234,330-->

### bit error rate with zero threshold

With {@{the hard decision rule}@} {@{$$\displaystyle \hat b_k=\begin{cases}1,& V>0\\[4pt]0,&V\le0\end{cases} \,,$$}@} {@{an error occurs}@} when $V$ has {@{opposite sign to the transmitted symbol}@}. <!--SR:!2026-11-05,291,330!2026-08-20,228,330!2026-09-23,254,330!2026-09-14,245,330-->

For {@{a transmitted '1'}@}: {@{$$P_e^{(1)} = P(N_T < -AT) = Q\!\left(\frac{AT}{\sigma_{n_T} }\right),$$}@} where {@{$\sigma_{n_T}=\sqrt{\tfrac{N_0}{2}\,T}$}@}. Thus {@{$$P_e^{(1)} = Q\!\left(\sqrt{\frac{2A^2T}{N_0} }\right) = Q\!\left(\sqrt{\frac{2E_b}{N_0} }\right) \,,$$}@} where {@{$E_b = A^2 T$}@} is {@{the signal \(excluding the noise\) power \(see below\)}@}. <!--SR:!2026-10-17,276,330!2026-10-28,285,330!2026-06-26,173,310!2026-10-11,272,330!2026-11-05,291,330!2026-09-20,251,330-->

For {@{a transmitted '0'}@}: {@{$$P_e^{(0)} = P(N_T > AT) = Q\!\left(\frac{AT}{\sigma_{n_T} }\right) = Q\!\left(\sqrt{\frac{2A^2T}{N_0} }\right) = Q\left(\sqrt {\frac{2E_b} {N_0} } \right) \,,$$}@} where {@{$E_b = A^2 T$}@} is {@{the signal \(excluding the noise\) power \(see below\)}@}. <!--SR:!2026-10-05,266,330!2026-05-28,160,310!2026-11-02,289,330!2026-11-04,290,330-->

Hence {@{the BER with arbitrary priors}@} is {@{$$\boxed{P_e = p_0\,Q\!\left(\sqrt{\tfrac{2E_b}{N_0} }\right)+p_1\,Q\!\left(\sqrt{\tfrac{2E_b}{N_0} }\right) = Q\!\left(\sqrt{\tfrac{2E_b}{N_0} }\right)} \,.$$}@} Notice how {@{the input bit probabilities}@} do not {@{affect the BER}@}. <!--SR:!2026-10-07,268,330!2026-07-03,180,310!2026-10-14,273,330!2026-10-09,270,330-->

### bit error rate with arbitrary threshold

If the receiver compares {@{$V$ to an arbitrary threshold $V_{\!th}\neq0$}@}, {@{the decision rule}@} becomes   {@{$$\displaystyle \hat b_k=\begin{cases}1,& V>V_{\!th}\\[4pt]0,&V\le V_{\!th}\end{cases} \,.$$}@} <!--SR:!2026-08-31,239,330!2026-08-26,234,330!2026-10-11,270,330-->

For {@{a transmitted '1'}@} {@{the error event}@} is {@{$\{AT+N_T=V\le V_{\!th}\}$}@}, giving {@{$$P_e^{(1)} = Q\!\left(\frac{AT-V_{\!th} }{\sigma_{n_T} }\right).$$}@} <!--SR:!2026-10-09,268,330!2026-11-04,290,330!2026-10-22,280,330!2026-10-01,262,330-->

For {@{a transmitted '0'}@} {@{the error event}@} is {@{$\{-AT+N_T=V>V_{\!th}\}$}@}, giving
$$P_e^{(0)} = Q\!\left(\frac{AT+V_{\!th} }{\sigma_{n_T} }\right).$$ <!--SR:!2026-09-08,239,330!2026-10-10,271,330!2026-10-16,275,330-->

{@{The overall BER with priors $p_0,p_1$}@} is therefore {@{$$\boxed{P_e(V_{\!th})= p_0\,Q\!\left(\frac{AT+V_{\!th} }{\sigma_{n_T} }\right) +p_1\,Q\!\left(\frac{AT-V_{\!th} }{\sigma_{n_T} }\right)} \,.$$}@} {@{Setting $V_{\!th}=0$}@} recovers {@{the zero-threshold result above}@}. <!--SR:!2026-10-10,271,330!2026-05-27,159,310!2026-09-12,243,330!2026-10-02,263,330-->

### bit error rate insight

{@{The variance term $\sigma^2_{n_T}$}@} governs how much {@{the decision statistic $V$ can deviate from its mean value $\pm AT$}@} before {@{causing a bit error}@}. <!--SR:!2026-10-09,270,330!2026-10-27,284,330!2026-10-09,268,330-->

In {@{a binary antipodal system}@}, {@{the probability of error}@} is governed by {@{the tail probability of a Gaussian distribution}@}; hence it {@{decays exponentially with the ratio}@} {@{$\frac{A^2T}{N_0} = \frac{E_b}{N_0}$}@}. \(The ratio comes from {@{the approximation for large $x$ below}@}.\) Because {@{the noise variance}@} scales {@{linearly with the symbol duration $T$}@}, {@{longer symbols}@} provide {@{more averaging and reduce $\sigma^2_{n_T}$}@}, improving {@{reliability}@}. <!--SR:!2026-08-22,230,330!2026-10-07,268,330!2026-08-19,227,330!2026-05-27,159,310!2026-06-20,167,310!2026-10-01,262,330!2026-10-16,275,330!2026-09-14,245,330!2026-09-01,240,330!2026-10-10,269,330!2026-03-03,92,362-->

Mathematically, for {@{a fixed energy per bit $E_b$}@}, {@{the error probability}@} is {@{$$P_e = Q\!\left(\sqrt{\frac{2E_b}{N_0} }\right) \;\approx\; \sqrt{\frac {N_0} {2E_b} } \frac{1}{\sqrt{2 \pi } }\exp\!\left(-\frac{E_b}{N_0}\right)\quad (E_b/N_0 \gg 1) \,,$$}@} using the approximation: {@{$$Q(x) \approx \frac {\phi(x)} x = \frac 1 {x \sqrt{2\pi} } e^{-x^2 / 2} \qquad x > 0 \,.$$}@} This approximation is {@{asymptotically exact}@} as {@{$x \to \infty$}@}. Thus, improving {@{the signal-to-noise ratio}@} by {@{increasing transmit power or reducing noise spectral density}@} directly {@{translates into a steep reduction in BER}@}. <!--SR:!2026-09-13,244,330!2026-05-28,160,310!2026-04-11,121,290!2026-10-30,286,330!2026-09-30,261,330!2026-10-09,268,330!2026-10-28,285,330!2026-10-27,284,330!2026-09-24,255,330-->

## signal energy

{@{The _signal energy_ \(excluding noise energy\) transmitted during one symbol}@} is {@{$$E_s = \int_{0}^{T} s_k^2(t)\,dt = A^2 \!\int_{0}^{T} p^2(t)\,dt = A^2 T \,,$$}@} since {@{$p(t)$}@} has {@{unit energy \(assumed to be a rectangular pulse of duration $T$\)}@}. <!--SR:!2026-10-02,263,330!2026-10-28,285,330!2026-09-15,246,330!2026-09-22,253,330-->

With {@{non-equiprobable bits}@} {@{the average energy per bit}@} is {@{$$E_b = p_0\,E_s^{(0)} + p_1\,E_s^{(1)} = (p_0+p_1) A^2 T = A^2 T,$$}@} because {@{both symbols}@} have {@{the same magnitude $A$ and duration $T$}@}. Thus, regardless of {@{the bit priors}@}, {@{$$E_b = A^2 T.$$}@} Note if {@{the two antipodal signals}@} had {@{different amplitudes}@}, the expression would {@{involve $p_0$ and $p_1$ explicitly}@}. <!--SR:!2026-10-04,265,330!2026-09-11,242,330!2026-10-08,269,330!2026-10-31,287,330!2026-10-02,263,330!2026-09-08,239,330!2026-10-04,265,330!2026-08-23,231,330!2026-09-07,238,330!2026-10-02,263,330-->

## signal-to-noise ratio

{@{The _signal-to-noise ratio_ (SNR) _per bit_}@} is defined as {@{$$\boxed{\text{bSNR}_{\text{lin} } := \frac{E_b}{N_0} } \,.$$}@} Note {@{the noise power spectral density uses one-sided power $N_0$}@} rather than {@{two-sided power $N_0 / 2$}@}. Thus, {@{the BER for _zero threshold_ can be expressed compactly}@} as {@{$$\boxed{\text{BER}=Q\!\left(\sqrt{\dfrac{2E_b}{N_0} }\right) = Q\!\left(\sqrt{2 \cdot \text{bSNR}_{\text{lin} } } \right) \qquad V_{\!th} = 0} \,.$$}@} It is {@{a classic result}@} for {@{binary antipodal signaling over an AWGN channel}@}. <!--SR:!2026-10-13,272,330!2026-11-03,290,330!2026-09-15,246,330!2026-11-03,290,330!2026-10-15,274,330!2026-10-04,265,330!2026-06-06,128,395!2026-05-29,121,395-->

## using Q-function

{@{The _Q-function_}@} is {@{the tail probability of a standard normal random variable}@}: {@{$$Q(x)\;=\;\Pr\{Z > x\}\quad \text{with } Z\sim\mathcal N(0,1) \;=\;\frac{1}{\sqrt{2\pi} }\int_{x}^{\infty} e^{-t^{2}/2}\,\mathrm dt \,.$$}@} It may be calculated by {@{using well-tabulated Q-functions}@} apart from {@{numerical integration or simulation}@}. However, most textbooks provide {@{a Q-table up to $x \approx 3$}@}; beyond that, {@{entries become negligible}@}. <!--SR:!2026-10-19,277,330!2026-09-09,240,330!2026-08-23,231,330!2026-10-11,272,330!2026-10-06,267,330!2026-09-09,240,330!2026-09-02,241,330-->

For {@{large positive arguments}@}, $Q(x)$ can be approximated by {@{$$Q(x)\approx \frac {\phi(x)} x = \frac{1}{\sqrt{2\pi}\,x}e^{-x^{2}/2} \,,$$}@} which is {@{asymptotically exact as $x\to\infty$}@}. {@{This approximation}@} simplifies {@{analytical work when deriving closed-form BER expressions}@}. It shows the function is {@{a strictly decreasing function}@}: {@{larger thresholds}@} lead to {@{smaller tail probabilities}@}; it also {@{rapid decays}@} for {@{large $x$}@}. <!--SR:!2026-08-25,233,330!2026-10-28,285,330!2026-10-14,273,330!2026-10-27,284,330!2026-11-01,288,330!2026-10-28,285,330!2026-10-23,281,330!2026-10-21,279,330!2026-10-11,272,330!2026-10-28,285,330-->

## using error function

{@{The Q-function}@} can be written in terms of {@{the _complementary_ error function $\operatorname{erfc}$}@}: {@{$$Q(x)=\frac12\,\operatorname{erfc}\!\left(\frac{x}{\sqrt{2} }\right) =\frac12-\frac12\,\operatorname{erf}\!\left(\frac{x}{\sqrt{2} }\right),$$}@} where {@{the ordinary error function}@} is {@{$$\operatorname{erf}(z)= \frac{2}{\sqrt{\pi} }\int_{0}^{z} e^{-t^2}\,dt.$$}@} <!--SR:!2026-09-19,250,330!2026-11-07,293,330!2026-10-30,286,330!2026-08-28,236,330!2026-06-19,166,290-->

{@{These identities are useful}@} when {@{numerical tables or software libraries}@} provide {@{$\operatorname{erfc}$ rather than $Q$}@}. For example, the equation for {@{the BER using zero threshold}@}: {@{$$\boxed{\text{BER}= \frac12\,\operatorname{erfc}\!\left(\sqrt{\dfrac{E_b}{N_0} }\right)} \,,$$}@} and the equation for {@{the BER using arbitrary threshold}@}: {@{$$\boxed{\text{BER}(V_{\!th})= p_{0}\,\tfrac12\,\operatorname{erfc}\!\left(\dfrac{AT+V_{\!th} } {\sqrt{2\,\sigma^2_{n_T} } }\right) +p_{1}\,\tfrac12\,\operatorname{erfc}\!\left(\dfrac{AT-V_{\!th} } {\sqrt{2\,\sigma^2_{n_T} } }\right)} \,.$$}@} <!--SR:!2026-10-20,278,330!2026-11-06,292,330!2026-11-05,291,330!2026-09-21,252,330!2026-07-02,179,310!2026-10-27,284,330!2026-04-07,118,290-->

## optimization

For {@{a given modulation and noise model}@}, BER depends on the {@{receiver front–end filter $h(t)$ and on the decision threshold $\gamma$}@}. {@{Minimizing $P_{\text{e} }$}@} can be {@{carried out in two stages}@}: {@{_fixed-filter optimization_}@} – for {@{a predetermined impulse response $h(t)$}@}, determine {@{the threshold $\gamma$ that gives the smallest BER}@}; and {@{_filter design optimization_}@} – search over {@{all admissible filters $h(t)$ \(an infinite–dimensional space\)}@} to find {@{the receiver that achieves the lowest possible BER}@}. Note here we are only considering {@{receivers that can be modeled by a LTI system}@}; there can be {@{non-linear receivers that achieve even lower BER}@}, showing that {@{the chosen receiver architecture}@} also {@{limits BER}@}. <!--SR:!2026-03-30,85,369!2026-04-05,91,379!2026-03-26,81,369!2026-04-01,87,369!2026-03-31,86,379!2026-04-03,89,369!2026-04-06,92,379!2026-03-24,79,369!2026-03-29,84,369!2026-03-29,84,369!2026-04-10,96,379!2026-03-30,85,369!2026-03-27,82,369!2026-03-31,86,379-->

Assuming {@{both bits are equiprobable}@} and {@{the optimal threshold is chosen \(middle\)}@}, we are also interested in {@{minimizing the energy of each bit}@} while {@{maintaining the error rate}@}. <!--SR:!2026-03-09,64,359!2026-03-31,86,377!2026-03-30,85,369!2026-03-30,85,369-->

### bit error rate optimization

As we have seen above, {@{the overall BER with priors $p_0,p_1$}@} is {@{$$\boxed{P_e(V_{\!th})= p_0\,Q\!\left(\frac{V_{\!th} - s_{o0} }{\sigma_{n_T} }\right) +p_1\,Q\!\left(\frac{s_{o1} -V_{\!th} }{\sigma_{n_T} }\right)} \,,$$}@} where {@{$s_{o0}, s_{o1}$}@} are {@{the convolution of the filter $h(t)$ with the received signal $s_0(t), s_1(t)$ \(assuming zero mean noise\) after time $T$}@}. For {@{a unit pulse signal and an integrator}@}, as {@{the filter function $h(t)$ for the integrator}@} is effectively {@{$h(t) = [0 \le t \le T]$}@}, so {@{$s_{o0} = -AT, s_{o1} = AT$ in the above special case}@}. To {@{minimize the above expression}@}, {@{differentiate with respect to $V_{th}$ and set to zero}@}: {@{$$\begin{aligned} 0 & = \frac 1 {\sigma_{n_T} } \left(-p_0 \phi\!\left(\frac{V_{\!th} - s_{o0} }{\sigma_{n_T} }\right) + p_1 \phi\!\left(\frac{s_{o1} -V_{\!th} }{\sigma_{n_T} }\right) \right) \\ \frac {p_0} {p_1} & = \frac {\phi\!\left(\frac{s_{o1} - V_{\!th} }{\sigma_{n_T} }\right)} {\phi\!\left(\frac{V_{\!th} - s_{o0} }{\sigma_{n_T} }\right)} \\ & = \exp\left(\frac 1 {2 \sigma_{n_T}^2 } \left((V_{th} - s_{o0})^2 - (s_{o1} - V_{th})^2 \right)\right) \\ \ln \frac {p_0} {p_1} & = \frac 1 {2 \sigma_{n_T}^2 } (s_{o1} - s_{o0})(2V_{th} - (s_{o0} + s_{o1})) \\ V_{th} & = \frac {\sigma_{n_T}^2} {s_{o1} - s_{o0} } \ln \frac {p_0} {p_1} + \frac {s_{o0} + s_{o1} } 2 \,. \end{aligned}$$}@} So {@{the optimal threshold is}@}: {@{$$\boxed{V_{th} = \frac {\sigma_{n_T}^2} {s_{o1} - s_{o0} } \ln \frac {p_0} {p_1} + \frac {s_{o0} + s_{o1} } 2 } \,.$$}@} When {@{both bits are equiprobable}@}, then {@{the first term vanishes}@}, and {@{$V_{th} = \frac {s_{o0} + s_{o1} } 2$}@} is {@{halfway between $s_{o0}$ and $s_{o1}$}@}, and {@{the overall BER with priors $p_0 = p_1 = 0.5$}@} is {@{$$\boxed{P_e(V_{\!th})= Q\!\left(\frac{s_{o1} - s_{o0} }{2\sigma_{n_T} }\right) = Q\!\left(\sqrt{\frac{(s_{o1} - s_{o0})^2 }{4\sigma_{n_T}^2 } }\right)} \,.$$}@} <!--SR:!2026-03-27,82,369!2026-04-06,92,379!2026-04-01,87,379!2026-03-26,81,369!2026-04-13,99,379!2026-03-26,81,369!2026-04-15,101,377!2026-04-02,88,369!2026-04-05,91,379!2026-03-28,83,369!2026-03-24,79,369!2026-04-04,90,377!2026-03-25,80,369!2026-04-05,91,379!2026-04-09,95,377!2026-03-28,83,369!2026-03-24,79,369!2026-03-25,80,369!2026-03-29,84,369-->

### filter optimization

Next is to {@{find the most optimal filter $h(t)$}@}. Assume {@{both bits are _equiprobable_}@}, otherwise {@{more complex mathematics are needed}@}. From {@{the overall BER formula for equiprobable bits}@}, we see {@{maximizing $\left\lvert \frac {s_{o0} - s_{o1} } {\sigma_{n_T} } \right\rvert$}@} or equivalently {@{$\rho := \frac {(s_{o0} - s_{o 1})^2} {\sigma_{n_T}^2}$}@} can {@{minimize $P_e$}@}. <!--SR:!2026-04-06,92,379!2026-03-24,79,369!2026-04-12,98,377!2026-03-24,79,369!2026-03-31,86,379!2026-04-06,92,379!2026-03-25,80,369-->

First, we generalize {@{$\sigma_{n_T}^2$ to arbitrary filter $h(t)$ and _WSS process_ \(not necessarily _white noise_!\) with power spectrum $S_{xx}(f)$}@}: {@{$$\begin{aligned} \sigma_{n_T}^2 & = \operatorname E\left[\left(\int_{-\infty}^\infty \! h(T - \tau) n(\tau) \,\mathrm d\tau\right)^2 \right] - \operatorname E\left[\int_{-\infty}^\infty \! h(T - \tau) n(\tau) \,\mathrm d\tau \right] \\ & = \operatorname{E} \left[\int_{\mathbb R^2} \! h(T - \tau_1) h(T - \tau_2) n(\tau_1) n(\tau_2) \,\mathrm d(\tau_1, \tau_2) \right] \\ & = \int_{\mathbb R^2} \! h(T - \tau_1) h(T - \tau_2) \operatorname E[n(\tau_1) n(\tau_2)] \,\mathrm d(\tau_1, \tau_2) \\ & = \int_{\mathbb R^2} \! h(T - \tau_1) h(T - \tau_2) r_{xx}(\tau_1 - \tau_2) \,\mathrm d(\tau_1, \tau_2) \,, \end{aligned}$$}@} which is {@{very ugly}@}. But we can make it {@{nicer using Fourier transform}@} and that {@{the Fourier transform of the \(WSS\) autocorrelation function $r_{xx}(t)$ is the power spectral density $S_{xx}(f)$}@}: {@{$$\begin{aligned} \sigma_{n_T}^2 & = \int_{\mathbb R^2} \! h(T - \tau_1) h(T - \tau_2) r_{xx}(\tau_1 - \tau_2) \,\mathrm d(\tau_1, \tau_2) \\ & = \int_{\mathbb R} h(\tau_1) \int_{\mathbb R} h(\tau_1 - \tau_2) r_{xx}(\tau_2) \,\mathrm d\tau_2 \,\mathrm d\tau_1 \\ & = \int_{\mathbb R} h(\tau_1) \int_{\mathbb R} H(f) S_{xx}(f) e^{j 2\pi f \tau_1} \,\mathrm df \,\mathrm d\tau_1 \\ & = \int_{\mathbb R} H(f) S_{xx}(f) \int_{\mathbb R} h(\tau_1) e^{j 2\pi f \tau_1} \,\mathrm d\tau_1 \,\mathrm df \\ & = \int_{\mathbb R} H(f) S_{xx}(f) \overline{H(f)} \,\mathrm df \\ & = \int_{\mathbb R} \lvert H(f) \rvert^2 S_{xx}(f) \,\mathrm df \,. \end{aligned}$$}@} Note we assumed that {@{$h(t)$ is a real function \(thus $\overline{h(t)} = h(t)$\)}@} when {@{manipulating the expression}@}. Thus, we have the following expression for {@{$\sigma_{n_T}^2$ for filter $h(t)$ and noise power spectral density $S_{xx}(f)$}@}: {@{$$\boxed{\sigma_{n_T}^2 = \int_{-\infty}^\infty \lvert H(f) \rvert^2 S_{xx}(f) \,\mathrm df} \,.$$}@} <!--SR:!2026-04-11,97,379!2026-04-10,96,377!2026-03-28,83,369!2026-02-16,59,349!2026-04-11,97,377!2026-03-06,61,359!2026-04-09,95,377!2026-03-26,81,369!2026-04-03,89,369!2026-03-27,82,369-->

Now, also {@{generalizing the numerator}@} to {@{accommodate for arbitrary $h(t)$}@}, we have {@{$$\begin{aligned} \rho = \frac {(s_{o0} - s_{o1})^2} {\sigma_{n_T}^2} & = \frac {\left(\int_{-\infty}^\infty h(T - \tau) s_1(\tau) \,\mathrm d\tau - \int_{-\infty}^\infty h(T - \tau) s_0(\tau) \,\mathrm d\tau \right)^2} {\sigma_{n_T}^2} \\ & = \frac {\left(\int_{-\infty}^\infty h(T - \tau) (s_1(\tau) - s_0(\tau)) \,\mathrm d\tau \right)^2} {\sigma_{n_T}^2} \,. \end{aligned}$$}@} Defining {@{$g(t) := s_1(t) - s_0(t)$}@}, we want to minimize {@{$$\frac {((h * g)(T))^2} {\sigma_{n_T}^2} \,.$$}@} Apply {@{the Fourier transform}@}, we have {@{$$\frac {((h * g)(T))^2} {\int_{\mathbb R} \lvert H(f) \rvert^2 S_{xx}(f) \,\mathrm df} = \frac {(\int_{\mathbb R} H(f)G(f) e^{j2\pi f T} \,\mathrm df)^2 } {\int_{\mathbb R} \lvert H(f) \rvert^2 S_{xx}(f) \,\mathrm df } \,.$$}@} Applying {@{the Cauchy–Schwarz inequality $(\mathbf x \cdot \mathbf y)^2 \le \lVert \mathbf x \rVert_2^2 \lVert \mathbf y \rVert_2^2$}@} \(treating {@{functions as infinite-dimensional vectors}@}\), we obtain {@{an upper bound for $\rho$}@}: {@{$$\begin{aligned} \rho = \frac {(s_{o0} - s_{o1})^2} {\sigma_{n_T}^2} & = \frac {\left(\int_{\mathbb R} H(f)G(f) e^{j2\pi f T} \,\mathrm df\right)^2 } {\int_{\mathbb R} \lvert H(f) \rvert^2 S_{xx}(f) \,\mathrm df } \\ & = \frac {\left(\int_{\mathbb R} \left ({H(f)} \sqrt{S_{xx}(f)} \right) \left( G(f) e^{j2\pi f T} / \sqrt{S_{xx}(f)} \right) \,\mathrm df\right)^2 } {\int_{\mathbb R} \lvert H(f) \rvert^2 S_{xx}(f) \,\mathrm df } \\ & \le \frac {\left(\int_{\mathbb R} \lvert H(f) \rvert^2 S_{xx}(f) \,\mathrm df \right) \left(\int_{\mathbb R} \lvert G(f)\rvert^2 / S_{xx}(f) \,\mathrm df \right) } {\int_{\mathbb R} \lvert H(f) \rvert^2 S_{xx}(f) \,\mathrm df } \\ & = \int_{\mathbb R} \frac {\lvert G(f) \rvert^2} {S_{xx}(f)} \,\mathrm df \,. \end{aligned}$$}@} Remember to {@{take the conjugate when writing $\lVert \mathbf x \rVert^2$ and $\lVert \mathbf y \rVert^2$}@}. <!--SR:!2026-03-27,82,369!2026-03-27,82,369!2026-04-11,97,377!2026-03-26,81,369!2026-04-05,91,379!2026-04-11,97,377!2026-02-25,67,349!2026-04-05,91,379!2026-03-26,81,369!2026-03-06,48,297!2026-02-25,39,297!2026-03-04,47,309-->

Thus, {@{the upper bound for $\rho = \frac {(s_{o0} - s_{o1})^2} {\sigma_{n_T}^2}$}@} is {@{$$\boxed{\rho \le \int_{\mathbb R} \frac {\lvert G(f) \rvert^2} {S_{xx}(f)} \,\mathrm df } \,.$$}@} We see it is limited by {@{the _maximum signal-to-noise ratio_ \(SNR\)}@}. Further, {@{the Cauchy–Schwarz inequality}@} tells us {@{_equality_ with the upper bound is achieved}@} {@{if and only if $$H_{\text{opt} }(f) \sqrt{S_{xx}(f)} = C \overline{\frac {G(f) e^{j 2\pi fT} } { \sqrt{S_{xx}(f)} } } \implies \boxed{H_{\text{opt} }(f) = C \frac {\overline {G(f) e^{j 2\pi f T} } } {S_{xx}(f)} = C \frac {G^*(f) e^{-j 2\pi f T} } {S_{xx}(f)} }$$}@} for {@{some arbitrary complex number $C$}@}. Intuitively, we see {@{the optimal filter}@} weights {@{each frequency component proportionally with the signal difference}@} and {@{inversely with the noise PSD \("whitening the noise"\)}@}. Note we have only assumed the noise is {@{a WSS process that _may or may not_ be white noise}@}. Also the process is {@{assumed _Gaussian_ as we are using the Q-function}@}. <!--SR:!2026-04-12,98,379!2026-03-26,81,369!2026-03-28,83,369!2026-04-05,91,377!2026-03-31,86,379!2026-03-30,85,377!2026-04-09,95,377!2026-03-31,86,377!2026-04-05,91,379!2026-03-28,83,369!2026-04-13,99,379!2026-04-09,95,377-->

In the _special case_ that {@{the noise is a white noise}@}, then {@{$S_{xx}(f) = N_0 / 2$ is a _constant_}@}. In that case, {@{the optimal filter}@} has the Fourier transform {@{$$H_{\text{opt} }(f) = C G^*(f) e^{-j 2\pi f T}$$}@} for {@{some arbitrary complex number $C$}@} which {@{absorbs the constant denominator}@}. Using {@{properties of the Fourier transform}@}, this means {@{the filter in the time domain}@} is {@{a _matched filter_}@}. That is, given {@{the input difference $g(t)$}@}, {@{the optimal filter}@} is {@{$$\boxed{h_{\text{opt} }(t) = \overline {g(-(t - T))} = g^*(T - t)} \,.$$}@} It is interpreted as {@{flipping $g(t)$ across $t = 0$, then shifting it to the right by $T$ \(note $t$ is now flipped\), and then have its conjugate taken}@}. Further, as {@{$\rho = \frac {(s_{o0} - s_{o1})^2} {\sigma_{n_T}^2} = \int_{\mathbb R} \frac {\lvert G(f) \rvert^2} {S_{xx}(f)} \,\mathrm df$}@} and noise has {@{constant (two-sided) power spectral density $S_{xx}(f) = N_0 / 2$}@}, so {@{$$\boxed{\rho = \frac {(s_{o0} - s_{o1})^2} {\sigma_{n_T}^2} = \frac {2E_g} {N_0} } \,.$$}@} <!--SR:!2026-04-12,98,377!2026-03-30,85,369!2026-04-03,89,369!2026-04-12,98,379!2026-04-09,95,377!2026-03-29,84,369!2026-04-15,101,379!2026-04-03,89,369!2026-04-04,90,377!2026-03-27,82,369!2026-04-11,97,379!2026-04-01,87,379!2026-03-28,83,369!2026-03-30,85,369!2026-03-26,81,369!2026-03-28,83,369-->

#### response of LTI system to WSS random signal

Given a {@{zero-mean WSS random signal with power spectrum $S_{xx}(f)$ \(not necessary Gaussian\)}@}, {@{the response of a LTI system characterized by the impulse function $h(t)$}@} is also {@{a zero-mean WSS random signal}@}. <!--SR:!2026-04-12,98,377!2026-03-28,83,369!2026-04-05,91,379-->

Above, when we generalize {@{$\sigma_{n_T}^2$ to arbitrary filter $h(t)$ and _WSS process_ \(not necessarily _white noise_!\) with power spectrum $S_{xx}(f)$}@}, we obtain somewhere in the middle {@{$$\begin{aligned} \sigma_{n_T}^2 & = \int_{\mathbb R} h(\tau_1) \int_{\mathbb R} h(\tau_1 - \tau_2) r_{xx}(\tau_2) \,\mathrm d\tau_2 \,\mathrm d\tau_1 \,, \end{aligned}$$}@} which is {@{very ugly}@}. There is a simpler way to {@{derive its Fourier transform}@} by noting there are {@{two convolutions happening here}@}: {@{$$\begin{aligned} \sigma_{n_T}^2 & = \int_{\mathbb R} h(\tau_1) \int_{\mathbb R} h(\tau_1 - \tau_2) r_{xx}(\tau_2) \,\mathrm d\tau_2 \,\mathrm d\tau_1 \\ & = \int_{\mathbb R} h(\tau_1) (h * r_{xx})(\tau_1) \,\mathrm d\tau_1 \\ & = ((h(-t)) * (h * r_{xx}))(0) \\ & = \int_{-\infty}^\infty H^*(f) H(f) S_{xx}(f) e^{j 2 \pi f 0} \,\mathrm df \\ & = \int_{-\infty}^\infty \lvert H(f) \rvert^2 S_{xx}(f) \,\mathrm df \,. \end{aligned}$$}@} Note we assumed that {@{$h(t)$ is a real function \(thus $\overline{h(t)} = h(t)$\)}@} when {@{manipulating the expression}@}. Thus, we have the following expression for {@{$\sigma_{n_T}^2$ for filter $h(t)$ and noise power spectral density $S_{xx}(f)$}@}: {@{$$\boxed{\sigma_{n_T}^2 = \int_{-\infty}^\infty \lvert H(f) \rvert^2 S_{xx}(f) \,\mathrm df} \,.$$}@} <!--SR:!2026-03-05,60,359!2026-03-27,82,369!2026-03-27,82,369!2026-04-11,97,377!2026-04-09,95,377!2026-02-17,60,349!2026-04-01,87,369!2026-03-24,79,369!2026-04-09,95,377!2026-04-11,97,379-->

Above, we have also _neglected_ to prove that {@{the response mean of a LTI system to a zero-mean WSS random signal}@} is {@{also zero}@}. We will prove it here quickly. Since {@{a WSS has a constant mean $m$}@}, so {@{the mean of LTI response}@} is simply {@{$$m \int_{-\infty}^\infty h(t) \,\mathrm dt = m H(0) \,.$$}@} We see if {@{$m = 0$}@} then {@{the mean of LTI response is also zero}@}. <!--SR:!2026-04-06,92,379!2026-04-02,88,369!2026-04-02,88,369!2026-03-25,80,369!2026-04-09,95,377!2026-03-27,82,369!2026-03-30,85,369-->

### matched filter

As mentioned above, in the _special case_ that {@{both bits are equiprobable and the noise is a white noise}@}, given {@{the input difference $g(t)$}@}, {@{the optimal filter}@} is {@{$$\boxed{h_{\text{opt} }(t) = \overline {g(-(t - T))} = g^*(T - t)} \,.$$}@} It is interpreted as {@{flipping $g(t)$ across $t = 0$, then shifting it to the right by $T$ \(note $t$ is now flipped\), and then have its conjugate taken}@}. This is known as a {@{_matched filter_}@}. <!--SR:!2026-04-05,91,379!2026-04-06,92,379!2026-03-27,82,369!2026-03-25,80,369!2026-04-04,90,377!2026-03-27,82,369-->

The formulation above takes {@{$g(t) = s_1(t) - s_0(t)$}@} to {@{create a matched filter}@}. Sometimes, {@{$s_0(t), s_1(t)$ are easily available}@} while {@{$g(t)$ is not}@}. In said cases, we can use {@{linearity of convolution}@} as follows: create {@{a matched filter for each of $s_0(t), s_1(t)$}@}: {@{$$h_0(t) := s_0^*(T - t) \qquad h_1(t) := s_1^*(T - t) \,,$$}@} then {@{subtract the output from the filter for $s_1(t)$ by that from the filter for $s_0(t)$}@}. Mathematically, the result is {@{equivalent to having a single filter}@}. <!--SR:!2026-03-28,83,369!2026-03-29,84,369!2026-04-13,99,379!2026-04-04,90,377!2026-04-01,87,379!2026-04-12,98,379!2026-04-01,87,379!2026-07-27,180,349!2026-03-28,83,369-->

#### correlator

{@{Implementing a matched filter}@} requires {@{flipping, shifting, and conjugating the $s_0(t), s_1(t)$}@}, which may {@{be inconvenient}@}. This can be {@{simplified}@} by noting that {@{the convolution at time $T$ can be simplified}@}: {@{$$\begin{aligned} (h * y)(T) & = \int_{\mathbb R} s^*(T - (T - t)) y(t) \,\mathrm dt \\ & = \int_{\mathbb R} s^*(t) y(t) \,\mathrm dt \\ & = \int_0^T s^*(t) y(t) \,\mathrm dt \,, \end{aligned}$$}@} where the last equality is {@{due to the support of $s^*(t)$ being $[0, T]$}@}. The above is {@{exactly the _cross-correlation_ of $y$ and $s$ at $t = 0$}@}! Thus, we can instead use {@{a _correlator receiver_}@}, represented in a diagram as {@{passing through $\otimes$ \(with an additional input $s(t)$\) and then an integrator over time $[0, T]$}@}. The receiver simply {@{takes the _product_ of the two inputs from time $[0, T]$}@}: {@{$$(y \star s)(0) = \int_{-\infty}^\infty y(t) s^*(t) \,\mathrm dt = \int_0^T y(t) s^*(t) \,\mathrm dt \,.$$}@} And again, we can use {@{two correlators instead of one}@} in the exact way to {@{using two matched filters instead of one}@}. <!--SR:!2026-04-13,99,379!2026-03-26,81,369!2026-03-28,83,369!2026-04-01,87,379!2026-03-25,80,369!2026-04-12,98,379!2026-04-05,91,379!2026-03-28,83,369!2026-04-06,92,379!2026-04-13,99,379!2026-04-04,90,377!2026-04-02,88,369!2026-04-10,96,379!2026-03-27,82,369-->

### energy optimization

Assuming {@{the optimal threshold is set \(middle\)}@}, {@{the overall BER with priors $p_0 = p_1 = 0.5$}@} is {@{$$\boxed{P_e(V_{\!th})= Q\!\left(\frac{s_{o1} - s_{o0} }{2\sigma_{n_T} }\right) = Q\!\left(\sqrt{\frac{(s_{o1} - s_{o0})^2 }{4\sigma_{n_T}^2 } }\right)} \,.$$}@} We can identify {@{the _signal difference_ $g = s_{o1} - s_{o0}$}@}. Then we can define {@{the _energy per bit_ of the signal difference}@} as: {@{$$\begin{aligned} E_g & := \int_0^T (s_1(t) - s_0(t))^2 \,\mathrm dt \\ & = \int_0^T (s_1(t))^2 \,\mathrm dt + \int_0^T (s_0(t))^2 \,\mathrm dt - 2 \int_0^T s_0(t) s_1(t) \,\mathrm dt \\ & = E_0 + E_1 - 2 \sqrt{E_0 E_1} \rho_{01} \,, \end{aligned}$$}@} where {@{$\rho_{01} := \frac 1 {\sqrt{E_0 E_1} } \int_0^T s_0(t) s_1(t) \,\mathrm dt$}@} is {@{the _normalized_ cross-correlation of $s_0(t)$ and $s_1(t)$}@} \("normalized" means {@{must be between −1 and 1, inclusive}@}\). Thus, {@{the energy per bit of the signal difference}@} can be {@{written in terms of the bit energies}@} as {@{$$\boxed{E_g = E_0 + E_1 - 2 \sqrt{E_0 E_1} \rho_{01} } \,.$$}@} <!--SR:!2026-03-29,84,369!2026-04-12,98,379!2026-04-09,95,377!2026-04-10,96,377!2026-04-14,100,379!2026-03-27,82,369!2026-04-11,97,379!2026-03-26,81,369!2026-03-29,84,369!2026-04-06,92,379!2026-03-27,82,369!2026-04-10,96,379-->

If we further assume {@{the noise is _white_ Gaussian noise}@}, then {@{$\rho = \frac {(s_{o0} - s_{o1})^2} {\sigma_{n_T}^2} = \frac {2E_g} {N_0}$ applies}@}, thus the above expression can be {@{rewritten in terms of the _bit difference_ "virtual" energy $E_g$}@}: {@{$$\boxed{P_e(V_{\!th})= Q\!\left(\sqrt{\frac{(s_{o1} - s_{o0})^2 }{4\sigma_{n_T}^2 } }\right) = Q\!\left(\sqrt {\frac {E_g} {2 N_0} } \right)} \,.$$}@} Later in {@{[signal space](signal%20space.md)}@}, we will see {@{this "virtual" energy}@} is {@{the squared distance of the two signals in the signal space}@}. <!--SR:!2026-04-05,91,379!2026-02-25,67,349!2026-03-27,82,369!2026-04-09,95,377!2026-04-02,88,369!2026-04-05,91,379!2026-04-04,90,377-->

Continue to assume {@{the noise is _white_ Gaussian noise}@}. In practice, when we {@{send a signal}@}, {@{the actual energy _per bit_}@} used is {@{$E_0$ or $E_1$}@}, not {@{the "_virtual_" energy $E_g$}@}, which {@{only determines the error rate}@}. Thus, the objective is to {@{minimize the _average bit energy_ $E_b$ while maximizing the "virtual" energy $E_g$}@}. Assuming {@{the bits to have equal energies $E_0 = E_1$}@} \(thus {@{the _average bit energy_}@} is {@{$E_b = (E_0 + E_1) / 2 = E_0 = E_1$}@}\) To {@{maximize $E_g$}@}, we want {@{$\rho = -1$}@}. This is easily achieved by {@{setting $s_0(t) = -s_1(t)$}@}, yield the {@{best possible error rate under _same bit energy_}@}: {@{$$\boxed{E_g = 4E_b \implies P_e(V_{th}) = Q\left(\sqrt {\frac {2 E_b} {N_0} } \right)} \,,$$}@} where the last equality {@{assumes AWGN of two-sided PSD $N_0 / 2$}@}. The last expression is {@{exactly the previously obtained error expression in terms of bit energy $E_b$}@} for {@{binary _antipodal_ signaling over an AWGN channel}@}. Examples of {@{common modulation schemes}@} using this are {@{_antipodal signaling_  and _phase shift keying_ \(PSK\)}@}. <!--SR:!2026-04-06,92,379!2026-04-04,90,377!2026-04-03,89,369!2026-03-25,80,369!2026-03-27,82,369!2026-03-31,86,379!2026-03-26,81,369!2026-03-25,80,369!2026-03-31,86,369!2026-04-10,96,379!2026-04-11,97,379!2026-03-31,86,379!2026-03-26,81,369!2026-04-12,98,379!2026-04-06,92,379!2026-04-08,94,379!2026-03-08,63,359!2026-04-05,91,379!2026-04-09,95,377!2026-04-11,97,379-->

Continue to assume {@{the noise is _white_ Gaussian noise}@}. An alternative is assuming {@{the zero bit has zero energy $E_0 = 0$}@}. The advantage is {@{the zero bit does not take energy}@}, so {@{_average energy bit_ is $E_b = E_1 / 2$}@}. The disadvantage is that {@{the error rate is lower}@}: {@{$$\boxed{E_g = 2E_b \implies P_e(V_{th}) = Q\left(\sqrt {\frac {E_b} {N_0} } \right)} \,,$$}@} where the last equality {@{assumes AWGN of two-sided PSD $N_0 / 2$}@}. Examples of {@{common modulation schemes}@} using this are {@{_non-return to zero_ \(NRZ\) and _amplitude shift keying_ \(ASK\)}@}. The above equation also holds for {@{equal bit energies but the bit signals are _orthogonal_ \(thus $\rho = 0$\)}@}, and an example of {@{common modulations scheme}@} using this is {@{_frequency shift keying_ \(FSK\)}@}. <!--SR:!2026-03-28,83,369!2026-03-28,83,369!2026-04-01,87,377!2026-03-25,80,369!2026-03-29,84,369!2026-04-05,91,379!2026-04-09,95,377!2026-04-04,90,377!2026-03-27,82,369!2026-03-04,59,349!2026-03-24,79,369!2026-04-04,90,377-->

## modulation schemes

{@{_Signal modulation_}@} refers to {@{the controlled variation of one or more attributes of a periodic carrier waveform}@}—such as {@{amplitude, phase, or frequency}@}—in order to {@{embed information onto it for transmission}@}. Modulation transforms {@{a baseband data stream}@} into {@{a form suitable for a particular channel}@}, allowing efficient {@{use of bandwidth}@} and improved {@{resilience to noise}@}. {@{The basic principle}@} is that {@{the transmitted signal $s(t)$}@} carries {@{the data through its modulated parameters}@} while {@{the receiver}@} demodulates {@{these changes to recover the original bit pattern}@}. <!--SR:!2026-04-06,92,379!2026-03-26,81,369!2026-03-26,81,369!2026-03-26,81,369!2026-03-24,79,369!2026-04-12,98,379!2026-04-10,96,377!2026-03-29,84,369!2026-03-25,80,369!2026-04-04,90,377!2026-03-27,82,369!2026-04-05,91,379!2026-04-10,96,377-->

### bipolar non-return-to-zero

In {@{bipolar or antipodal NRZ \(__this course__: "_antipodal signaling_"\)}@} {@{the binary digits}@} are represented by {@{two opposite voltage levels}@}, typically {@{$+A$ and $-A$}@}, without {@{a neutral level between transitions}@}. {@{The transmitter}@} sends {@{one of these fixed amplitudes}@} for {@{each bit interval $T$}@}, returning {@{immediately to the next level}@} at {@{the end of the symbol period}@}. {@{A convenient mathematical description}@} is  {@{$$s_0(t)= -A,\qquad s_1(t)=+A \qquad (t\in[0,T]) \,.$$}@} <!--SR:!2026-04-06,92,379!2026-04-10,96,377!2026-03-29,84,369!2026-03-24,79,369!2026-04-04,90,377!2026-04-06,92,379!2026-03-28,83,369!2026-03-28,83,369!2026-03-28,83,369!2026-04-06,92,379!2026-04-13,99,379!2026-03-26,81,369-->

### unipolar non-return-to-zero

{@{Unipolar NRZ \(__this course__: "_non-return to zero_"\)}@} is {@{a simple line-coding scheme}@} in which {@{binary "1" is represented by a positive voltage level $+A$}@} and {@{binary "0" is represented by the absence of that level}@}—i.e., the signal {@{sits at 0&nbsp;V}@}. {@{The waveform for one bit interval $[0,T]$}@} can be written as {@{$$s_0(t)=0,\qquad s_1(t)=A\qquad (t\in[0,T]) \,.$$}@} <!--SR:!2026-03-28,83,369!2026-04-10,96,379!2026-03-26,81,369!2026-04-12,98,379!2026-03-26,81,369!2026-04-12,98,377!2026-03-31,86,369-->

### amplitude-shift keying

{@{Amplitude-shift keying \(ASK\)}@} conveys {@{binary data}@} by altering {@{the amplitude of a carrier wave while keeping its frequency and phase fixed}@}. In {@{the simplest case}@}, {@{one bit value}@} is transmitted as {@{the full-amplitude carrier}@} and {@{the other as zero or a reduced amplitude}@}. A typical representation for {@{a single-bit symbol over the interval $0\le t \le T$}@} is {@{$$s_0(t)=0,\qquad s_1(t)=A\cos(\omega t+\theta) \qquad (t \in [0, T]) \,,$$}@} where {@{$\omega$ and $\theta$ are chosen}@} so that {@{$\omega T$ is an integer multiple of $2\pi$}@} to ensure {@{smooth phase transition between symbols}@}. <!--SR:!2026-04-04,90,377!2026-04-14,100,379!2026-03-25,80,369!2026-03-29,84,369!2026-04-06,92,379!2026-03-26,81,369!2026-04-04,90,369!2026-04-03,89,369!2026-04-01,87,379!2026-03-30,85,369!2026-03-26,81,369!2026-04-10,96,379-->

### phase-shift keying

{@{Phase-shift keying \(PSK\)}@} modulates {@{the carrier's phase while maintaining constant amplitude and frequency}@}. {@{Binary PSK \(BPSK\)}@} uses {@{two phases separated by $180^\circ$}@}, making it {@{a simple form of digital modulation}@}. {@{The transmitted waveforms for one bit interval}@} {@{are $$s_0(t)= +A\cos(\omega t+\theta), \qquad s_1(t)= -A\cos(\omega t+\theta) \qquad (t \in [0, T]) \,,$$}@} again with {@{$\omega T$ an integer multiple of $2\pi$}@} to ensure {@{smooth phase transition between symbols}@} and {@{signal orthogonality}@}. <!--SR:!2026-04-02,88,369!2026-03-29,84,369!2026-03-25,80,369!2026-04-10,96,379!2026-04-14,100,379!2026-03-30,85,369!2026-04-05,91,379!2026-03-25,80,369!2026-04-11,97,379!2026-03-27,82,369-->

### frequency-shift keying

In {@{frequency-shift keying \(FSK\)}@} {@{the data are encoded}@} by switching {@{between discrete carrier frequencies rather than altering amplitude or phase}@}. {@{Each symbol}@} is represented by {@{a sinusoid of a particular frequency during its interval $0\le t \le T$}@}. For {@{binary FSK}@} {@{one convenient pair of signals}@} is {@{$$s_0(t)=A\cos(\omega_1 t+\theta), \qquad s_1(t)=A\cos(\omega_2 t+\theta) \qquad (t \in [0, T]) \,,$$}@} with {@{$\omega_1$ and $\omega_2$ chosen}@} such that {@{$\omega_1 \ne \omega_2$ and both $\omega_1 T$ and $\omega_2 T$ are integer multiples of $2\pi$}@} to ensure {@{smooth phase transition between symbols}@}. <!--SR:!2026-04-03,89,369!2026-04-09,95,377!2026-03-30,85,369!2026-04-05,91,379!2026-04-06,92,379!2026-04-02,88,369!2026-04-05,91,379!2026-03-27,82,369!2026-03-28,83,369!2026-04-06,92,379!2026-03-28,83,369-->
