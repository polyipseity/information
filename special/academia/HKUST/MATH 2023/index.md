---
aliases:
  - HKUST MATH 2023
  - HKUST MATH 2023 index
  - HKUST MATH2023
  - HKUST MATH2023 index
  - MATH 2023
  - MATH 2023 index
  - MATH2023
  - MATH2023 index
  - Multivariable Calculus
  - Multivariable Calculus index
tags:
  - flashcard/active/special/academia/HKUST/MATH_2023/index
  - function/index
  - language/in/English
---

# index

- HKUST MATH 2023
- name: Multivariable Calculus

The content is in teaching order.

- grading
  - scheme
    - homework ×?: 10%
    - midterm exam: 40%
    - final exam: 50%
- logistics

## children

- [assignments](assignments/index.md)
- [questions](questions.md)

## week 1 lecture

- datetime: 2025-02-04T13:30:00+08:00/2025-02-04T14:50:00+08:00
- [function](../../../../general/function%20(mathematics).md) ::@:: image, pre-image, graph of a function, continuity
  - function / image ::@:: It of an input value $x$ is the single output value produced by $f$ when passed $x$. It of a set of input values $X$ is the set of output values produced by $f$ when passed each value of $X$.
  - function / pre-image ::@:: It of an output value $y$ is the set of input values that produce $y$ when passed to $f$. It of a set of output values $Y$ is the set of input values that produce any value in $Y$ when passed to $f$.
  - function / graph of a function ::@:: The set of points $(x, f(x))$ for all $x$ in the domain of $f$ on the 2D Cartesian plane. <p> Not all set of points $(x, y)$ is the graph of a function. Use the vertical line test: If any vertical line pass through at most one point in the set, then the graph is a function.
  - function / [continuity](../../../../general/continuous%20function.md) ::@:: The limit of the function at each input value equals the function evaluated at that value. (Some special cases regarding domain boundaries have been omitted here...)
- [calculus](../../../../general/calculus.md) ::@:: rate of change, differentiation, integration, fundamental theorem of calculus
  - calculus / [differentiation](../../../../general/derivative.md) ::@:: Definition: $$f'(x) = \lim_{\delta \to 0} \frac {f(x + \delta) - f(x)} {\delta} \,$$ if it exists. But in practice, we apply some rules. <p> Related to the rate of change.
  - calculus / [integration](../../../../general/integral.md) ::@:: Inverse operation to differentiation. <p> Related to area under the curve.
  - calculus / [fundamental theorem of calculus](../../../../general/fundamental%20theorem%20of%20calculus.md) ::@:: There are 2 parts. It relates a function to its antiderivatives. <p> The first part: We have a continuous function $f$. Then we can define a function as the definite integral of $f$ from $a$ to $x$. The function is an antiderivative of $f$. <p> The second part: We have a Riemann integrable function $f$. We also have a function $F$ that is the antiderivaive of $f$. Then the definite integral of $f$ from $a$ to $b$ is $F(b) - F(a)$.
- function
  - function / extension in this course ::@:: We will study functions with multiple inputs and/or multiple outputs.
- [function of several real variables](../../../../general/function%20of%20a%20several%20real%20variables.md) ::@:: In general, $$\mathbb R^n \to X \,.$$ <p> For this course in particular, we will focus on $X = \mathbb R^m$.
- [Cartesian coordinate system](../../../../general/Cartesian%20coordinate%20system.md) ::@:: A coordinate system that specifies each point uniquely by a pair of real numbers called _coordinates_, which are the signed distances to the point from two fixed perpendicular oriented lines, called _coordinate lines_, _coordinate axes_ or just _axes_ (plural of _axis_) of the system.
  - Cartesian coordinate system / dimension ::@:: number of points to describe a point
  - Cartesian coordinate system / symbols ::@:: Using real numbers, $\mathbb R^n$, where $n$ is the dimension of the entire space.
  - Cartesian coordinate system / orientation ::@:: left-hand side, right-hand side; by most convention (including this course), we use right-hand side
  - Cartesian coordinate system / shape description ::@:: Equations can describe a subset of points of the entire space. Intersection and/or union may be used to define shapes defined by multiple equations. <p> For example: $x^2 + y^2 + z^2 = r^2$ is a 3D sphere of radius $r$.
  - Cartesian coordinate system / projection ::@:: Orthogonal projection may be used to extract a specific coordinate of a point.
- [Euclidean vector](../../../../general/Euclidean%20vector.md) ::@:: A set whose elements, often called _vectors_, can be added together and multiplied ("scaled") by numbers called _scalars_.
  - Euclidean vector / vector ::@:: A line segment with a direction, up to translation. That is, two vectors are the same if they have the same length and direction. <p> The zero vector has 0 length and no direction.
  - Euclidean vector / scalar ::@:: A real number. In general, the field associated with the vector space.
  - Euclidean vector / position vector ::@:: A vector used to represent the position of a point. <p> For example, in 3D space, the point $(x, y, z)$ can be represented by the position vector $\langle x, y, z \rangle$ (note the angle brackets).
  - Euclidean vector / element-wise operations ::@:: vector addition, vector subtraction: perform the operation on each coordinate separately to get the resulting vector
  - Euclidean vector / scalar multiplication ::@:: multiply each coordinate by the specified scalar to get the resulting vector
  - Euclidean vector / magnitude or length ::@:: The square root of the sum of the squared coordinates. This is a specific case of the _p_-norm. <p> For example, the vector $\vec v = \langle x, y, z \rangle$ has the magnitude $\lvert \vec v \rvert = \sqrt{x^2 + y^2 + z^2}$.
  - [unit vector](../../../../general/unit%20vector.md) ::@:: A vector of magnitude 1. The unit vector associated with an arbitrary vector $\vec v$ is $\vec v / \lvert \vec v \rvert$. The zero vector has no unit vector, as it has no direction. <p> For example, in 3D space, the unit vectors along the coordinate axes (and their notations): $\vec i = \langle 1, 0, 0 \rangle, \vec j = \langle 0, 1, 0 \rangle, \vec k = \langle 0, 0, 1 \rangle$. These 3 unit vectors form is _an_ (not _the_) orthonormal basis of the 3D space.
  - [orthonormal basis](../../../../general/orthonormal%20basis.md) ::@:: The set of unit vectors along the coordinate axes form an orthonormal basis. This means any vector can be _uniquely_ written as a linear combination of the orthonormal basis. <p> For example, in 3D space, each vector $\vec v$ can be written uniquely as $\vec v = x \vec i + y \vec j + z \vec k$, where $x, y, z$ are scalars.

## week 1 tutorial

- datetime: 2025-02-04T16:00:00+08:00/2025-02-04T16:50:00+08:00
- [week 1 lecture](#week%201%20lecture)
- [week 1 lecture 2](#week%201%20lecture%202)

## week 1 lecture 2

- datetime: 2025-02-06T13:30:00+08:00/2025-02-06T14:50:00+08:00
- [dot product](../../../../general/dot%20product.md)
  - dot product / algebraically ::@:: It is the sum of the products of the corresponding entries of the two sequences of numbers. Algebraic and geometric definitions are equivalent when using Cartesian coordinates.
  - dot product / geometrically ::@:: It is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them. Algebraic and geometric definitions are equivalent when using Cartesian coordinates.
  - dot product / properties
    - dot product / properties / magnitude ::@:: $\vec v \cdot \vec v = \lvert \vec v \rvert^2$
    - dot product / properties / commutativity ::@:: $\vec v \cdot \vec w = \vec w \cdot \vec v$
    - dot product / properties / distributivity ::@:: $\vec u \cdot (\vec v + \vec w) = \vec u \cdot \vec v + \vec u \cdot \vec w \qquad (\vec u + \vec v) \cdot \vec w = \vec u \cdot \vec w + \vec v \cdot \vec w$
    - dot product / properties / homogeneity ::@:: $(c \vec u) \cdot \vec v = c (\vec u \cdot \vec v) = \vec u \cdot (c \vec v)$
- [polarization identity](../../../../general/polarization%20identity.md) ::@:: $2 \lVert \vec x \rVert^2 + 2 \lVert \vec y \rVert^2 = \lVert \vec x + \vec y \rVert^2 - \lVert \vec x - \vec y \rVert^2$
  - polarization identity / variant ::@:: $\langle \vec x, \vec y \rangle = \frac 1 4 \left(\lVert x + y \rVert^2 - \lVert x - y \rVert^2 \right)$
- [dot product](../../../../general/dot%20product.md)
  - dot product / cosine ::@:: $\cos \theta = \frac {\vec u \cdot \vec v} {\lVert \vec u \rVert \lVert \vec v \rVert}$
    - dot product / cosine / applications ::@:: inscribed triangle with diameter has a right angle, law of cosine/generalized Pythagoras theorem
  - dot product / projection ::@:: Project $\vec b$ onto $\vec a$: $\operatorname{proj}_{\vec a} \vec b = \frac {\vec a \cdot \vec b} {\vec a \cdot \vec a} \vec a$. $\frac {\vec a \cdot \vec b} {\vec a \cdot \vec a}$ can be interpreted as $\cos \theta$, where $\theta$ is the angle between them.
    - dot product / projection / scalar ::@:: It is simply the signed length of the projection, which is $\operatorname{comp}_{\vec a} \vec b = \frac {\vec a \cdot \vec b} {\lvert \vec a \rvert}$.
- [cross product](../../../../general/cross%20product.md) ::@:: Given two linearly independent vectors $\mathbf a$ and $\mathbf b$, the cross product, $\mathbf a \times \mathbf b$ (read "a cross b"), is a vector that is perpendicular to both $\mathbf a$ and $\mathbf b$, and thus normal to the plane containing them. <p> The magnitude of the cross product equals the area of a parallelogram with the vectors for sides; in particular, the magnitude of the product of two perpendicular vectors is the product of their lengths, i.e. $\lVert \mathbf a \rVert \lVert \mathbf b \rVert \sin \theta$, where $\theta$ is the angle between them.
  - cross product / properties
    - cross product / properties / anti-commutative ::@:: $\mathbf a \times \mathbf b = -\mathbf b \times \mathbf a$
    - cross product / properties / distributivity ::@:: $\mathbf a \times (\mathbf b + \mathbf c) = \mathbf a \times \mathbf b + \mathbf a \times \mathbf c \qquad (\mathbf a + \mathbf b) \times \mathbf c = \mathbf a \times \mathbf c + \mathbf b \times \mathbf c$
    - cross product / properties / zero element ::@:: $\mathbf a \times \mathbf 0 = \mathbf 0 \times \mathbf a = \mathbf 0$
    - cross product / properties / parallel ::@:: $\mathbf a \times \mathbf a = \mathbf 0$
  - cross product / calculation
    - cross product / calculation / element-wise ::@:: $\mathbf a \times \mathbf b = \langle a_2 b_3 - a_3 b_2, a_3 b_1 - a_1 b_3, a_1 b_2 - a_2 b_1 \rangle$
    - cross product / calculation / matrix determinant ::@:: $\mathbf a \times \mathbf b = \begin{vmatrix} \mathbf i & \mathbf j & \mathbf k \\ a_1 & a_2 & a_3 \\ b_1 & b_2 & b_3 \end{vmatrix}$

## week 2 lecture

- datetime: 2025-02-11T13:30:00+08:00/2025-02-11T14:50:00+08:00
- [cross product](../../../../general/cross%20product.md)
  - cross product / calculation
    - cross product / calculation / 3D basis ::@:: $\hat i \times \hat j = \hat k \qquad \hat j \times \hat k = \hat i \qquad \hat k \times \hat i = \hat j$
  - cross product / properties
    - cross product / properties / perpendicularity ::@:: $(\mathbf u \times \mathbf v) \cdot \mathbf u = (\mathbf u \times \mathbf v) \cdot \mathbf v = 0$
    - cross product / properties / parallelism ::@:: $\mathbf u \times \mathbf v = \mathbf 0$ iff the 2 vectors are parallel or at least 1 of them is $\mathbf 0$.
  - cross product / area and volume ::@:: $\lvert \mathbf u \times \mathbf v \rvert$ is the area of the parallelogram formed by the 2 vectors. $\lvert \mathbf u \cdot (\mathbf v \times \mathbf w) \rvert$ is the volume of the parallelepiped (analogy of parallelogram to 3D space) formed by the 3 vectors.
- [parametric equation](../../../../general/parmetric%20equation.md) ::@:: It expresses several quantities, such as the coordinates of a point, as functions of one or several variables called parameters. <p> We consider there are exactly 1 parameter, in which case the equation describes a curve (it need not be a line).
  - parametric equation / notation ::@:: For example, to describe a curve in 3D space, we have several ways: $$\begin{aligned} \gamma(t) & = (x(t), y(t), z(t)) \\ & \begin{cases} x(t) = \text{function of }t \\ y(t) = \text{function of }t \\ z(t) = \text{function of }t \end{cases} \\ & \begin{cases} x = \text{function of }t \\ y = \text{function of }t \\ z = \text{function of }t \end{cases} \end{aligned}$$
  - parametric equation / construction
    - parametric equation / construction / line through a point parallel to a vector ::@:: If the point is $(x_0, y_0, z_0)$ and the vector is $\langle a, b, c \rangle$, then $\gamma(t) = (x_0, y_0, z_0) + t (a, b, c) = (x_0 + at, y_0 + bt, z_0 + ct)$. <p> We notice that actually, as long as the multiplication factor ($t$ in this context) has a domain of $\mathbb R$ and are the same for $a, b, c$, the equation describes the same line. So we can replace $t$ with $2t$, but not $0t$ or $t^2$.
  - parametric equation / non-uniqueness ::@:: Different equations can represent the same curve. <p> The simplest would be to replace $t$ by $\alpha t$, where $\alpha$ is a nonzero real number. The curve is the same but we can think of it as $t$ going "faster" or "slower" along the curve.

## week 2 tutorial

- datetime: 2025-02-11T16:00:00+08:00/2025-02-11T16:50:00+08:00
- [week 2 lecture](#week%202%20lecture)
- [week 2 lecture 2](#week%202%20lecture%202)

## week 2 lecture 2

- datetime: 2025-02-13T13:30:00+08:00/2025-02-13T14:50:00+08:00
- parametric equation
  - parametric equation / symmetric equation ::@:: Solve for $t$ in the parametric equation $\gamma(t) = (x_0 + at, y_0 + bt, z_0 + ct)$ to get this form: $$\frac {x - x_0} a = \frac {y - y_0} b = \frac {z - z_0} c \,.$$ Note that if one or more of $a$, $b$, or $c$ is 0, it means the curve does not change in coordinate for said axes. In that case, we simply have resp. $x = x_0$, $y = y_0$, $z = z_0$.
- [line](../../../../general/line%20(geometry).md) ::@:: an infinitely long object with no width, depth, or curvature, an idealization of such physical objects as a straightedge, a taut string, or a ray of light
  - line / relations with other lines ::@:: same: same line <br/> intersect: two _distinct_ lines intersecting at one point (they must be in the same plane) <br/> parallel: two _distinct_ lines in the same plane (always true for 2D space) and does not intersect <br/> skew: two _distinct_ lines not in the same plane (always false for 2D space) and does not intersect
    - line / relations with other lines / checking ::@:: Check direction first. If the same direction, if they intersect then same line, otherwise parallel lines. If not the same direction, if they intersect, then intersect, otherwise skew.
- [Euclidean plane](../../../../general/Euclidean%20plane.md) ::@:: A Euclidean space of dimension two. In 3D space, it can be described by a point it passes through and its _nonzero_ normal vector (which reduces the dimensionality by 1).
  - Euclidean plane / equation ::@:: Given a point $(x_0, y_0, z_0)$ and a _nonzero_ normal vector $\langle a, b, c \rangle$, the plane described has the equation: $$a(x - x_0) + b(y - y_0) + c(z - z_0) = 0$$ which is more commonly written as $$ax + by + cz - d = 0 \,.$$ You can think of _d_ as the _signed_ (positive in the direction of the normal vector) distance of the plane from the origin scaled by the normal vector length. <p> Conversely, any equation in the above form, where $a, b, c$ are not all zero, describes a plane. <p> (You can also describe it using linear algebra.)
- [Hesse normal form](../../../../general/Hesse%20normal%20form.md) ::@:: It describes a _n_−1 Euclidean space in a _n_-dimensional space. For 3D space, it describes a plane.
  - Hesse normal form / expression ::@:: It is written in vector notation as $${\vec {r} }\cdot {\vec {n} }_{0}-d=0.\,$$ The dot $\cdot$ indicates the [dot product](../../../../general/dot%20product.md) \(or scalar product\). Vector ${\vec {r} }$ points from the origin of the coordinate system, _O_, to any point _P_ that lies precisely in plane or on line _E_. The vector ${\vec {n} }_{0}$ represents the [unit](../../../../general/unit%20vector.md) [normal vector](../../../../general/normal%20vector.md) of plane or line _E_. The distance $d$ is the shortest _signed_ (positive in the direction of $\vec n_0$) distance from the origin _O_ to the plane or line.
- Euclidean plane
  - Euclidean plane / cross product ::@:: In 3D space, we can find the plane two non-parallel vectors lie in. Find the cross product of the two vectors. Then the resulting vector is a normal vector of the plane.
  - Euclidean plane / angle between two planes ::@:: The angle is the same as the angle between their two normal vectors. So you can use dot product (and cross product if in 3D space) to find it. <p> Note: What if the normal of one is reversed? It turns out there are two angles definable between two planes, but they must add up to 180 degrees. Use the lower one.
  - Euclidean plane / relations with other planes ::@:: same: same plane <br/> intersect: _distinct_ planes that intersect at a line <br/> parallel: _distinct_ planes that do not intersect
    - Euclidean plane / relations with other planes / checking ::@:: Check if their normal vectors are parallel. If so, they are either the same or parallel. Otherwise, they are intersect.
  - Euclidean plane / distance to a point ::@:: This is the length of the line connecting the point to the plane parallel to the plane normal vector.
    - Euclidean plane / distance to a point / intuition ::@:: Consider the plane equation $ax + by + cz - d = 0$. In particular, the left hand side without $d$ is the component of $\langle x, y, z \rangle$ in the normal vector direction, scaled by the length of the normal vector. The $d$ represents the _signed_ (positive in the direction of the normal vector) distance of the plane from the origin scaled by the normal vector length. So the entire left hand side is the _signed_ distance from the plane to the point $(x, y, z)$. <p> So clearly, the (_unsigned_) distance to the point $(x, y, z)$ is: $$\text{distance} = \frac {\lvert ax + by + cz - d \rvert} {\sqrt {a^2 + b^2 + c^2} } \,.$$ <p> If you use the Hesse normal form, since the normal vector is normalized, the denominator (length of the normal vector) is 1 and can be omitted.
    - Euclidean plane / distance to a point / derivation ::@:: Consider the plane equation $ax + by + cz - d = 0$. Find its distance to $P_0 = (x_0, y_0, z_0)$. <p> Consider any point on the plane $P_1 = (x_1, y_1, z_1)$. Construct the vector $\overrightarrow{P_1 P_0}$. The distance is the component of the vector in the normal vector $\langle a, b, c \rangle$ direction. So we have $$\text{distance} = \left\lvert \frac {a(x_0 - x_1) + b(y_0 - y_1) + c(z_0 - z_1)} {\sqrt{a^2 + b^2 + c^2} } \right\rvert = \frac {\lvert a x_0 + b y_0 + c z_0 - d \rvert} {\sqrt{a^2 + b^2 + c^2} } \,.$$ <p> The above derivation also applies to higher dimensions for hyperplanes.

## week 3 lecture

- datetime: 2025-02-18T13:30:00+08:00/2025-02-18T14:50:00+08:00
- parametric equation
  - parametric equation / parametric curve ::@:: A curve described by a parametric equation: $\gamma : A \to \mathbb R^n$, where $A \subseteq \mathbb R$.
    - parametric equation / parametric curve / circle ::@:: $$(x, y) = (x_0 + r \cos at, y_0 + r \sin at)$$, where $(x_0, y_0)$ is the center, $r$ is the radius, $a \ne 0$ is "the rate of running along the shape".
    - parametric equation / parametric curve / graph of a function ::@:: $$(x, y) = (t, f(t))$$, where $f$ is a function.
    - parametric equation / parametric curve / cycloid ::@:: Consider the cycloid through the origin, generated by a circle of radius _r_ rolling over the _x_-axis on the positive side (_y_ ≥ 0). <p> To derive the equation, consider a non-rolling circle: $\gamma'(\theta) = (r \sin \theta, r(1 - \cos \theta))$. Then consider the circle rolling to the right by $r \, \mathrm d\theta$ per $\mathrm d\theta$ (compare when $\theta = 0$ and $\theta = 2\pi$), producing a cycloid. So we have: $$\gamma(\theta) = (r(\theta - \sin \theta), r(1- \cos \theta))$$.
- [cycloid](../../../../general/cycloid.md) ::@:: the curve traced by a point on a circle as it rolls along a straight line without slipping
- [parametric derivative](../../../../general/parametric%20derivative.md) ::@:: It is a derivative of a dependent variable with respect to another dependent variable that is taken when both variables depend on an independent third variable, usually thought of as "time" (that is, when the dependent variables are _x_ and _y_ and are given by parametric equations in _t_).
  - parametric derivative / intuition ::@:: Recall we can consider changing $t$ as running along the curve. Then its parametric derivative is the velocity (vector) of running.
  - parametric derivative / slope ::@:: To extract the slope for a curve on 2D, use $y'(t) / x'(t)$.
    - parametric derivative / slope / indeterminate slope ::@::  Note that the slope as defined above may be indeterminate, e.g. when $x'(t) = y'(t) = 0$. That does not necessarily mean that slope is undefined there (sometimes it really is undefined though, e.g. sharp corners). <p> In said cases, we can try to compute $\lim_{\tau \to t} \frac {y'(\tau)} {x'(\tau)}$, and maybe make use of the L'Hoptial rule if needed.
    - parametric derivative / slope / horizontal slope ::@:: Find $t$ for which $x'(t) \ne 0$ and $y'(t) = 0$. Also check if there are indeterminate slopes and see if they are actually also horizontal.
    - parametric derivative / slope / vertical slope ::@:: Find $t$ for which $x'(t) = 0$ and $y'(t) \ne 0$. Also check if there are indeterminate slopes and see if they are actually also vertical.
- [function of several real variables](../../../../general/function%20of%20several%20real%20variables.md) (multivariable functions) ::@:: It is a function with more than one argument, with all arguments being real variables. <p> In this course, we study real-valued functions, i.e. the codomain is also real.
  - function of several real variables / domain ::@:: The [domain](../../../../general/domain%20of%20a%20function.md) of a function of _n_ variables is the [subset](../../../../general/subset.md) of ⁠$\mathbb {R} ^{n}$⁠ for which the function is defined. As usual, the domain of a function of several real variables is supposed to contain a nonempty [open](../../../../general/open%20set.md) subset of ⁠$\mathbb {R} ^{n}$⁠. <p> When the domain is not specified, it is usually implicitly understood as the largest subset that makes the function defined.
  - function of several real variables / graph ::@:: For _n_ real arguments, we can visualize it in a _n_+1-dimensional space. It also satisfies the vertical line test. <p> For example, when _n_ = 1, we get the graph of a function in 2D. When _n_ = 2, we get a surface in 3D satisfying the vertical line test. When _n_ = 3, we need to visualize in 4D, and good luck with that!
- [level set](../../../../general/level%20set.md) ::@:: It of a [real-valued function](../../../../general/real-valued%20function.md) _f_ of _n_ [real variables](../../../../general/function%20of%20several%20real%20variables.md) is a [set](../../../../general/set%20(mathematics).md) where the function takes on a given [constant](../../../../general/constant%20(mathematics).md) value _c_, that is: $$L_{c}(f)=\left\{(x_{1},\ldots ,x_{n})\mid f(x_{1},\ldots ,x_{n})=c\right\}~.$$
  - level set / names ::@:: It depends on the number of independent variables to the function. If 2, it is also called a _level curve_. If 3, it is also called a _level surface_. If more than 3, it is also called a _level hypersurface_.
  - level set / visualization ::@:: It can be visualized using _contour lines_ for functions of 2 independent variables. <p> Note that a level curve (_n_ = 2) does not always have to be a curve: it could be a point or a surface. Similarly, a level surface (_n_ = 3) does not always have to be a surface.
  - level set / interpretations ::@:: It is the preimage of the function over $c$. It is also the intersection of the graph of the function with the (hyper)plane $y = c$. <p> Further, if $f$ is _differentiable_, then the gradient of $f$ at a point is either zero, or _perpendicular_ to the level set of $f$ at that point.
- reading: [cycloid](../../../../general/cycloid.md)

## week 3 tutorial

- datetime: 2025-02-18T16:00:00+08:00/2025-02-18T16:50:00+08:00
- [week 3 lecture](#week%203%20lecture)
- [week 3 lecture 2](#week%203%20lecture%202)

## week 3 lecture 2

- datetime: 2025-02-20T13:30:00+08:00/2025-02-20T14:50:00+08:00
- function of several real variables
- level set
- [limit of a function](../../../../general/limit%20of%20a%20function.md) ::@:: a fundamental concept in calculus and analysis concerning the behavior of that function near a particular input which may or may not be in the domain of the function
  - limit of a function / definition (univariate) ::@:: __The limit of _f_ of _x_, as _x_ approaches _p_, exists, and it equals _L_<!-- markdown separator -->__ and write, $$\lim _{x\to p}f(x)=L,$$ if the following property holds: for every real _ε_ \> 0, there exists a real _δ_ \> 0 such that for all real _x_, 0 \< \|_x_ − _p_\| \< _δ_ implies \|_f_\(_x_\) − _L_\| \< _ε_.
  - limit of a function / definition (multivariate) ::@:: __The limit of _f_ as \(_x_, _y_\) approaches \(_p_, _q_\) is _L_<!-- markdown separator -->__, written $$\lim _{(x,y)\to (p,q)}f(x,y)=L$$ if the following condition holds: For every _ε_ \> 0, there exists a _δ_ \> 0 such that for all _x_ in _S_ and _y_ in _T_, whenever $0<{\sqrt {(x-p)^{2}+(y-q)^{2} } }<\delta$, we have \|_f_\(_x_, _y_\) − _L_\| \< _ε_
    - limit of a function / definition / proving ::@:: To disprove it, it suffices to show there exists two paths to \(_x_, _y_\) with different limits. <p> To prove it, the definition is required, since if we use try to use paths to prove (which we will usually not in practice), we need to check the infinite many ways to make a path, which includes curved paths.
  - algebraic limit theorem ::@:: Taking the limit of algebraic operations (addition, subtraction, multiplication, division, exponentiation) on two functions are compatible with the operation on the limits of the two functions _under some condition_. <p> The _main_ condition is that the limits of the two functions exist (finite, not indeterminate/infinity). Division and exponentiation has some _extra_ conditions.
    - algebraic limit theorem / division ::@:: The limit of the divisor (i.e. $g(x)$ in $f(x)/g(x)$) is non-zero.
    - algebraic limit theorem / exponentiation ::@:: The limit of the base (i.e. $f(x)$ in $(f(x))^{g(x)}$) is either positive, or zero while the limit of the exponent (i.e. $g(x)$ in $(f(x))^{g(x)}$) is positive (finite).
    - algebraic limit theorem / polynomials, rational functions ::@:: This basically allows us to say polynomials have limits everywhere, and rational functions have limits _almost_ everywhere (where the denominator is nonzero).
- [squeeze theorem](../../../../general/squeeze%20theorem.md) ::@:: a theorem regarding the limit of a function that is bounded between two other functions
  - squeeze theorem / theorem (univariate) ::@:: Let _I_ be an [interval](../../../../general/interval%20(mathematics).md) containing the point _a_. Let _g_, _f_, and _h_ be [functions](../../../../general/function%20(mathematics).md) defined on _I_, except possibly at _a_ itself. Suppose that for every _x_ in _I_ not equal to _a_, we have $$g(x)\leq f(x)\leq h(x)$$ and also suppose that $$\lim _{x\to a}g(x)=\lim _{x\to a}h(x)=L.$$ Then $\lim _{x\to a}f(x)=L$.
  - squeeze theorem / theorem (multivariate) ::@:: The squeeze theorem can still be used in multivariable calculus but the lower (and upper functions) must be below (and above) the target function not just along a path but around the entire neighborhood of the point of interest and it only works if the function really does have a limit there. It can, therefore, be used to prove that a function has a limit at a point, but it can never be used to prove that a function does not have a limit at a point.
- [questions § week 3 lecture 2](questions.md#week%203%20lecture%202)

## week 4 lecture

- datetime: 2025-02-25T13:30:00+08:00/2025-02-25T14:50:00+08:00
- [continuous function](../../../../general/continuous%20function.md) ::@:: a function such that a small variation of the argument induces a small variation of the value of the function; no abrupt changes in value, known as _discontinuities_
  - continuous function / definition ::@:: The function _f_ is _continuous at some point_ _c_ of its domain if the [limit](../../../../geneal/limit%20of%20a%20function.md) of $f(x)$, as _x_ approaches _c_ through the domain of _f_, exists and is equal to $f(c)$. In mathematical notation, this is written as $$\lim _{x\to c}{f(x)}=f(c).$$
  - continuous function / construction ::@:: Sum (& subtraction), product, and composition of functions preserve continuity. Division preserves continuity excluding where the denominator vanishes (equals 0).
    - continuous function / construction / examples ::@:: Recall that the functions mentioned below have limits everywhere/almost everywhere. <p> Polynomial functions are continuous everywhere. Rational functions are continuous almost everywhere except where the denominator vanishes.
- [partial derivative](../../../../general/partial%20derivative.md) ::@:: It of a function of several variables is its derivative with respect to one of those variables, with the others held constant (as opposed to the total derivative, in which all variables are allowed to vary).
  - partial derivative / in univariate calculus ::@:: There is only one (or two) direction $x$ can change in. Thus in this case, the derivative definition is quite simple and we only have one (first) derivative.
  - partial derivative / intuition ::@:: Fix all arguments to a multivariate function except for one. Then differentiate it with respect to that argument. This is the _partial derivative with respect to that argument_. <p> One partial derivative can be constructed for each argument in this way.
  - partial derivative / notations ::@:: The partial derivative of a function $f(x,y,\dots )$ with respect to the variable $x$ is variously denoted by <p> $f_{x}$, $f'_{x}$, $\partial _{x}f$, $\ D_{x}f$, $D_{1}f$, ${\frac {\partial }{\partial x} }f$, or ${\frac {\partial f}{\partial x} }$.
  - partial derivative / computation ::@:: Assume all other variables are constant. Then differentiate as if it is a univariate function. <p> Implicit differentiation is another technique. It works similar to that applied to a univariate function. If a variable is neither the variable being differentiated against (output) nor being differentiated with respect to (input), consider it fixed.
  - partial derivative / higher order ::@:: They are defined analogously to the higher order derivatives of univariate functions: Just partial differentiate multiple times.
    - partial derivative / higher order / notations ::@:: For the function $f(x,y,...)$ the "own" second partial derivative with respect to _x_ is simply the partial derivative of the partial derivative \(both with respect to _x_\): $${\frac {\partial ^{2}f}{\partial x^{2} } }\equiv \partial {\frac {\partial f/\partial x}{\partial x} }\equiv {\frac {\partial f_{x} }{\partial x} }\equiv f_{xx}.$$ <p> The cross partial derivative with respect to _x_ and _y_ is obtained by taking the partial derivative of _f_ with respect to _x_, and then taking the partial derivative of the result with respect to _y_, to obtain $${\frac {\partial ^{2}f}{\partial y\,\partial x} }\equiv \partial {\frac {\partial f/\partial x}{\partial y} }\equiv {\frac {\partial f_{x} }{\partial y} }\equiv f_{xy}.$$
- [symmetry of second derivatives](../../../../general/symmetry%20of%20second%20derivatives.md) ::@:: It is the fact that exchanging the order of [partial derivatives](../../../../general/partial%20derivative.md) of a [multivariate function](../../../../general/multivariate%20function.md#multivariate%20functions) $$f\left(x_{1},\,x_{2},\,\ldots ,\,x_{n}\right)$$ does not change the result if some [continuity](../../../../general/continuous%20function.md) conditions are satisfied \(see below\); that is, the second-order partial derivatives satisfy the [identities](../../../../general/identity%20(mathematics).md) $${\frac {\partial }{\partial x_{i} } }\left({\frac {\partial f}{\partial x_{j} } }\right)\ =\ {\frac {\partial }{\partial x_{j} } }\left({\frac {\partial f}{\partial x_{i} } }\right).$$
  - symmetry of second derivatives / Hessian matrix ::@:: In other words, the matrix of the second-order partial derivatives, known as the [Hessian matrix](../../../../general/Hessian%20matrix.md), is a [symmetric matrix](../../../../general/symmetric%20matrix.md).
  - symmetry of second derivatives / Schwarz's theorem ::@:: It states that for a function $f\colon \Omega \to \mathbb {R}$ defined on a set $\Omega \subset \mathbb {R} ^{n}$, if $\mathbf {p} \in \mathbb {R} ^{n}$ is a point such that some [neighborhood](../../../../general/neighbourhood%20(mathematics).md) of $\mathbf {p}$ is contained in $\Omega$ and $f$ has [continuous](../../../../general/continuous%20function.md) second [partial derivatives](../../../../general/partial%20derivatives.md) on that neighborhood of $\mathbf {p}$, then for all _i_ and _j_ in $\{1,2\ldots ,\,n\}$, $${\frac {\partial ^{2} }{\partial x_{i}\,\partial x_{j} } }f(\mathbf {p} )={\frac {\partial ^{2} }{\partial x_{j}\,\partial x_{i} } }f(\mathbf {p} ).$$ The partial derivatives of this function commute at that point.
  - symmetry of second derivatives / usage ::@:: After checking the conditions hold, this may be used to simplify expressions involving mixed derivatives.

## week 4 tutorial

- datetime: 2025-02-25T16:00:00+08:00/2025-02-25T16:50:00+08:00
- [week 4 lecture](#week%204%20lecture)
- [week 4 lecture 2](#week%204%20lecture%202)

## week 4 lecture 2

- datetime: 2025-02-27T13:30:00+08:00/2025-02-27T14:50:00+08:00
- [tangent](../../../../general/tangent.md) ::@:: It to a plane curve at a given point is, intuitively, the straight line that "just touches" the curve at that point.
  - tangent / tangent plane ::@:: It to a surface at a given point is the plane that "just touches" the surface at that point.
  - tangent / tangent space ::@:: It of a manifold is a generalization of tangent lines to curves in two-dimensional space and tangent planes to surfaces in three-dimensional space in higher dimensions.
  - tangent / existence ::@:: Note that for the tangent to be defined, the function needs to be _differentiable_ at that point. Otherwise, either the tangent is vertical (infinite slope), the point is a cusp (infinite slope, different sign), the point is a corner (finite slope, different slope), or the point is discontinuous. <p> We assume the functions are _differentiable_ at the relevant points here and below.
  - tangent / tangent plane
    - tangent / tangent plane / interpretation ::@:: It is the best approximation of the surface by a plane at _p_, and can be obtained as the limiting position of the planes passing through 3 distinct points on the surface close to _p_ as these points converge to _p_.
    - tangent / tangent plane / equation ::@:: if the surface is given by a function $z=f(x,y)$, the equation of the tangent plane at point $(x_{0},y_{0},z_{0})$ can be expressed as: $$z-z_{0}={\frac {\partial f}{\partial x} }(x_{0},y_{0})(x-x_{0})+{\frac {\partial f}{\partial y} }(x_{0},y_{0})(y-y_{0}) \,.$$ Here, ${\frac {\partial f}{\partial x} }$ and ${\frac {\partial f}{\partial y} }$ are the partial derivatives of the function $f$ with respect to $x$ and $y$ respectively, evaluated at the point $(x_{0},y_{0})$. It can be written into $${\frac {\partial f}{\partial x} }(x_{0},y_{0})(x-x_{0})+{\frac {\partial f}{\partial y} }(x_{0},y_{0})(y-y_{0}) + (-1) (z - z_0) = 0 \,,$$ which has a normal vector $\left\langle \frac {\partial f} {\partial x}, \frac {\partial f} {\partial y}, -1 \right\rangle$ that can be interpreted as the surface normal pointing "downwards".
    - tangent / tangent plane / level surface ::@:: We have a function $f: \mathbb R^3 \to \mathbb R$. Define the level surface $f$ at $k$ as $f(x, y, z) = k$. To generalize, this level surface is not necessary a surface. <p> Find its _tangent plane_ (tangent space) by first considering its orthogonal vector (space), its _normal vector_ (normal space). Conveniently, gradient of the function at that point is either orthogonal to the level set, which makes the gradient a normal vector; or zero, which means the normal space is the zero vector space. So we have: $$\frac {\partial f} {\partial x} (x_0, y_0, z_0) (x - x_0) + \frac {\partial f} {\partial y} (x_0, y_0, z_0) (y - y_0) + \frac {\partial f} {\partial z} (x_0, y_0, z_0) (z - z_0) = 0 \,,$$ and check that the above "makes sense" when the gradient is zero (tangent space is the whole space).
- [differentiable function](../../../../general/differentiale%20function.md)
  - differentiable function / approximation ::@:: A differentiable function is locally well approximated as a linear function at each interior point. That is, an approximation at $x_0$ has an approximation error that shrinks faster than $\lvert x - x_0 \rvert$, i.e. in $o(\lvert x - x_0 \rvert)$ using small O notation.
- function of several real variables
  - function of several real variables / multivariate differentiability ::@:: A function _f_\(___x___\) is __differentiable__ in a neighborhood of a point ___a___ if there is an _n_-tuple of numbers dependent on ___a___ in general, ___A___\(___a___\) = \(_A_<sub>1</sub>\(___a___\), _A_<sub>2</sub>\(___a___\), …, _A_<sub>_n_</sub>\(___a___\)\), so that: $$f({\boldsymbol {x} })=f({\boldsymbol {a} })+{\boldsymbol {A} }({\boldsymbol {a} })\cdot ({\boldsymbol {x} }-{\boldsymbol {a} })+\alpha ({\boldsymbol {x} })|{\boldsymbol {x} }-{\boldsymbol {a} }|$$ where $\alpha ({\boldsymbol {x} })\to 0$ as $|{\boldsymbol {x} }-{\boldsymbol {a} }|\to 0$. If _f_ is differentiable at ___a___ then the first order partial derivatives exist at ___a___ and: $$\left.{\frac {\partial f({\boldsymbol {x} })}{\partial x_{i} } }\right|_{ {\boldsymbol {x} }={\boldsymbol {a} } }=A_{i}({\boldsymbol {a} })$$ for _i_ = 1, 2, …, _n_, which can be found from the definitions of the individual partial derivatives, so the partial derivatives of _f_ exist (but they are not necessarily _continuous_).
    - function of several real variables / multivariate differentiability / limits ::@:: Assuming the function is _differentiable_ at $\mathbf a$ (otherwise the directional derivative may exist but not the gradient, or vice versa). If you prefer limits instead: $$\lim_{\mathbf x \to \mathbf a} \frac {f(\mathbf x) - f(\mathbf a)} {\lvert \mathbf x - \mathbf a \rvert} = ((\mathbf x - \mathbf a) \cdot \nabla) f = (\mathbf x - \mathbf a) \cdot (\nabla f) \,,$$ or $$\lim_{\mathbf x \to \mathbf a} \frac {f(\mathbf x) - f(\mathbf a) - ((\mathbf x - \mathbf a) \cdot \nabla) f} {\lvert \mathbf x - \mathbf a \rvert} = \lim_{\mathbf x \to \mathbf a} \frac {f(\mathbf x) - f(\mathbf a) - (\mathbf x - \mathbf a) \cdot (\nabla f)} {\lvert \mathbf x - \mathbf a \rvert} = 0 \,.$$
    - function of several real variables / multivariate differentiability / intuition ::@:: Intuitively, the tangent plane at point ___a___ has an approximation error that shrinks faster than $\lvert \mathbf x - \mathbf a \rvert$, i.e. in $o(\lvert \mathbf x - \mathbf a \rvert)$ using small O notation.
    - function of several real variables / multivariate differentiability / relations ::@:: continuity: Multivariate differentiability implies multivariate continuity. The converse is not true. <p> partial differentiability: Having all partial derivatives (including mixed) up to order $p$ is _not_ sufficient to imply $p$-times multivariate differentiability. Having all _continuous_ partial derivatives (including mixed) up to order $p$ is sufficient, but not necessary: there are multivariate differentiable functions that do not have all _continuous_ partial derivatives (but they do have all partial derivatives). Examples can be constructed using $\sin(1 / x)$.
    - function of several real variables / multivariate differentiability / approximation ::@:: Assuming the function is _differentiable_ at $\mathbf a$ (otherwise the directional derivative may exist but not the gradient, or vice versa). Then the _linear_ approximation is: $$f(\mathbf x) \approx f(\mathbf a) + ((\mathbf x - \mathbf a) \cdot \nabla) f(\mathbf a) = f(\mathbf a) + (\mathbf x - \mathbf a) \cdot (\nabla f(\mathbf a)) \,.$$

## week 5 lecture

- datetime: 2025-03-04T13:30:00+08:00/2025-03-04T14:50:00+08:00
- [chain rule](../../../../general/chain%20rule.md) ::@:: It is a [formula](../../../../general/formula.md) that expresses the [derivative](../../../../general/derivative.md) of the [composition](../../../../general/function%20composition.md) of two [differentiable functions](../../../../general/differentiable%20function.md) _f_ and _g_ in terms of the derivatives of _f_ and _g_.
  - chain rule / univariate ::@:: It is expressed as $${\frac {dz}{dx} }={\frac {dz}{dy} }\cdot {\frac {dy}{dx} },$$ and $$\left.{\frac {dz}{dx} }\right|_{x}=\left.{\frac {dz}{dy} }\right|_{y(x)}\cdot \left.{\frac {dy}{dx} }\right|_{x},$$for indicating at which points the derivatives have to be evaluated.
  - chain rule / scalar-valued multivariate ::@:: Denote by $D_{i}f$ the partial derivative of _f_ with respect to its _i_<!-- markdown separator -->th argument, and by $D_{i}f(z)$ the value of this derivative at _z_. <p> With this notation, the chain rule is $${\frac {d}{dx} }f(g_{1}(x),\dots ,g_{k}(x))=\sum _{i=1}^{k}\left({\frac {d}{dx} }{g_{i} }(x)\right)D_{i}f(g_{1}(x),\dots ,g_{k}(x)).$$
    - chain rule / scalar-valued multivariate / intuition ::@:: In a scalar-valued multivariate function $f$, there are multiple inputs $g_i$. Each input can contribute to change in the output. If all inputs are functions of another variable $x$, $x$ contributes to change in the output of $f$ via all inputs $g_i$. Thus, we need to add up the changes.
  - chain rule / vector-valued multivariate ::@:: The simplest way for writing the chain rule in the general case is to use the [total derivative](../../../../general/total%20derivative.md#the%20total%20derivative%20as%20a%20linear%20map), which is a linear transformation that captures all [directional derivatives](../../../../general/directional%20derivative.md) in a single formula. Consider differentiable functions _f_ : __R__<sup>_m_</sup> → __R__<sup>_k_</sup> and _g_ : __R__<sup>_n_</sup> → __R__<sup>_m_</sup>, and a point __a__ in __R__<sup>_n_</sup>. Let _D_<sub>__a__</sub> _g_ denote the total derivative of _g_ at __a__ and _D_<sub>_g_\(__a__\)</sub> _f_ denote the total derivative of _f_ at _g_\(__a__\). These two derivatives are linear transformations __R__<sup>_n_</sup> → __R__<sup>_m_</sup> and __R__<sup>_m_</sup> → __R__<sup>_k_</sup> \(annotation: same domain and codomain as the original functions\), respectively, so they can be composed. The chain rule for total derivatives is that their composite is the total derivative of _f_ ∘ _g_ at __a__: $$D_{\mathbf {a} }(f\circ g)=D_{g(\mathbf {a} )}f\circ D_{\mathbf {a} }g,$$ or for short, $$D(f\circ g)=Df\circ Dg.$$
    - chain rule / vector-valued multivariate / matrix ::@:: Because the total derivative is a linear transformation, the functions appearing in the formula can be rewritten as matrices. The matrix corresponding to a total derivative is called a [Jacobian matrix](../../../../general/Jacobian%20matrix.md), and the composite of two derivatives corresponds to the product of their Jacobian matrices. From this perspective the chain rule therefore says: $$J_{f\circ g}(\mathbf {a} )=J_{f}(g(\mathbf {a} ))J_{g}(\mathbf {a} ),$$or for short, $$J_{f\circ g}=(J_{f}\circ g)J_{g}.$$ That is, the Jacobian of a composite function is the product of the Jacobians of the composed functions \(evaluated at the appropriate points\).
- [Jacobian matrix](../../../../general/Jacobian%20matrix.md) ::@:: It of a [vector-valued function](../../../../general/vector-valued%20function.md) of several variables is the [matrix](../../../../general/matrix%20(mathematics).md) of all its first-order [partial derivatives](../../../../general/partial%20derivative.md).
  - Jacobian matrix / definition ::@:: Suppose __f__ : __R__<sup>_n_</sup> → __R__<sup>_m_</sup> is a function such that each of its first-order partial derivatives exists on __R__<sup>_n_</sup>. This function takes a point __x__ ∈ __R__<sup>_n_</sup> as input and produces the vector __f__\(__x__\) ∈ __R__<sup>_m_</sup> as output. Then the Jacobian matrix of __f__, denoted __J<sub>f</sub>__ ∈ __R__<sup>_m_<!-- markdown separator -->×<!-- markdown separator -->_n_</sup>, is defined such that its \(_i_,_j_\)<sup>th</sup> entry is ${\frac {\partial f_{i} }{\partial x_{j} } }$, or explicitly $$\mathbf {J_{f} } ={\begin{bmatrix}{\dfrac {\partial \mathbf {f} }{\partial x_{1} } }&\cdots &{\dfrac {\partial \mathbf {f} }{\partial x_{n} } }\end{bmatrix} }={\begin{bmatrix}\nabla ^{\mathsf {T} }f_{1}\\\vdots \\\nabla ^{\mathsf {T} }f_{m}\end{bmatrix} }={\begin{bmatrix}{\dfrac {\partial f_{1} }{\partial x_{1} } }&\cdots &{\dfrac {\partial f_{1} }{\partial x_{n} } }\\\vdots &\ddots &\vdots \\{\dfrac {\partial f_{m} }{\partial x_{1} } }&\cdots &{\dfrac {\partial f_{m} }{\partial x_{n} } }\end{bmatrix} }$$ where $\nabla ^{\mathsf {T} }f_{i}$ is the transpose \(row vector\) of the [gradient](../../../../general/gradient.md) of the $i$-th component.
- [directional derivative](../../../../general/directional%20derivative.md) ::@:: It measures the rate at which a function changes in a particular direction at a given point.
  - directional derivative / definition ::@:: It of a [scalar function](../../../../general/scalar%20function.md) $$f(\mathbf {x} )=f(x_{1},x_{2},\ldots ,x_{n})$$ along a vector $$\mathbf {v} =(v_{1},\ldots ,v_{n})$$ is the [function](../../../../general/function%20(mathematics).md) $\nabla _{\mathbf {v} }{f}$ defined by the [limit](../../../../general/limit%20(mathematics).md) $$\nabla _{\mathbf {v} }{f}(\mathbf {x} )=\lim _{h\to 0}{\frac {f(\mathbf {x} +h\mathbf {v} )-f(\mathbf {x} )}{h} }.$$ <p> This definition is valid in a broad range of contexts, for example where the [norm](../../../../general/Euclidean%20norm.md#Euclidean%20norm) of a vector \(and hence a unit vector\) is undefined. <p> \(__this course__: __important__, we do _not_ use this definition here, since we require a unit vector\)
    - directional derivative / definition / direction only ::@:: In a [Euclidean space](../../../../general/Euclidean%20space.md), some authors define the directional derivative to be with respect to an arbitrary nonzero vector __v__ after [normalization](../../../../general/normalized%20vector.md), thus being independent of its magnitude and depending only on its direction. <p> This definition gives the rate of increase of _f_ per unit of distance moved in the direction given by __v__. In this case, one has $$\nabla _{\mathbf {v} }{f}(\mathbf {x} )=\lim _{h\to 0}{\frac {f(\mathbf {x} +h\mathbf {v} )-f(\mathbf {x} )}{h|\mathbf {v} |} },$$ or in case _f_ is differentiable at __x__, $$\nabla _{\mathbf {v} }{f}(\mathbf {x} )=\nabla f(\mathbf {x} )\cdot {\frac {\mathbf {v} }{|\mathbf {v} |} }.$$ <p> \(__this course__: __important__, we use this definition here, as we require a unit vector\)
    - directional derivative / definition / differentiable ::@:: If the function _f_ is [differentiable](../../../../general/differentiable%20function.md#differentiability%20in%20higher%20dimensions) at __x__, then the directional derivative exists along any unit vector __v__ at x, and one has $$\nabla _{\mathbf {v} }{f}(\mathbf {x} )=\nabla f(\mathbf {x} )\cdot \mathbf {v}$$ where the $\nabla$ on the right denotes the _[gradient](../../../../general/gradient.md)_, $\cdot$ is the [dot product](../../../../general/dot%20product.md) and __v__ is a unit vector.
  - directional derivative / intuition ::@:: The partial derivatives we have considered above are actually special cases of directional derivatives, where the direction is along the coordinate directions. Directional derivative generalizes this by allowing directions that are _linear combinations_ of coordinate directions. <p> When the multivariate function is _differentiable_, the directional derivative along all directions _exist_, and is just the dot product of the direction vector with the gradient.
- [total derivative](../../../../general/total%20derivative.md) ::@:: It of a function _f_ at a point is the best [linear approximation](../../../../general/linear%20approximation.md) near this point of the function with respect to its arguments. Unlike [partial derivatives](../../../../general/partial%20derivative.md), the total derivative approximates the function with respect to all of its arguments, not just a single one. In many situations, this is the same as considering all partial derivatives simultaneously.
  - total derivative / differential form ::@:: When the function under consideration is real-valued, the total derivative can be recast using [differential forms](../../../../general/differential%20form.md). For example, suppose that $f\colon \mathbb {R} ^{n}\to \mathbb {R}$ is a differentiable function of variables $x_{1},\ldots ,x_{n}$. The total derivative of $f$ at $a$ may be written in terms of its Jacobian matrix, which in this instance is a row matrix: $$Df_{a}={\begin{bmatrix}{\frac {\partial f}{\partial x_{1} } }(a)&\cdots &{\frac {\partial f}{\partial x_{n} } }(a)\end{bmatrix} }.$$ The linear approximation property of the total derivative implies that if $$\Delta x={\begin{bmatrix}\Delta x_{1}&\cdots &\Delta x_{n}\end{bmatrix} }^{\mathsf {T} }$$ is a small vector \(where the ${\mathsf {T} }$ denotes transpose, so that this vector is a column vector\), then $$f(a+\Delta x)-f(a)\approx Df_{a}\cdot \Delta x=\sum _{i=1}^{n}{\frac {\partial f}{\partial x_{i} } }(a)\cdot \Delta x_{i}.$$ Heuristically, this suggests that if $dx_{1},\ldots ,dx_{n}$ are [infinitesimal](infinitesimal.md) increments in the coordinate directions, then $$df_{a}=\sum _{i=1}^{n}{\frac {\partial f}{\partial x_{i} } }(a)\cdot dx_{i}.$$

## week 5 tutorial

- datetime: 2025-03-04T16:00:00+08:00/2025-03-04T16:50:00+08:00
- [week 5 lecture](#week%205%20lecture)
- [week 5 lecture 2](#week%205%20lecture%202)
- [implicit differentiation](../../../../general/implicit%20differentiation.md)
  - implicit differentiation / implicit functions ::@:: If you have a _explicit_ function in the form of $y = f(...)$, you can always write it in terms of $g(..., y) := f(...) - y = 0$. Then calculate its partial derivatives. Then partial derivatives between inputs of the original function $f$ can easily be obtained, by realizing that the inputs \(including $y$\) to the new function $g$ must change in a way such that $g$ remains zero, i.e. the directional derivatives of $g$ are always zero. <p> This method may seem cumbersome, but it would make sense if you consider implicit functions that have no or complicated explicit forms. <p> For example, $y = f(x)$ can be changed into $g(x, y) := f(x) - y = 0$. Then, $\frac {\partial y} {\partial x} = -\frac {\partial g} {\partial x} / \frac {\partial g} {\partial y}$ \(note the negative sign, which shows why you should not be "simplifying" partial derivatives\). This is because $\frac {\partial g} {\partial x} = - \frac {\partial y} {\partial x} \frac {\partial g} {\partial y}$, which is interpreted as: Assuming we only change $x$ and $y$. The rates of change between $x$ and $y$ must be such that the change in $g$ caused by the change in $x$ is neutralized by the change in $g$ caused by the change in $y$, in order to keep $g = 0$.

## week 5 lecture 2

- datetime: 2025-03-06T13:30:00+08:00/2025-03-06T14:50:00+08:00
- [gradient](../../../../general/gradient.md) ::@:: It of a scalar-valued differentiable function $f$ of several variables is the vector field (or vector-valued function) $\nabla f$ whose value at a point $p$ gives the direction and the rate of fastest increase.
  - gradient / intuition ::@:: Recall when a function is differentiable, the directional derivative along all directions _exist_, and is just the dot product of the direction vector with the gradient. The dot product, hence direction derivative, has the greatest value \(_steepest ascent_\) when it has the same direction as the gradient. Similarly, it has the least value \(_steepest descent_\) when it is antiparallel as the gradient. <p> However, do note that gradient also encodes the _rate_ of fastest increase, not only just the _direction_.
  - gradient / direction ::@:: The gradient is not necessarily a unit vector. You need to normalize it to get its direction. Also, be aware of zero gradient, for which the gradient has no direction.
  - gradient / level set ::@:: If $f$ is _differentiable_, then the gradient of $f$ at a point is either zero, or _perpendicular_ to the level set of $f$ at that point.
  - gradient / visualization ::@:: On a contour map, the gradient, visualized as arrows, is perpendicular to the contour lines. Its magnitude is higher when the contour lines are denser, and vice versa.
- level set
- [maximum and minimum](../../../../general/maximum%20and%20minimum.md) ::@:: It of a [function](../../../../general/function%20(mathematics).md) are, respectively, the greatest and least value taken by the function. Known generically as __extremum__, they may be defined either within a given [range](../../../../general/interval%20(mathematics).md) \(the _local_ or _relative_ extrema\) or on the entire [domain](../../../../general/domain%20of%20a%20function.md) \(the _global_ or _absolute_ extrema\) of a function.
  - maximum and minimum / definition ::@:: An extremum can be _local_ or _global_ \(implies local\). An extremum can be additionally said to be _weak_ \(usually not explicitly written out\) or _strict_.
    - maximum and minimum / definition / local ::@:: If the domain _X_ is a [metric space](../../../../general/metric%20space.md), then _f_ is said to have a __local__ \(or __relative__\) __maximum point__ at the point _x_<sup>∗</sup>, if there exists some _ε_ \> 0 such that _f_\(_x_<sup>∗</sup>\) ≥ _f_\(_x_\) for all _x_ in _X_ within distance _ε_ of _x_<sup>∗</sup>. Similarly, the function has a __local minimum point__ at _x_<sup>∗</sup>, if _f_\(_x_<sup>∗</sup>\) ≤ _f_\(_x_\) for all _x_ in _X_ within distance _ε_ of _x_<sup>∗</sup>. A similar definition can be used when _X_ is a [topological space](../../../../general/topological%20space.md), since the definition just given can be rephrased in terms of neighbourhoods. Mathematically, the given definition is written as follows: <p> &emsp; Let $(X,d_{X})$ be a metric space and function $f:X\to \mathbb {R}$. Then $x_{0}\in X$ is a local maximum point of function $f$ if $(\exists \varepsilon >0)$ such that $(\forall x\in X)\,d_{X}(x,x_{0})<\varepsilon \implies f(x_{0})\geq f(x)$. <p> The definition of local minimum point can also proceed similarly.
  - maximum and minimum / gradient ::@:: The gradient is zero. <p> Consider any line parametrization through the an extremum of a multivariate function. The parameterized function has an extremum when it passes through the extremum of the multivariate function, which has a derivative of zero. So the directional derivatives \(in particular, the partial derivatives\) are all zero.
- [critical point](../../../../general/critical%20point%20(mathematics).md) ::@:: It is the argument of a function where the function derivative is zero \(or undefined, as specified below\).
  - critical point / multivariate ::@:: It is a point where the gradient is zero or undefined.
  - critical point / interpretation ::@:: A critical point \(where the function is _differentiable_\) may be either a [local maximum](../../../../general/local%20maximum.md), a [local minimum](../../../../general/local%20minimum.md) or a [saddle point](../../../../general/saddle%20point.md). <p> In univariate calculus, saddle point is a [point](../../../../general/point%20(geometry).md) which is both a [stationary point](../../../../general/stationary%20point.md) and a [point of inflection](../../../../general/inflection%20point.md).
- [saddle point](../../../../general/saddle%20point.md) ::@:: It is a point on the surface of the graph of a function where the slopes \(derivatives\) in orthogonal directions are all zero \(a _critical point_\), but which is not a local extremum of the function.

## week 6 lecture

- datetime: 2025-03-11T13:30:00+08:00/2025-03-11T14:50:00+08:00
- [derivative test](../../../../general/derivative%20test.md) ::@:: It uses the derivatives of a function to locate the critical points of a function and determine whether each point is a local maximum, a local minimum, or a saddle point. Derivative tests can also give information about the concavity of a function.
  - derivative test / second-derivative test ::@:: After establishing the [critical points](../../../../general/critical%20point%20(mathematics).md) of a function, the _second-derivative test_ uses the value of the [second derivative](../../../../general/second%20derivative.md) at those points to determine whether such points are a local [maximum](../../../../general/maxima%20and%20minima.md) or a local minimum. If the function _f_ is twice-differentiable at a critical point _x_ \(i.e. a point where _f′_\(_x_\) = 0\), then: <p> - If $f''(x)<0$, then $f$ has a local maximum at $x$. <br/> - If $f''(x)>0$, then $f$ has a local minimum at $x$. <br/> - If $f''(x)=0$, the test is inconclusive. <p> In the last case, [Taylor's theorem](../../../../general/Taylor's%20theorem.md#Taylor's%20theorem%20in%20one%20real%20variable) may sometimes be used to determine the behavior of _f_ near _x_ using [higher derivatives](../../../../general/higher%20derivative.md#higher-order%20derivatives).
- [second partial derivative test](../../../../general/second%20partial%20derivative%20test.md) ::@:: It is a method in multivariable calculus used to determine if a critical point of a function is a local minimum, maximum or saddle point.
  - second partial derivative test / functions of two variables ::@:: Suppose that _f_\(_x_, _y_\) is a differentiable [real function](../../../../general/real%20function.md) of two variables whose second [partial derivatives](../../../../general/partial%20derivative.md) exist and are [continuous](../../../../general/continuous%20function.md). <p> Find its Hessian matrix \(Some definitions may have this matrix transposed, but the results are equivalent for a $C_2$ function.\). Check the signs of its determinant $D$ and its upper left element $f_{xx}$ evaluated at a _critical point_ \(its partial first derivatives are zero\) to see if that point is a local minimum, local maximum, saddle point, or inconclusive \(could be local minimum, local maximum, or saddle point\).
    - second partial derivative test / functions of two variables / detail ::@:: The [Hessian matrix](../../../../general/Hessian%20matrix.md) _H_ of _f_ is the 2 × 2 matrix of partial derivatives of _f_: $$H(x,y)={\begin{bmatrix}f_{xx}(x,y)&f_{xy}(x,y)\\f_{yx}(x,y)&f_{yy}(x,y)\end{bmatrix} }.$$ \(annotation: Some definitions may have this matrix transposed, but the results are equivalent for a $C_2$ function; __this course__: the above form is in the lecture notes\) Define _D_\(_x_, _y_\) to be the [determinant](../../../../general/determinant.md) $$D(x,y)=\det(H(x,y))=f_{xx}(x,y)f_{yy}(x,y)-\left(f_{xy}(x,y)\right)^{2}$$ of _H_. Finally, suppose that \(_a_, _b_\) is a critical point of _f_, that is, that _f_<sub>_x_</sub>\(_a_, _b_\) = _f_<sub>_y_</sub>\(_a_, _b_\) = 0. Then the second partial derivative test asserts the following: <p> 1. If _D_\(_a_, _b_\) \> 0 and _f<sub>xx</sub>_\(_a_, _b_\) \> 0 then \(_a_, _b_\) is a local minimum of _f_. <br/> 2. If _D_\(_a_, _b_\) \> 0 and _f<sub>xx</sub>_\(_a_, _b_\) \< 0 then \(_a_, _b_\) is a local maximum of _f_. <br/> 3. If _D_\(_a_, _b_\) \< 0 then \(_a_, _b_\) is a [saddle point](saddle%20point.md) of _f_. <br/> 4. If _D_\(_a_, _b_\) = 0 then the point \(_a_, _b_\) could be any of a minimum, maximum, or saddle point \(that is, the test is inconclusive\).
    - second partial derivative test / functions of two variables / corollary ::@:: A condition implicit in the statement of the test is that if $f_{xx}=0$ or $f_{yy}=0$, it must be the case that $D(a,b)\leq 0$, and therefore only cases 3 or 4 are possible.
  - second partial derivative test / functions of many variables ::@:: For a function _f_ of three or more variables, there is a generalization of the rule shown above. In this context, instead of examining the determinant of the Hessian matrix, one must look at the [eigenvalues](../../../../general/eigenvalues%20and%20eigenvectors.md) of the Hessian matrix at the critical point.
    - second partial derivative test / functions of many variables / detail ::@:: The following test can be applied at any critical point _a_ for which the Hessian matrix is [invertible](../../../../general/invertible%20matrix.md): <p> 1. If the Hessian is [positive definite](../../../../general/positive-definite%20matrix.md) \(equivalently, has all eigenvalues positive\) at _a_, then _f_ attains a local minimum at _a_. <br/> 2. If the Hessian is negative definite \(equivalently, has all eigenvalues negative\) at _a_, then _f_ attains a local maximum at _a_. <br/> 3. If the Hessian has both positive and negative eigenvalues then _a_ is a saddle point for _f_ \(and in fact this is true even if _a_ is degenerate\). <p> In those cases not listed above \(annotation: e.g. non-invertible Hessian\), the test is inconclusive.
    - second partial derivative test / functions of many variables / two variables ::@:: In the two variable case, $D(a,b)$ and $f_{xx}(a,b)$ are the principal [minors](../../../../general/minor%20(linear%20algebra).md) of the Hessian. The first two conditions listed above on the signs of these minors are the conditions for the positive or negative definiteness of the Hessian.
    - second partial derivative test / functions of many variables / Sylvester's criterion ::@:: For the general case of an arbitrary number _n_ of variables, there are _n_ sign conditions on the _n_ principal minors of the Hessian matrix that together are equivalent to positive or negative definiteness of the Hessian \([Sylvester's criterion](../../../../general/Sylvester's%20criterion.md)\): for a local minimum, all the principal minors need to be positive, while for a local maximum, the minors with an odd number of rows and columns need to be negative and the minors with an even number of rows and columns need to be positive.
  - second partial derivative test / intuition ::@:: The intuition is a bit hard to grasp. Below assumes functions of two variables \(of class $C_2$\) only. <p> If the Hessian determinant \> 0, then along all directions, the resulting univariate functions have the same convexity, so looking at $f_{xx}$ suffices to determine if it is a local maximum or local minimum. If the Hessian determinant \< 0, then the resulting univariate functions do not have the same convexity, so it is a saddle point.
  - second partial derivative test / proof ::@:: For functions of two variables \(of class $C_2$\), a proof is by considering the second derivatives of all resulting univariate functions through a critical point along all directions, and then apply the second derivative test. <p> Consider the second directional derivative of a function through a critical point, where the \(non-unit\) direction vector is $u = \langle a, b \rangle$, and assuming $f_{xx} \ne 0$ and $f_{yy} \ne 0$: $$\begin{aligned} (D_u)^2 f & = D_u(a f_x + b f_y) \\ & = a D_u(f_x) + b D_u(f_y) \\ & = a (a f_{xx} + b f_{xy}) + b (a f_{yx} + b f_{yy}) \\ & = a^2 f_{xx} + 2ab f_{xy} + b^2 f_{yy} \\ & = f_{xx} \left(a^2 + \frac {f_{xy} } {f_{xx} } 2ab \right) + b^2 f_{yy} \\ & = f_{xx} \left(a + \frac {f_{xy} } {f_{xx} } b \right)^2 - \frac {f_{xy}^2 } {f_{xx} } b^2 + b^2 f_{yy} \\ & = f_{xx} \left(a + \frac {f_{xy} } {f_{xx} } b \right)^2 + \frac {b^2} {f_{xx} } \left(f_{xx} f_{yy} - f_{xy}^2 \right) \,. \end{aligned}$$ The above derivation uses symmetry of second derivatives and completing the square. We can see if the determinant is positive, the sign of the above expression is controlled by $f_{xx}$ and always nonzero \(note that $a = b = 0$ is not allowed\). If the determinant is negative, the expression can be made positive, negative, or zero arbitrarily by adjusting $a$ and $b$. \(If the determinant is zero, we could not tell.\) <p> In the case where $f_{xx} = 0$ or $f_{yy} = 0$, the determinant must be non-positive. When $f_{xy} = f_{yx} \ne 0$, the determinant is negative. The previous expression is also valid for $f_{xx} \ne 0$ and $f_{yy} = 0$. The following expression is valid for $f_{xx} = 0$ and $f_{yy} \ne 0$: $$(D_u)^2 f = f_{yy} \left(b + \frac {f_{xy} } {f_{yy} } a \right)^2 + \frac {a^2} {f_{yy} } \left(f_{xx} f_{yy} - f_{xy}^2 \right) \,.$$ Finally, the following expression is valid for $f_{xx} = f_{yy} = 0$: $$(D_u)^2 f = 2ab f_{xy} \,,$$ where we see if $f_{xy} \ne 0$ \(determinant is negative\), then the expression can be made positive, negative, or zero arbitrarily. When $f_{xy} = 0$ \(determinant is zero\), then the test is inconclusive.
- [Sylvester's criterion](../../../../general/Sylvester's%20criterion.md): \(untaught\) It is a [necessary and sufficient](../../../../general/necessary%20and%20sufficient%20condition.md) criterion to determine whether a [Hermitian matrix](../../../../general/Hermitian%20matrix.md) is [positive-definite](../../../../general/definite%20matrix.md).
- maximum and minimum
  - maximum and minimum / definition
    - maximum and minimum / definition / global ::@:: A real-valued [function](function%20(mathematics).md) _f_ defined on a [domain](../../../../general/domain%20of%20a%20function.md) _X_ has a __global__ \(or __absolute__\) __maximum point__ at _x_<sup>∗</sup>, if _f_\(_x_<sup>∗</sup>\) ≥ _f_\(_x_\) for all _x_ in _X_. Similarly, the function has a __global__ \(or __absolute__\) __minimum point__ at _x_<sup>∗</sup>, if _f_\(_x_<sup>∗</sup>\) ≤ _f_\(_x_\) for all _x_ in _X_.
- [extreme value theorem](../../../../general/extreme%20value%20theorem.md) ::@:: It states that if a real-valued [function](../../../../general/function%20(mathematics).md) $f$ is [continuous](../../../../general/continuous%20function.md) on the [closed](../../../../general/bounded%20interval.md#classification%20of%20intervals) and [bounded](../../../../general/bounded%20set.md) interval $[a,b]$, then $f$ must attain a [maximum](../../../../general/maximum.md) and a [minimum](../../../../general/minimum.md), each at least once.
  - extreme value theorem / multivariate ::@:: For Euclidean spaces $\mathbb R^n$, the above readily generalizes to closed and bounded subsets of the space. Here, _bounded_ means every point in the subset is at most some finite distance away from the origin. _Closed_ means the subset contains all of its limit points. A _limit point_ of a subset is a point \(that may not be in the subset\) that has arbitrary small nonzero distance from any point in the subset. <p> For [metric spaces](../../../../general/metric%20spaces.md) and general [topological spaces](../../../../general/topological%20spaces.md), the appropriate generalization of a closed bounded interval is a [compact set](../../../../general/compact%20space.md).

## week 6 tutorial

- datetime: 2025-03-11T16:00:00+08:00/2025-03-11T16:50:00+08:00
- [week 6 lecture](#week%206%20lecture)
- [week 6 lecture 2](#week%206%20lecture%202)
- [square pyramidal number](../../../../general/square%20pyramidal%20number.md) ::@:: It is a natural number that counts the stacked spheres in a pyramid with a square base.
  - square pyramidal number / formula ::@:: The total number $P_{n}$ of spheres can be counted as the sum of the number of spheres in each square, $$P_{n}=\sum _{k=1}^{n}k^{2}=1+4+9+\cdots +n^{2},$$ and this [summation](../../../../general/summation.md) can be solved to give a [cubic polynomial](../../../../general/cubic%20polynomial.md), which can be written in several equivalent ways: $$P_{n}={\frac {n(n+1)(2n+1)}{6} }={\frac {2n^{3}+3n^{2}+n}{6} }={\frac {n^{3} }{3} }+{\frac {n^{2} }{2} }+{\frac {n}{6} }.$$ This equation for a sum of squares is a special case of [Faulhaber's formula](../../../../general/Faulhaber's%20formula.md) for sums of powers, and may be proved by [mathematical induction](../../../../general/mathematical%20induction.md).

## week 6 lecture 2

- datetime: 2025-03-13T13:30:00+08:00/2025-03-13T14:50:00+08:00
- maximum and minimum
  - maximum and minimum / search ::@:: Finding global maxima and minima is the goal of [mathematical optimization](../../../../general/mathematical%20optimization.md). If a function is continuous on a closed interval, then by the [extreme value theorem](../../../../general/extreme%20value%20theorem.md), global maxima and minima exist. Furthermore, a global maximum \(or minimum\) either must be a local maximum \(or minimum\) in the interior of the domain, or must lie on the boundary of the domain. So a method of finding a global maximum \(or minimum\) is to look at all the local maxima \(or minima\) in the interior, and also look at the maxima \(or minima\) of the points on the boundary, and take the greatest \(or least\) one.
    - maximum and minium / search / detail ::@:: Note that the boundary of a domain may itself has a boundary. In general, for a _n_-dimensional subset, its boundary is a _n_–1-dimensional subset. <P> For example, a rectangle has 4 edges. Using 4 different parameterization to represent its 4 edge, each parameterization represents an edge. Each edge itself has a boundary, which are the rectangle vertices.
- [Lagrange multiplier](../../../../general/Lagrange%20multiplier.md) ::@:: It is a strategy for finding the local [maxima and minima](../../../../general/maxima%20and%20minima.md) of a [function](../../../../general/function%20(mathematics).md) subject to [equation constraints](../../../../general/constraint%20(mathematics).md) \(i.e., subject to the condition that one or more [equations](../../../../general/equation.md) have to be satisfied exactly by the chosen values of the [variables](../../../../general/variable%20(mathematics).md)\). It is named after the mathematician [Joseph-Louis Lagrange](../../../../general/Joseph-Louis%20Lagrange.md).
  - Lagrange multiplier / rationale ::@:: The basic idea is to convert a constrained problem into a form such that the [derivative test](../../../../general/derivative%20test.md) of an unconstrained problem can still be applied. <p> Consider a multivariate function of two variables $f(x, y)$ and a single constraint function $g(x, y) = 0$ \(with _constraint qualification_: its gradient is nonzero\). Then, the constraint is a curve on the graph surface of the multivariate function. A local extremum on the constraint curve must have the curve at that point parallel to the level sets, otherwise, one could "walk" along the constraint curve to get higher or lower values. This also means the constraint curve is perpendicular to the gradient of the multivariate function. And the constraint curve is perpendicular to its own gradient. So the gradient of the multivariate function can be expressed as a linear combination of the gradient of the constraint. Thus we have $$D f(x, y) = \lambda D g(x, y) \,,$$ for some arbitrary constant $\lambda$ subject to $g(x, y)$.
  - Lagrange multiplier / summary ::@:: In the general case, the Lagrangian is defined as $${\mathcal {L} }(x,\lambda )\equiv f(x)+\langle \lambda ,g(x)\rangle$$ for \(annotation: $C^1$, having _continuous first derivatives_\) functions $f,g$; the notation $\langle \cdot ,\cdot \rangle$ denotes an [inner product](../../../../general/inner%20product.md). The value $\lambda$ is called the Lagrange multiplier. <p> In simple cases, where the inner product is defined as the [dot product](../../../../general/dot%20product.md), the Lagrangian is $${\mathcal {L} }(x,\lambda )\equiv f(x)+\lambda \cdot g(x)$$ <p> The method can be summarized as follows: in order to find the maximum or minimum of a function $f$ subject to the equality constraint $g(x)=0$, find the [stationary points](../../../../general/stationary%20point.md) of ${\mathcal {L} }$ considered as a function of $x$ and the Lagrange multiplier $\lambda ~$. This means that all [partial derivatives](../../../../general/partial%20derivative.md) should be zero, including the partial derivative with respect to $\lambda ~$. <p> &emsp; ${\frac {\partial {\mathcal {L} } }{\partial x} }=0$ and ${\frac {\ \partial {\mathcal {L} }\ }{\partial \lambda } }=0\ ;$ <p> or equivalently <p> &emsp; ${\frac {\partial f(x)}{\partial x} }+\lambda \cdot {\frac {\partial g(x)}{\partial x} }=0$ and $g(x)=0~$.

## week 7 lecture

- datetime: 2025-03-18T13:30:00+08:00/2025-03-18T14:50:00+08:00, PT1H20M
- topic: midterm examination review

## week 7 tutorial

- datetime: 2025-03-18T16:00:00+08:00/2025-03-18T16:50:00+08:00, PT50M
- [week 7 lecture](#week%207%20lecture)
- [week 7 lecture 2](#week%207%20lecture%202)

## week 7 lecture 2

- datetime: 2025-03-20T13:30:00+08:00/2025-03-20T14:50:00+08:00, PT1H20M
- topic: midterm examination
- [§ midterm examination](#midterm%20examination)

## midterm examination

- datetime: 2025-03-20T13:45:00+08:00/2025-03-20T14:45:00+08:00, PT1H
- venue: Lecture Theater A, Academic Building
- format
  - calculator: no
  - cheatsheet: no
  - referencing: closed book, closed notes
  - provided: \(none\)
  - questions: long questions ×9
- grades: 84/100 → 84/100
  - statistics
    - timestamps: 2025-04-08T11+08:00 → 2025-04-18T16:51:40+08:00
    - count: ? → ?
    - mean: 56.95 \(provided: 57\) → 58.02
    - standard deviation: ? \(provided: 25.3\) → ?
    - low: 0 \(provided: 14\) → 0
    - lower quartile: 36 → 36.75
    - median: 57 \(provided: 57\) → 58.5
    - upper quartile: 78 → 80
    - high: 100 \(provided: 100\) → 100
    - distribution: ? → ?
    - data: ? → ?
- report
  - time limit ::@:: Time was too short. Or that, you cannot hesitate on any question. <p> The instructor had not even mentioned the time limit in examination announcements.
  - deriving perpendicular distance to a plane \(–1\) ::@:: I don't know? ?\_?
  - existence of limit \(–10\) ::@:: Due to time limit and , I could not see $$f(x, y) = \frac {x^{2024} y^{2025} } {x^{4048} + y^{4050} }$$ has no limits. <p> Sometimes, substitutions like $a = x^{2024}$ and $b = y^{2025}$ are very helpful for seeing the solution. Also see [questions § midterm examination](questions.md#midterm%20examination).
  - Lagrange multiplier \(–5\) ::@:: The steps were fine. But time was not enough to find the final solution, as it involves solving non-linear system of equations.
- check
  - datetime: 2025-04-11T16:00:00+08:00/2025-04-11T17:00:00+08:00, PT1H
  - venue: Room 2503, Academic Building
  - report
    - \(none\)

> __<big><big>Midterm exam</big></big>__
>
> Dear all,
>
> <span style="color: #ba372a;">The following message concerns the midterm exam arrangement. Please read it carefully in its entirety.</span>
>
> The midterm exam will take place during the usual class time \(1:30pm\) this Thursday, Mar 20, 2025. It will take place at
>
> - __LTA__ for non-SEN students \(i.e., most students\)
> - __Room 5506__ for SEN students. \(I've added all the SEN students I'm aware of to a special Canvas group. If you're not in that group, please ask your SEN advisor to notify me ASAP.\)
>
> You will need to bring the following items to the exam: your student ID, pen\(s\), and a ruler. You need to present the student ID to be allowed to do the exam. For the LTA venue, there will be a seating plan. Moreover, you are only allowed to have those items listed above at your desk when you work on the exam \(the rest will have to be left somewhere else in the lecture hall\).
>
> It will cover everything we've discussed in the course so far. You will be responsible for remembering, understanding, and knowing how to apply all the concepts presented in class so far. As indicated in the course syllabus, no make-up exam is allowed, under any circumstance. If you have a valid reason to miss the exam, please let me know as soon as possible and before the actual exam date. Please consult the course syllabus for more details.
>
> As indicated in the syllabus, no materials \(books/notes/cheat sheets\) are allowed during the exam. The use of calculators, phones, or any electronic devices is also not allowed. Moreover, no discussion with fellow students is allowed during the exam.
>
> Please let me know if you have any questions.
>
> Good luck!
>
> Best,
>
> \[redacted\]

## week 8 lecture

- datetime: 2025-03-25T13:30:00+08:00/2025-03-25T14:50:00+08:00, PT1H20M
- Lagrange multiplier
  - Lagrange multiplier / techniques ::@:: To solve problems with Lagrange multipliers, you can solve the problem directly. For constraints with both interiors and boundaries, separate them into interiors and boundaries and treat these two separately. <p> Alternative ways to speed up computation include <p> - Re-parameterize the constraints to reduce the number of variables, e.g. turning the constraint $x^2 + y^2 = 1$ into $\gamma(t) = (\cos t, \sin t)$ and rewrite the value function in terms of $t$. <br/> - Substituting constraints into the value function directly to reduce the number of constraints, e.g. substituting $x^2 + y^2 = 1$ into $2x^2 + 3y^2$ to get $2 + y^2$. Each substitution reduces the number of variables in the value function by 1.
- [Riemann integral](../../../../general/Riemann%20integral.md) ::@:: It was the first rigorous definition of the integral of a function on an interval. <p> When $f$ is Riemann integrable, the limit below exists and defines the integral: $$\int_a^b \! f(x) \,\mathrm dx = \lim_{n \to +\infty} \sum_{k = 1}^n f(x_i^*) \Delta x_i \,.$$ The interval $[a, b]$ is split into $n$ partitions. Each partition has a length of $\Delta x_i$ and is tagged by a number $x_i^*$ in the partition.
  - Riemann integral / practice ::@:: In practice, the definition is not used directly. Various tools and results, proved using the definition. are used to calculate integrals.
- [multiple integral](../../../../general/multiple%20integral.md) ::@:: It is a [definite integral](../../../../general/definite%20integral.md) of a [function of several real variables](../../../../general/function%20of%20several%20real%20variables.md), for instance, _f_\(_x_, _y_\) or _f_\(_x_, _y_, _z_\).
  - multiple integral / mathematical definition ::@:: domain definition, partitioning, Riemann sum, diameter, existence, notation, extension
    - multiple integral / mathematical definition / domain definition ::@:: For _n_ \> 1, consider a so-called "half-open" _n_-dimensional [hyperrectangular](../../../../general/hyperrectangle.md) domain _T_, defined as $$T=[a_{1},b_{1})\times [a_{2},b_{2})\times \cdots \times [a_{n},b_{n})\subseteq \mathbb {R} ^{n} \,.$$
    - multiple integral / mathematical definition / partitioning ::@:: [Partition](../../../../general/partition%20(set%20theory).md) each interval \[_a_<sub>_j_</sub>, _b_<sub>_j_</sub>\) into a finite family _I<sub>j</sub>_ of non-overlapping subintervals _i<sub>j<sub>α</sub></sub>_, with each subinterval closed at the left end, and open at the right end. <p> Then the finite family of subrectangles _C_ given by $$C=I_{1}\times I_{2}\times \cdots \times I_{n}$$ is a [partition](../../../../partition%20(set%20theory).md) of _T_; that is, the subrectangles _C<sub>k</sub>_ are non-overlapping and their union is _T_.
    - multiple integral / mathematical definition / Riemann sum ::@:: Let _f_ : _T_ → __R__ be a function defined on _T_. Consider a partition _C_ of _T_ as defined above, such that _C_ is a family of _m_ subrectangles _C<sub>m</sub>_ and $$T=C_{1}\cup C_{2}\cup \cdots \cup C_{m}$$ We can approximate the total \(_n_ + 1\)-dimensional volume bounded below by the _n_-dimensional hyperrectangle _T_ and above by the _n_-dimensional graph of _f_ with the following [Riemann sum](../../../../general/Riemann%20sum.md): $$\sum _{k=1}^{m}f(P_{k})\,\operatorname {m} (C_{k})$$ where _P<sub>k</sub>_ is a point in _C<sub>k</sub>_ and m\(_C_<sub>_k_</sub>\) is the product of the lengths of the intervals whose Cartesian product is _C<sub>k</sub>_, also known as the measure of _C<sub>k</sub>_.
    - multiple integral / mathematical definition / diameter ::@:: The __diameter__ of a subrectangle _C<sub>k</sub>_ is the largest of the lengths of the intervals whose [Cartesian product](../../../../general/Cartesian%20product.md) is _C<sub>k</sub>_. The diameter of a given partition of _T_ is defined as the largest of the diameters of the subrectangles in the partition. Intuitively, as the diameter of the partition _C_ is restricted smaller and smaller, the number of subrectangles _m_ gets larger, and the measure m\(_C_<sub>_k_</sub>\) of each subrectangle grows smaller.
    - multiple integral / mathematical definition / existence ::@:: Intuitively, as the diameter of the partition _C_ is restricted smaller and smaller, the number of subrectangles _m_ gets larger, and the measure m\(_C_<sub>_k_</sub>\) of each subrectangle grows smaller. The function _f_ is said to be __Riemann integrable__ if the [limit](../../../../general/limit%20(mathematics).md) $$S=\lim _{\delta \to 0}\sum _{k=1}^{m}f(P_{k})\,\operatorname {m} (C_{k})$$ exists, where the limit is taken over all possible partitions of _T_ of diameter at most _δ_.
    - multiple integral / mathematical definition / notation ::@:: If _f_ is Riemann integrable, _S_ is called the __Riemann integral__ of _f_ over _T_ and is denoted $$\int \cdots \int _{T}\,f(x_{1},x_{2},\ldots ,x_{n})\,dx_{1}\!\cdots dx_{n} \,.$$ Frequently this notation is abbreviated as $$\int _{T}\!f(\mathbf {x} )\,d^{n}\mathbf {x} \,,$$ where __x__ represents the _n_-tuple \(_x_<sub>1</sub>, ..., _x<sub>n</sub>_\) and _d_<sup>_n_</sup>__x__ is the _n_-dimensional volume [differential](../../../../general/differential%20(infinitesimal).md).
    - multiple integral / mathematical definition / extension ::@:: The Riemann integral of a function defined over an arbitrary bounded n-dimensional set can be defined by extending that function to a function defined over a half-open rectangle whose values are zero outside the domain of the original function. Then the integral of the original function over the original domain is defined to be the integral of the extended function over its rectangular domain, if it exists.
- [Fubini's theorem](../../../../general/Fubini's%20theorem.md) ::@:: It characterizes the conditions under which it is possible to compute a double integral by using an iterated integral.
  - Fubini's theorem / statement ::@:: The theorem states that if a function is [Lebesgue integrable](../../../../general/Lebesgue%20integral.md) on a rectangle $X\times Y$, then one can evaluate the double integral as an iterated integral: $$\,\iint \limits _{X\times Y}f(x,y)\,{\text{d} }(x,y)=\int _{X}\left(\int _{Y}f(x,y)\,{\text{d} }y\right){\text{d} }x=\int _{Y}\left(\int _{X}f(x,y)\,{\text{d} }x\right){\text{d} }y.$$ This formula is generally not true for the [Riemann integral](../../../../general/Riemann%20integral.md), but it is true if the function is continuous on the rectangle. In [multivariable calculus](../../../../general/multivariable%20calculus.md), this weaker result is sometimes also called Fubini's theorem. <p> \(__this course__: We introduce the weaker result.\)
- [iterated integral](../../../../general/iterated%20integral.md) ::@:: It is the result of applying [integrals](../../../../general/integral.md) to a [function](../../../../general/function%20(mathematics).md) of [more than one variable](../../../../general/function%20of%20several%20real%20variables.md) \(for example $f(x,y)$ or $f(x,y,z)$\) in such a way that each of the integrals considers some of the variables as given [constants](../../../../general/constant%20(mathematics).md).
  - iterated integral / techniques ::@:: Assuming Fubini's theorem applies. Sometimes, an integration order is more convenient to compute than others, e.g. by creating new terms or having better integration bounds. Sometimes, it makes the computation _much easier_, e.g. consider $$\int_0^1 \! \int_0^y \! \sin\left(y^2\right) \,\mathrm dx \,\mathrm dy \,.$$ <p> Other techniques include change of variables, constant functions, linear decomposition, normal domains, symmetry, etc.
- multiple integral
  - multiple integral / measure ::@:: Integrating over the indicator function of a set \(1 if in the set, otherwise 0\) gives its measure. If 1D, it is length; if 2D, area; if 3D, volume.
  - multiple integral / average value ::@:: The average value is simply the multiple integral divided by the _measure_ of the domain \(considered as a set\).

## week 8 tutorial

- datetime: 2025-03-25T16:00:00+08:00/2025-03-25T16:50:00+08:00, PT50M
- [week 8 lecture](#week%208%20lecture)
- [week 8 lecture 2](#week%208%20lecture%202)
- [improper integral](../../../../general/improper%20integral.md) ::@:: It is an extension of the notion of a definite integral to cases that violate the usual assumptions for that kind of integral. In the context of Riemann integrals \(or, equivalently, Darboux integrals\), this typically involves unboundedness, either of the set over which the integral is taken or of the integrand \(the function being integrated\), or both. It may also involve bounded but not closed sets or bounded but not continuous functions.
  - improper integral / nature ::@:: While an improper integral is typically written symbolically just like a standard definite integral, it actually represents a limit of a definite integral or a sum of such limits; thus improper integrals are said to converge or diverge. If a regular definite integral \(which may retronymically be called a proper integral\) is worked out as if it is improper, the same answer will result.

## week 8 lecture 2

- datetime: 2025-03-27T13:30:00+08:00/2025-03-27T14:50:00+08:00, PT1H20M
- multiple integral
  - multiple integral / integration bounds ::@:: The integration bounds of an inner integral can be functions of dummy variables of outer integrals.
  - multiple integral / normal domains on __R__<sup>2</sup> ::@:: type I: $y$ is bounded by two functions <br/> type II: $x$ is bounded by two functions
  - multiple integral / inclusion–exclusion principle ::@:: The inclusion–exclusion principle may be used to calculate the multiple integral over a complicated domain. <p> In particular, multiple integral over a domain consisting of two sets with no overlap \(except for their boundaries; more generally, the overlap must have measure zero\) is the sum of two multiple integrals each over each of the two sets.
  - multiple integral / bounded function ::@:: Given a function bounded from above and below within the integration domain $A$: $$L \le f(\mathbf x) \le U \qquad \forall \mathbf x \in A \,,$$ then the integral over the integration domain is in between $L \mu(A)$ and $U \mu(A)$ \(inclusive\), where $\mu(A)$ is the measure \(length, area, volume, ...\) of $A$.

## week 9 lecture

- datetime: 2025-04-01T13:30:00+08:00/2025-04-01T14:50:00+08:00, PT1H20M
- status: unscheduled, midterm break

## week 9 tutorial

- datetime: 2025-04-01T16:00:00+08:00/2025-04-01T16:50:00+08:00, PT50M
- status: unscheduled, midterm break
- [week 9 lecture](#week%209%20lecture)
- [week 9 lecture 2](#week%209%20lecture%202)

## week 9 lecture 2

- datetime: 2025-04-03T13:30:00+08:00/2025-04-03T14:50:00+08:00, PT1H20M
- status: unscheduled, midterm break

## week 10 lecture

- datetime: 2025-04-08T13:30:00+08:00/2025-04-08T14:50:00+08:00, PT1H20M
- multiple integral
  - multiple integral / change of variables ::@:: The limits of integration are often not easily interchangeable \(without normality or with complex formulae to integrate\). One makes a [change of variables](../../../../general/change%20of%20variables.md) to rewrite the integral in a more "comfortable" region, which can be described in simpler formulae. To do so, the function must be adapted to the new coordinates.
- [integration by substitution](../../../../general/integration%20of%20substitution.md) ::@:: It is a method for evaluating integrals and antiderivatives. It is the counterpart to the chain rule for differentiation, and can loosely be thought of as using the chain rule "backwards."
  - integration by substitution / statement ::@:: Let $g:[a,b]\to I$ be a [differentiable function](../../../../general/differentiable%20function.md) with a [continuous](../../../../general/continuous%20function.md) derivative, where $I\subset \mathbb {R}$ is an [interval](../../../../general/interval%20(mathematics).md). Suppose that $f:I\to \mathbb {R}$ is a [continuous function](../../../../general/continuous%20function.md). Then: $$\int _{a}^{b}f(g(x))\cdot g'(x)\,dx=\int _{g(a)}^{g(b)}f(u)\ du.$$ In Leibniz notation, the substitution $u=g(x)$ yields: $${\frac {du}{dx} }=g'(x).$$ Working heuristically with [infinitesimals](../../../../general/infinitesimal.md) yields the equation $$du=g'(x)\,dx,$$ which suggests the substitution formula above.
  - integration by substitution / multiple variables ::@:: __Theorem__—Let _U_ be an open set in __R__<sup>_n_</sup> and _φ_ : _U_ → __R__<sup>_n_</sup> an [injective](../../../../general/injective%20function.md) differentiable function with continuous partial derivatives, the Jacobian of which is nonzero for every _x_ in _U_. Then for any real-valued, compactly supported, continuous function _f_, with support contained in _φ_\(_U_\): $$\int _{\varphi (U)}f(\mathbf {v} )\,d\mathbf {v} =\int _{U}f(\varphi (\mathbf {u} ))\,\,\left|\!\det(D\varphi )(\mathbf {u} )\right|\,d\mathbf {u} .$$
    - integration by substitution / multiple variables / conditions ::@:: The conditions on the theorem can be weakened in various ways. First, the requirement that _φ_ be continuously differentiable can be replaced by the weaker assumption that _φ_ be merely differentiable and have a continuous inverse. This is guaranteed to hold if _φ_ is continuously differentiable by the [inverse function theorem](../../../../general/inverse%20function%20theorem.md).
    - integration by substitution / multiple variables / conditions ::@:: Intuitively, the absolute value of the determinant of a matrix equals the volume of the parallelotope spanned by its columns or rows. <p> In the above equation, $\mathrm d\mathbf u$ can be considered as an infinitesimal change in the new coordinates. The Jacobian determinant $\lvert \det(D \varphi)(\mathbf u) \rvert$ gives the ratio of the volume of the parallelotope in the old coordinates over that in the new coordinates, given an infinitesimal change $\mathbf d\mathbf u$ in the new coordinates.
- Jacobian matrix
  - Jacobian matrix / note ::@:: \(__this course__: Seems like the definition for this course is transposed. For calculating determinant, this is fine, since the determinant remains the same after a transpose.\)
- [polar coordinate system](../../../../general/polar%20coordinate%20system.md) ::@:: It specifies a given [point](../../../../general/point%20(mathematics).md) in a [plane](../../../../general/plane%20(mathematics).md) by using a distance and an angle as its two [coordinates](../../../../general/coordinate%20system.md). These are <p> - the point's distance from a reference point called the _pole_, and <br/> - the point's direction from the pole relative to the direction of the _polar axis_, a [ray](../../../../general/ray%20(geometry).md#ray) drawn from the pole.
  - polar coordinate system / notation ::@:: $$(r, \theta)$$
  - polar coordinate system / redundancies ::@:: Unlike Cartesian coordinates, where a point can be represented by exactly one set of coordinates, in this coordinate system, a point can be represented by many sets of coordinates.
  - polar coordinate system / conversion between Cartesian coordinates ::@:: The polar coordinates _r_ and _φ_ can be converted to the Cartesian coordinates _x_ and _y_ by using the [trigonometric functions](../../../../general/trigonometric%20function.md) sine and cosine: $${\begin{aligned}x&=r\cos \varphi ,\\y&=r\sin \varphi .\end{aligned} }$$ The Cartesian coordinates _x_ and _y_ can be converted to polar coordinates _r_ and _φ_ with _r_ ≥ 0 and _φ_ in the interval \(−<!-- markdown separator -->_π_, _π_\] by: $${\begin{aligned}r&={\sqrt {x^{2}+y^{2} } }=\operatorname {hypot} (x,y)\\\varphi &=\operatorname {atan2} (y,x),\end{aligned} }$$ where hypot is the [Pythagorean sum](../../../../general/Pythagorean%20addition.md) and [atan2](../../../../general/atan2.md) is a common variation on the [arctangent](../../../../general/arctangent.md) function.
    - polar coordinate system / conversion between Cartesian coordinates / atan2 ::@:: [atan2](../../../../general/atan2.md) is a common variation on the [arctangent](../../../../general/arctangent.md) function defined as $$\operatorname {atan2} (y,x)={\begin{cases}\arctan \left({\frac {y}{x} }\right)&{\text{if } }x>0\\\arctan \left({\frac {y}{x} }\right)+\pi &{\text{if } }x<0{\text{ and } }y\geq 0\\\arctan \left({\frac {y}{x} }\right)-\pi &{\text{if } }x<0{\text{ and } }y<0\\{\frac {\pi }{2} }&{\text{if } }x=0{\text{ and } }y>0\\-{\frac {\pi }{2} }&{\text{if } }x=0{\text{ and } }y<0\\{\text{undefined} }&{\text{if } }x=0{\text{ and } }y=0.\end{cases} }$$
  - polar coordinate system / polar equation ::@:: The equation defining a [plane curve](../../../../general/plane%20curve.md) expressed in polar coordinates is known as a _polar equation_. In many cases, such an equation can simply be specified by defining _r_ as a [function](../../../../general/function%20(mathematics).md) of _φ_. The resulting curve then consists of points of the form \(_r_\(_φ_\), _φ_\) and can be regarded as the [graph](../../../../general/graph%20of%20a%20function.md) of the polar function _r_. Note that, in contrast to Cartesian coordinates, the independent variable _φ_ is the _second_ entry in the ordered pair.
  - polar coordinate system / integral calculus ::@:: Using [Cartesian coordinates](../../../../general/Cartesian%20coordinates.md), an infinitesimal area element can be calculated as _dA_ = _dx_ _dy_. The [substitution rule for multiple integrals](../../../../general/integration%20by%20substitution.md#substitution%20for%20multiple%20variables) states that, when using other coordinates, the [Jacobian](../../../../general/Jacobian%20matrix%20and%20determinant.md) determinant of the coordinate conversion formula has to be considered: $$J=\det {\frac {\partial (x,y)}{\partial (r,\varphi )} }={\begin{vmatrix}{\frac {\partial x}{\partial r} }&{\frac {\partial x}{\partial \varphi } }\\[2pt]{\frac {\partial y}{\partial r} }&{\frac {\partial y}{\partial \varphi } }\end{vmatrix} }={\begin{vmatrix}\cos \varphi &-r\sin \varphi \\\sin \varphi &r\cos \varphi \end{vmatrix} }=r\cos ^{2}\varphi +r\sin ^{2}\varphi =r.$$ Hence, an area element in polar coordinates can be written as $$dA=dx\,dy\ =J\,dr\,d\varphi =r\,dr\,d\varphi .$$ Now, a function, that is given in polar coordinates, can be integrated as follows \(annotation: the absolute sign enclosing $r$ is dropped, since $r \ge 0$\): $$\iint _{R}f(x,y)\,dA=\int _{a}^{b}\int _{0}^{r(\varphi )}f(r,\varphi )\,r\,dr\,d\varphi .$$ Here, _R_ is the same region as above, namely, the region enclosed by a curve _r_\(_φ_\) and the rays _φ_ = _a_ and _φ_ = _b_. The formula for the area of _R_ is retrieved by taking _f_ identically equal to 1.
    - polar coordinate system / integral calculus / intuition ::@:: Consider a very very small slice of the infinitesimal area $\mathrm dr \, \mathrm d\theta$. Changing $r$, we see the area is proportional to $r$. Changing $\theta$, we see the area remains the same. So the factor is $r$.

## week 10 tutorial

- datetime: 2025-04-08T16:00:00+08:00/2025-04-08T16:50:00+08:00, PT50M
- [week 10 lecture](#week%2010%20lecture)
- [week 10 lecture 2](#week%2010%20lecture%202)
- Fubini's theorem
  - Fubini's theorem / note ::@:: When changing integration order, beware of functions in the integration boundaries. It is best to visualize before deciding on the new integration boundaries after changing the integration order.
- [cylindrical coordinate system](../../../../general/cylindrical%20coordinate%20system.md) ::@:: It is a three-dimensional coordinate system that specifies point positions around a main axis \(a chosen directed line\) and an auxiliary axis \(a reference ray\).
  - cylindrical coordinate system / convention ::@:: The notation for cylindrical coordinates is not uniform. The [ISO](../../../../general/International%20Organization%20for%20Standardization.md) standard [31-11](../../../../general/ISO%2031-11.md) recommends \(_ρ_, _φ_, _z_\), where _ρ_ is the radial coordinate, _φ_ the azimuth, and _z_ the height. However, the radius is also often denoted _r_ or _s_, the azimuth by _θ_ or _t_, and the third coordinate by _h_ or \(if the cylindrical axis is considered horizontal\) _x_, or any context-specific letter. <p> \(__this course__: $$(r, \theta, z) \,,$$ where $\theta$ is the _azimuth_ \(counterclockwise\)\)
    - cylindrical coordinate system / convention / integration order ::@:: \(__this course__: $$\int_0^Z \! \int_0^{2\pi} \! \int_0^R \! f \, r \, \mathrm dr \,\mathrm d\theta \,\mathrm dz \,,$$ but you can change the integration order as needed using Fubini's theorem\)
- [spherical coordinate system](../../../../general/spherical%20coordinate%20system.md) ::@:: It specifies a given point in three-dimensional space by using a distance and two angles as its three coordinates.
  - spherical coordinate system / convention ::@:: Several different conventions exist for representing spherical coordinates and prescribing the naming order of their symbols. The 3-tuple number set $(r,\theta ,\varphi )$ denotes radial distance, the polar angle—"inclination", or as the alternative, "elevation"—and the azimuthal angle. It is the common practice within the physics convention, as specified by [ISO](../../../../general/International%20Organization%20for%20Standardization.md) standard [80000-2:2019](../../../../general/ISO%2080000-2.md), and earlier in [ISO 31-11](../../../../general/ISO%2031-11.md) \(1992\). <p> However, some authors \(including mathematicians\) use the symbol _ρ_ \(rho\) for radius, or radial distance, _φ_ for inclination \(or elevation\) and _θ_ for azimuth—while others keep the use of _r_ for the radius; all which "provides a logical extension of the usual polar coordinates notation". As to order, some authors list the azimuth _before_ the inclination \(or the elevation\) angle. Some combinations of these choices result in a [left-handed](../../../../general/right-hand%20rule.md) coordinate system. The standard "physics convention" 3-tuple set $(r,\theta ,\varphi )$ conflicts with the usual notation for two-dimensional [polar coordinates](../../../../general/polar%20coordinate%20system.md) and three-dimensional [cylindrical coordinates](../../../../general/cylindrical%20coordinate%20system.md), where _θ_ is often used for the azimuth. <p> \(__this course__: $$(\rho, \theta, \varphi) \,,$$ where $\theta$ is the _azimuth_ \(counterclockwise\) and $\varphi$ is the _polar_ angle \(from the _positive_ _z_-axis\)\)
    - spherical coordinate system / convention / integration order ::@:: \(__this course__: $$\int_0^\pi \! \int_0^{2\pi} \! \int_0^R \! f \, \rho^2 \sin \varphi \, \mathrm d\rho \,\mathrm d\theta \,\mathrm d\varphi \,,$$ but you can change the integration order as needed using Fubini's theorem\)

## week 10 lecture 2

- datetime: 2025-04-10T13:30:00+08:00/2025-04-10T14:50:00+08:00, PT1H20M
- improper integral
- [Gaussian integral](../../../../general/Gaussian%20integral.md) ::@:: It is the integral of the [Gaussian function](../../../../general/Gaussian%20function.md) $f(x)=e^{-x^{2} }$ over the entire real line. Named after the German mathematician [Carl Friedrich Gauss](../../../../general/Carl%20Friedrich%20Gauss.md), the integral is $$\int _{-\infty }^{\infty }e^{-x^{2} }\,dx={\sqrt {\pi } }.$$
  - Gaussian integral / polar coordinates ::@:: $$\begin{aligned} & \phantom = \int_{-\infty}^\infty \! e^{-x^2} \,\mathrm dx \\ & = \sqrt{\left(\int_{-\infty}^\infty \! e^{-x^2} \,\mathrm dx \right) \left(\int_{-\infty}^\infty \! e^{-y^2} \,\mathrm dy \right)} \\ & = \sqrt{\iint_{\mathbb R^2} \! e^{-\left(x^2 + y^2\right)} \,\mathrm dx \,\mathrm dy} \\ & = \sqrt{\int_0^{2\pi} \! \int_0^\infty \! e^{-r^2} r \,\mathrm dr \,\mathrm d\theta} \\ & = \sqrt{2\pi \left[-\frac 1 2 e^{-r^2}\right]_{r = 0}^{r = \infty} } \\ & = \sqrt \pi \,. \end{aligned}$$ Rigorously, we should also justify the improper double integrals and equating the two expressions.
- polar coordinate system
  - polar coordinate system / integral calculus of area ::@:: Let _R_ denote the region enclosed by a curve _r_\(_φ_\) and the rays _φ_ = _a_ and _φ_ = _b_, where 0 \< _b_ − _a_ ≤ 2<!-- markdown separator -->_π_. Then, the area of _R_ is $${\frac {1}{2} }\int _{a}^{b}\left[r(\varphi )\right]^{2}\,d\varphi .$$
    - polar coordinate system / integral calculus of area / derivation ::@:: This result can be found as follows. First, the interval \[_a_, _b_\] is divided into _n_ subintervals, where _n_ is some positive integer. Thus Δ<!-- markdown separator -->_φ_, the angle measure of each subinterval, is equal to _b_ − _a_ \(the total angle measure of the interval\), divided by _n_, the number of subintervals. For each subinterval _i_ = 1, 2, ..., _n_, let _φ_<sub>_i_</sub> be the midpoint of the subinterval, and construct a [sector](../../../../general/circular%20sector.md) with the center at the pole, radius _r_\(_φ_<sub>_i_</sub>\), central angle Δ<!-- markdown separator -->_φ_ and arc length _r_\(_φ_<sub>_i_</sub>\)Δ<!-- markdown separator -->_φ_. The area of each constructed sector is therefore equal to $$\left[r(\varphi _{i})\right]^{2}\pi \cdot {\frac {\Delta \varphi }{2\pi } }={\frac {1}{2} }\left[r(\varphi _{i})\right]^{2}\Delta \varphi .$$ Hence, the total area of all of the sectors is $$\sum _{i=1}^{n}{\tfrac {1}{2} }r(\varphi _{i})^{2}\,\Delta \varphi .$$ As the number of subintervals _n_ is increased, the approximation of the area improves. Taking _n_ → ∞, the sum becomes the [Riemann sum](../../../../general/Riemann%20sum.md) for the above integral.
- multiple integral
  - multiple integral / change of variables
    - multiple integral / change of variables / others ::@:: Common changes of variables involve conversion between the Cartesian, polar, cylindrical, and spherical coordinate systems.
- [surface integral](../../../../general/surface%20integral.md) ::@:: It is a generalization of multiple integrals to integration over surfaces. It can be thought of as the double integral analogue of the line integral.
  - surface integral / surface area of the graph of a function ::@:: For example, if we want to find the [surface area](../../../../general/surface%20area.md) of the graph of some scalar function, say _z_ = _f_\(_x_, _y_\), we have $$A=\iint _{S}\,\mathrm {d} S=\iint _{T}\left\|{\partial \mathbf {r}  \over \partial x}\times {\partial \mathbf {r}  \over \partial y}\right\|\mathrm {d} x\,\mathrm {d} y$$ where __r__ = \(_x_, _y_, _z_\) = \(_x_, _y_, _f_\(_x_, _y_\)\). So that ${\partial \mathbf {r}  \over \partial x}=(1,0,f_{x}(x,y))$, and ${\partial \mathbf {r}  \over \partial y}=(0,1,f_{y}(x,y))$. So, $${\begin{aligned}A&{}=\iint _{T}\left\|\left(1,0,{\partial f \over \partial x}\right)\times \left(0,1,{\partial f \over \partial y}\right)\right\|\mathrm {d} x\,\mathrm {d} y\\&{}=\iint _{T}\left\|\left(-{\partial f \over \partial x},-{\partial f \over \partial y},1\right)\right\|\mathrm {d} x\,\mathrm {d} y\\&{}=\iint _{T}{\sqrt {\left({\partial f \over \partial x}\right)^{2}+\left({\partial f \over \partial y}\right)^{2}+1} }\,\,\mathrm {d} x\,\mathrm {d} y\end{aligned} }$$ which is the standard formula for the area of a surface described this way. One can recognize the vector in the second-last line above as the [normal vector](../../../../general/surface%20normal.md) to the surface.

## week 11 lecture

- datetime: 2025-04-15T13:30:00+08:00/2025-04-15T14:50:00+08:00, PT1H20M
- [volume integral](../../../../general/volume%20integral.md) ::@:: It is an integral over a 3-dimensional domain; that is, it is a special case of multiple integrals.
  - volume integral / techniques ::@:: Again, they include change of variables, integration boundaries, integration order, visualization, etc.

## week 11 tutorial

- datetime: 2025-04-15T16:00:00+08:00/2025-04-15T16:50:00+08:00, PT50M
- [week 11 lecture](#week%2011%20lecture)
- [week 11 lecture 2](#week%2011%20lecture%202)
- Gaussian integral
- [trigonometric substitution](../../../../general/trigonometric%20substitution.md) ::@:: It replaces a trigonometric function for another expression. In calculus, trigonometric substitutions are a technique for evaluating integrals. In this case, an expression involving a radical function is replaced with a trigonometric one. Trigonometric identities may help simplify the answer.
- [list of trigonometric identities](../../../../general/list%20of%20trigonometric%20identities.md) ::@:: They are equalities that involve trigonometric functions and are true for every value of the occurring variables for which both sides of the equality are defined. Geometrically, these are identities involving certain functions of one or more angles.
- [Bioche's rules](../../../../general/Bioche's%20rules.md) ::@:: They, formulated by the French mathematician Charles Bioche \(1859–1949\), are rules to aid in the computation of certain indefinite integrals in which the integrand contains sines and cosines.
  - Bioche's rules / conditions ::@:: In the following, $f(t)$ is a [rational expression](../../../../general/rational%20function.md) in $\sin t$ and $\cos t$. In order to calculate $\int f(t)\,dt$, consider the integrand $\omega (t)=f(t)\,dt$. We consider the behavior of this entire integrand, including the $dt$, under translation and reflections of the _t_ axis. The translations and reflections are ones that correspond to the symmetries and periodicities of the basic trigonometric functions.
  - Bioche's rules / statement ::@:: Bioche's rules state that: <p> 1. If $\omega (-t)=\omega (t)$, a good change of variables is $u=\cos t$. <br/> 2. If $\omega (\pi -t)=\omega (t)$, a good change of variables is $u=\sin t$. <br/> 3. If $\omega (\pi +t)=\omega (t)$, a good change of variables is $u=\tan t$. <br/> 4. If two of the preceding relations both hold, a good change of variables is $u=\cos 2t$. <br/> 5. In all other cases, use $u=\tan(t/2)$.

## week 11 lecture 2

- datetime: 2025-04-17T13:30:00+08:00/2025-04-17T14:50:00+08:00, PT1H20M
- volume integral
  - volume integral / techniques
- cylindrical coordinate system
  - cylindrical coordinate system / Cartesian coordinates ::@:: For the conversion between cylindrical and Cartesian coordinates, it is convenient to assume that the reference plane of the former is the Cartesian _xy_-plane \(with equation _z_ = 0\), and the cylindrical axis is the Cartesian _z_-axis. Then the _z_-coordinate is the same in both systems, and the correspondence between cylindrical \(_ρ_, _φ_, _z_\) and Cartesian \(_x_, _y_, _z_\) are the same as for polar coordinates, namely $${\begin{aligned}x&=\rho \cos \varphi \\y&=\rho \sin \varphi \\z&=z\end{aligned} }$$ in one direction, and $${\begin{aligned}\rho &={\sqrt {x^{2}+y^{2} } }\\\varphi &={\begin{cases}{\text{indeterminate} }&{\text{if } }x=0{\text{ and } }y=0\\\arcsin \left({\frac {y}{\rho } }\right)&{\text{if } }x\geq 0\\-\arcsin \left({\frac {y}{\rho } }\right)+\pi &{\text{if } }x<0{\text{ and } }y\geq 0\\-\arcsin \left({\frac {y}{\rho } }\right)-\pi &{\text{if } }x<0{\text{ and } }y<0\end{cases} }\end{aligned} }$$ in the other. The [arcsine](../../../../general/arcsine.md) function is the inverse of the [sine](../../../../general/sine.md) function, and is assumed to return an angle in the range \[−⁠π/2⁠, +⁠π/2⁠\] = \[−90°, +90°\]. These formulas yield an azimuth _φ_ in the range \[−180°, +180°\].
    - cylindrical coordinate system / Cartesian coordinates / change of variables ::@:: The factor from Cartesian coordinates to cylindrical coordinates is: $$\begin{aligned} \lVert J \rVert & = \left\lVert \frac {\partial(x, y, z)} {\partial(\rho, \varphi, z)} \right\rVert \\ & = \begin{Vmatrix} \cos\varphi & \sin\varphi & 0 \\ -\rho \sin \varphi & \rho \cos \varphi & 0 \\ 0 & 0 & 1 \end{Vmatrix} \\ & = \lvert \rho \rvert \,. \end{aligned}$$ For the inverse transformation, the factor is the reciprocal: $$\frac 1 \rho = \frac 1 {\sqrt{x^2 + y^2} } \,.$$ <p> Consider a very very small slice of the infinitesimal area $\mathrm d\rho \, \mathrm d\varphi \,\mathrm dz$. Changing $\rho$, we see the area is proportional to $\rho$. Changing $\varphi$ or $z$, we see the area remains the same. So the factor is $r$.
- spherical coordinate system
  - spherical coordinate system / Cartesian coordinates ::@:: The spherical coordinates of a point in the ISO convention \(i.e. for physics: radius _r_, inclination _θ_, azimuth _φ_\) can be obtained from its [Cartesian coordinates](../../../../general/Cartesian%20coordinate%20system.md) \(_x_, _y_, _z_\) by the formulae $${\begin{aligned}r&={\sqrt {x^{2}+y^{2}+z^{2} } }\\\theta &=\arccos {\frac {z}{\sqrt {x^{2}+y^{2}+z^{2} } } }=\arccos {\frac {z}{r} } \\\varphi &=\operatorname {sgn}(y)\arccos {\frac {x}{\sqrt {x^{2}+y^{2} } } } \end{aligned} }$$ The [inverse tangent](../../../../general/inverse%20tangent.md) denoted in _φ_ = arctan ⁠_y_<!-- markdown separator -->/<!-- markdown separator -->_x_⁠ must be suitably defined, taking into account the correct quadrant of \(_x_, _y_\), as done in the equations above. See the article on [atan2](../../../../general/atan2.md). <p> Conversely, the Cartesian coordinates may be retrieved from the spherical coordinates \(_radius_ _r_, _inclination_ _θ_, _azimuth_ _φ_\), where _r_ ∈ \[0, ∞\), _θ_ ∈ \[0, _π_\], _φ_ ∈ \[0, 2<!-- markdown separator -->_π_\), by $${\begin{aligned}x&=r\sin \theta \,\cos \varphi ,\\y&=r\sin \theta \,\sin \varphi ,\\z&=r\cos \theta .\end{aligned} }$$
    - spherical coordinate system / Cartesian coordinates / change of variables ::@:: The factor from Cartesian coordinates to spherical coordinates is \($\theta$ is the _polar_ angle\): $$\begin{aligned} \lVert J \rVert & = \left\lVert \frac {\partial(x, y, z)} {\partial(r, \theta, \varphi)} \right\rVert \\ & = \begin{Vmatrix} \cos\varphi \sin\theta & \sin\varphi \sin\theta & \cos\theta \\ r \cos\varphi \cos\theta & r \sin\varphi \cos\theta & r \sin\theta \\ -r \sin\varphi \sin\theta & r \cos\varphi \sin\theta & 0 \end{Vmatrix} \\ & = \left\lvert -r^2 \sin^2 \varphi \sin^3 \theta + r^2 \cos^2 \varphi \sin \theta \cos^2 \theta + r^2 \sin^2 \varphi \sin \theta \cos^2 \theta - r^2 \cos^2 \varphi \sin^3 \theta  \right\rvert \\ & = \lvert -r^2 \sin^3 \theta + r^2 \sin \theta \cos^2 \theta \rvert \\ & = \lvert r^2 \sin \theta \rvert \,. \end{aligned}$$ For the inverse transformation, the factor is the reciprocal: $$\frac 1 {r \sin \theta} = \frac 1 {\sqrt{x^2 + y^2} } \,.$$ <p> Consider a very very small slice of the infinitesimal area $\mathrm dr \, \mathrm d\theta \,\mathrm d\varphi$. Changing $r$, we see the area is proportional to $r^2$. Changing $\theta$, we see the area is \(roughly\) proportional to $\sin\theta$. Changing $\varphi$, we see the area remains the same. So the factor is $r^2 \sin\theta$.

## week 12 lecture

- datetime: 2025-04-22T13:30:00+08:00/2025-04-22T14:50:00+08:00, PT1H20M
- spherical coordinate system
- [vector field](../../../../general/vector%20field.md) ::@:: It is an assignment of a vector to each point in a space, most commonly Euclidean space $\mathbb R^n$. A vector field on a plane can be visualized as a collection of arrows with given magnitudes and directions, each attached to a point on the plane.
  - vector field / examples ::@:: A simple one: $$F(x, y, z) = zi + yj + xk = \langle z, y, x \rangle \,.$$ _Radial_ vector fields: $$F(x, y, z) = \frac {\langle x, y, z \rangle} {\lVert \langle x, y, z \rangle \rVert_2^p}$$ for some real number $p$.
- [conservative vector field](../../../../general/conservative%20field.md) ::@:: It is a vector field that is the gradient of some function.
  - conservative vector field / definition ::@:: A [vector field](../../../../general/vector%20field.md) $\mathbf {v} :U\to \mathbb {R} ^{n}$, where $U$ is an open subset of $\mathbb {R} ^{n}$, is said to be conservative if there exists a $C^{1}$ \([continuously differentiable](../../../../general/smoothness.md#multivariate%20differentiability%20classes)\) [scalar field](../../../../general/scalar%20field.md) $\varphi$ on $U$ such that $$\mathbf {v} =\nabla \varphi .$$ Here, $\nabla \varphi$ denotes the [gradient](../../../../general/gradient.md) of $\varphi$. Since $\varphi$ is continuously differentiable, $\mathbf {v}$ is continuous. When the equation above holds, $\varphi$ is called a [scalar potential](../../../../general/scalar%20potential.md) for $\mathbf {v}$.

## week 12 tutorial

- datetime: 2025-04-22T16:00:00+08:00/2025-04-22T16:50:00+08:00, PT50M
- [week 12 lecture](#week%2012%20lecture)
- [week 12 lecture 2](#week%2012%20lecture%202)
- conservative vector field
  - conservative vector field / scalar field uniqueness ::@:: The scalar field \(potential field\) is unique up to a constant.
- [line integral](../../../../general/line%20integral.md) ::@:: An integral where the function to be integrated is evaluated along a curve. The function to be integrated can be a scalar field or a vector field.
  - line integral / integrability ::@:: If the scalar/vector field to be integrated over is continuous along the integration path, along with the other conditions for the integration path itself, the integral exists. This is similar to the fundamental theorem of calculus, part 1. <p> However, the above condition is not necessary: integrals can exist over noncontinuous scalar/vector fields. This would depend on the notion of integrability used \(__this course__: Riemann integrability\).
- [Riemann integral](../../../../general/Riemann%20integral.md) ::@:: It, created by Bernhard Riemann, was the first rigorous definition of the integral of a function on an interval.
  - Riemann integral / mesh ::@:: The __mesh__ or __norm__ of a partition is defined to be the length of the longest sub-interval, that is, $$\max \left(x_{i+1}-x_{i}\right),\quad i\in [0,n-1].$$
  - Riemann integral / Riemann sum ::@:: For all _ε_ \> 0, there exists _δ_ \> 0 such that for any [tagged partition](../../../../general/partition%20of%20an%20interval.md) _x_<sub>0</sub>, ..., _x<sub>n</sub>_ and _t_<sub>0</sub>, ..., _t_<sub>_n_ − 1</sub> whose mesh is less than _δ_, we have $$\left|\left(\sum _{i=0}^{n-1}f(t_{i})(x_{i+1}-x_{i})\right)-s\right|<\varepsilon .$$

## week 12 lecture 2

- datetime: 2025-04-24T13:30:00+08:00/2025-04-24T14:50:00+08:00, PT1H20M
- conservative vector field
- [gravitational field](../../../../general/gravitational%20field.md) ::@:: It is a [vector](../../../../general/vector%20field.md) [field](../../../../general/field%20(physics).md) used to explain the influences that a body extends into the space around itself. A gravitational field is used to explain [gravitational](../../../../general/gravity.md) phenomena, such as the _[gravitational force](../../../../general/gravitational%20force.md) field_ exerted on another massive body.
  - gravitational field / classical mechanics ::@:: The gravitational field equation is $$\mathbf {g} ={\frac {\mathbf {F} }{m} }={\frac {d^{2}\mathbf {R} }{dt^{2} } }=-GM{\frac {\mathbf {R} }{\left|\mathbf {R} \right|^{3} } }=-\nabla \Phi ,$$ where __F__ is the [gravitational force](../../../../general/gravitational%20force.md), _m_ is the mass of the [test particle](../../../../general/test%20mass.md), __R__ is the radial vector of the test particle relative to the mass \(or for Newton's second law of motion which is a time dependent function, a set of positions of test particles each occupying a particular point in space for the start of testing\), _t_ is [time](../../../../general/time.md), _G_ is the [gravitational constant](../../../../general/gravitational%20constant.md), and ∇ is the [del operator](../../../../general/del%20operator.md).
- [gravitational potential](../../../../general/gravitational%20potential.md) ::@:: It is a scalar potential associating with each point in space the work \(energy transferred\) per unit mass that would be needed to move an object to that point from a fixed reference point in the conservative gravitational field.
  - gravitational potential / mathematical form ::@:: The gravitational [potential](../../../../general/scalar%20potential.md) _V_ at a distance _x_ from a [point mass](../../../../general/point%20particle.md) of mass _M_ can be defined as the work _W_ that needs to be done by an external agent to bring a unit mass in from infinity to that point: $$V(\mathbf {x} )={\frac {W}{m} }={\frac {1}{m} }\int _{\infty }^{x}\mathbf {F} \cdot d\mathbf {x} ={\frac {1}{m} }\int _{\infty }^{x}{\frac {GmM}{x^{2} } }dx=-{\frac {GM}{x} },$$ where _G_ is the [gravitational constant](../../../../general/gravitational%20constant.md), and __F__ is the gravitational force.
  - gravitational potential / gravitational field ::@:: It, and thus the acceleration of a small body in the space around the massive object, is the negative [gradient](../../../../general/gradient.md) of the gravitational potential. Thus the negative of a negative gradient yields positive acceleration toward a massive object. Because the potential has no angular components, its gradient is $$\mathbf {a} =-{\frac {GM}{x^{3} } }\mathbf {x} =-{\frac {GM}{x^{2} } }{\hat {\mathbf {x} } },$$ where __x__ is a vector of length _x_ pointing from the point mass toward the small body and ${\hat {\mathbf {x} } }$ is a [unit vector](../../../../general/unit%20vector.md) pointing from the point mass toward the small body.
- line integral
  - line integral / over a scalar field ::@:: For some [scalar field](../../../../general/scalar%20field.md) $f\colon U\to \mathbb {R}$ where $U\subseteq \mathbb {R} ^{n}$, the line integral along a [piecewise smooth](../../../../general/../../../../general/piecewise%20smooth.md) [curve](../../../../general/curve.md) ${\mathcal {C} }\subset U$ is defined as $$\int _{\mathcal {C} }f\,ds=\int _{a}^{b}f\left(\mathbf {r} (t)\right)\left|\mathbf {r} '(t)\right|\,dt,$$ where $\mathbf {r} \colon [a,b]\to {\mathcal {C} }$ is an arbitrary [bijective](../../../../general/bijective.md) [parametrization](../../../../general/parametric%20equation.md) of the curve ${\mathcal {C} }$ such that __r__\(_a_\) and __r__\(_b_\) give the endpoints of ${\mathcal {C} }$ and _a_ \< _b_. <p> If $C$ is a closed curve, we can use $\oint$ instead of $\int$. <p> special case: If $\mathbf r(t) = \langle x(t), y(t) \rangle$, then $\lvert \mathbf r'(t) \rvert = \lvert \langle x'(t), y'(t) \rangle \rvert = \sqrt{x'(t)^2 + y'(t)^2}$.
    - line integral / over a scalar field / piecewise ::@:: If the curve is a piecewise smooth curve, you can split the curve into several \(one-piece\) smooth curves, integrate over them separately, and then add the integral results together to get the original integral result.
  - line integral / single-variable calculus ::@:: The single-variable calculus you have been doing has always been a line integral! It is just that the space is 1D, i.e. the real number line, and obviously, the only curves in a 1D space are straight lines.
  - line integral / over a vector field ::@:: For a [vector field](../../../../general/vector%20field.md) __F__: _U_ ⊆ __R__<sup>_n_</sup> → __R__<sup>_n_</sup>, the line integral along a [piecewise smooth](../../../../general/piecewise%20smooth.md) [curve](../../../../general/curve.md) _C_ ⊂ _U_, in the direction of __r__, is defined as $$\int _{C}\mathbf {F} (\mathbf {r} )\cdot d\mathbf {r} =\int _{a}^{b}\mathbf {F} (\mathbf {r} (t))\cdot \mathbf {r} '(t)\,dt$$ where · is the [dot product](../../../../general/dot%20product.md), and __r__: \[_a_, _b_\] → _C_ is a regular [parametrization](../../../../general/parametric%20equation.md) \(i.e: $||\mathbf {r} '(t)||\neq 0\;\;\forall t\in [a,b]$\) of the curve _C_ such that __r__\(_a_\) and __r__\(_b_\) give the endpoints of _C_. <p> If $C$ is a closed curve, we can use $\oint$ instead of $\int$.
    - line integral / over a vector field / 2 dimensions ::@:: We have $$\begin{aligned} & \phantom = \int_C \! \mathbf F(\mathbf r) \cdot \mathrm d\mathbf r \\ & = \int_C \! \mathbf F(\mathbf r(t)) \cdot \mathbf r'(t) \,\mathrm dt \\ & = \int_C \! (P(x(t), y(t)) x'(t) + Q(x(t), y(t)) y'(t)) \,\mathrm dt \\ & = \int_C \! (P \,\mathrm dx + Q \,\mathrm dy) \,. \end{aligned}$$
    - line integral / over a vector field / 3 dimensions ::@:: We have $$\begin{aligned} & \phantom = \int_C \! \mathbf F(\mathbf r) \cdot \mathrm d\mathbf r \\ & = \int_C \! \mathbf F(\mathbf r(t)) \cdot \mathbf r'(t) \,\mathrm dt \\ & = \int_C \! (P(x(t), y(t), z(t)) x'(t) + Q(x(t), y(t), z(t)) y'(t) + R(x(t), y(t), z(t)) z'(t)) \,\mathrm dt \\ & = \int_C \! (P \,\mathrm dx + Q \,\mathrm dy + R \,\mathrm dz) \,. \end{aligned}$$
    - line integral / over a vector field / examples ::@:: A line integral over the gravitational field \(direction is in the acceleration direction\) from a reference point to any point yields the negative of the gravitational potential at that point. This applies to inverse-square laws in general.

## final examination

## aftermath

### total
