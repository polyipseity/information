---
aliases:
  - HKUST MATH 2023
  - HKUST MATH 2023 index
  - HKUST MATH2023
  - HKUST MATH2023 index
  - MATH 2023
  - MATH 2023 index
  - MATH2023
  - MATH2023 index
  - Multivariable Calculus
  - Multivariable Calculus index
tags:
  - flashcard/active/special/academia/HKUST/MATH_2023/index
  - function/index
  - language/in/English
---

# index

- HKUST MATH 2023
- name: Multivariable Calculus

The content is in teaching order.

- grading
  - scheme
    - homework ×?: 10%
    - midterm exam: 40%
    - final exam: 50%
- logistics

## children

- [questions](questions.md)

## week 1 lecture

- datetime: 2025-02-04T13:30:00+08:00/2025-02-04T14:50:00+08:00
- [function](../../../../general/function%20(mathematics).md) ::@:: image, pre-image, graph of a function, continuity <!--SR:!2025-04-25,58,310!2025-04-24,57,310-->
  - function / image ::@:: It of an input value $x$ is the single output value produced by $f$ when passed $x$. It of a set of input values $X$ is the set of output values produced by $f$ when passed each value of $X$. <!--SR:!2025-04-21,54,310!2025-04-11,47,290-->
  - function / pre-image ::@:: It of an output value $y$ is the set of input values that produce $y$ when passed to $f$. It of a set of output values $Y$ is the set of input values that produce any value in $Y$ when passed to $f$. <!--SR:!2025-04-25,58,310!2025-04-23,56,310-->
  - function / graph of a function ::@:: The set of points $(x, f(x))$ for all $x$ in the domain of $f$ on the 2D Cartesian plane. <p> Not all set of points $(x, y)$ is the graph of a function. Use the vertical line test: If any vertical line pass through at most one point in the set, then the graph is a function. <!--SR:!2025-04-24,57,310!2025-04-21,54,310-->
  - function / [continuity](../../../../general/continuous%20function.md) ::@:: The limit of the function at each input value equals the function evaluated at that value. (Some special cases regarding domain boundaries have been omitted here...) <!--SR:!2025-03-27,31,270!2025-04-22,55,310-->
- [calculus](../../../../general/calculus.md) ::@:: rate of change, differentiation, integration, fundamental theorem of calculus <!--SR:!2025-04-21,54,310!2025-04-24,57,310-->
  - calculus / [differentiation](../../../../general/derivative.md) ::@:: Definition: $$f'(x) = \lim_{\delta \to 0} \frac {f(x + \delta) - f(x)} {\delta} \,$$ if it exists. But in practice, we apply some rules. <p> Related to the rate of change. <!--SR:!2025-04-21,54,310!2025-04-22,55,310-->
  - calculus / [integration](../../../../general/integral.md) ::@:: Inverse operation to differentiation. <p> Related to area under the curve. <!--SR:!2025-04-24,57,310!2025-04-22,55,310-->
  - calculus / [fundamental theorem of calculus](../../../../general/fundamental%20theorem%20of%20calculus.md) ::@:: There are 2 parts. It relates a function to its antiderivatives. <p> The first part: We have a continuous function $f$. Then we can define a function as the definite integral of $f$ from $a$ to $x$. The function is an antiderivative of $f$. <p> The second part: We have a Riemann integrable function $f$. We also have a function $F$ that is the antiderivaive of $f$. Then the definite integral of $f$ from $a$ to $b$ is $F(b) - F(a)$. <!--SR:!2025-04-23,56,310!2025-04-09,42,290-->
- function
  - function / extension in this course ::@:: We will study functions with multiple inputs and/or multiple outputs. <!--SR:!2025-04-25,58,310!2025-04-25,58,310-->
- [function of several real variables](../../../../general/function%20of%20a%20several%20real%20variables.md) ::@:: In general, $$\mathbb R^n \to X \,.$$ <p> For this course in particular, we will focus on $X = \mathbb R^m$. <!--SR:!2025-04-25,58,310!2025-04-21,54,310-->
- [Cartesian coordinate system](../../../../general/Cartesian%20coordinate%20system.md) ::@:: A coordinate system that specifies each point uniquely by a pair of real numbers called _coordinates_, which are the signed distances to the point from two fixed perpendicular oriented lines, called _coordinate lines_, _coordinate axes_ or just _axes_ (plural of _axis_) of the system. <!--SR:!2025-04-10,46,290!2025-03-31,38,290-->
  - Cartesian coordinate system / dimension ::@:: number of points to describe a point <!--SR:!2025-04-06,43,290!2025-04-24,57,310-->
  - Cartesian coordinate system / symbols ::@:: Using real numbers, $\mathbb R^n$, where $n$ is the dimension of the entire space. <!--SR:!2025-04-23,56,310!2025-04-01,39,290-->
  - Cartesian coordinate system / orientation ::@:: left-hand side, right-hand side; by most convention (including this course), we use right-hand side <!--SR:!2025-04-22,55,310!2025-04-21,54,310-->
  - Cartesian coordinate system / shape description ::@:: Equations can describe a subset of points of the entire space. Intersection and/or union may be used to define shapes defined by multiple equations. <p> For example: $x^2 + y^2 + z^2 = r^2$ is a 3D sphere of radius $r$. <!--SR:!2025-04-22,55,310!2025-04-23,56,310-->
  - Cartesian coordinate system / projection ::@:: Orthogonal projection may be used to extract a specific coordinate of a point. <!--SR:!2025-04-10,43,290!2025-04-22,55,310-->
- [Euclidean vector](../../../../general/Euclidean%20vector.md) ::@:: A set whose elements, often called _vectors_, can be added together and multiplied ("scaled") by numbers called _scalars_. <!--SR:!2025-04-08,41,290!2025-04-25,58,310-->
  - Euclidean vector / vector ::@:: A line segment with a direction, up to translation. That is, two vectors are the same if they have the same length and direction. <p> The zero vector has 0 length and no direction. <!--SR:!2025-04-24,57,310!2025-04-21,54,310-->
  - Euclidean vector / scalar ::@:: A real number. In general, the field associated with the vector space. <!--SR:!2025-04-22,55,310!2025-04-24,57,310-->
  - Euclidean vector / position vector ::@:: A vector used to represent the position of a point. <p> For example, in 3D space, the point $(x, y, z)$ can be represented by the position vector $\langle x, y, z \rangle$ (note the angle brackets). <!--SR:!2025-04-23,56,310!2025-04-22,55,310-->
  - Euclidean vector / element-wise operations ::@:: vector addition, vector subtraction: perform the operation on each coordinate separately to get the resulting vector <!--SR:!2025-04-22,55,310!2025-04-25,58,310-->
  - Euclidean vector / scalar multiplication ::@:: multiply each coordinate by the specified scalar to get the resulting vector <!--SR:!2025-04-24,57,310!2025-04-25,58,310-->
  - Euclidean vector / magnitude or length ::@:: The square root of the sum of the squared coordinates. This is a specific case of the _p_-norm. <p> For example, the vector $\vec v = \langle x, y, z \rangle$ has the magnitude $\lvert \vec v \rvert = \sqrt{x^2 + y^2 + z^2}$. <!--SR:!2025-04-23,56,310!2025-04-24,57,310-->
  - [unit vector](../../../../general/unit%20vector.md) ::@:: A vector of magnitude 1. The unit vector associated with an arbitrary vector $\vec v$ is $\vec v / \lvert \vec v \rvert$. The zero vector has no unit vector, as it has no direction. <p> For example, in 3D space, the unit vectors along the coordinate axes (and their notations): $\vec i = \langle 1, 0, 0 \rangle, \vec j = \langle 0, 1, 0 \rangle, \vec k = \langle 0, 0, 1 \rangle$. These 3 unit vectors form is _an_ (not _the_) orthonormal basis of the 3D space. <!--SR:!2025-04-21,54,310!2025-04-23,56,310-->
  - [orthonormal basis](../../../../general/orthonormal%20basis.md) ::@:: The set of unit vectors along the coordinate axes form an orthonormal basis. This means any vector can be _uniquely_ written as a linear combination of the orthonormal basis. <p> For example, in 3D space, each vector $\vec v$ can be written uniquely as $\vec v = x \vec i + y \vec j + z \vec k$, where $x, y, z$ are scalars. <!--SR:!2025-04-08,43,290!2025-04-23,56,310-->

## week 1 tutorial

- datetime: 2025-02-04T16:00:00+08:00/2025-02-04T16:50:00+08:00
- [week 1 lecture](#week%201%20lecture)
- [week 1 lecture 2](#week%201%20lecture%202)

## week 1 lecture 2

- datetime: 2025-02-06T13:30:00+08:00/2025-02-06T14:50:00+08:00
- [dot product](../../../../general/dot%20product.md)
  - dot product / algebraically ::@:: It is the sum of the products of the corresponding entries of the two sequences of numbers. Algebraic and geometric definitions are equivalent when using Cartesian coordinates. <!--SR:!2025-05-29,83,347!2025-04-14,45,327-->
  - dot product / geometrically ::@:: It is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them. Algebraic and geometric definitions are equivalent when using Cartesian coordinates. <!--SR:!2025-05-30,84,347!2025-04-14,45,327-->
  - dot product / properties
    - dot product / properties / magnitude ::@:: $\vec v \cdot \vec v = \lvert \vec v \rvert^2$ <!--SR:!2025-05-30,84,347!2025-05-30,84,347-->
    - dot product / properties / commutativity ::@:: $\vec v \cdot \vec w = \vec w \cdot \vec v$ <!--SR:!2025-05-30,84,347!2025-05-30,84,347-->
    - dot product / properties / distributivity ::@:: $\vec u \cdot (\vec v + \vec w) = \vec u \cdot \vec v + \vec u \cdot \vec w \qquad (\vec u + \vec v) \cdot \vec w = \vec u \cdot \vec w + \vec v \cdot \vec w$ <!--SR:!2025-05-30,84,347!2025-05-30,84,347-->
    - dot product / properties / homogeneity ::@:: $(c \vec u) \cdot \vec v = c (\vec u \cdot \vec v) = \vec u \cdot (c \vec v)$ <!--SR:!2025-05-29,83,347!2025-05-29,83,347-->
- [polarization identity](../../../../general/polarization%20identity.md) ::@:: $2 \lVert \vec x \rVert^2 + 2 \lVert \vec y \rVert^2 = \lVert \vec x + \vec y \rVert^2 - \lVert \vec x - \vec y \rVert^2$ <!--SR:!2025-05-29,83,347!2025-05-29,83,347-->
  - polarization identity / variant ::@:: $\langle \vec x, \vec y \rangle = \frac 1 4 \left(\lVert x + y \rVert^2 - \lVert x - y \rVert^2 \right)$ <!--SR:!2025-05-30,84,347!2025-05-30,84,346-->
- [dot product](../../../../general/dot%20product.md)
  - dot product / cosine ::@:: $\cos \theta = \frac {\vec u \cdot \vec v} {\lVert \vec u \rVert \lVert \vec v \rVert}$ <!--SR:!2025-05-30,84,347!2025-05-30,84,347-->
    - dot product / cosine / applications ::@:: inscribed triangle with diameter has a right angle, law of cosine/generalized Pythagoras theorem <!--SR:!2025-05-30,84,347!2025-04-04,35,307-->
  - dot product / projection ::@:: Project $\vec b$ onto $\vec a$: $\operatorname{proj}_{\vec a} \vec b = \frac {\vec a \cdot \vec b} {\vec a \cdot \vec a} \vec a$. $\frac {\vec a \cdot \vec b} {\vec a \cdot \vec a}$ can be interpreted as $\cos \theta$, where $\theta$ is the angle between them. <!--SR:!2025-04-04,35,307!2025-05-05,59,327-->
    - dot product / projection / scalar ::@:: It is simply the signed length of the projection, which is $\operatorname{comp}_{\vec a} \vec b = \frac {\vec a \cdot \vec b} {\lvert \vec a \rvert}$. <!--SR:!2025-05-30,84,346!2025-05-30,84,347-->
- [cross product](../../../../general/cross%20product.md) ::@:: Given two linearly independent vectors $\mathbf a$ and $\mathbf b$, the cross product, $\mathbf a \times \mathbf b$ (read "a cross b"), is a vector that is perpendicular to both $\mathbf a$ and $\mathbf b$, and thus normal to the plane containing them. <p> The magnitude of the cross product equals the area of a parallelogram with the vectors for sides; in particular, the magnitude of the product of two perpendicular vectors is the product of their lengths, i.e. $\lVert \mathbf a \rVert \lVert \mathbf b \rVert \sin \theta$, where $\theta$ is the angle between them. <!--SR:!2025-05-30,84,347!2025-05-29,83,347-->
  - cross product / properties
    - cross product / properties / anti-commutative ::@:: $\mathbf a \times \mathbf b = -\mathbf b \times \mathbf a$ <!--SR:!2025-05-29,83,347!2025-05-30,84,347-->
    - cross product / properties / distributivity ::@:: $\mathbf a \times (\mathbf b + \mathbf c) = \mathbf a \times \mathbf b + \mathbf a \times \mathbf c \qquad (\mathbf a + \mathbf b) \times \mathbf c = \mathbf a \times \mathbf c + \mathbf b \times \mathbf c$ <!--SR:!2025-05-29,83,347!2025-05-30,84,347-->
    - cross product / properties / zero element ::@:: $\mathbf a \times \mathbf 0 = \mathbf 0 \times \mathbf a = \mathbf 0$ <!--SR:!2025-05-30,84,347!2025-05-30,84,347-->
    - cross product / properties / parallel ::@:: $\mathbf a \times \mathbf a = \mathbf 0$ <!--SR:!2025-05-29,83,347!2025-05-05,59,327-->
  - cross product / calculation
    - cross product / calculation / element-wise ::@:: $\mathbf a \times \mathbf b = \langle a_2 b_3 - a_3 b_2, a_3 b_1 - a_1 b_3, a_1 b_2 - a_2 b_1 \rangle$ <!--SR:!2025-04-14,45,326!2025-05-30,84,347-->
    - cross product / calculation / matrix determinant ::@:: $\mathbf a \times \mathbf b = \begin{vmatrix} \mathbf i & \mathbf j & \mathbf k \\ a_1 & a_2 & a_3 \\ b_1 & b_2 & b_3 \end{vmatrix}$ <!--SR:!2025-05-30,84,347!2025-05-30,84,347-->

## week 2 lecture

- datetime: 2025-02-11T13:30:00+08:00/2025-02-11T14:50:00+08:00
- [cross product](../../../../general/cross%20product.md)
  - cross product / calculation
    - cross product / calculation / 3D basis ::@:: $\hat i \times \hat j = \hat k \qquad \hat j \times \hat k = \hat i \qquad \hat k \times \hat i = \hat j$ <!--SR:!2025-06-15,93,363!2025-06-15,93,363-->
  - cross product / properties
    - cross product / properties / perpendicularity ::@:: $(\mathbf u \times \mathbf v) \cdot \mathbf u = (\mathbf u \times \mathbf v) \cdot \mathbf v = 0$ <!--SR:!2025-06-15,93,363!2025-06-15,93,363-->
    - cross product / properties / parallelism ::@:: $\mathbf u \times \mathbf v = \mathbf 0$ iff the 2 vectors are parallel or at least 1 of them is $\mathbf 0$. <!--SR:!2025-06-15,93,363!2025-06-04,83,363-->
  - cross product / area and volume ::@:: $\lvert \mathbf u \times \mathbf v \rvert$ is the area of the parallelogram formed by the 2 vectors. $\lvert \mathbf u \cdot (\mathbf v \times \mathbf w) \rvert$ is the volume of the parallelepiped (analogy of parallelogram to 3D space) formed by the 3 vectors. <!--SR:!2025-06-15,93,363!2025-06-15,93,363-->
- [parametric equation](../../../../general/parmetric%20equation.md) ::@:: It expresses several quantities, such as the coordinates of a point, as functions of one or several variables called parameters. <p> We consider there are exactly 1 parameter, in which case the equation describes a curve (it need not be a line). <!--SR:!2025-06-08,87,363!2025-06-15,93,363-->
  - parametric equation / notation ::@:: For example, to describe a curve in 3D space, we have several ways: $$\begin{aligned} \gamma(t) & = (x(t), y(t), z(t)) \\ & \begin{cases} x(t) = \text{function of }t \\ y(t) = \text{function of }t \\ z(t) = \text{function of }t \end{cases} \\ & \begin{cases} x = \text{function of }t \\ y = \text{function of }t \\ z = \text{function of }t \end{cases} \end{aligned}$$ <!--SR:!2025-06-09,88,363!2025-06-15,93,363-->
  - parametric equation / construction
    - parametric equation / construction / line through a point parallel to a vector ::@:: If the point is $(x_0, y_0, z_0)$ and the vector is $\langle a, b, c \rangle$, then $\gamma(t) = (x_0, y_0, z_0) + t (a, b, c) = (x_0 + at, y_0 + bt, z_0 + ct)$. <p> We notice that actually, as long as the multiplication factor ($t$ in this context) has a domain of $\mathbb R$ and are the same for $a, b, c$, the equation describes the same line. So we can replace $t$ with $2t$, but not $0t$ or $t^2$. <!--SR:!2025-06-09,88,363!2025-06-06,85,363-->
  - parametric equation / non-uniqueness ::@:: Different equations can represent the same curve. <p> The simplest would be to replace $t$ by $\alpha t$, where $\alpha$ is a nonzero real number. The curve is the same but we can think of it as $t$ going "faster" or "slower" along the curve. <!--SR:!2025-06-07,86,363!2025-06-09,88,363-->

## week 2 tutorial

- datetime: 2025-02-11T16:00:00+08:00/2025-02-11T16:50:00+08:00
- [week 2 lecture](#week%202%20lecture)
- [week 2 lecture 2](#week%202%20lecture%202)

## week 2 lecture 2

- datetime: 2025-02-13T13:30:00+08:00/2025-02-13T14:50:00+08:00
- parametric equation
  - parametric equation / symmetric equation ::@:: Solve for $t$ in the parametric equation $\gamma(t) = (x_0 + at, y_0 + bt, z_0 + ct)$ to get this form: $$\frac {x - x_0} a = \frac {y - y_0} b = \frac {z - z_0} c \,.$$ Note that if one or more of $a$, $b$, or $c$ is 0, it means the curve does not change in coordinate for said axes. In that case, we simply have resp. $x = x_0$, $y = y_0$, $z = z_0$. <!--SR:!2025-06-20,94,370!2025-06-18,92,370-->
- [line](../../../../general/line%20(geometry).md) ::@:: an infinitely long object with no width, depth, or curvature, an idealization of such physical objects as a straightedge, a taut string, or a ray of light <!--SR:!2025-06-14,89,370!2025-06-19,93,370-->
  - line / relations with other lines ::@:: same: same line <br/> intersect: two _distinct_ lines intersecting at one point (they must be in the same plane) <br/> parallel: two _distinct_ lines in the same plane (always true for 2D space) and does not intersect <br/> skew: two _distinct_ lines not in the same plane (always false for 2D space) and does not intersect <!--SR:!2025-06-20,94,370!2025-06-20,94,370-->
    - line / relations with other lines / checking ::@:: Check direction first. If the same direction, if they intersect then same line, otherwise parallel lines. If not the same direction, if they intersect, then intersect, otherwise skew. <!--SR:!2025-06-19,93,370!2025-06-14,89,370-->
- [Euclidean plane](../../../../general/Euclidean%20plane.md) ::@:: A Euclidean space of dimension two. In 3D space, it can be described by a point it passes through and its _nonzero_ normal vector (which reduces the dimensionality by 1). <!--SR:!2025-06-15,90,370!2025-06-09,85,370-->
  - Euclidean plane / equation ::@:: Given a point $(x_0, y_0, z_0)$ and a _nonzero_ normal vector $\langle a, b, c \rangle$, the plane described has the equation: $$a(x - x_0) + b(y - y_0) + c(z - z_0) = 0$$ which is more commonly written as $$ax + by + cz - d = 0 \,.$$ You can think of _d_ as the _signed_ (positive in the direction of the normal vector) distance of the plane from the origin scaled by the normal vector length. <p> Conversely, any equation in the above form, where $a, b, c$ are not all zero, describes a plane. <p> (You can also describe it using linear algebra.) <!--SR:!2025-06-15,90,370!2025-06-09,85,370-->
- [Hesse normal form](../../../../general/Hesse%20normal%20form.md) ::@:: It describes a _n_−1 Euclidean space in a _n_-dimensional space. For 3D space, it describes a plane. <!--SR:!2025-06-09,85,370!2025-06-18,92,370-->
  - Hesse normal form / expression ::@:: It is written in vector notation as $${\vec {r} }\cdot {\vec {n} }_{0}-d=0.\,$$ The dot $\cdot$ indicates the [dot product](../../../../general/dot%20product.md) \(or scalar product\). Vector ${\vec {r} }$ points from the origin of the coordinate system, _O_, to any point _P_ that lies precisely in plane or on line _E_. The vector ${\vec {n} }_{0}$ represents the [unit](../../../../general/unit%20vector.md) [normal vector](../../../../general/normal%20vector.md) of plane or line _E_. The distance $d$ is the shortest _signed_ (positive in the direction of $\vec n_0$) distance from the origin _O_ to the plane or line. <!--SR:!2025-06-19,93,370!2025-05-18,67,350-->
- Euclidean plane
  - Euclidean plane / cross product ::@:: In 3D space, we can find the plane two non-parallel vectors lie in. Find the cross product of the two vectors. Then the resulting vector is a normal vector of the plane. <!--SR:!2025-06-12,87,370!2025-06-12,87,370-->
  - Euclidean plane / angle between two planes ::@:: The angle is the same as the angle between their two normal vectors. So you can use dot product (and cross product if in 3D space) to find it.  <p> Note: What if the normal of one is reversed? It turns out there are two angles definable between two planes, but they must add up to 180 degrees. Use the lower one. <!--SR:!2025-05-12,61,350!2025-06-14,89,370-->
  - Euclidean plane / relations with other planes ::@:: same: same plane <br/> intersect: _distinct_ planes that intersect at a line <br/> parallel: _distinct_ planes that do not intersect <!--SR:!2025-06-18,92,370!2025-06-09,85,370-->
    - Euclidean plane / relations with other planes / checking ::@:: Check if their normal vectors are parallel. If so, they are either the same or parallel. Otherwise, they are intersect. <!--SR:!2025-06-09,85,370!2025-06-20,94,370-->
  - Euclidean plane / distance to a point ::@:: This is the length of the line connecting the point to the plane parallel to the plane normal vector. <!--SR:!2025-06-11,86,370!2025-06-09,85,370-->
    - Euclidean plane / distance to a point / intuition ::@:: Consider the plane equation $ax + by + cz - d = 0$. In particular, the left hand side without $d$ is the component of $\langle x, y, z \rangle$ in the normal vector direction, scaled by the length of the normal vector. The $d$ represents the _signed_ (positive in the direction of the normal vector) distance of the plane from the origin scaled by the normal vector length. So the entire left hand side is the _signed_ distance from the plane to the point $(x, y, z)$. <p> So clearly, the (_unsigned_) distance to the point $(x, y, z)$ is: $$\text{distance} = \frac {\lvert ax + by + cz - d \rvert} {\sqrt {a^2 + b^2 + c^2} } \,.$$ <p> If you use the Hesse normal form, since the normal vector is normalized, the denominator (length of the normal vector) is 1 and can be omitted. <!--SR:!2025-06-15,90,370!2025-05-12,61,350-->
    - Euclidean plane / distance to a point / derivation ::@:: Consider the plane equation $ax + by + cz - d = 0$. Find its distance to $P_0 = (x_0, y_0, z_0)$. <p> Consider any point on the plane $P_1 = (x_1, y_1, z_1)$. Construct the vector $\overrightarrow{P_1 P_0}$. The distance is the component of the vector in the normal vector $\langle a, b, c \rangle$ direction. So we have $$\text{distance} = \left\lvert \frac {a(x_0 - x_1) + b(y_0 - y_1) + c(z_0 - z_1)} {\sqrt{a^2 + b^2 + c^2} } \right\rvert = \frac {\lvert a x_0 + b y_0 + c z_0 - d \rvert} {\sqrt{a^2 + b^2 + c^2} } \,.$$ <p> The above derivation also applies to higher dimensions for hyperplanes. <!--SR:!2025-05-18,63,350!2025-06-19,93,370-->

## week 3 lecture

- datetime: 2025-02-18T13:30:00+08:00/2025-02-18T14:50:00+08:00
- parametric equation
  - parametric equation / parametric curve ::@:: A curve described by a parametric equation: $\gamma : A \to \mathbb R^n$, where $A \subseteq \mathbb R$. <!--SR:!2025-06-20,94,370!2025-06-12,87,370-->
    - parametric equation / parametric curve / circle ::@:: $$(x, y) = (x_0 + r \cos at, y_0 + r \sin at)$$, where $(x_0, y_0)$ is the center, $r$ is the radius, $a \ne 0$ is "the rate of running along the shape". <!--SR:!2025-06-12,87,370!2025-06-14,89,370-->
    - parametric equation / parametric curve / graph of a function ::@:: $$(x, y) = (t, f(t))$$, where $f$ is a function. <!--SR:!2025-06-09,85,370!2025-06-19,93,370-->
    - parametric equation / parametric curve / cycloid ::@:: Consider the cycloid through the origin, generated by a circle of radius _r_ rolling over the _x_-axis on the positive side (_y_ ≥ 0). <p> To derive the equation, consider a non-rolling circle: $\gamma'(\theta) = (r \sin \theta, r(1 - \cos \theta))$. Then consider the circle rolling to the right by $r \, \mathrm d\theta$ per $\mathrm d\theta$ (compare when $\theta = 0$ and $\theta = 2\pi$), producing a cycloid. So we have: $$\gamma(\theta) = (r(\theta - \sin \theta), r(1- \cos \theta))$$. <!--SR:!2025-04-01,26,310!2025-05-18,63,350-->
- [cycloid](../../../../general/cycloid.md) ::@:: the curve traced by a point on a circle as it rolls along a straight line without slipping <!--SR:!2025-06-11,86,370!2025-06-20,94,370-->
- [parametric derivative](../../../../general/parametric%20derivative.md) ::@:: It is a derivative of a dependent variable with respect to another dependent variable that is taken when both variables depend on an independent third variable, usually thought of as "time" (that is, when the dependent variables are _x_ and _y_ and are given by parametric equations in _t_). <!--SR:!2025-05-19,67,350!2025-06-13,88,370-->
  - parametric derivative / intuition ::@:: Recall we can consider changing $t$ as running along the curve. Then its parametric derivative is the velocity (vector) of running. <!--SR:!2025-06-13,88,370!2025-06-09,85,370-->
  - parametric derivative / slope ::@:: To extract the slope for a curve on 2D, use $y'(t) / x'(t)$. <!--SR:!2025-06-09,85,370!2025-06-13,88,370-->
    - parametric derivative / slope / indeterminate slope ::@::  Note that the slope as defined above may be indeterminate, e.g. when $x'(t) = y'(t) = 0$. That does not necessarily mean that slope is undefined there (sometimes it really is undefined though, e.g. sharp corners). <p> In said cases, we can try to compute $\lim_{\tau \to t} \frac {y'(\tau)} {x'(\tau)}$, and maybe make use of the L'Hoptial rule if needed. <!--SR:!2025-06-13,88,370!2025-06-19,93,370-->
    - parametric derivative / slope / horizontal slope ::@:: Find $t$ for which $x'(t) \ne 0$ and $y'(t) = 0$. Also check if there are indeterminate slopes and see if they are actually also horizontal. <!--SR:!2025-06-09,85,370!2025-06-18,92,370-->
    - parametric derivative / slope / vertical slope ::@:: Find $t$ for which $x'(t) = 0$ and $y'(t) \ne 0$. Also check if there are indeterminate slopes and see if they are actually also vertical. <!--SR:!2025-06-18,92,370!2025-06-18,92,370-->
- [function of several real variables](../../../../general/function%20of%20several%20real%20variables.md) (multivariable functions) ::@:: It is a function with more than one argument, with all arguments being real variables. <p> In this course, we study real-valued functions, i.e. the codomain is also real. <!--SR:!2025-05-19,67,350!2025-05-19,67,350-->
  - function of several real variables / domain ::@:: The [domain](../../../../general/domain%20of%20a%20function.md) of a function of _n_ variables is the [subset](../../../../general/subset.md) of ⁠$\mathbb {R} ^{n}$⁠ for which the function is defined. As usual, the domain of a function of several real variables is supposed to contain a nonempty [open](../../../../general/open%20set.md) subset of ⁠$\mathbb {R} ^{n}$⁠. <p> When the domain is not specified, it is usually implicitly understood as the largest subset that makes the function defined. <!--SR:!2025-06-20,94,370!2025-06-09,85,370-->
  - function of several real variables / graph ::@:: For _n_ real arguments, we can visualize it in a _n_+1-dimensional space. It also satisfies the vertical line test. <p> For example, when _n_ = 1, we get the graph of a function in 2D. When _n_ = 2, we get a surface in 3D satisfying the vertical line test. When _n_ = 3, we need to visualize in 4D, and good luck with that! <!--SR:!2025-06-09,85,370!2025-06-19,93,370-->
- [level set](../../../../general/level%20set.md) ::@:: It of a [real-valued function](../../../../general/real-valued%20function.md) _f_ of _n_ [real variables](../../../../general/function%20of%20several%20real%20variables.md) is a [set](../../../../general/set%20(mathematics).md) where the function takes on a given [constant](../../../../general/constant%20(mathematics).md) value _c_, that is: $$L_{c}(f)=\left\{(x_{1},\ldots ,x_{n})\mid f(x_{1},\ldots ,x_{n})=c\right\}~.$$ <!--SR:!2025-06-15,90,370!2025-05-18,67,350-->
  - level set / names ::@:: It depends on the number of independent variables to the function. If 2, it is also called a _level curve_. If 3, it is also called a _level surface_. If more than 3, it is also called a _level hypersurface_. <!--SR:!2025-06-20,94,370!2025-05-18,63,350-->
  - level set / visualization ::@:: It can be visualized using _contour lines_ for functions of 2 independent variables. <p> Note that a level curve (_n_ = 2) does not always have to be a curve: it could be a point or a surface. Similarly, a level surface (_n_ = 3) does not always have to be a surface. <!--SR:!2025-06-19,93,370!2025-06-19,93,370-->
  - level set / interpretations ::@:: It is the preimage of the function over $c$. It is also the intersection of the graph of the function with the (hyper)plane $y = c$. <p> Further, if $f$ is _differentiable_, then the gradient of $f$ at a point is either zero, or _perpendicular_ to the level set of $f$ at that point. <!--SR:!2025-06-20,94,370!2025-05-18,63,350-->
- reading: [cycloid](../../../../general/cycloid.md)

## week 3 tutorial

- datetime: 2025-02-18T16:00:00+08:00/2025-02-18T16:50:00+08:00
- [week 3 lecture](#week%203%20lecture)
- [week 3 lecture 2](#week%203%20lecture%202)

## week 3 lecture 2

- datetime: 2025-02-20T13:30:00+08:00/2025-02-20T14:50:00+08:00
- function of several real variables
- level set
- [limit of a function](../../../../general/limit%20of%20a%20function.md) ::@:: a fundamental concept in calculus and analysis concerning the behavior of that function near a particular input which may or may not be in the domain of the function <!--SR:!2025-03-29,24,370!2025-03-30,25,370-->
  - limit of a function / definition (univariate) ::@:: __The limit of _f_ of _x_, as _x_ approaches _p_, exists, and it equals _L_<!-- markdown separator -->__ and write, $$\lim _{x\to p}f(x)=L,$$ if the following property holds: for every real _ε_ \> 0, there exists a real _δ_ \> 0 such that for all real _x_, 0 \< \|_x_ − _p_\| \< _δ_ implies \|_f_\(_x_\) − _L_\| \< _ε_. <!--SR:!2025-03-30,25,370!2025-06-15,85,370-->
  - limit of a function / definition (multivariate) ::@:: __The limit of _f_ as \(_x_, _y_\) approaches \(_p_, _q_\) is _L_<!-- markdown separator -->__, written $$\lim _{(x,y)\to (p,q)}f(x,y)=L$$ if the following condition holds: For every _ε_ \> 0, there exists a _δ_ \> 0 such that for all _x_ in _S_ and _y_ in _T_, whenever $0<{\sqrt {(x-p)^{2}+(y-q)^{2} } }<\delta$, we have \|_f_\(_x_, _y_\) − _L_\| \< _ε_ <!--SR:!2025-03-28,23,370!2025-03-28,23,370-->
    - limit of a function / definition / proving ::@:: To disprove it, it suffices to show there exists two paths to \(_x_, _y_\) with different limits. <p> To prove it, the definition is required, since if we use try to use paths to prove (which we will usually not in practice), we need to check the infinite many ways to make a path, which includes curved paths. <!--SR:!2025-03-28,23,370!2025-03-29,24,370-->
  - algebraic limit theorem ::@:: Taking the limit of algebraic operations (addition, subtraction, multiplication, division, exponentiation) on two functions are compatible with the operation on the limits of the two functions _under some condition_. <p> The _main_ condition is that the limits of the two functions exist (finite, not indeterminate/infinity). Division and exponentiation has some _extra_ conditions. <!--SR:!2025-03-30,25,370!2025-03-29,24,370-->
    - algebraic limit theorem / division ::@:: The limit of the divisor (i.e. $g(x)$ in $f(x)/g(x)$) is non-zero. <!--SR:!2025-03-28,23,370!2025-03-29,24,370-->
    - algebraic limit theorem / exponentiation ::@:: The limit of the base (i.e. $f(x)$ in $(f(x))^{g(x)}$) is either positive, or zero while the limit of the exponent (i.e. $g(x)$ in $(f(x))^{g(x)}$) is positive (finite). <!--SR:!2025-03-29,24,370!2025-03-29,24,370-->
    - algebraic limit theorem / polynomials, rational functions ::@:: This basically allows us to say polynomials have limits everywhere, and rational functions have limits _almost_ everywhere (where the denominator is nonzero). <!--SR:!2025-03-30,25,370!2025-03-28,23,370-->
- [squeeze theorem](../../../../general/squeeze%20theorem.md) ::@:: a theorem regarding the limit of a function that is bounded between two other functions <!--SR:!2025-03-28,23,370!2025-03-28,23,370-->
  - squeeze theorem / theorem (univariate) ::@:: Let _I_ be an [interval](../../../../general/interval%20(mathematics).md) containing the point _a_. Let _g_, _f_, and _h_ be [functions](../../../../general/function%20(mathematics).md) defined on _I_, except possibly at _a_ itself. Suppose that for every _x_ in _I_ not equal to _a_, we have $$g(x)\leq f(x)\leq h(x)$$ and also suppose that $$\lim _{x\to a}g(x)=\lim _{x\to a}h(x)=L.$$ Then $\lim _{x\to a}f(x)=L$. <!--SR:!2025-03-30,25,370!2025-03-30,25,370-->
  - squeeze theorem / theorem (multivariate) ::@:: The squeeze theorem can still be used in multivariable calculus but the lower (and upper functions) must be below (and above) the target function not just along a path but around the entire neighborhood of the point of interest and it only works if the function really does have a limit there. It can, therefore, be used to prove that a function has a limit at a point, but it can never be used to prove that a function does not have a limit at a point. <!--SR:!2025-03-30,25,370!2025-06-04,75,370-->
- [questions § week 3 lecture 2](questions.md#week%203%20lecture%202)

## week 4 lecture

- datetime: 2025-02-25T13:30:00+08:00/2025-02-25T14:50:00+08:00
- [continuous function](../../../../general/continuous%20function.md) ::@:: a function such that a small variation of the argument induces a small variation of the value of the function; no abrupt changes in value, known as _discontinuities_ <!--SR:!2025-04-05,24,374!2025-04-05,24,374-->
  - continuous function / definition ::@:: The function _f_ is _continuous at some point_ _c_ of its domain if the [limit](../../../../geneal/limit%20of%20a%20function.md) of $f(x)$, as _x_ approaches _c_ through the domain of _f_, exists and is equal to $f(c)$. In mathematical notation, this is written as $$\lim _{x\to c}{f(x)}=f(c).$$ <!--SR:!2025-04-04,23,374!2025-04-05,24,374-->
  - continuous function / construction ::@:: Sum (& subtraction), product, and composition of functions preserve continuity. Division preserves continuity excluding where the denominator vanishes (equals 0). <!--SR:!2025-04-04,23,374!2025-04-04,23,374-->
    - continuous function / construction / examples ::@:: Recall that the functions mentioned below have limits everywhere/almost everywhere. <p> Polynomial functions are continuous everywhere. Rational functions are continuous almost everywhere except where the denominator vanishes. <!--SR:!2025-04-06,25,374!2025-04-04,23,374-->
- [partial derivative](../../../../general/partial%20derivative.md) ::@:: It of a function of several variables is its derivative with respect to one of those variables, with the others held constant (as opposed to the total derivative, in which all variables are allowed to vary). <!--SR:!2025-04-03,23,374!2025-03-31,19,354-->
  - partial derivative / in univariate calculus ::@:: There is only one (or two) direction $x$ can change in. Thus in this case, the derivative definition is quite simple and we only have one (first) derivative. <!--SR:!2025-04-06,25,374!2025-04-04,23,374-->
  - partial derivative / intuition ::@:: Fix all arguments to a multivariate function except for one. Then differentiate it with respect to that argument. This is the _partial derivative with respect to that argument_. <p> One partial derivative can be constructed for each argument in this way. <!--SR:!2025-04-06,25,374!2025-04-06,25,374-->
  - partial derivative / notations ::@:: The partial derivative of a function $f(x,y,\dots )$ with respect to the variable $x$ is variously denoted by <p> $f_{x}$, $f'_{x}$, $\partial _{x}f$, $\ D_{x}f$, $D_{1}f$, ${\frac {\partial }{\partial x} }f$, or ${\frac {\partial f}{\partial x} }$. <!--SR:!2025-04-06,25,374!2025-04-05,24,374-->
  - partial derivative / computation ::@:: Assume all other variables are constant. Then differentiate as if it is a univariate function. <p> Implicit differentiation is another technique. It works similar to that applied to a univariate function. If a variable is neither the variable being differentiated against (output) nor being differentiated with respect to (input), consider it fixed. <!--SR:!2025-04-06,25,374!2025-05-27,66,374-->
  - partial derivative / higher order ::@:: They are defined analogously to the higher order derivatives of univariate functions: Just partial differentiate multiple times. <!--SR:!2025-04-06,25,374!2025-04-06,25,374-->
    - partial derivative / higher order / notations ::@:: For the function $f(x,y,...)$ the "own" second partial derivative with respect to _x_ is simply the partial derivative of the partial derivative \(both with respect to _x_\): $${\frac {\partial ^{2}f}{\partial x^{2} } }\equiv \partial {\frac {\partial f/\partial x}{\partial x} }\equiv {\frac {\partial f_{x} }{\partial x} }\equiv f_{xx}.$$ <p> The cross partial derivative with respect to _x_ and _y_ is obtained by taking the partial derivative of _f_ with respect to _x_, and then taking the partial derivative of the result with respect to _y_, to obtain $${\frac {\partial ^{2}f}{\partial y\,\partial x} }\equiv \partial {\frac {\partial f/\partial x}{\partial y} }\equiv {\frac {\partial f_{x} }{\partial y} }\equiv f_{xy}.$$ <!--SR:!2025-04-06,25,374!2025-04-05,24,374-->
- [symmetry of second derivatives](../../../../general/symmetry%20of%20second%20derivatives.md) ::@:: It is the fact that exchanging the order of [partial derivatives](../../../../general/partial%20derivative.md) of a [multivariate function](../../../../general/multivariate%20function.md#multivariate%20functions) $$f\left(x_{1},\,x_{2},\,\ldots ,\,x_{n}\right)$$ does not change the result if some [continuity](../../../../general/continuous%20function.md) conditions are satisfied \(see below\); that is, the second-order partial derivatives satisfy the [identities](../../../../general/identity%20(mathematics).md) $${\frac {\partial }{\partial x_{i} } }\left({\frac {\partial f}{\partial x_{j} } }\right)\ =\ {\frac {\partial }{\partial x_{j} } }\left({\frac {\partial f}{\partial x_{i} } }\right).$$ <!--SR:!2025-04-06,25,374!2025-04-04,23,374-->
  - symmetry of second derivatives / Hessian matrix ::@:: In other words, the matrix of the second-order partial derivatives, known as the [Hessian matrix](../../../../general/Hessian%20matrix.md), is a [symmetric matrix](../../../../general/symmetric%20matrix.md). <!--SR:!2025-04-05,24,374!2025-04-05,24,374-->
  - symmetry of second derivatives / Schwarz's theorem ::@:: It states that for a function $f\colon \Omega \to \mathbb {R}$ defined on a set $\Omega \subset \mathbb {R} ^{n}$, if $\mathbf {p} \in \mathbb {R} ^{n}$ is a point such that some [neighborhood](../../../../general/neighbourhood%20(mathematics).md) of $\mathbf {p}$ is contained in $\Omega$ and $f$ has [continuous](../../../../general/continuous%20function.md) second [partial derivatives](../../../../general/partial%20derivatives.md) on that neighborhood of $\mathbf {p}$, then for all _i_ and _j_ in $\{1,2\ldots ,\,n\}$, $${\frac {\partial ^{2} }{\partial x_{i}\,\partial x_{j} } }f(\mathbf {p} )={\frac {\partial ^{2} }{\partial x_{j}\,\partial x_{i} } }f(\mathbf {p} ).$$ The partial derivatives of this function commute at that point. <!--SR:!2025-04-06,25,374!2025-03-31,19,354-->
  - symmetry of second derivatives / usage ::@:: After checking the conditions hold, this may be used to simplify expressions involving mixed derivatives. <!--SR:!2025-04-05,24,374!2025-04-05,24,374-->

## week 4 tutorial

- datetime: 2025-02-25T16:00:00+08:00/2025-02-25T16:50:00+08:00
- [week 4 lecture](#week%204%20lecture)
- [week 4 lecture 2](#week%204%20lecture%202)

## week 4 lecture 2

- datetime: 2025-02-27T13:30:00+08:00/2025-02-27T14:50:00+08:00
- [tangent](../../../../general/tangent.md) ::@:: It to a plane curve at a given point is, intuitively, the straight line that "just touches" the curve at that point. <!--SR:!2025-04-05,24,374!2025-04-06,25,374-->
  - tangent / tangent plane ::@:: It to a surface at a given point is the plane that "just touches" the surface at that point. <!--SR:!2025-04-06,25,374!2025-04-05,24,374-->
  - tangent / tangent space ::@:: It of a manifold is a generalization of tangent lines to curves in two-dimensional space and tangent planes to surfaces in three-dimensional space in higher dimensions. <!--SR:!2025-04-05,24,374!2025-04-06,25,374-->
  - tangent / existence ::@:: Note that for the tangent to be defined, the function needs to be _differentiable_ at that point. Otherwise, either the tangent is vertical (infinite slope), the point is a cusp (infinite slope, different sign), the point is a corner (finite slope, different slope), or the point is discontinuous. <p> We assume the functions are _differentiable_ at the relevant points here and below. <!--SR:!2025-04-04,23,374!2025-04-05,24,374-->
  - tangent / tangent plane
    - tangent / tangent plane / interpretation ::@:: It is the best approximation of the surface by a plane at _p_, and can be obtained as the limiting position of the planes passing through 3 distinct points on the surface close to _p_ as these points converge to _p_. <!--SR:!2025-04-04,23,374!2025-04-06,25,374-->
    - tangent / tangent plane / equation ::@:: if the surface is given by a function $z=f(x,y)$, the equation of the tangent plane at point $(x_{0},y_{0},z_{0})$ can be expressed as: $$z-z_{0}={\frac {\partial f}{\partial x} }(x_{0},y_{0})(x-x_{0})+{\frac {\partial f}{\partial y} }(x_{0},y_{0})(y-y_{0}) \,.$$ Here, ${\frac {\partial f}{\partial x} }$ and ${\frac {\partial f}{\partial y} }$ are the partial derivatives of the function $f$ with respect to $x$ and $y$ respectively, evaluated at the point $(x_{0},y_{0})$. It can be written into $${\frac {\partial f}{\partial x} }(x_{0},y_{0})(x-x_{0})+{\frac {\partial f}{\partial y} }(x_{0},y_{0})(y-y_{0}) + (-1) (z - z_0) = 0 \,,$$ which has a normal vector $\left\langle \frac {\partial f} {\partial x}, \frac {\partial f} {\partial y}, -1 \right\rangle$ that can be interpreted as the surface normal pointing "downwards". <!--SR:!2025-04-04,23,374!2025-04-04,23,374-->
    - tangent / tangent plane / level surface ::@:: We have a function $f: \mathbb R^3 \to \mathbb R$. Define the level surface $f$ at $k$ as $f(x, y, z) = k$. To generalize, this level surface is not necessary a surface. <p> Find its _tangent plane_ (tangent space) by first considering its orthogonal vector (space), its _normal vector_ (normal space). Conveniently, gradient of the function at that point is either orthogonal to the level set, which makes the gradient a normal vector; or zero, which means the normal space is the zero vector space. So we have: $$\frac {\partial f} {\partial x} (x_0, y_0, z_0) (x - x_0) + \frac {\partial f} {\partial y} (x_0, y_0, z_0) (y - y_0) + \frac {\partial f} {\partial z} (x_0, y_0, z_0) (z - z_0) = 0 \,,$$ and check that the above "makes sense" when the gradient is zero (tangent space is the whole space). <!--SR:!2025-04-27,37,334!2025-05-13,53,354-->
- [differentiable function](../../../../general/differentiale%20function.md)
  - differentiable function / approximation ::@:: A differentiable function is locally well approximated as a linear function at each interior point. That is, an approximation at $x_0$ has an approximation error that shrinks faster than $\lvert x - x_0 \rvert$, i.e. in $o(\lvert x - x_0 \rvert)$ using small O notation. <!--SR:!2025-04-06,25,374!2025-04-05,24,374-->
- function of several real variables
  - function of several real variables / multivariate differentiability ::@:: A function _f_\(___x___\) is __differentiable__ in a neighborhood of a point ___a___ if there is an _n_-tuple of numbers dependent on ___a___ in general, ___A___\(___a___\) = \(_A_<sub>1</sub>\(___a___\), _A_<sub>2</sub>\(___a___\), …, _A_<sub>_n_</sub>\(___a___\)\), so that: $$f({\boldsymbol {x} })=f({\boldsymbol {a} })+{\boldsymbol {A} }({\boldsymbol {a} })\cdot ({\boldsymbol {x} }-{\boldsymbol {a} })+\alpha ({\boldsymbol {x} })|{\boldsymbol {x} }-{\boldsymbol {a} }|$$ where $\alpha ({\boldsymbol {x} })\to 0$ as $|{\boldsymbol {x} }-{\boldsymbol {a} }|\to 0$. If _f_ is differentiable at ___a___ then the first order partial derivatives exist at ___a___ and: $$\left.{\frac {\partial f({\boldsymbol {x} })}{\partial x_{i} } }\right|_{ {\boldsymbol {x} }={\boldsymbol {a} } }=A_{i}({\boldsymbol {a} })$$ for _i_ = 1, 2, …, _n_, which can be found from the definitions of the individual partial derivatives, so the partial derivatives of _f_ exist (but they are not necessarily _continuous_). <!--SR:!2025-04-04,23,374!2025-04-03,23,374-->
    - function of several real variables / multivariate differentiability / limits ::@:: Assuming the function is _differentiable_ at $\mathbf a$ (otherwise the directional derivative may exist but not the gradient, or vice versa). If you prefer limits instead: $$\lim_{\mathbf x \to \mathbf a} \frac {f(\mathbf x) - f(\mathbf a)} {\lvert \mathbf x - \mathbf a \rvert} = ((\mathbf x - \mathbf a) \cdot \nabla) f = (\mathbf x - \mathbf a) \cdot (\nabla f) \,,$$ or $$\lim_{\mathbf x \to \mathbf a} \frac {f(\mathbf x) - f(\mathbf a) - ((\mathbf x - \mathbf a) \cdot \nabla) f} {\lvert \mathbf x - \mathbf a \rvert} = \lim_{\mathbf x \to \mathbf a} \frac {f(\mathbf x) - f(\mathbf a) - (\mathbf x - \mathbf a) \cdot (\nabla f)} {\lvert \mathbf x - \mathbf a \rvert} = 0 \,.$$ <!--SR:!2025-04-04,23,374!2025-05-11,51,354-->
    - function of several real variables / multivariate differentiability / intuition ::@:: Intuitively, the tangent plane at point ___a___ has an approximation error that shrinks faster than $\lvert \mathbf x - \mathbf a \rvert$, i.e. in $o(\lvert \mathbf x - \mathbf a \rvert)$ using small O notation. <!--SR:!2025-04-04,23,374!2025-04-04,23,374-->
    - function of several real variables / multivariate differentiability / relations ::@:: continuity: Multivariate differentiability implies multivariate continuity. The converse is not true. <p> partial differentiability: Having all partial derivatives (including mixed) up to order $p$ is _not_ sufficient to imply $p$-times multivariate differentiability. Having all _continuous_ partial derivatives (including mixed) up to order $p$ is sufficient, but not necessary: there are multivariate differentiable functions that do not have all _continuous_ partial derivatives (but they do have all partial derivatives). Examples can be constructed using $\sin(1 / x)$. <!--SR:!2025-04-05,24,374!2025-04-04,23,374-->
    - function of several real variables / multivariate differentiability / approximation ::@:: Assuming the function is _differentiable_ at $\mathbf a$ (otherwise the directional derivative may exist but not the gradient, or vice versa). Then the _linear_ approximation is: $$f(\mathbf x) \approx f(\mathbf a) + ((\mathbf x - \mathbf a) \cdot \nabla) f(\mathbf a) = f(\mathbf a) + (\mathbf x - \mathbf a) \cdot (\nabla f(\mathbf a)) \,.$$ <!--SR:!2025-04-05,24,374!2025-04-04,23,374-->

## week 5 lecture

- datetime: 2025-03-04T13:30:00+08:00/2025-03-04T14:50:00+08:00
- [chain rule](../../../../general/chain%20rule.md) ::@:: It is a [formula](../../../../general/formula.md) that expresses the [derivative](../../../../general/derivative.md) of the [composition](../../../../general/function%20composition.md) of two [differentiable functions](../../../../general/differentiable%20function.md) _f_ and _g_ in terms of the derivatives of _f_ and _g_. <!--SR:!2025-04-12,26,386!2025-03-29,14,365-->
  - chain rule / univariate ::@:: It is expressed as $${\frac {dz}{dx} }={\frac {dz}{dy} }\cdot {\frac {dy}{dx} },$$ and $$\left.{\frac {dz}{dx} }\right|_{x}=\left.{\frac {dz}{dy} }\right|_{y(x)}\cdot \left.{\frac {dy}{dx} }\right|_{x},$$for indicating at which points the derivatives have to be evaluated. <!--SR:!2025-04-12,26,386!2025-04-12,26,386-->
  - chain rule / scalar-valued multivariate ::@:: Denote by $D_{i}f$ the partial derivative of _f_ with respect to its _i_<!-- markdown separator -->th argument, and by $D_{i}f(z)$ the value of this derivative at _z_. <p> With this notation, the chain rule is $${\frac {d}{dx} }f(g_{1}(x),\dots ,g_{k}(x))=\sum _{i=1}^{k}\left({\frac {d}{dx} }{g_{i} }(x)\right)D_{i}f(g_{1}(x),\dots ,g_{k}(x)).$$ <!--SR:!2025-04-10,24,386!2025-04-03,17,365-->
    - chain rule / scalar-valued multivariate / intuition ::@:: In a scalar-valued multivariate function $f$, there are multiple inputs $g_i$. Each input can contribute to change in the output. If all inputs are functions of another variable $x$, $x$ contributes to change in the output of $f$ via all inputs $g_i$. Thus, we need to add up the changes. <!--SR:!2025-04-12,26,386!2025-04-12,26,386-->
  - chain rule / vector-valued multivariate ::@:: The simplest way for writing the chain rule in the general case is to use the [total derivative](../../../../general/total%20derivative.md#the%20total%20derivative%20as%20a%20linear%20map), which is a linear transformation that captures all [directional derivatives](../../../../general/directional%20derivative.md) in a single formula. Consider differentiable functions _f_ : __R__<sup>_m_</sup> → __R__<sup>_k_</sup> and _g_ : __R__<sup>_n_</sup> → __R__<sup>_m_</sup>, and a point __a__ in __R__<sup>_n_</sup>. Let _D_<sub>__a__</sub> _g_ denote the total derivative of _g_ at __a__ and _D_<sub>_g_\(__a__\)</sub> _f_ denote the total derivative of _f_ at _g_\(__a__\). These two derivatives are linear transformations __R__<sup>_n_</sup> → __R__<sup>_m_</sup> and __R__<sup>_m_</sup> → __R__<sup>_k_</sup> \(annotation: same domain and codomain as the original functions\), respectively, so they can be composed. The chain rule for total derivatives is that their composite is the total derivative of _f_ ∘ _g_ at __a__: $$D_{\mathbf {a} }(f\circ g)=D_{g(\mathbf {a} )}f\circ D_{\mathbf {a} }g,$$ or for short, $$D(f\circ g)=Df\circ Dg.$$ <!--SR:!2025-04-12,26,386!2025-04-03,17,366-->
    - chain rule / vector-valued multivariate / matrix ::@:: Because the total derivative is a linear transformation, the functions appearing in the formula can be rewritten as matrices. The matrix corresponding to a total derivative is called a [Jacobian matrix](../../../../general/Jacobian%20matrix.md), and the composite of two derivatives corresponds to the product of their Jacobian matrices. From this perspective the chain rule therefore says: $$J_{f\circ g}(\mathbf {a} )=J_{f}(g(\mathbf {a} ))J_{g}(\mathbf {a} ),$$or for short, $$J_{f\circ g}=(J_{f}\circ g)J_{g}.$$ That is, the Jacobian of a composite function is the product of the Jacobians of the composed functions \(evaluated at the appropriate points\). <!--SR:!2025-04-12,26,385!2025-04-12,26,386-->
- [Jacobian matrix](../../../../general/Jacobian%20matrix.md) ::@:: It of a [vector-valued function](../../../../general/vector-valued%20function.md) of several variables is the [matrix](../../../../general/matrix%20(mathematics).md) of all its first-order [partial derivatives](../../../../general/partial%20derivative.md). <!--SR:!2025-04-10,24,385!2025-04-11,25,386-->
  - Jacobian matrix / definition ::@:: Suppose __f__ : __R__<sup>_n_</sup> → __R__<sup>_m_</sup> is a function such that each of its first-order partial derivatives exists on __R__<sup>_n_</sup>. This function takes a point __x__ ∈ __R__<sup>_n_</sup> as input and produces the vector __f__\(__x__\) ∈ __R__<sup>_m_</sup> as output. Then the Jacobian matrix of __f__, denoted __J<sub>f</sub>__ ∈ __R__<sup>_m_<!-- markdown separator -->×<!-- markdown separator -->_n_</sup>, is defined such that its \(_i_,_j_\)<sup>th</sup> entry is ${\frac {\partial f_{i} }{\partial x_{j} } }$, or explicitly $$\mathbf {J_{f} } ={\begin{bmatrix}{\dfrac {\partial \mathbf {f} }{\partial x_{1} } }&\cdots &{\dfrac {\partial \mathbf {f} }{\partial x_{n} } }\end{bmatrix} }={\begin{bmatrix}\nabla ^{\mathsf {T} }f_{1}\\\vdots \\\nabla ^{\mathsf {T} }f_{m}\end{bmatrix} }={\begin{bmatrix}{\dfrac {\partial f_{1} }{\partial x_{1} } }&\cdots &{\dfrac {\partial f_{1} }{\partial x_{n} } }\\\vdots &\ddots &\vdots \\{\dfrac {\partial f_{m} }{\partial x_{1} } }&\cdots &{\dfrac {\partial f_{m} }{\partial x_{n} } }\end{bmatrix} }$$ where $\nabla ^{\mathsf {T} }f_{i}$ is the transpose \(row vector\) of the [gradient](../../../../general/gradient.md) of the $i$-th component. <!--SR:!2025-04-11,25,386!2025-04-11,25,386-->
- [directional derivative](../../../../general/directional%20derivative.md) ::@:: It measures the rate at which a function changes in a particular direction at a given point. <!--SR:!2025-04-12,26,386!2025-04-12,26,386-->
  - directional derivative / definition ::@:: It of a [scalar function](../../../../general/scalar%20function.md) $$f(\mathbf {x} )=f(x_{1},x_{2},\ldots ,x_{n})$$ along a vector $$\mathbf {v} =(v_{1},\ldots ,v_{n})$$ is the [function](../../../../general/function%20(mathematics).md) $\nabla _{\mathbf {v} }{f}$ defined by the [limit](../../../../general/limit%20(mathematics).md) $$\nabla _{\mathbf {v} }{f}(\mathbf {x} )=\lim _{h\to 0}{\frac {f(\mathbf {x} +h\mathbf {v} )-f(\mathbf {x} )}{h} }.$$ <p> This definition is valid in a broad range of contexts, for example where the [norm](../../../../general/Euclidean%20norm.md#Euclidean%20norm) of a vector \(and hence a unit vector\) is undefined. <p> \(this course: __important__, we do _not_ use this definition here, since we require a unit vector\) <!--SR:!2025-04-12,26,386!2025-04-12,26,386-->
    - directional derivative / definition / direction only ::@:: In a [Euclidean space](../../../../general/Euclidean%20space.md), some authors define the directional derivative to be with respect to an arbitrary nonzero vector __v__ after [normalization](../../../../general/normalized%20vector.md), thus being independent of its magnitude and depending only on its direction. <p> This definition gives the rate of increase of _f_ per unit of distance moved in the direction given by __v__. In this case, one has $$\nabla _{\mathbf {v} }{f}(\mathbf {x} )=\lim _{h\to 0}{\frac {f(\mathbf {x} +h\mathbf {v} )-f(\mathbf {x} )}{h|\mathbf {v} |} },$$ or in case _f_ is differentiable at __x__, $$\nabla _{\mathbf {v} }{f}(\mathbf {x} )=\nabla f(\mathbf {x} )\cdot {\frac {\mathbf {v} }{|\mathbf {v} |} }.$$ <p> \(this course: __important__, we use this definition here, as we require a unit vector\) <!--SR:!2025-04-12,26,385!2025-04-12,26,386-->
    - directional derivative / definition / differentiable ::@:: If the function _f_ is [differentiable](../../../../general/differentiable%20function.md#differentiability%20in%20higher%20dimensions) at __x__, then the directional derivative exists along any unit vector __v__ at x, and one has $$\nabla _{\mathbf {v} }{f}(\mathbf {x} )=\nabla f(\mathbf {x} )\cdot \mathbf {v}$$ where the $\nabla$ on the right denotes the _[gradient](../../../../general/gradient.md)_, $\cdot$ is the [dot product](../../../../general/dot%20product.md) and __v__ is a unit vector. <!--SR:!2025-04-12,26,386!2025-04-11,25,385-->
  - directional derivative / intuition ::@:: The partial derivatives we have considered above are actually special cases of directional derivatives, where the direction is along the coordinate directions. Directional derivative generalizes this by allowing directions that are _linear combinations_ of coordinate directions. <p> When the multivariate function is _differentiable_, the directional derivative along all directions _exist_, and is just the dot product of the direction vector with the gradient. <!--SR:!2025-04-12,26,386!2025-04-12,26,386-->
- [total derivative](../../../../general/total%20derivative.md) ::@:: It of a function _f_ at a point is the best [linear approximation](../../../../general/linear%20approximation.md) near this point of the function with respect to its arguments. Unlike [partial derivatives](../../../../general/partial%20derivative.md), the total derivative approximates the function with respect to all of its arguments, not just a single one. In many situations, this is the same as considering all partial derivatives simultaneously. <!--SR:!2025-04-12,26,386!2025-04-11,25,386-->
  - total derivative / differential form ::@:: When the function under consideration is real-valued, the total derivative can be recast using [differential forms](../../../../general/differential%20form.md). For example, suppose that $f\colon \mathbb {R} ^{n}\to \mathbb {R}$ is a differentiable function of variables $x_{1},\ldots ,x_{n}$. The total derivative of $f$ at $a$ may be written in terms of its Jacobian matrix, which in this instance is a row matrix: $$Df_{a}={\begin{bmatrix}{\frac {\partial f}{\partial x_{1} } }(a)&\cdots &{\frac {\partial f}{\partial x_{n} } }(a)\end{bmatrix} }.$$ The linear approximation property of the total derivative implies that if $$\Delta x={\begin{bmatrix}\Delta x_{1}&\cdots &\Delta x_{n}\end{bmatrix} }^{\mathsf {T} }$$ is a small vector \(where the ${\mathsf {T} }$ denotes transpose, so that this vector is a column vector\), then $$f(a+\Delta x)-f(a)\approx Df_{a}\cdot \Delta x=\sum _{i=1}^{n}{\frac {\partial f}{\partial x_{i} } }(a)\cdot \Delta x_{i}.$$ Heuristically, this suggests that if $dx_{1},\ldots ,dx_{n}$ are [infinitesimal](infinitesimal.md) increments in the coordinate directions, then $$df_{a}=\sum _{i=1}^{n}{\frac {\partial f}{\partial x_{i} } }(a)\cdot dx_{i}.$$ <!--SR:!2025-04-03,17,366!2025-04-12,26,386-->

## week 5 tutorial

- datetime: 2025-03-04T16:00:00+08:00/2025-03-04T16:50:00+08:00
- [week 5 lecture](#week%205%20lecture)
- [week 5 lecture 2](#week%205%20lecture%202)
- [implicit differentiation](../../../../general/implicit%20differentiation.md)
  - implicit differentiation / implicit functions ::@:: If you have a _explicit_ function in the form of $y = f(...)$, you can always write it in terms of $g(..., y) := f(...) - y = 0$. Then calculate its partial derivatives. Then partial derivatives between inputs of the original function $f$ can easily be obtained, by realizing that the inputs \(including $y$\) to the new function $g$ must change in a way such that $g$ remains zero, i.e. the directional derivatives of $g$ are always zero. <!--SR:!2025-04-11,25,386!2025-04-12,26,386--> <p> This method may seem cumbersome, but it would make sense if you consider implicit functions that have no or complicated explicit forms. <p> For example, $y = f(x)$ can be changed into $g(x, y) := f(x) - y = 0$. Then, $\frac {\partial y} {\partial x} = -\frac {\partial g} {\partial x} / \frac {\partial g} {\partial y}$ \(note the negative sign, which shows why you should not be "simplifying" partial derivatives\). This is because $\frac {\partial g} {\partial x} = - \frac {\partial y} {\partial x} \frac {\partial g} {\partial y}$, which is interpreted as: Assuming we only change $x$ and $y$. The rates of change between $x$ and $y$ must be such that the change in $g$ caused by the change in $x$ is neutralized by the change in $g$ caused by the change in $y$, in order to keep $g = 0$.

## week 5 lecture 2

- datetime: 2025-03-06T13:30:00+08:00/2025-03-06T14:50:00+08:00
- [gradient](../../../../general/gradient.md) ::@:: It of a scalar-valued differentiable function $f$ of several variables is the vector field (or vector-valued function) $\nabla f$ whose value at a point $p$ gives the direction and the rate of fastest increase. <!--SR:!2025-04-12,26,386!2025-04-10,24,386-->
  - gradient / intuition ::@:: Recall when a function is differentiable, the directional derivative along all directions _exist_, and is just the dot product of the direction vector with the gradient. The dot product, hence direction derivative, has the greatest value \(_steepest ascent_\) when it has the same direction as the gradient. Similarly, it has the least value \(_steepest descent_\) when it is antiparallel as the gradient. <p> However, do note that gradient also encodes the _rate_ of fastest increase, not only just the _direction_. <!--SR:!2025-04-12,26,386!2025-04-10,24,385-->
  - gradient / direction ::@:: The gradient is not necessarily a unit vector. You need to normalize it to get its direction. Also, be aware of zero gradient, for which the gradient has no direction. <!--SR:!2025-04-11,25,386!2025-04-11,25,386-->
  - gradient / level set ::@:: If $f$ is _differentiable_, then the gradient of $f$ at a point is either zero, or _perpendicular_ to the level set of $f$ at that point. <!--SR:!2025-04-11,25,386!2025-04-11,25,386-->
  - gradient / visualization ::@:: On a contour map, the gradient, visualized as arrows, is perpendicular to the contour lines. Its magnitude is higher when the contour lines are denser, and vice versa. <!--SR:!2025-04-10,24,386!2025-04-12,26,386-->
- level set
- [maximum and minimum](../../../../general/maximum%20and%20minimum.md) ::@:: It of a [function](../../../../general/function%20(mathematics).md) are, respectively, the greatest and least value taken by the function. Known generically as __extremum__, they may be defined either within a given [range](../../../../general/interval%20(mathematics).md) \(the _local_ or _relative_ extrema\) or on the entire [domain](../../../../general/domain%20of%20a%20function.md) \(the _global_ or _absolute_ extrema\) of a function. <!--SR:!2025-04-10,24,386!2025-04-10,24,386-->
  - maximum and minimum / definition ::@:: An extremum can be _local_ or _global_ \(implies local\). An extremum can be additionally said to be _weak_ \(usually not explicitly written out\) or _strict_. <!--SR:!2025-04-12,26,386!2025-04-12,26,386-->
    - maximum and minimum / definition / local ::@:: If the domain _X_ is a [metric space](../../../../general/metric%20space.md), then _f_ is said to have a __local__ \(or __relative__\) __maximum point__ at the point _x_<sup>∗</sup>, if there exists some _ε_ \> 0 such that _f_\(_x_<sup>∗</sup>\) ≥ _f_\(_x_\) for all _x_ in _X_ within distance _ε_ of _x_<sup>∗</sup>. Similarly, the function has a __local minimum point__ at _x_<sup>∗</sup>, if _f_\(_x_<sup>∗</sup>\) ≤ _f_\(_x_\) for all _x_ in _X_ within distance _ε_ of _x_<sup>∗</sup>. A similar definition can be used when _X_ is a [topological space](../../../../general/topological%20space.md), since the definition just given can be rephrased in terms of neighbourhoods. Mathematically, the given definition is written as follows: <p> &emsp; Let $(X,d_{X})$ be a metric space and function $f:X\to \mathbb {R}$. Then $x_{0}\in X$ is a local maximum point of function $f$ if $(\exists \varepsilon >0)$ such that $(\forall x\in X)\,d_{X}(x,x_{0})<\varepsilon \implies f(x_{0})\geq f(x)$. <p> The definition of local minimum point can also proceed similarly. <!--SR:!2025-04-12,26,385!2025-04-12,26,386-->
  - maximum and minimum / gradient ::@:: The gradient is zero. <p> Consider any line parametrization through the an extremum of a multivariate function. The parameterized function has an extremum when it passes through the extremum of the multivariate function, which has a derivative of zero. So the directional derivatives \(in particular, the partial derivatives\) are all zero. <!--SR:!2025-04-10,24,386!2025-04-12,26,386-->
- [critical point](../../../../general/critical%20point%20(mathematics).md) ::@:: It is the argument of a function where the function derivative is zero \(or undefined, as specified below\). <!--SR:!2025-04-12,26,386!2025-04-12,26,386-->
  - critical point / multivariate ::@:: It is a point where the gradient is zero or undefined. <!--SR:!2025-04-12,26,386!2025-04-10,24,386-->
  - critical point / interpretation ::@:: A critical point \(where the function is _differentiable_\) may be either a [local maximum](../../../../general/local%20maximum.md), a [local minimum](../../../../general/local%20minimum.md) or a [saddle point](../../../../general/saddle%20point.md). <p> In univariate calculus, saddle point is a [point](../../../../general/point%20(geometry).md) which is both a [stationary point](../../../../general/stationary%20point.md) and a [point of inflection](../../../../general/inflection%20point.md). <!--SR:!2025-04-03,17,366!2025-04-12,26,386-->

## week 6 lecture

- datetime: 2025-03-11T13:30:00+08:00/2025-03-11T14:50:00+08:00
- [derivative test](../../../../general/derivative%20test.md) ::@:: It uses the derivatives of a function to locate the critical points of a function and determine whether each point is a local maximum, a local minimum, or a saddle point. Derivative tests can also give information about the concavity of a function. <!--SR:!2025-03-23,5,377!2025-04-12,21,377-->
  - derivative test / second-derivative test ::@:: After establishing the [critical points](../../../../general/critical%20point%20(mathematics).md) of a function, the _second-derivative test_ uses the value of the [second derivative](../../../../general/second%20derivative.md) at those points to determine whether such points are a local [maximum](../../../../general/maxima%20and%20minima.md) or a local minimum. If the function _f_ is twice-differentiable at a critical point _x_ \(i.e. a point where _f′_\(_x_\) = 0\), then: <p> - If $f''(x)<0$, then $f$ has a local maximum at $x$. <br/> - If $f''(x)>0$, then $f$ has a local minimum at $x$. <br/> - If $f''(x)=0$, the test is inconclusive. <p> In the last case, [Taylor's theorem](../../../../general/Taylor's%20theorem.md#Taylor's%20theorem%20in%20one%20real%20variable) may sometimes be used to determine the behavior of _f_ near _x_ using [higher derivatives](../../../../general/higher%20derivative.md#higher-order%20derivatives). <!--SR:!2025-03-23,5,377!2025-03-23,5,377-->
- [second partial derivative test](../../../../general/second%20partial%20derivative%20test.md) ::@:: It is a method in multivariable calculus used to determine if a critical point of a function is a local minimum, maximum or saddle point. <!--SR:!2025-03-23,5,377!2025-03-23,5,377-->
  - second partial derivative test / functions of two variables ::@:: Suppose that _f_\(_x_, _y_\) is a differentiable [real function](../../../../general/real%20function.md) of two variables whose second [partial derivatives](../../../../general/partial%20derivative.md) exist and are [continuous](../../../../general/continuous%20function.md). <p> Find its Hessian matrix \(Some definitions may have this matrix transposed, but the results are equivalent for a $C_2$ function.\). Check the signs of its determinant $D$ and its upper left element $f_{xx}$ evaluated at a _critical point_ \(its partial first derivatives are zero\) to see if that point is a local minimum, local maximum, saddle point, or inconclusive \(could be local minimum, local maximum, or saddle point\). <!--SR:!2025-03-23,5,377!2025-04-11,20,377-->
    - second partial derivative test / functions of two variables / detail ::@:: The [Hessian matrix](../../../../general/Hessian%20matrix.md) _H_ of _f_ is the 2 × 2 matrix of partial derivatives of _f_: $$H(x,y)={\begin{bmatrix}f_{xx}(x,y)&f_{xy}(x,y)\\f_{yx}(x,y)&f_{yy}(x,y)\end{bmatrix} }.$$ \(annotation: Some definitions may have this matrix transposed, but the results are equivalent for a $C_2$ function; this course: the above form is in the lecture notes\) Define _D_\(_x_, _y_\) to be the [determinant](../../../../general/determinant.md) $$D(x,y)=\det(H(x,y))=f_{xx}(x,y)f_{yy}(x,y)-\left(f_{xy}(x,y)\right)^{2}$$ of _H_. Finally, suppose that \(_a_, _b_\) is a critical point of _f_, that is, that _f_<sub>_x_</sub>\(_a_, _b_\) = _f_<sub>_y_</sub>\(_a_, _b_\) = 0. Then the second partial derivative test asserts the following: <p> 1. If _D_\(_a_, _b_\) \> 0 and _f<sub>xx</sub>_\(_a_, _b_\) \> 0 then \(_a_, _b_\) is a local minimum of _f_. <br/> 2. If _D_\(_a_, _b_\) \> 0 and _f<sub>xx</sub>_\(_a_, _b_\) \< 0 then \(_a_, _b_\) is a local maximum of _f_. <br/> 3. If _D_\(_a_, _b_\) \< 0 then \(_a_, _b_\) is a [saddle point](saddle%20point.md) of _f_. <br/> 4. If _D_\(_a_, _b_\) = 0 then the point \(_a_, _b_\) could be any of a minimum, maximum, or saddle point \(that is, the test is inconclusive\). <!--SR:!2025-03-23,5,377!2025-03-23,5,377-->
    - second partial derivative test / functions of two variables / corollary ::@:: A condition implicit in the statement of the test is that if $f_{xx}=0$ or $f_{yy}=0$, it must be the case that $D(a,b)\leq 0$, and therefore only cases 3 or 4 are possible. <!--SR:!2025-03-23,5,377!2025-03-23,5,377-->
  - second partial derivative test / functions of many variables ::@:: For a function _f_ of three or more variables, there is a generalization of the rule shown above. In this context, instead of examining the determinant of the Hessian matrix, one must look at the [eigenvalues](../../../../general/eigenvalues%20and%20eigenvectors.md) of the Hessian matrix at the critical point. <!--SR:!2025-03-23,5,377!2025-03-23,5,377-->
    - second partial derivative test / functions of many variables / detail ::@:: The following test can be applied at any critical point _a_ for which the Hessian matrix is [invertible](../../../../general/invertible%20matrix.md): <p> 1. If the Hessian is [positive definite](../../../../general/positive-definite%20matrix.md) \(equivalently, has all eigenvalues positive\) at _a_, then _f_ attains a local minimum at _a_. <br/> 2. If the Hessian is negative definite \(equivalently, has all eigenvalues negative\) at _a_, then _f_ attains a local maximum at _a_. <br/> 3. If the Hessian has both positive and negative eigenvalues then _a_ is a saddle point for _f_ \(and in fact this is true even if _a_ is degenerate\). <p> In those cases not listed above \(annotation: e.g. non-invertible Hessian\), the test is inconclusive. <!--SR:!2025-03-23,5,377!2025-04-11,20,377-->
    - second partial derivative test / functions of many variables / two variables ::@:: In the two variable case, $D(a,b)$ and $f_{xx}(a,b)$ are the principal [minors](../../../../general/minor%20(linear%20algebra).md) of the Hessian. The first two conditions listed above on the signs of these minors are the conditions for the positive or negative definiteness of the Hessian. <!--SR:!2025-03-23,5,377!2025-03-23,5,377-->
    - second partial derivative test / functions of many variables / Sylvester's criterion ::@:: For the general case of an arbitrary number _n_ of variables, there are _n_ sign conditions on the _n_ principal minors of the Hessian matrix that together are equivalent to positive or negative definiteness of the Hessian \([Sylvester's criterion](../../../../general/Sylvester's%20criterion.md)\): for a local minimum, all the principal minors need to be positive, while for a local maximum, the minors with an odd number of rows and columns need to be negative and the minors with an even number of rows and columns need to be positive. <!--SR:!2025-03-23,5,377!2025-03-23,5,377-->
  - second partial derivative test / intuition ::@:: The intuition is a bit hard to grasp. Below assumes functions of two variables \(of class $C_2$\) only. <p> If the Hessian determinant \> 0, then along all directions, the resulting univariate functions have the same convexity, so looking at $f_{xx}$ suffices to determine if it is a local maximum or local minimum. If the Hessian determinant \< 0, then the resulting univariate functions do not have the same convexity, so it is a saddle point. <!--SR:!2025-03-23,5,377!2025-03-23,5,377-->
  - second partial derivative test / proof ::@:: For functions of two variables \(of class $C_2$\), a proof is by considering the second derivatives of all resulting univariate functions through a critical point along all directions, and then apply the second derivative test. <p> Consider the second directional derivative of a function through a critical point, where the \(non-unit\) direction vector is $u = \langle a, b \rangle$, and assuming $f_{xx} \ne 0$ and $f_{yy} \ne 0$: $$\begin{aligned} (D_u)^2 f & = D_u(a f_x + b f_y) \\ & = a D_u(f_x) + b D_u(f_y) \\ & = a (a f_{xx} + b f_{xy}) + b (a f_{yx} + b f_{yy}) \\ & = a^2 f_{xx} + 2ab f_{xy} + b^2 f_{yy} \\ & = f_{xx} \left(a^2 + \frac {f_{xy} } {f_{xx} } 2ab \right) + b^2 f_{yy} \\ & = f_{xx} \left(a + \frac {f_{xy} } {f_{xx} } b \right)^2 - \frac {f_{xy}^2 } {f_{xx} } b^2 + b^2 f_{yy} \\ & = f_{xx} \left(a + \frac {f_{xy} } {f_{xx} } b \right)^2 + \frac {b^2} {f_{xx} } \left(f_{xx} f_{yy} - f_{xy}^2 \right) \,. \end{aligned}$$ The above derivation uses symmetry of second derivatives and completing the square. We can see if the determinant is positive, the sign of the above expression is controlled by $f_{xx}$ and always nonzero \(note that $a = b = 0$ is not allowed\). If the determinant is negative, the expression can be made positive, negative, or zero arbitrarily by adjusting $a$ and $b$. \(If the determinant is zero, we could not tell.\) <p> In the case where $f_{xx} = 0$ or $f_{yy} = 0$, the determinant must be non-positive. When $f_{xy} = f_{yx} \ne 0$, the determinant is negative. The previous expression is also valid for $f_{xx} \ne 0$ and $f_{yy} = 0$. The following expression is valid for $f_{xx} = 0$ and $f_{yy} \ne 0$: $$(D_u)^2 f = f_{yy} \left(b + \frac {f_{xy} } {f_{yy} } a \right)^2 + \frac {a^2} {f_{yy} } \left(f_{xx} f_{yy} - f_{xy}^2 \right) \,.$$ Finally, the following expression is valid for $f_{xx} = f_{yy} = 0$: $$(D_u)^2 f = 2ab f_{xy} \,,$$ where we see if $f_{xy} \ne 0$ \(determinant is negative\), then the expression can be made positive, negative, or zero arbitrarily. When $f_{xy} = 0$ \(determinant is zero\), then the test is inconclusive. <!--SR:!2025-03-31,9,337!2025-03-23,5,377-->
- [Sylvester's criterion](../../../../general/Sylvester's%20criterion.md): \(untaught\) It is a [necessary and sufficient](../../../../general/necessary%20and%20sufficient%20condition.md) criterion to determine whether a [Hermitian matrix](../../../../general/Hermitian%20matrix.md) is [positive-definite](../../../../general/definite%20matrix.md).
- maximum and minimum
  - maximum and minimum / definition
    - maximum and minimum / definition / global ::@:: A real-valued [function](function%20(mathematics).md) _f_ defined on a [domain](../../../../general/domain%20of%20a%20function.md) _X_ has a __global__ \(or __absolute__\) __maximum point__ at _x_<sup>∗</sup>, if _f_\(_x_<sup>∗</sup>\) ≥ _f_\(_x_\) for all _x_ in _X_. Similarly, the function has a __global__ \(or __absolute__\) __minimum point__ at _x_<sup>∗</sup>, if _f_\(_x_<sup>∗</sup>\) ≤ _f_\(_x_\) for all _x_ in _X_. <!--SR:!2025-03-23,5,377!2025-03-23,5,377-->
- [extreme value theorem](../../../../general/extreme%20value%20theorem.md) ::@:: It states that if a real-valued [function](../../../../general/function%20(mathematics).md) $f$ is [continuous](../../../../general/continuous%20function.md) on the [closed](../../../../general/bounded%20interval.md#classification%20of%20intervals) and [bounded](../../../../general/bounded%20set.md) interval $[a,b]$, then $f$ must attain a [maximum](../../../../general/maximum.md) and a [minimum](../../../../general/minimum.md), each at least once. <!--SR:!2025-03-23,5,377!2025-03-23,5,377-->
  - extreme value theorem / multivariate ::@:: For Euclidean spaces $\mathbb R^n$, the above readily generalizes to closed and bounded subsets of the space. Here, _bounded_ means every point in the subset is at most some finite distance away from the origin. _Closed_ means the subset contains all of its limit points. A _limit point_ of a subset is a point \(that may not be in the subset\) that has arbitrary small nonzero distance from any point in the subset. <p> For [metric spaces](../../../../general/metric%20spaces.md) and general [topological spaces](../../../../general/topological%20spaces.md), the appropriate generalization of a closed bounded interval is a [compact set](../../../../general/compact%20space.md). <!--SR:!2025-03-23,5,377!2025-03-23,5,377-->

## week 6 tutorial

- datetime: 2025-03-11T16:00:00+08:00/2025-03-11T16:50:00+08:00
- [week 6 lecture](#week%206%20lecture)
- [week 6 lecture 2](#week%206%20lecture%202)
- [square pyramidal number](../../../../general/square%20pyramidal%20number.md) ::@:: It is a natural number that counts the stacked spheres in a pyramid with a square base. <!--SR:!2025-03-23,5,377!2025-03-23,5,377-->
  - square pyramidal number / formula ::@:: The total number $P_{n}$ of spheres can be counted as the sum of the number of spheres in each square, $$P_{n}=\sum _{k=1}^{n}k^{2}=1+4+9+\cdots +n^{2},$$ and this [summation](../../../../general/summation.md) can be solved to give a [cubic polynomial](../../../../general/cubic%20polynomial.md), which can be written in several equivalent ways: $$P_{n}={\frac {n(n+1)(2n+1)}{6} }={\frac {2n^{3}+3n^{2}+n}{6} }={\frac {n^{3} }{3} }+{\frac {n^{2} }{2} }+{\frac {n}{6} }.$$ This equation for a sum of squares is a special case of [Faulhaber's formula](../../../../general/Faulhaber's%20formula.md) for sums of powers, and may be proved by [mathematical induction](../../../../general/mathematical%20induction.md). <!--SR:!2025-04-04,13,357!2025-03-23,5,377-->

## week 6 lecture 2

- datetime: 2025-03-13T13:30:00+08:00/2025-03-13T14:50:00+08:00
- maximum and minimum
  - maximum and minimum / search ::@:: Finding global maxima and minima is the goal of [mathematical optimization](../../../../general/mathematical%20optimization.md). If a function is continuous on a closed interval, then by the [extreme value theorem](../../../../general/extreme%20value%20theorem.md), global maxima and minima exist. Furthermore, a global maximum \(or minimum\) either must be a local maximum \(or minimum\) in the interior of the domain, or must lie on the boundary of the domain. So a method of finding a global maximum \(or minimum\) is to look at all the local maxima \(or minima\) in the interior, and also look at the maxima \(or minima\) of the points on the boundary, and take the greatest \(or least\) one. <!--SR:!2025-03-23,5,377!2025-03-23,5,377-->
    - maximum and minium / search / detail ::@:: Note that the boundary of a domain may itself has a boundary. In general, for a _n_-dimensional subset, its boundary is a _n_–1-dimensional subset. <P> For example, a rectangle has 4 edges. Using 4 different parameterization to represent its 4 edge, each parameterization represents an edge. Each edge itself has a boundary, which are the rectangle vertices. <!--SR:!2025-03-23,5,377!2025-03-23,5,377-->
- [Lagrange multiplier](../../../../general/Lagrange%20multiplier.md) ::@:: It is a strategy for finding the local [maxima and minima](../../../../general/maxima%20and%20minima.md) of a [function](../../../../general/function%20(mathematics).md) subject to [equation constraints](../../../../general/constraint%20(mathematics).md) \(i.e., subject to the condition that one or more [equations](../../../../general/equation.md) have to be satisfied exactly by the chosen values of the [variables](../../../../general/variable%20(mathematics).md)\). It is named after the mathematician [Joseph-Louis Lagrange](../../../../general/Joseph-Louis%20Lagrange.md). <!--SR:!2025-03-23,5,377!2025-03-23,5,377-->
  - Lagrange multiplier / rationale ::@:: The basic idea is to convert a constrained problem into a form such that the [derivative test](../../../../general/derivative%20test.md) of an unconstrained problem can still be applied. <p> Consider a multivariate function of two variables $f(x, y)$ and a single constraint function $g(x, y) = 0$ \(with _constraint qualification_: its gradient is nonzero\). Then, the constraint is a curve on the graph surface of the multivariate function. A local extremum on the constraint curve must have the curve at that point parallel to the level sets, otherwise, one could "walk" along the constraint curve to get higher or lower values. This also means the constraint curve is perpendicular to the gradient of the multivariate function. And the constraint curve is perpendicular to its own gradient. So the gradient of the multivariate function can be expressed as a linear combination of the gradient of the constraint. Thus we have $$D f(x, y) = \lambda D g(x, y) \,,$$ for some arbitrary constant $\lambda$ subject to $g(x, y)$. <!--SR:!2025-04-10,19,377!2025-03-23,5,377-->
  - Lagrange multiplier / summary ::@:: In the general case, the Lagrangian is defined as $${\mathcal {L} }(x,\lambda )\equiv f(x)+\langle \lambda ,g(x)\rangle$$ for \(annotation: $C^1$, having _continuous first derivatives_\) functions $f,g$; the notation $\langle \cdot ,\cdot \rangle$ denotes an [inner product](../../../../general/inner%20product.md). The value $\lambda$ is called the Lagrange multiplier. <p> In simple cases, where the inner product is defined as the [dot product](../../../../general/dot%20product.md), the Lagrangian is $${\mathcal {L} }(x,\lambda )\equiv f(x)+\lambda \cdot g(x)$$ <p> The method can be summarized as follows: in order to find the maximum or minimum of a function $f$ subject to the equality constraint $g(x)=0$, find the [stationary points](../../../../general/stationary%20point.md) of ${\mathcal {L} }$ considered as a function of $x$ and the Lagrange multiplier $\lambda ~$. This means that all [partial derivatives](../../../../general/partial%20derivative.md) should be zero, including the partial derivative with respect to $\lambda ~$. <p> &emsp; ${\frac {\partial {\mathcal {L} } }{\partial x} }=0$ and ${\frac {\ \partial {\mathcal {L} }\ }{\partial \lambda } }=0\ ;$ <p> or equivalently <p> &emsp; ${\frac {\partial f(x)}{\partial x} }+\lambda \cdot {\frac {\partial g(x)}{\partial x} }=0$ and $g(x)=0~$. <!--SR:!2025-03-23,5,377!2025-03-23,5,377-->

## assignments

## midterm examination

## final examination

## aftermath

### total
